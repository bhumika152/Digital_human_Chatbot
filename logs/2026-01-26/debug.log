2026-01-26 23:56:38,685 | DEBUG | httpcore.connection | connect_tcp.started host='raw.githubusercontent.com' port=443 local_address=None timeout=5 socket_options=None
2026-01-26 23:56:38,734 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002127FAB2890>
2026-01-26 23:56:38,735 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x000002127E7EE7C0> server_hostname='raw.githubusercontent.com' timeout=5
2026-01-26 23:56:38,768 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002127FAB2860>
2026-01-26 23:56:38,768 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'GET']>
2026-01-26 23:56:38,768 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-26 23:56:38,768 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'GET']>
2026-01-26 23:56:38,768 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-26 23:56:38,768 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'GET']>
2026-01-26 23:56:38,808 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Connection', b'keep-alive'), (b'Content-Length', b'62322'), (b'Cache-Control', b'max-age=300'), (b'Content-Security-Policy', b"default-src 'none'; style-src 'unsafe-inline'; sandbox"), (b'Content-Type', b'text/plain; charset=utf-8'), (b'ETag', b'W/"5f0be0ed2fe5ecf561c4b4488d64c304c35b980a45012ef1d1253b4b5d438af1"'), (b'Strict-Transport-Security', b'max-age=31536000'), (b'X-Content-Type-Options', b'nosniff'), (b'X-Frame-Options', b'deny'), (b'X-XSS-Protection', b'1; mode=block'), (b'X-GitHub-Request-Id', b'415E:3865D1:30D7E6:7A896E:69753835'), (b'Content-Encoding', b'gzip'), (b'Accept-Ranges', b'bytes'), (b'Date', b'Mon, 26 Jan 2026 18:26:38 GMT'), (b'Via', b'1.1 varnish'), (b'X-Served-By', b'cache-bom-vanm7210044-BOM'), (b'X-Cache', b'HIT'), (b'X-Cache-Hits', b'11'), (b'X-Timer', b'S1769451999.763715,VS0,VE0'), (b'Vary', b'Authorization,Accept-Encoding'), (b'Access-Control-Allow-Origin', b'*'), (b'Cross-Origin-Resource-Policy', b'cross-origin'), (b'X-Fastly-Request-ID', b'9b7439ba44d7489eb9d81b870637a03b756f7877'), (b'Expires', b'Mon, 26 Jan 2026 18:31:38 GMT'), (b'Source-Age', b'230')])
2026-01-26 23:56:38,808 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'GET']>
2026-01-26 23:56:38,851 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-26 23:56:38,851 | DEBUG | httpcore.http11 | response_closed.started
2026-01-26 23:56:38,851 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-26 23:56:38,859 | DEBUG | httpcore.connection | close.started
2026-01-26 23:56:38,859 | DEBUG | httpcore.connection | close.complete
2026-01-26 23:56:39,620 | DEBUG | LiteLLM | [Non-Blocking] Unable to import GenericAPILogger - LiteLLM Enterprise Feature - No module named 'litellm_enterprise'
2026-01-26 23:56:40,118 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-26 23:56:40,135 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-26 23:56:40,135 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-26 23:56:40,135 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-26 23:56:40,135 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-26 23:56:40,135 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-26 23:56:40,135 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-26 23:56:40,145 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-26 23:56:40,145 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-26 23:56:40,146 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-26 23:56:40,146 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-26 23:56:40,147 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-26 23:56:40,268 | INFO | main | üöÄ FastAPI application starting
2026-01-26 23:56:40,371 | INFO | main | üóÑÔ∏è Database tables ensured
2026-01-26 23:56:50,249 | INFO | chat | üì© Chat request | user_id=6 | request_id=772fa783-ac36-4b5a-b6a7-ddc6bdb9eb60
2026-01-26 23:56:50,293 | INFO | chat | üßë User message | me 
2026-01-26 23:56:50,318 | INFO | chat | ü§ñ Stream started | session_id=ae4b8d0d-4e2b-4c2d-96c5-1acf39b38367 | request_id=772fa783-ac36-4b5a-b6a7-ddc6bdb9eb60
2026-01-26 23:56:50,318 | INFO | chat | ü§ñ Running digital human
2026-01-26 23:56:50,318 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-26 23:56:50,318 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-26 23:56:50,318 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_7168501d8aeb45c990d294613934214e
2026-01-26 23:56:50,318 | DEBUG | openai.agents | Setting current trace: trace_7168501d8aeb45c990d294613934214e
2026-01-26 23:56:50,318 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000002120390C220> with id None
2026-01-26 23:56:50,333 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-26 23:56:50,336 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000002120399B940> with id None
2026-01-26 23:56:50,336 | DEBUG | openai.agents | Calling LLM
2026-01-26 23:56:50,336 | DEBUG | LiteLLM | 

2026-01-26 23:56:50,336 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-26 23:56:50,336 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n', 'role': 'system'}, {'role': 'user', 'content': 'me '}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.5'}, api_key=None, base_url=None)[0m
2026-01-26 23:56:50,336 | DEBUG | LiteLLM | 

2026-01-26 23:56:50,336 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-26 23:56:50,336 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-26 23:56:50,343 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-26 23:56:50,351 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-26 23:56:50,351 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-26 23:56:50,351 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n', 'role': 'system'}, {'role': 'user', 'content': 'me '}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-26 23:56:50,351 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-26 23:56:50,351 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-26 23:56:50,366 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-26 23:56:50,366 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:56:50,592 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:56:50,596 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.5' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'me '}]}], 'system_instruction': {'parts': [{'text': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n'}]}, 'generationConfig': {}}'
[0m

2026-01-26 23:56:52,612 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "SAFE"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 189,
    "candidatesTokenCount": 1,
    "totalTokenCount": 273,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 189
      }
    ],
    "thoughtsTokenCount": 83
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "7LF3aYSOGvncqfkP9uPt-Qw"
}



2026-01-26 23:56:52,618 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-26 23:56:52,618 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-26 23:56:52,618 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-26 23:56:52,618 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-26 23:56:52,618 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:56:52,618 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:56:52,618 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:56:52,618 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:56:52,618 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:56:52,618 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:56:52,626 | DEBUG | LiteLLM | response_cost: 0.00026670000000000003
2026-01-26 23:56:52,626 | DEBUG | LiteLLM | response_cost: 0.00026670000000000003
2026-01-26 23:56:52,628 | DEBUG | openai.agents | Received model response
2026-01-26 23:56:52,628 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-26 23:56:52,639 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x0000021203AB5480>>
2026-01-26 23:56:52,640 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:56:52,641 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-26 23:56:52,641 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:56:52,642 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-26 23:56:52,642 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-26 23:56:52,642 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-26 23:56:52,643 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-26 23:56:52,644 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-26 23:56:52,644 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-26 23:56:52,645 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:56:52,645 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-26 23:56:52,645 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:56:52,646 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:56:52,646 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:56:52,646 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:56:52,646 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:56:52,646 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-26 23:56:52,646 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.00026670000000000003
2026-01-26 23:56:52,646 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-26 23:56:52,649 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:56:52,650 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:56:52,651 | DEBUG | openai.agents | Resetting current trace
2026-01-26 23:56:52,652 | INFO | orchestrator | ‚úÖ Safety passed
2026-01-26 23:56:52,652 | INFO | orchestrator | üß≠ Router agent called
2026-01-26 23:56:52,653 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_442f4f3b5a8b4b739f31e5c19ae64df3
2026-01-26 23:56:52,653 | DEBUG | openai.agents | Setting current trace: trace_442f4f3b5a8b4b739f31e5c19ae64df3
2026-01-26 23:56:52,653 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000002127CD76610> with id None
2026-01-26 23:56:52,653 | DEBUG | openai.agents | Running agent RouterAgent (turn 1)
2026-01-26 23:56:52,653 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000021203A68B20> with id None
2026-01-26 23:56:52,653 | DEBUG | openai.agents | Calling LLM
2026-01-26 23:56:52,653 | DEBUG | LiteLLM | 

2026-01-26 23:56:52,655 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-26 23:56:52,655 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'me '}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.5'}, api_key=None, base_url=None)[0m
2026-01-26 23:56:52,655 | DEBUG | LiteLLM | 

2026-01-26 23:56:52,655 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-26 23:56:52,655 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-26 23:56:52,655 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-26 23:56:52,655 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-26 23:56:52,655 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-26 23:56:52,655 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'me '}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-26 23:56:52,655 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-26 23:56:52,655 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-26 23:56:52,655 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-26 23:56:52,662 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:56:52,662 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:56:52,663 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.5' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'me '}]}], 'system_instruction': {'parts': [{'text': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n'}]}, 'generationConfig': {}}'
[0m

2026-01-26 23:56:54,440 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\n  \"use_memory\": false,\n  \"use_tool\": false,\n  \"tool_name\": \"none\",\n  \"tool_arguments\": null,\n  \"intent\": \"none\",\n  \"memory_key\": null\n}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 487,
    "candidatesTokenCount": 56,
    "totalTokenCount": 660,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 487
      }
    ],
    "thoughtsTokenCount": 117
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "7rF3adeCEMrc4-EPucnEsQk"
}



2026-01-26 23:56:54,441 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-26 23:56:54,442 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-26 23:56:54,442 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-26 23:56:54,443 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:56:54,443 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-26 23:56:54,444 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:56:54,444 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:56:54,444 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:56:54,445 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:56:54,445 | DEBUG | LiteLLM | response_cost: 0.0005786000000000001
2026-01-26 23:56:54,445 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:56:54,446 | DEBUG | openai.agents | Received model response
2026-01-26 23:56:54,446 | DEBUG | LiteLLM | response_cost: 0.0005786000000000001
2026-01-26 23:56:54,447 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x0000021203AB4E80>>
2026-01-26 23:56:54,448 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-26 23:56:54,448 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-26 23:56:54,448 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:56:54,449 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-26 23:56:54,449 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-26 23:56:54,450 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:56:54,450 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-26 23:56:54,451 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-26 23:56:54,451 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-26 23:56:54,452 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-26 23:56:54,452 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:56:54,452 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-26 23:56:54,453 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:56:54,453 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:56:54,454 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:56:54,454 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:56:54,455 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:56:54,455 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-26 23:56:54,456 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0005786000000000001
2026-01-26 23:56:54,456 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-26 23:56:54,457 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:56:54,457 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:56:54,458 | DEBUG | openai.agents | Resetting current trace
2026-01-26 23:56:54,459 | INFO | orchestrator | üß≠ Router | tool=False memory=False intent=none
2026-01-26 23:56:54,459 | INFO | orchestrator | üß† Reasoning agent called
2026-01-26 23:56:54,460 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_fdfaef2807c64e63927071e3b5b3cfae
2026-01-26 23:56:54,460 | DEBUG | openai.agents | Setting current trace: trace_fdfaef2807c64e63927071e3b5b3cfae
2026-01-26 23:56:54,460 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x0000021203A34810> with id None
2026-01-26 23:56:54,461 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000021203AB28C0> with id None
2026-01-26 23:56:54,461 | DEBUG | openai.agents | Calling LLM
2026-01-26 23:56:54,462 | DEBUG | LiteLLM | 

2026-01-26 23:56:54,462 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-26 23:56:54,462 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.\n\nIf the user mentions vague locations like:\n- nearest city\n- near me\n- nearby\n- around here\n\nand user location is provided,\nrewrite the query using the nearest major city.\n\nOutput ONLY the rewritten query.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n\\nYou may answer ANY general knowledge question.\\n\\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n\\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n\\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "system", "content": "Conversation summary:\\nAshu Singh is planning a special birthday gift for her 22-year-old boyfriend, who she describes as mature and intelligent."}, {"role": "assistant", "content": "Okay, perfect! I\'m ready when you are.\\n\\nPlease tell me more about your boyfriend so I can suggest the best gift ideas for his 22nd birthday. For instance:\\n\\n*   **What are his hobbies or interests?** (e.g., gaming, sports, music, reading, outdoors, cooking, technology)\\n*   **What\'s his personality like?** (e.g., adventurous, laid-back, creative, practical)\\n*   **Is there anything he\'s mentioned wanting or needing recently?**\\n*   **What\'s your approximate budget?**"}, {"role": "user", "content": "ok"}, {"role": "assistant", "content": "Okay, I\'m ready when you are! Please share some details about your boyfriend\'s hobbies, personality, or budget so I can help you find a great gift for his birthday."}, {"role": "user", "content": "lets discuss about m e"}, {"role": "assistant", "content": "I\'m not sure I understand what \\"m e\\" refers to. Could you please clarify what you\'d like to discuss?"}, {"role": "user", "content": "me "}, {"role": "user", "content": "me "}], "memory_action": {}, "memory_data": [], "tool_context": {}}'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=True, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.5'}, api_key=None, base_url=None)[0m
2026-01-26 23:56:54,462 | DEBUG | LiteLLM | 

2026-01-26 23:56:54,462 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-26 23:56:54,464 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-26 23:56:54,464 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-26 23:56:54,465 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-26 23:56:54,466 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-26 23:56:54,466 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.\n\nIf the user mentions vague locations like:\n- nearest city\n- near me\n- nearby\n- around here\n\nand user location is provided,\nrewrite the query using the nearest major city.\n\nOutput ONLY the rewritten query.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n\\nYou may answer ANY general knowledge question.\\n\\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n\\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n\\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "system", "content": "Conversation summary:\\nAshu Singh is planning a special birthday gift for her 22-year-old boyfriend, who she describes as mature and intelligent."}, {"role": "assistant", "content": "Okay, perfect! I\'m ready when you are.\\n\\nPlease tell me more about your boyfriend so I can suggest the best gift ideas for his 22nd birthday. For instance:\\n\\n*   **What are his hobbies or interests?** (e.g., gaming, sports, music, reading, outdoors, cooking, technology)\\n*   **What\'s his personality like?** (e.g., adventurous, laid-back, creative, practical)\\n*   **Is there anything he\'s mentioned wanting or needing recently?**\\n*   **What\'s your approximate budget?**"}, {"role": "user", "content": "ok"}, {"role": "assistant", "content": "Okay, I\'m ready when you are! Please share some details about your boyfriend\'s hobbies, personality, or budget so I can help you find a great gift for his birthday."}, {"role": "user", "content": "lets discuss about m e"}, {"role": "assistant", "content": "I\'m not sure I understand what \\"m e\\" refers to. Could you please clarify what you\'d like to discuss?"}, {"role": "user", "content": "me "}, {"role": "user", "content": "me "}], "memory_action": {}, "memory_data": [], "tool_context": {}}'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-26 23:56:54,468 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': True}
2026-01-26 23:56:54,468 | DEBUG | LiteLLM | Final returned optional params: {'stream': True}
2026-01-26 23:56:54,469 | DEBUG | LiteLLM | self.optional_params: {'stream': True}
2026-01-26 23:56:54,469 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:56:54,469 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:56:54,469 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?key=*****=sse \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.5' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n\\nYou may answer ANY general knowledge question.\\n\\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n\\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n\\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "system", "content": "Conversation summary:\\nAshu Singh is planning a special birthday gift for her 22-year-old boyfriend, who she describes as mature and intelligent."}, {"role": "assistant", "content": "Okay, perfect! I\'m ready when you are.\\n\\nPlease tell me more about your boyfriend so I can suggest the best gift ideas for his 22nd birthday. For instance:\\n\\n*   **What are his hobbies or interests?** (e.g., gaming, sports, music, reading, outdoors, cooking, technology)\\n*   **What\'s his personality like?** (e.g., adventurous, laid-back, creative, practical)\\n*   **Is there anything he\'s mentioned wanting or needing recently?**\\n*   **What\'s your approximate budget?**"}, {"role": "user", "content": "ok"}, {"role": "assistant", "content": "Okay, I\'m ready when you are! Please share some details about your boyfriend\'s hobbies, personality, or budget so I can help you find a great gift for his birthday."}, {"role": "user", "content": "lets discuss about m e"}, {"role": "assistant", "content": "I\'m not sure I understand what \\"m e\\" refers to. Could you please clarify what you\'d like to discuss?"}, {"role": "user", "content": "me "}, {"role": "user", "content": "me "}], "memory_action": {}, "memory_data": [], "tool_context": {}}'}]}], 'system_instruction': {'parts': [{'text': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.\n\nIf the user mentions vague locations like:\n- nearest city\n- near me\n- nearby\n- around here\n\nand user location is provided,\nrewrite the query using the nearest major city.\n\nOutput ONLY the rewritten query.'}]}, 'generationConfig': {}}'
[0m

2026-01-26 23:56:54,471 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-26 23:56:54,472 | DEBUG | LiteLLM | messages in token_counter: None, text in token_counter: 
2026-01-26 23:56:54,474 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:56:54,475 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:56:54,475 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:56:54,476 | DEBUG | LiteLLM | response_cost: 0.0
2026-01-26 23:56:54,605 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-26 23:56:54,605 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-26 23:56:54,606 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-26 23:56:55,334 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-26 23:56:55,400 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000212048404C0>
2026-01-26 23:56:55,400 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x000002127D66B4C0> server_hostname='api.openai.com' timeout=5.0
2026-01-26 23:56:55,402 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000021204840100>
2026-01-26 23:56:55,402 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-26 23:56:55,402 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-26 23:56:55,402 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-26 23:56:55,402 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-26 23:56:55,417 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-26 23:56:56,655 | DEBUG | LiteLLM | RAW RESPONSE:
first stream response received


2026-01-26 23:56:56,655 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': 'It seems like you might be trying to tell me something about yourself, or perhaps about your boyfriend. Could you please elaborate'}], 'role': 'model'}, 'index': 0}], 'usageMetadata': {'promptTokenCount': 600, 'candidatesTokenCount': 24, 'totalTokenCount': 709, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 600}], 'thoughtsTokenCount': 85}, 'modelVersion': 'gemini-2.5-flash', 'responseId': '77F3aZDEMeKZjuMPsao0'}
2026-01-26 23:56:56,655 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='77F3aZDEMeKZjuMPsao0', created=1769452016, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='It seems like you might be trying to tell me something about yourself, or perhaps about your boyfriend. Could you please elaborate', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=109, prompt_tokens=600, total_tokens=709, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=85, rejected_prediction_tokens=None, text_tokens=24, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=600, image_tokens=None)))
2026-01-26 23:56:56,655 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content='It seems like you might be trying to tell me something about yourself, or perhaps about your boyfriend. Could you please elaborate', role='assistant', function_call=None, tool_calls=None, audio=None)
2026-01-26 23:56:56,664 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='77F3aZDEMeKZjuMPsao0', created=1769452016, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='It seems like you might be trying to tell me something about yourself, or perhaps about your boyfriend. Could you please elaborate', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=109, prompt_tokens=600, total_tokens=709, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=85, rejected_prediction_tokens=None, text_tokens=24, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=600, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-26 23:56:56,677 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': ' on what you mean by "me"? I\'m here to help, whether it\'s about finding a gift or something else!'}], 'role': 'model'}, 'finishReason': 'STOP', 'index': 0}], 'usageMetadata': {'promptTokenCount': 600, 'candidatesTokenCount': 51, 'totalTokenCount': 736, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 600}], 'thoughtsTokenCount': 85}, 'modelVersion': 'gemini-2.5-flash', 'responseId': '77F3aZDEMeKZjuMPsao0'}
2026-01-26 23:56:56,677 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='77F3aZDEMeKZjuMPsao0', created=1769452016, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content=' on what you mean by "me"? I\'m here to help, whether it\'s about finding a gift or something else!', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=136, prompt_tokens=600, total_tokens=736, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=85, rejected_prediction_tokens=None, text_tokens=51, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=600, image_tokens=None)))
2026-01-26 23:56:56,681 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content=' on what you mean by "me"? I\'m here to help, whether it\'s about finding a gift or something else!', role=None, function_call=None, tool_calls=None, audio=None)
2026-01-26 23:56:56,682 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='77F3aZDEMeKZjuMPsao0', created=1769452016, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' on what you mean by "me"? I\'m here to help, whether it\'s about finding a gift or something else!', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=136, prompt_tokens=600, total_tokens=736, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=85, rejected_prediction_tokens=None, text_tokens=51, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=600, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-26 23:56:56,693 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=False
2026-01-26 23:56:56,693 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=False
2026-01-26 23:56:56,693 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-26 23:56:56,693 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-26 23:56:56,699 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-26 23:56:56,699 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-26 23:56:56,699 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:56:56,701 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:56:56,701 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:56:56,701 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:56:56,701 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:56:56,702 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:56:56,702 | DEBUG | LiteLLM | response_cost: 0.0005200000000000001
2026-01-26 23:56:56,703 | DEBUG | LiteLLM | response_cost: 0.0005200000000000001
2026-01-26 23:56:56,703 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-26 23:56:56,703 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0005200000000000001
2026-01-26 23:56:56,703 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:56:56,706 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-26 23:56:56,706 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:56:56,706 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:56:56,706 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:56:56,708 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-26 23:56:56,708 | DEBUG | openai.agents | Resetting current trace
2026-01-26 23:56:56,755 | DEBUG | LiteLLM | 

2026-01-26 23:56:56,755 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-26 23:56:56,755 | DEBUG | LiteLLM | [92mlitellm.completion(model='gemini/gemini-2.5-flash', provider='gemini', messages=[{'role': 'system', 'content': 'You are a precise memory compression engine.'}, {'role': 'user', 'content': '\nYou are a memory compression system.\n \nTASK:\nUpdate the existing conversation summary using the new dialogue below.\n \nRULES:\n- Preserve facts, decisions, preferences, and open tasks\n- Preserve key events and discussion topics\n- Remove chit-chat and repetition\n- Do NOT invent new information\n- Keep the summary concise and factual\n \nEXISTING SUMMARY:\nAshu Singh is planning a special birthday gift for her 22-year-old boyfriend, who she describes as mature and intelligent.\n \nNEW DIALOGUE:\nUSER: hi \nASSISTANT: Hello! How can I help you today?\nUSER: hi\nASSISTANT: Hello! How can I help you today?\nUSER: my self ashu singh \nASSISTANT: Hello Ashu Singh! How can I help you today?\nUSER: i am planning to do something interesting \nASSISTANT: That sounds great, Ashu! What kind of interesting things are you considering? I can help you brainstorm some ideas if you tell me a little more about what you\'re looking for.\nUSER: i have my bf birthday coming i want to do something special for him \nASSISTANT: That\'s very thoughtful of you, Ashu! To help me suggest something really special, could you tell me a little more about your boyfriend\'s interests or what kind of things he enjoys?\n\nIn the meantime, here are some general ideas:\n\n*   **Plan a romantic dinner:** Cook his favorite meal at home, or book a table at a restaurant he\'s always wanted to try.\n*   **Weekend getaway:** A surprise trip to a nearby city, a cozy cabin, or a relaxing beach destination.\n*   **Experience gift:** Tickets to a concert, a sporting event, a workshop (like cooking or brewing), or an adventure activity he\'d love (e.g., skydiving, go-karting).\n*   **Personalized gift:** A custom photo album, a piece of art, or a handmade gift that reflects your shared memories.\n*   **Adventure day:** Plan a day filled with activities he loves, like hiking, visiting a museum, playing his favorite sport, or exploring a new area.\n*   **Memory lane:** Create a "memory jar" filled with notes about your favorite moments together, or put together a video montage of your best memories.\nUSER: he is somewhat u know very different from others like so mature at the age of 22 and u know he is so intelligent also \nASSISTANT: That\'s a wonderful insight, Ashu! Knowing he\'s mature and intelligent at 22 helps a lot. For someone with those qualities, experiences that stimulate the mind, offer growth, or provide unique, high-quality enjoyment often resonate more than typical gifts.\n\nHere are some ideas tailored for someone like him:\n\n1.  **Intellectual Stimulation/Learning:**\n    *   **Masterclass/Online Course Subscription:** A subscription to a platform like MasterClass or a specific online course in a field he\'s passionate about (e.g., advanced coding, philosophy, creative writing, science, history).\n    *   **High-Quality Books/Rare Edition:** A special edition of a classic, a first edition of a book he loves, or a carefully curated selection of books by an author or in a genre he admires.\n    *   **Subscription to a Thought-Provoking Magazine/Journal:** Something like The Economist, Smithsonian, National Geographic (if he\'s into science/exploration), or a literary review.\n    *   **Telescope/Microscope:** If he has an interest in astronomy or biology, a good quality one can be a fascinating gift.\n\n2.  **Unique Experiences & Culture:**\n    *   **Tickets to a Special Exhibition/Lecture:** This could be at a museum, a science center, or a university event featuring a speaker he\'d find engaging.\n    *   **Fine Dining Experience:** Not just any restaurant, but perhaps a Michelin-starred experience, a chef\'s tasting menu, or a place known for its innovative cuisine and sophisticated ambiance.\n    *   **Weekend Getaway with a Cultural Focus:** A trip to a city known for its history, art, or architecture, rather than just a party destination.\n    *   **Private Tour:** A private tour of a historical site, a specialized museum, or a unique local attraction that aligns with his interests.\n\n3.  **Sophisticated & Thoughtful Gifts:**\n    *   **Quality Watch or Accessory:** A classic, well-designed watch, a premium pen, or a high-quality leather wallet/briefcase if he appreciates craftsmanship.\n    *   **Curated Collection:** A selection of gourmet coffee, rare teas, artisanal chocolates, or fine spirits (if he enjoys them), presented thoughtfully.\n    *   **Contribution to a Cause:** Make a donation in his name to a charity or organization he supports, perhaps related to education, science, or social justice.\n    *   **Personalized Experience:** A custom-made piece of art that reflects his intellectual pursuits, or a beautifully framed map of a place significant to him.\n\nTo narrow it down further, think about what specific subjects or hobbies capture his intelligent mind the most!\n \nOUTPUT:\nUpdated summary only.\n'}], temperature=0.2, max_tokens=400)[0m
2026-01-26 23:56:56,755 | DEBUG | LiteLLM | 

2026-01-26 23:56:56,755 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-26 23:56:56,755 | DEBUG | LiteLLM | SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False
2026-01-26 23:56:56,755 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-26 23:56:56,755 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-26 23:56:56,755 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': 0.2, 'top_p': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': 400, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': 'You are a precise memory compression engine.'}, {'role': 'user', 'content': '\nYou are a memory compression system.\n \nTASK:\nUpdate the existing conversation summary using the new dialogue below.\n \nRULES:\n- Preserve facts, decisions, preferences, and open tasks\n- Preserve key events and discussion topics\n- Remove chit-chat and repetition\n- Do NOT invent new information\n- Keep the summary concise and factual\n \nEXISTING SUMMARY:\nAshu Singh is planning a special birthday gift for her 22-year-old boyfriend, who she describes as mature and intelligent.\n \nNEW DIALOGUE:\nUSER: hi \nASSISTANT: Hello! How can I help you today?\nUSER: hi\nASSISTANT: Hello! How can I help you today?\nUSER: my self ashu singh \nASSISTANT: Hello Ashu Singh! How can I help you today?\nUSER: i am planning to do something interesting \nASSISTANT: That sounds great, Ashu! What kind of interesting things are you considering? I can help you brainstorm some ideas if you tell me a little more about what you\'re looking for.\nUSER: i have my bf birthday coming i want to do something special for him \nASSISTANT: That\'s very thoughtful of you, Ashu! To help me suggest something really special, could you tell me a little more about your boyfriend\'s interests or what kind of things he enjoys?\n\nIn the meantime, here are some general ideas:\n\n*   **Plan a romantic dinner:** Cook his favorite meal at home, or book a table at a restaurant he\'s always wanted to try.\n*   **Weekend getaway:** A surprise trip to a nearby city, a cozy cabin, or a relaxing beach destination.\n*   **Experience gift:** Tickets to a concert, a sporting event, a workshop (like cooking or brewing), or an adventure activity he\'d love (e.g., skydiving, go-karting).\n*   **Personalized gift:** A custom photo album, a piece of art, or a handmade gift that reflects your shared memories.\n*   **Adventure day:** Plan a day filled with activities he loves, like hiking, visiting a museum, playing his favorite sport, or exploring a new area.\n*   **Memory lane:** Create a "memory jar" filled with notes about your favorite moments together, or put together a video montage of your best memories.\nUSER: he is somewhat u know very different from others like so mature at the age of 22 and u know he is so intelligent also \nASSISTANT: That\'s a wonderful insight, Ashu! Knowing he\'s mature and intelligent at 22 helps a lot. For someone with those qualities, experiences that stimulate the mind, offer growth, or provide unique, high-quality enjoyment often resonate more than typical gifts.\n\nHere are some ideas tailored for someone like him:\n\n1.  **Intellectual Stimulation/Learning:**\n    *   **Masterclass/Online Course Subscription:** A subscription to a platform like MasterClass or a specific online course in a field he\'s passionate about (e.g., advanced coding, philosophy, creative writing, science, history).\n    *   **High-Quality Books/Rare Edition:** A special edition of a classic, a first edition of a book he loves, or a carefully curated selection of books by an author or in a genre he admires.\n    *   **Subscription to a Thought-Provoking Magazine/Journal:** Something like The Economist, Smithsonian, National Geographic (if he\'s into science/exploration), or a literary review.\n    *   **Telescope/Microscope:** If he has an interest in astronomy or biology, a good quality one can be a fascinating gift.\n\n2.  **Unique Experiences & Culture:**\n    *   **Tickets to a Special Exhibition/Lecture:** This could be at a museum, a science center, or a university event featuring a speaker he\'d find engaging.\n    *   **Fine Dining Experience:** Not just any restaurant, but perhaps a Michelin-starred experience, a chef\'s tasting menu, or a place known for its innovative cuisine and sophisticated ambiance.\n    *   **Weekend Getaway with a Cultural Focus:** A trip to a city known for its history, art, or architecture, rather than just a party destination.\n    *   **Private Tour:** A private tour of a historical site, a specialized museum, or a unique local attraction that aligns with his interests.\n\n3.  **Sophisticated & Thoughtful Gifts:**\n    *   **Quality Watch or Accessory:** A classic, well-designed watch, a premium pen, or a high-quality leather wallet/briefcase if he appreciates craftsmanship.\n    *   **Curated Collection:** A selection of gourmet coffee, rare teas, artisanal chocolates, or fine spirits (if he enjoys them), presented thoughtfully.\n    *   **Contribution to a Cause:** Make a donation in his name to a charity or organization he supports, perhaps related to education, science, or social justice.\n    *   **Personalized Experience:** A custom-made piece of art that reflects his intellectual pursuits, or a beautifully framed map of a place significant to him.\n\nTo narrow it down further, think about what specific subjects or hobbies capture his intelligent mind the most!\n \nOUTPUT:\nUpdated summary only.\n'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None, 'provider': 'gemini'}
2026-01-26 23:56:56,755 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'temperature': 0.2, 'max_tokens': 400}
2026-01-26 23:56:56,755 | DEBUG | LiteLLM | Final returned optional params: {'temperature': 0.2, 'max_output_tokens': 400, 'provider': 'gemini'}
2026-01-26 23:56:56,755 | DEBUG | LiteLLM | self.optional_params: {'temperature': 0.2, 'max_tokens': 400, 'provider': 'gemini'}
2026-01-26 23:56:56,755 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:56:56,767 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:56:56,767 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '\nYou are a memory compression system.\n \nTASK:\nUpdate the existing conversation summary using the new dialogue below.\n \nRULES:\n- Preserve facts, decisions, preferences, and open tasks\n- Preserve key events and discussion topics\n- Remove chit-chat and repetition\n- Do NOT invent new information\n- Keep the summary concise and factual\n \nEXISTING SUMMARY:\nAshu Singh is planning a special birthday gift for her 22-year-old boyfriend, who she describes as mature and intelligent.\n \nNEW DIALOGUE:\nUSER: hi \nASSISTANT: Hello! How can I help you today?\nUSER: hi\nASSISTANT: Hello! How can I help you today?\nUSER: my self ashu singh \nASSISTANT: Hello Ashu Singh! How can I help you today?\nUSER: i am planning to do something interesting \nASSISTANT: That sounds great, Ashu! What kind of interesting things are you considering? I can help you brainstorm some ideas if you tell me a little more about what you\'re looking for.\nUSER: i have my bf birthday coming i want to do something special for him \nASSISTANT: That\'s very thoughtful of you, Ashu! To help me suggest something really special, could you tell me a little more about your boyfriend\'s interests or what kind of things he enjoys?\n\nIn the meantime, here are some general ideas:\n\n*   **Plan a romantic dinner:** Cook his favorite meal at home, or book a table at a restaurant he\'s always wanted to try.\n*   **Weekend getaway:** A surprise trip to a nearby city, a cozy cabin, or a relaxing beach destination.\n*   **Experience gift:** Tickets to a concert, a sporting event, a workshop (like cooking or brewing), or an adventure activity he\'d love (e.g., skydiving, go-karting).\n*   **Personalized gift:** A custom photo album, a piece of art, or a handmade gift that reflects your shared memories.\n*   **Adventure day:** Plan a day filled with activities he loves, like hiking, visiting a museum, playing his favorite sport, or exploring a new area.\n*   **Memory lane:** Create a "memory jar" filled with notes about your favorite moments together, or put together a video montage of your best memories.\nUSER: he is somewhat u know very different from others like so mature at the age of 22 and u know he is so intelligent also \nASSISTANT: That\'s a wonderful insight, Ashu! Knowing he\'s mature and intelligent at 22 helps a lot. For someone with those qualities, experiences that stimulate the mind, offer growth, or provide unique, high-quality enjoyment often resonate more than typical gifts.\n\nHere are some ideas tailored for someone like him:\n\n1.  **Intellectual Stimulation/Learning:**\n    *   **Masterclass/Online Course Subscription:** A subscription to a platform like MasterClass or a specific online course in a field he\'s passionate about (e.g., advanced coding, philosophy, creative writing, science, history).\n    *   **High-Quality Books/Rare Edition:** A special edition of a classic, a first edition of a book he loves, or a carefully curated selection of books by an author or in a genre he admires.\n    *   **Subscription to a Thought-Provoking Magazine/Journal:** Something like The Economist, Smithsonian, National Geographic (if he\'s into science/exploration), or a literary review.\n    *   **Telescope/Microscope:** If he has an interest in astronomy or biology, a good quality one can be a fascinating gift.\n\n2.  **Unique Experiences & Culture:**\n    *   **Tickets to a Special Exhibition/Lecture:** This could be at a museum, a science center, or a university event featuring a speaker he\'d find engaging.\n    *   **Fine Dining Experience:** Not just any restaurant, but perhaps a Michelin-starred experience, a chef\'s tasting menu, or a place known for its innovative cuisine and sophisticated ambiance.\n    *   **Weekend Getaway with a Cultural Focus:** A trip to a city known for its history, art, or architecture, rather than just a party destination.\n    *   **Private Tour:** A private tour of a historical site, a specialized museum, or a unique local attraction that aligns with his interests.\n\n3.  **Sophisticated & Thoughtful Gifts:**\n    *   **Quality Watch or Accessory:** A classic, well-designed watch, a premium pen, or a high-quality leather wallet/briefcase if he appreciates craftsmanship.\n    *   **Curated Collection:** A selection of gourmet coffee, rare teas, artisanal chocolates, or fine spirits (if he enjoys them), presented thoughtfully.\n    *   **Contribution to a Cause:** Make a donation in his name to a charity or organization he supports, perhaps related to education, science, or social justice.\n    *   **Personalized Experience:** A custom-made piece of art that reflects his intellectual pursuits, or a beautifully framed map of a place significant to him.\n\nTo narrow it down further, think about what specific subjects or hobbies capture his intelligent mind the most!\n \nOUTPUT:\nUpdated summary only.\n'}]}], 'system_instruction': {'parts': [{'text': 'You are a precise memory compression engine.'}]}, 'generationConfig': {'temperature': 0.2, 'max_output_tokens': 400}}'
[0m

2026-01-26 23:56:56,769 | DEBUG | httpcore.connection | connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None
2026-01-26 23:56:56,773 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000212048434F0>
2026-01-26 23:56:56,773 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x000002120369BDC0> server_hostname='generativelanguage.googleapis.com' timeout=600.0
2026-01-26 23:56:56,773 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000212048434C0>
2026-01-26 23:56:56,773 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-26 23:56:56,773 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-26 23:56:56,784 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-26 23:56:56,784 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-26 23:56:56,784 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-26 23:56:56,983 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Mon, 26 Jan 2026 18:26:56 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_9a26384848bda8aa5339324b6d861406'), (b'openai-processing-ms', b'375'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'user-u2cbhcbswv5bt4bmhvzdmhwd'), (b'openai-project', b'proj_oIIqE3fW3A3IDZXBAVOtc4gK'), (b'x-envoy-upstream-service-time', b'378'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=ZmulNqqCndqsgo7I0Xgv0w_Zb73GnEgU7E9AJGOg8oU-1769452016-1.0.1.1-xfDv1_vEiafTWPkVhYpOeuD5UotdQGLAEaA1aqywVEpn3lCbVBAumCKiAOhLC5i6A1_imyJ93wZtTq9kA.8LFrtRn2maCHxV.k8o3ZrlaI4; path=/; expires=Mon, 26-Jan-26 18:56:56 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=Fs9RjM4cedmfiWlr0_2yopX4XaW6AqPHIVD34PXXJgo-1769452016931-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c420fb80beb1b73-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-26 23:56:56,993 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-26 23:56:56,993 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-26 23:56:56,993 | DEBUG | httpcore.http11 | response_closed.started
2026-01-26 23:56:56,993 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-26 23:56:56,993 | DEBUG | openai.agents | Exported 7 items
2026-01-26 23:56:56,993 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-26 23:56:56,998 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-26 23:56:56,998 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-26 23:56:56,998 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-26 23:56:56,998 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-26 23:56:57,467 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Mon, 26 Jan 2026 18:26:57 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_cfc0c7a4fd9c3465391a02a6fa5d54ae'), (b'openai-processing-ms', b'137'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'user-u2cbhcbswv5bt4bmhvzdmhwd'), (b'openai-project', b'proj_oIIqE3fW3A3IDZXBAVOtc4gK'), (b'x-envoy-upstream-service-time', b'140'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c420fc1e86a1b73-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-26 23:56:57,467 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-26 23:56:57,467 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-26 23:56:57,467 | DEBUG | httpcore.http11 | response_closed.started
2026-01-26 23:56:57,467 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-26 23:56:57,467 | DEBUG | openai.agents | Exported 2 items
2026-01-26 23:56:59,462 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Mon, 26 Jan 2026 18:26:59 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=2673'), (b'Alt-Svc', b'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])
2026-01-26 23:56:59,462 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-26 23:56:59,463 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-26 23:56:59,463 | DEBUG | httpcore.http11 | response_closed.started
2026-01-26 23:56:59,463 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-26 23:56:59,464 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "Ashu Singh is planning a special birthday gift for her 22-year-old boyfriend, who she describes as mature and intelligent."
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 1131,
    "candidatesTokenCount": 28,
    "totalTokenCount": 1426,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 1131
      }
    ],
    "thoughtsTokenCount": 267
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "87F3adacEc2z4-EPkoOU6Ak"
}



2026-01-26 23:56:59,465 | INFO | LiteLLM | Wrapper: Completed Call, calling success_handler
2026-01-26 23:56:59,466 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-26 23:56:59,467 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-26 23:56:59,468 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:56:59,468 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-26 23:56:59,468 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:56:59,469 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:56:59,469 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:56:59,470 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:56:59,470 | DEBUG | LiteLLM | response_cost: 0.0010768
2026-01-26 23:56:59,470 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:56:59,473 | DEBUG | LiteLLM | response_cost: 0.0010768
2026-01-26 23:56:59,472 | INFO | chat | ‚úÖ Stream complete | tokens=2 | time=9.22s
2026-01-26 23:56:59,473 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-26 23:56:59,476 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:56:59,476 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:56:59,477 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-26 23:56:59,478 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-26 23:56:59,480 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:56:59,481 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:57:20,364 | INFO | chat | üì© Chat request | user_id=6 | request_id=fa15f90f-c89f-407a-b63b-786758370d01
2026-01-26 23:57:20,364 | INFO | chat | üßë User message | myself 
2026-01-26 23:57:20,379 | INFO | chat | ü§ñ Stream started | session_id=ae4b8d0d-4e2b-4c2d-96c5-1acf39b38367 | request_id=fa15f90f-c89f-407a-b63b-786758370d01
2026-01-26 23:57:20,383 | INFO | chat | ü§ñ Running digital human
2026-01-26 23:57:20,385 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-26 23:57:20,385 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-26 23:57:20,385 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_ff7c2c3c60584099943aec1f665bb876
2026-01-26 23:57:20,385 | DEBUG | openai.agents | Setting current trace: trace_ff7c2c3c60584099943aec1f665bb876
2026-01-26 23:57:20,387 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x00000212039544A0> with id None
2026-01-26 23:57:20,387 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-26 23:57:20,388 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000021203AB3160> with id None
2026-01-26 23:57:20,388 | DEBUG | openai.agents | Calling LLM
2026-01-26 23:57:20,388 | DEBUG | LiteLLM | 

2026-01-26 23:57:20,388 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-26 23:57:20,388 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n', 'role': 'system'}, {'role': 'user', 'content': 'myself '}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.5'}, api_key=None, base_url=None)[0m
2026-01-26 23:57:20,388 | DEBUG | LiteLLM | 

2026-01-26 23:57:20,388 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-26 23:57:20,388 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-26 23:57:20,388 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-26 23:57:20,388 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-26 23:57:20,388 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-26 23:57:20,388 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n', 'role': 'system'}, {'role': 'user', 'content': 'myself '}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-26 23:57:20,388 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-26 23:57:20,388 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-26 23:57:20,388 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-26 23:57:20,388 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:20,388 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:20,388 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.5' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'myself '}]}], 'system_instruction': {'parts': [{'text': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n'}]}, 'generationConfig': {}}'
[0m

2026-01-26 23:57:22,238 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "SAFE"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 190,
    "candidatesTokenCount": 1,
    "totalTokenCount": 363,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 190
      }
    ],
    "thoughtsTokenCount": 172
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "CbJ3aZfxOu704-EPuu3I-AU"
}



2026-01-26 23:57:22,238 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-26 23:57:22,238 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-26 23:57:22,238 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-26 23:57:22,238 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:22,238 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-26 23:57:22,238 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:22,238 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:22,238 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:57:22,238 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:22,251 | DEBUG | LiteLLM | response_cost: 0.0004895
2026-01-26 23:57:22,251 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:57:22,252 | DEBUG | openai.agents | Received model response
2026-01-26 23:57:22,252 | DEBUG | LiteLLM | response_cost: 0.0004895
2026-01-26 23:57:22,252 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x00000212039491E0>>
2026-01-26 23:57:22,252 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-26 23:57:22,252 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-26 23:57:22,252 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:22,252 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-26 23:57:22,252 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-26 23:57:22,252 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:57:22,252 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-26 23:57:22,252 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-26 23:57:22,252 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-26 23:57:22,252 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-26 23:57:22,252 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:22,252 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-26 23:57:22,252 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:22,252 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:57:22,267 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:22,267 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:57:22,268 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:57:22,268 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-26 23:57:22,268 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0004895
2026-01-26 23:57:22,268 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-26 23:57:22,268 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:22,268 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:57:22,268 | DEBUG | openai.agents | Resetting current trace
2026-01-26 23:57:22,268 | INFO | orchestrator | ‚úÖ Safety passed
2026-01-26 23:57:22,268 | INFO | orchestrator | üß≠ Router agent called
2026-01-26 23:57:22,268 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_8cd0198ffb0b425a8af58852386c09cc
2026-01-26 23:57:22,268 | DEBUG | openai.agents | Setting current trace: trace_8cd0198ffb0b425a8af58852386c09cc
2026-01-26 23:57:22,268 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x0000021203B4B380> with id None
2026-01-26 23:57:22,268 | DEBUG | openai.agents | Running agent RouterAgent (turn 1)
2026-01-26 23:57:22,268 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000002120407A560> with id None
2026-01-26 23:57:22,268 | DEBUG | openai.agents | Calling LLM
2026-01-26 23:57:22,268 | DEBUG | LiteLLM | 

2026-01-26 23:57:22,268 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-26 23:57:22,284 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'myself '}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.5'}, api_key=None, base_url=None)[0m
2026-01-26 23:57:22,284 | DEBUG | LiteLLM | 

2026-01-26 23:57:22,284 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-26 23:57:22,284 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-26 23:57:22,284 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-26 23:57:22,284 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-26 23:57:22,284 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-26 23:57:22,284 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'myself '}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-26 23:57:22,284 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-26 23:57:22,284 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-26 23:57:22,284 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-26 23:57:22,284 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:22,284 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:22,284 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.5' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'myself '}]}], 'system_instruction': {'parts': [{'text': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n'}]}, 'generationConfig': {}}'
[0m

2026-01-26 23:57:23,249 | DEBUG | httpcore.connection | close.started
2026-01-26 23:57:23,250 | DEBUG | httpcore.connection | close.complete
2026-01-26 23:57:23,250 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-26 23:57:24,281 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000021204085420>
2026-01-26 23:57:24,281 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x000002127D66B4C0> server_hostname='api.openai.com' timeout=5.0
2026-01-26 23:57:24,281 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000212040855A0>
2026-01-26 23:57:24,281 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-26 23:57:24,281 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-26 23:57:24,281 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-26 23:57:24,281 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-26 23:57:24,281 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-26 23:57:24,801 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\n  \"use_memory\": false,\n  \"use_tool\": false,\n  \"tool_name\": \"none\",\n  \"tool_arguments\": null,\n  \"intent\": \"none\",\n  \"memory_key\": null\n}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 488,
    "candidatesTokenCount": 56,
    "totalTokenCount": 714,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 488
      }
    ],
    "thoughtsTokenCount": 170
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "DLJ3abejItqHqfkPkP6gqAk"
}



2026-01-26 23:57:24,801 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-26 23:57:24,801 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-26 23:57:24,801 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-26 23:57:24,816 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:24,816 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-26 23:57:24,817 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:24,817 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:24,817 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:57:24,817 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:24,817 | DEBUG | LiteLLM | response_cost: 0.0007114
2026-01-26 23:57:24,817 | DEBUG | openai.agents | Received model response
2026-01-26 23:57:24,817 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:57:24,817 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x00000212048438E0>>
2026-01-26 23:57:24,817 | DEBUG | LiteLLM | response_cost: 0.0007114
2026-01-26 23:57:24,820 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-26 23:57:24,821 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-26 23:57:24,821 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-26 23:57:24,821 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-26 23:57:24,821 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:24,822 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-26 23:57:24,822 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-26 23:57:24,822 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:57:24,823 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-26 23:57:24,823 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:24,824 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-26 23:57:24,825 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:24,825 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:57:24,825 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-26 23:57:24,825 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:57:24,825 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:24,825 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-26 23:57:24,825 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:57:24,825 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0007114
2026-01-26 23:57:24,825 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-26 23:57:24,825 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:24,825 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:57:24,825 | DEBUG | openai.agents | Resetting current trace
2026-01-26 23:57:24,825 | INFO | orchestrator | üß≠ Router | tool=False memory=False intent=none
2026-01-26 23:57:24,831 | INFO | orchestrator | üß† Reasoning agent called
2026-01-26 23:57:24,832 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_ea8417221c0b4456ba07b6e17ce7f5e3
2026-01-26 23:57:24,832 | DEBUG | openai.agents | Setting current trace: trace_ea8417221c0b4456ba07b6e17ce7f5e3
2026-01-26 23:57:24,832 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000002120407DF80> with id None
2026-01-26 23:57:24,832 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000002120407B280> with id None
2026-01-26 23:57:24,834 | DEBUG | openai.agents | Calling LLM
2026-01-26 23:57:24,834 | DEBUG | LiteLLM | 

2026-01-26 23:57:24,834 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-26 23:57:24,835 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.\n\nIf the user mentions vague locations like:\n- nearest city\n- near me\n- nearby\n- around here\n\nand user location is provided,\nrewrite the query using the nearest major city.\n\nOutput ONLY the rewritten query.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n\\nYou may answer ANY general knowledge question.\\n\\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n\\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n\\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "system", "content": "Conversation summary:\\nAshu Singh is planning a special birthday gift for her 22-year-old boyfriend, who she describes as mature and intelligent."}, {"role": "assistant", "content": "Okay, I\'m ready when you are! Please share some details about your boyfriend\'s hobbies, personality, or budget so I can help you find a great gift for his birthday."}, {"role": "user", "content": "lets discuss about m e"}, {"role": "assistant", "content": "I\'m not sure I understand what \\"m e\\" refers to. Could you please clarify what you\'d like to discuss?"}, {"role": "user", "content": "me "}, {"role": "assistant", "content": "It seems like you might be trying to tell me something about yourself, or perhaps about your boyfriend. Could you please elaborate on what you mean by \\"me\\"? I\'m here to help, whether it\'s about finding a gift or something else!"}, {"role": "user", "content": "myself "}, {"role": "user", "content": "myself "}], "memory_action": {}, "memory_data": [], "tool_context": {}}'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=True, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.5'}, api_key=None, base_url=None)[0m
2026-01-26 23:57:24,835 | DEBUG | LiteLLM | 

2026-01-26 23:57:24,835 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-26 23:57:24,835 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-26 23:57:24,835 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-26 23:57:24,835 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-26 23:57:24,835 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-26 23:57:24,835 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.\n\nIf the user mentions vague locations like:\n- nearest city\n- near me\n- nearby\n- around here\n\nand user location is provided,\nrewrite the query using the nearest major city.\n\nOutput ONLY the rewritten query.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n\\nYou may answer ANY general knowledge question.\\n\\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n\\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n\\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "system", "content": "Conversation summary:\\nAshu Singh is planning a special birthday gift for her 22-year-old boyfriend, who she describes as mature and intelligent."}, {"role": "assistant", "content": "Okay, I\'m ready when you are! Please share some details about your boyfriend\'s hobbies, personality, or budget so I can help you find a great gift for his birthday."}, {"role": "user", "content": "lets discuss about m e"}, {"role": "assistant", "content": "I\'m not sure I understand what \\"m e\\" refers to. Could you please clarify what you\'d like to discuss?"}, {"role": "user", "content": "me "}, {"role": "assistant", "content": "It seems like you might be trying to tell me something about yourself, or perhaps about your boyfriend. Could you please elaborate on what you mean by \\"me\\"? I\'m here to help, whether it\'s about finding a gift or something else!"}, {"role": "user", "content": "myself "}, {"role": "user", "content": "myself "}], "memory_action": {}, "memory_data": [], "tool_context": {}}'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-26 23:57:24,835 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': True}
2026-01-26 23:57:24,835 | DEBUG | LiteLLM | Final returned optional params: {'stream': True}
2026-01-26 23:57:24,840 | DEBUG | LiteLLM | self.optional_params: {'stream': True}
2026-01-26 23:57:24,841 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:24,841 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:24,841 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?key=*****=sse \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.5' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n\\nYou may answer ANY general knowledge question.\\n\\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n\\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n\\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "system", "content": "Conversation summary:\\nAshu Singh is planning a special birthday gift for her 22-year-old boyfriend, who she describes as mature and intelligent."}, {"role": "assistant", "content": "Okay, I\'m ready when you are! Please share some details about your boyfriend\'s hobbies, personality, or budget so I can help you find a great gift for his birthday."}, {"role": "user", "content": "lets discuss about m e"}, {"role": "assistant", "content": "I\'m not sure I understand what \\"m e\\" refers to. Could you please clarify what you\'d like to discuss?"}, {"role": "user", "content": "me "}, {"role": "assistant", "content": "It seems like you might be trying to tell me something about yourself, or perhaps about your boyfriend. Could you please elaborate on what you mean by \\"me\\"? I\'m here to help, whether it\'s about finding a gift or something else!"}, {"role": "user", "content": "myself "}, {"role": "user", "content": "myself "}], "memory_action": {}, "memory_data": [], "tool_context": {}}'}]}], 'system_instruction': {'parts': [{'text': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.\n\nIf the user mentions vague locations like:\n- nearest city\n- near me\n- nearby\n- around here\n\nand user location is provided,\nrewrite the query using the nearest major city.\n\nOutput ONLY the rewritten query.'}]}, 'generationConfig': {}}'
[0m

2026-01-26 23:57:24,842 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-26 23:57:24,842 | DEBUG | LiteLLM | messages in token_counter: None, text in token_counter: 
2026-01-26 23:57:24,843 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:24,843 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:24,843 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:57:24,844 | DEBUG | LiteLLM | response_cost: 0.0
2026-01-26 23:57:25,517 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Mon, 26 Jan 2026 18:27:25 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_db90bbbe8614829d775989476341468b'), (b'openai-processing-ms', b'117'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'user-u2cbhcbswv5bt4bmhvzdmhwd'), (b'openai-project', b'proj_oIIqE3fW3A3IDZXBAVOtc4gK'), (b'x-envoy-upstream-service-time', b'120'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c42106c7b07a346-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-26 23:57:25,519 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-26 23:57:25,519 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-26 23:57:25,519 | DEBUG | httpcore.http11 | response_closed.started
2026-01-26 23:57:25,519 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-26 23:57:25,519 | DEBUG | openai.agents | Exported 4 items
2026-01-26 23:57:25,521 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-26 23:57:25,521 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-26 23:57:25,521 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-26 23:57:25,521 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-26 23:57:25,521 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-26 23:57:26,426 | DEBUG | LiteLLM | RAW RESPONSE:
first stream response received


2026-01-26 23:57:26,427 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': 'Okay, what would you like to tell me about yourself'}], 'role': 'model'}, 'index': 0}], 'usageMetadata': {'promptTokenCount': 525, 'candidatesTokenCount': 11, 'totalTokenCount': 642, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 525}], 'thoughtsTokenCount': 106}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'DbJ3aZjNIq7tg8UPzpXc0Qo'}
2026-01-26 23:57:26,428 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='DbJ3aZjNIq7tg8UPzpXc0Qo', created=1769452046, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='Okay, what would you like to tell me about yourself', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=117, prompt_tokens=525, total_tokens=642, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=106, rejected_prediction_tokens=None, text_tokens=11, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=525, image_tokens=None)))
2026-01-26 23:57:26,429 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content='Okay, what would you like to tell me about yourself', role='assistant', function_call=None, tool_calls=None, audio=None)
2026-01-26 23:57:26,429 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='DbJ3aZjNIq7tg8UPzpXc0Qo', created=1769452046, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Okay, what would you like to tell me about yourself', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=117, prompt_tokens=525, total_tokens=642, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=106, rejected_prediction_tokens=None, text_tokens=11, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=525, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-26 23:57:26,433 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': "? Are you looking to provide more details to help with the gift for your boyfriend, or is there something else you'd like to discuss?"}], 'role': 'model'}, 'finishReason': 'STOP', 'index': 0}], 'usageMetadata': {'promptTokenCount': 525, 'candidatesTokenCount': 40, 'totalTokenCount': 671, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 525}], 'thoughtsTokenCount': 106}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'DbJ3aZjNIq7tg8UPzpXc0Qo'}
2026-01-26 23:57:26,434 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='DbJ3aZjNIq7tg8UPzpXc0Qo', created=1769452046, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content="? Are you looking to provide more details to help with the gift for your boyfriend, or is there something else you'd like to discuss?", role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=146, prompt_tokens=525, total_tokens=671, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=106, rejected_prediction_tokens=None, text_tokens=40, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=525, image_tokens=None)))
2026-01-26 23:57:26,435 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content="? Are you looking to provide more details to help with the gift for your boyfriend, or is there something else you'd like to discuss?", role=None, function_call=None, tool_calls=None, audio=None)
2026-01-26 23:57:26,436 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='DbJ3aZjNIq7tg8UPzpXc0Qo', created=1769452046, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content="? Are you looking to provide more details to help with the gift for your boyfriend, or is there something else you'd like to discuss?", role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=146, prompt_tokens=525, total_tokens=671, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=106, rejected_prediction_tokens=None, text_tokens=40, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=525, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-26 23:57:26,439 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=False
2026-01-26 23:57:26,439 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=False
2026-01-26 23:57:26,439 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-26 23:57:26,439 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-26 23:57:26,441 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-26 23:57:26,441 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-26 23:57:26,441 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:26,442 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:26,442 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:26,443 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:26,443 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:57:26,443 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:57:26,444 | DEBUG | LiteLLM | response_cost: 0.0005225
2026-01-26 23:57:26,444 | DEBUG | LiteLLM | response_cost: 0.0005225
2026-01-26 23:57:26,445 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0005225
2026-01-26 23:57:26,446 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-26 23:57:26,446 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-26 23:57:26,447 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:26,447 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:26,447 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:57:26,448 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:57:26,449 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-26 23:57:26,449 | DEBUG | openai.agents | Resetting current trace
2026-01-26 23:57:26,456 | DEBUG | LiteLLM | 

2026-01-26 23:57:26,457 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-26 23:57:26,457 | DEBUG | LiteLLM | [92mlitellm.completion(model='gemini/gemini-2.5-flash', provider='gemini', messages=[{'role': 'system', 'content': 'You are a precise memory compression engine.'}, {'role': 'user', 'content': '\nYou are a memory compression system.\n \nTASK:\nUpdate the existing conversation summary using the new dialogue below.\n \nRULES:\n- Preserve facts, decisions, preferences, and open tasks\n- Preserve key events and discussion topics\n- Remove chit-chat and repetition\n- Do NOT invent new information\n- Keep the summary concise and factual\n \nEXISTING SUMMARY:\nAshu Singh is planning a special birthday gift for her 22-year-old boyfriend, who she describes as mature and intelligent.\n \nNEW DIALOGUE:\nUSER: hi \nASSISTANT: Hello! How can I help you today?\nUSER: hi\nASSISTANT: Hello! How can I help you today?\nUSER: my self ashu singh \nASSISTANT: Hello Ashu Singh! How can I help you today?\nUSER: i am planning to do something interesting \nASSISTANT: That sounds great, Ashu! What kind of interesting things are you considering? I can help you brainstorm some ideas if you tell me a little more about what you\'re looking for.\nUSER: i have my bf birthday coming i want to do something special for him \nASSISTANT: That\'s very thoughtful of you, Ashu! To help me suggest something really special, could you tell me a little more about your boyfriend\'s interests or what kind of things he enjoys?\n\nIn the meantime, here are some general ideas:\n\n*   **Plan a romantic dinner:** Cook his favorite meal at home, or book a table at a restaurant he\'s always wanted to try.\n*   **Weekend getaway:** A surprise trip to a nearby city, a cozy cabin, or a relaxing beach destination.\n*   **Experience gift:** Tickets to a concert, a sporting event, a workshop (like cooking or brewing), or an adventure activity he\'d love (e.g., skydiving, go-karting).\n*   **Personalized gift:** A custom photo album, a piece of art, or a handmade gift that reflects your shared memories.\n*   **Adventure day:** Plan a day filled with activities he loves, like hiking, visiting a museum, playing his favorite sport, or exploring a new area.\n*   **Memory lane:** Create a "memory jar" filled with notes about your favorite moments together, or put together a video montage of your best memories.\nUSER: he is somewhat u know very different from others like so mature at the age of 22 and u know he is so intelligent also \nASSISTANT: That\'s a wonderful insight, Ashu! Knowing he\'s mature and intelligent at 22 helps a lot. For someone with those qualities, experiences that stimulate the mind, offer growth, or provide unique, high-quality enjoyment often resonate more than typical gifts.\n\nHere are some ideas tailored for someone like him:\n\n1.  **Intellectual Stimulation/Learning:**\n    *   **Masterclass/Online Course Subscription:** A subscription to a platform like MasterClass or a specific online course in a field he\'s passionate about (e.g., advanced coding, philosophy, creative writing, science, history).\n    *   **High-Quality Books/Rare Edition:** A special edition of a classic, a first edition of a book he loves, or a carefully curated selection of books by an author or in a genre he admires.\n    *   **Subscription to a Thought-Provoking Magazine/Journal:** Something like The Economist, Smithsonian, National Geographic (if he\'s into science/exploration), or a literary review.\n    *   **Telescope/Microscope:** If he has an interest in astronomy or biology, a good quality one can be a fascinating gift.\n\n2.  **Unique Experiences & Culture:**\n    *   **Tickets to a Special Exhibition/Lecture:** This could be at a museum, a science center, or a university event featuring a speaker he\'d find engaging.\n    *   **Fine Dining Experience:** Not just any restaurant, but perhaps a Michelin-starred experience, a chef\'s tasting menu, or a place known for its innovative cuisine and sophisticated ambiance.\n    *   **Weekend Getaway with a Cultural Focus:** A trip to a city known for its history, art, or architecture, rather than just a party destination.\n    *   **Private Tour:** A private tour of a historical site, a specialized museum, or a unique local attraction that aligns with his interests.\n\n3.  **Sophisticated & Thoughtful Gifts:**\n    *   **Quality Watch or Accessory:** A classic, well-designed watch, a premium pen, or a high-quality leather wallet/briefcase if he appreciates craftsmanship.\n    *   **Curated Collection:** A selection of gourmet coffee, rare teas, artisanal chocolates, or fine spirits (if he enjoys them), presented thoughtfully.\n    *   **Contribution to a Cause:** Make a donation in his name to a charity or organization he supports, perhaps related to education, science, or social justice.\n    *   **Personalized Experience:** A custom-made piece of art that reflects his intellectual pursuits, or a beautifully framed map of a place significant to him.\n\nTo narrow it down further, think about what specific subjects or hobbies capture his intelligent mind the most!\n \nOUTPUT:\nUpdated summary only.\n'}], temperature=0.2, max_tokens=400)[0m
2026-01-26 23:57:26,457 | DEBUG | LiteLLM | 

2026-01-26 23:57:26,457 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-26 23:57:26,458 | DEBUG | LiteLLM | SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False
2026-01-26 23:57:26,458 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-26 23:57:26,459 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-26 23:57:26,460 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': 0.2, 'top_p': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': 400, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': 'You are a precise memory compression engine.'}, {'role': 'user', 'content': '\nYou are a memory compression system.\n \nTASK:\nUpdate the existing conversation summary using the new dialogue below.\n \nRULES:\n- Preserve facts, decisions, preferences, and open tasks\n- Preserve key events and discussion topics\n- Remove chit-chat and repetition\n- Do NOT invent new information\n- Keep the summary concise and factual\n \nEXISTING SUMMARY:\nAshu Singh is planning a special birthday gift for her 22-year-old boyfriend, who she describes as mature and intelligent.\n \nNEW DIALOGUE:\nUSER: hi \nASSISTANT: Hello! How can I help you today?\nUSER: hi\nASSISTANT: Hello! How can I help you today?\nUSER: my self ashu singh \nASSISTANT: Hello Ashu Singh! How can I help you today?\nUSER: i am planning to do something interesting \nASSISTANT: That sounds great, Ashu! What kind of interesting things are you considering? I can help you brainstorm some ideas if you tell me a little more about what you\'re looking for.\nUSER: i have my bf birthday coming i want to do something special for him \nASSISTANT: That\'s very thoughtful of you, Ashu! To help me suggest something really special, could you tell me a little more about your boyfriend\'s interests or what kind of things he enjoys?\n\nIn the meantime, here are some general ideas:\n\n*   **Plan a romantic dinner:** Cook his favorite meal at home, or book a table at a restaurant he\'s always wanted to try.\n*   **Weekend getaway:** A surprise trip to a nearby city, a cozy cabin, or a relaxing beach destination.\n*   **Experience gift:** Tickets to a concert, a sporting event, a workshop (like cooking or brewing), or an adventure activity he\'d love (e.g., skydiving, go-karting).\n*   **Personalized gift:** A custom photo album, a piece of art, or a handmade gift that reflects your shared memories.\n*   **Adventure day:** Plan a day filled with activities he loves, like hiking, visiting a museum, playing his favorite sport, or exploring a new area.\n*   **Memory lane:** Create a "memory jar" filled with notes about your favorite moments together, or put together a video montage of your best memories.\nUSER: he is somewhat u know very different from others like so mature at the age of 22 and u know he is so intelligent also \nASSISTANT: That\'s a wonderful insight, Ashu! Knowing he\'s mature and intelligent at 22 helps a lot. For someone with those qualities, experiences that stimulate the mind, offer growth, or provide unique, high-quality enjoyment often resonate more than typical gifts.\n\nHere are some ideas tailored for someone like him:\n\n1.  **Intellectual Stimulation/Learning:**\n    *   **Masterclass/Online Course Subscription:** A subscription to a platform like MasterClass or a specific online course in a field he\'s passionate about (e.g., advanced coding, philosophy, creative writing, science, history).\n    *   **High-Quality Books/Rare Edition:** A special edition of a classic, a first edition of a book he loves, or a carefully curated selection of books by an author or in a genre he admires.\n    *   **Subscription to a Thought-Provoking Magazine/Journal:** Something like The Economist, Smithsonian, National Geographic (if he\'s into science/exploration), or a literary review.\n    *   **Telescope/Microscope:** If he has an interest in astronomy or biology, a good quality one can be a fascinating gift.\n\n2.  **Unique Experiences & Culture:**\n    *   **Tickets to a Special Exhibition/Lecture:** This could be at a museum, a science center, or a university event featuring a speaker he\'d find engaging.\n    *   **Fine Dining Experience:** Not just any restaurant, but perhaps a Michelin-starred experience, a chef\'s tasting menu, or a place known for its innovative cuisine and sophisticated ambiance.\n    *   **Weekend Getaway with a Cultural Focus:** A trip to a city known for its history, art, or architecture, rather than just a party destination.\n    *   **Private Tour:** A private tour of a historical site, a specialized museum, or a unique local attraction that aligns with his interests.\n\n3.  **Sophisticated & Thoughtful Gifts:**\n    *   **Quality Watch or Accessory:** A classic, well-designed watch, a premium pen, or a high-quality leather wallet/briefcase if he appreciates craftsmanship.\n    *   **Curated Collection:** A selection of gourmet coffee, rare teas, artisanal chocolates, or fine spirits (if he enjoys them), presented thoughtfully.\n    *   **Contribution to a Cause:** Make a donation in his name to a charity or organization he supports, perhaps related to education, science, or social justice.\n    *   **Personalized Experience:** A custom-made piece of art that reflects his intellectual pursuits, or a beautifully framed map of a place significant to him.\n\nTo narrow it down further, think about what specific subjects or hobbies capture his intelligent mind the most!\n \nOUTPUT:\nUpdated summary only.\n'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None, 'provider': 'gemini'}
2026-01-26 23:57:26,460 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'temperature': 0.2, 'max_tokens': 400}
2026-01-26 23:57:26,461 | DEBUG | LiteLLM | Final returned optional params: {'temperature': 0.2, 'max_output_tokens': 400, 'provider': 'gemini'}
2026-01-26 23:57:26,461 | DEBUG | LiteLLM | self.optional_params: {'temperature': 0.2, 'max_tokens': 400, 'provider': 'gemini'}
2026-01-26 23:57:26,462 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:26,462 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:26,463 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '\nYou are a memory compression system.\n \nTASK:\nUpdate the existing conversation summary using the new dialogue below.\n \nRULES:\n- Preserve facts, decisions, preferences, and open tasks\n- Preserve key events and discussion topics\n- Remove chit-chat and repetition\n- Do NOT invent new information\n- Keep the summary concise and factual\n \nEXISTING SUMMARY:\nAshu Singh is planning a special birthday gift for her 22-year-old boyfriend, who she describes as mature and intelligent.\n \nNEW DIALOGUE:\nUSER: hi \nASSISTANT: Hello! How can I help you today?\nUSER: hi\nASSISTANT: Hello! How can I help you today?\nUSER: my self ashu singh \nASSISTANT: Hello Ashu Singh! How can I help you today?\nUSER: i am planning to do something interesting \nASSISTANT: That sounds great, Ashu! What kind of interesting things are you considering? I can help you brainstorm some ideas if you tell me a little more about what you\'re looking for.\nUSER: i have my bf birthday coming i want to do something special for him \nASSISTANT: That\'s very thoughtful of you, Ashu! To help me suggest something really special, could you tell me a little more about your boyfriend\'s interests or what kind of things he enjoys?\n\nIn the meantime, here are some general ideas:\n\n*   **Plan a romantic dinner:** Cook his favorite meal at home, or book a table at a restaurant he\'s always wanted to try.\n*   **Weekend getaway:** A surprise trip to a nearby city, a cozy cabin, or a relaxing beach destination.\n*   **Experience gift:** Tickets to a concert, a sporting event, a workshop (like cooking or brewing), or an adventure activity he\'d love (e.g., skydiving, go-karting).\n*   **Personalized gift:** A custom photo album, a piece of art, or a handmade gift that reflects your shared memories.\n*   **Adventure day:** Plan a day filled with activities he loves, like hiking, visiting a museum, playing his favorite sport, or exploring a new area.\n*   **Memory lane:** Create a "memory jar" filled with notes about your favorite moments together, or put together a video montage of your best memories.\nUSER: he is somewhat u know very different from others like so mature at the age of 22 and u know he is so intelligent also \nASSISTANT: That\'s a wonderful insight, Ashu! Knowing he\'s mature and intelligent at 22 helps a lot. For someone with those qualities, experiences that stimulate the mind, offer growth, or provide unique, high-quality enjoyment often resonate more than typical gifts.\n\nHere are some ideas tailored for someone like him:\n\n1.  **Intellectual Stimulation/Learning:**\n    *   **Masterclass/Online Course Subscription:** A subscription to a platform like MasterClass or a specific online course in a field he\'s passionate about (e.g., advanced coding, philosophy, creative writing, science, history).\n    *   **High-Quality Books/Rare Edition:** A special edition of a classic, a first edition of a book he loves, or a carefully curated selection of books by an author or in a genre he admires.\n    *   **Subscription to a Thought-Provoking Magazine/Journal:** Something like The Economist, Smithsonian, National Geographic (if he\'s into science/exploration), or a literary review.\n    *   **Telescope/Microscope:** If he has an interest in astronomy or biology, a good quality one can be a fascinating gift.\n\n2.  **Unique Experiences & Culture:**\n    *   **Tickets to a Special Exhibition/Lecture:** This could be at a museum, a science center, or a university event featuring a speaker he\'d find engaging.\n    *   **Fine Dining Experience:** Not just any restaurant, but perhaps a Michelin-starred experience, a chef\'s tasting menu, or a place known for its innovative cuisine and sophisticated ambiance.\n    *   **Weekend Getaway with a Cultural Focus:** A trip to a city known for its history, art, or architecture, rather than just a party destination.\n    *   **Private Tour:** A private tour of a historical site, a specialized museum, or a unique local attraction that aligns with his interests.\n\n3.  **Sophisticated & Thoughtful Gifts:**\n    *   **Quality Watch or Accessory:** A classic, well-designed watch, a premium pen, or a high-quality leather wallet/briefcase if he appreciates craftsmanship.\n    *   **Curated Collection:** A selection of gourmet coffee, rare teas, artisanal chocolates, or fine spirits (if he enjoys them), presented thoughtfully.\n    *   **Contribution to a Cause:** Make a donation in his name to a charity or organization he supports, perhaps related to education, science, or social justice.\n    *   **Personalized Experience:** A custom-made piece of art that reflects his intellectual pursuits, or a beautifully framed map of a place significant to him.\n\nTo narrow it down further, think about what specific subjects or hobbies capture his intelligent mind the most!\n \nOUTPUT:\nUpdated summary only.\n'}]}], 'system_instruction': {'parts': [{'text': 'You are a precise memory compression engine.'}]}, 'generationConfig': {'temperature': 0.2, 'max_output_tokens': 400}}'
[0m

2026-01-26 23:57:26,464 | DEBUG | httpcore.connection | close.started
2026-01-26 23:57:26,464 | DEBUG | httpcore.connection | close.complete
2026-01-26 23:57:26,465 | DEBUG | httpcore.connection | connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None
2026-01-26 23:57:26,483 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000021204843C40>
2026-01-26 23:57:26,483 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x000002120369BDC0> server_hostname='generativelanguage.googleapis.com' timeout=600.0
2026-01-26 23:57:26,489 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000212048425F0>
2026-01-26 23:57:26,489 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-26 23:57:26,489 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-26 23:57:26,489 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-26 23:57:26,490 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-26 23:57:26,490 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-26 23:57:26,533 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Mon, 26 Jan 2026 18:27:26 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_71c84d93d95ec8a0e8ba81b99331c4e0'), (b'openai-processing-ms', b'90'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'user-u2cbhcbswv5bt4bmhvzdmhwd'), (b'openai-project', b'proj_oIIqE3fW3A3IDZXBAVOtc4gK'), (b'x-envoy-upstream-service-time', b'96'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4210742fbda346-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-26 23:57:26,533 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-26 23:57:26,533 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-26 23:57:26,533 | DEBUG | httpcore.http11 | response_closed.started
2026-01-26 23:57:26,533 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-26 23:57:26,533 | DEBUG | openai.agents | Exported 3 items
2026-01-26 23:57:26,533 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-26 23:57:26,533 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-26 23:57:26,533 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-26 23:57:26,533 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-26 23:57:26,533 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-26 23:57:26,967 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Mon, 26 Jan 2026 18:27:26 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_65cfe40ecf0e3c0e91278f2b0cf6ea58'), (b'openai-processing-ms', b'117'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'user-u2cbhcbswv5bt4bmhvzdmhwd'), (b'openai-project', b'proj_oIIqE3fW3A3IDZXBAVOtc4gK'), (b'x-envoy-upstream-service-time', b'120'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c42107a8acca346-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-26 23:57:26,967 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-26 23:57:26,967 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-26 23:57:26,967 | DEBUG | httpcore.http11 | response_closed.started
2026-01-26 23:57:26,967 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-26 23:57:26,967 | DEBUG | openai.agents | Exported 2 items
2026-01-26 23:57:29,818 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Mon, 26 Jan 2026 18:27:29 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=3329'), (b'Alt-Svc', b'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])
2026-01-26 23:57:29,818 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-26 23:57:29,818 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-26 23:57:29,818 | DEBUG | httpcore.http11 | response_closed.started
2026-01-26 23:57:29,818 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-26 23:57:29,818 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "Ashu Singh is planning a special birthday gift for her 22-year-old boyfriend, who she describes as mature and intelligent."
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 1131,
    "candidatesTokenCount": 28,
    "totalTokenCount": 1466,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 1131
      }
    ],
    "thoughtsTokenCount": 307
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "EbJ3aeSLIpzG4-EPr5SFwQk"
}



2026-01-26 23:57:29,818 | INFO | LiteLLM | Wrapper: Completed Call, calling success_handler
2026-01-26 23:57:29,818 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-26 23:57:29,818 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-26 23:57:29,818 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:29,818 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-26 23:57:29,818 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:29,818 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:29,818 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:57:29,818 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:29,818 | DEBUG | LiteLLM | response_cost: 0.0011768
2026-01-26 23:57:29,818 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:57:29,832 | DEBUG | LiteLLM | response_cost: 0.0011768
2026-01-26 23:57:29,818 | INFO | chat | ‚úÖ Stream complete | tokens=2 | time=9.46s
2026-01-26 23:57:29,833 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-26 23:57:29,835 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:29,835 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:57:29,836 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-26 23:57:29,837 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-26 23:57:29,837 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:29,837 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:57:52,014 | INFO | chat | üì© Chat request | user_id=6 | request_id=acbe85dd-0da1-4784-809e-c04617e8fd65
2026-01-26 23:57:52,015 | INFO | chat | üßë User message | hi
2026-01-26 23:57:52,016 | INFO | chat | ü§ñ Stream started | session_id=7ca900f1-14b6-4d0f-8ff8-094a9fc2b87c | request_id=acbe85dd-0da1-4784-809e-c04617e8fd65
2026-01-26 23:57:52,016 | INFO | chat | ü§ñ Running digital human
2026-01-26 23:57:52,031 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-26 23:57:52,032 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-26 23:57:52,032 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_7626d609e7774c11845c6c76c3ee6c5b
2026-01-26 23:57:52,033 | DEBUG | openai.agents | Setting current trace: trace_7626d609e7774c11845c6c76c3ee6c5b
2026-01-26 23:57:52,033 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000002120407EAC0> with id None
2026-01-26 23:57:52,033 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-26 23:57:52,034 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000021203AB3B20> with id None
2026-01-26 23:57:52,034 | DEBUG | openai.agents | Calling LLM
2026-01-26 23:57:52,034 | DEBUG | LiteLLM | 

2026-01-26 23:57:52,035 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-26 23:57:52,035 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n', 'role': 'system'}, {'role': 'user', 'content': 'hi'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.5'}, api_key=None, base_url=None)[0m
2026-01-26 23:57:52,036 | DEBUG | LiteLLM | 

2026-01-26 23:57:52,037 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-26 23:57:52,037 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-26 23:57:52,038 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-26 23:57:52,040 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-26 23:57:52,041 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-26 23:57:52,043 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n', 'role': 'system'}, {'role': 'user', 'content': 'hi'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-26 23:57:52,043 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-26 23:57:52,044 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-26 23:57:52,045 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-26 23:57:52,045 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:52,047 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:52,048 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.5' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'hi'}]}], 'system_instruction': {'parts': [{'text': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n'}]}, 'generationConfig': {}}'
[0m

2026-01-26 23:57:52,683 | DEBUG | httpcore.connection | close.started
2026-01-26 23:57:52,685 | DEBUG | httpcore.connection | close.complete
2026-01-26 23:57:52,685 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-26 23:57:52,715 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000021204817E20>
2026-01-26 23:57:52,715 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x000002127D66B4C0> server_hostname='api.openai.com' timeout=5.0
2026-01-26 23:57:52,717 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000212048170D0>
2026-01-26 23:57:52,717 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-26 23:57:52,717 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-26 23:57:52,717 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-26 23:57:52,717 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-26 23:57:52,717 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-26 23:57:53,250 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "SAFE"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 188,
    "candidatesTokenCount": 1,
    "totalTokenCount": 189,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 188
      }
    ]
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "KLJ3aZj3PN674-EPutfdsAE"
}



2026-01-26 23:57:53,250 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-26 23:57:53,250 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-26 23:57:53,250 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-26 23:57:53,250 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:53,250 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-26 23:57:53,250 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:53,250 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:53,250 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:57:53,250 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:53,250 | DEBUG | LiteLLM | response_cost: 5.8899999999999995e-05
2026-01-26 23:57:53,250 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:57:53,265 | DEBUG | openai.agents | Received model response
2026-01-26 23:57:53,267 | DEBUG | LiteLLM | response_cost: 5.8899999999999995e-05
2026-01-26 23:57:53,268 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x0000021204842740>>
2026-01-26 23:57:53,268 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-26 23:57:53,268 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-26 23:57:53,268 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-26 23:57:53,268 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:53,268 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-26 23:57:53,268 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-26 23:57:53,268 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:57:53,273 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-26 23:57:53,273 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-26 23:57:53,275 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-26 23:57:53,275 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:53,275 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:53,278 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-26 23:57:53,278 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:57:53,278 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:57:53,280 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:53,282 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-26 23:57:53,282 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:57:53,282 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=5.8899999999999995e-05
2026-01-26 23:57:53,283 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-26 23:57:53,284 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:53,284 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:57:53,284 | DEBUG | openai.agents | Resetting current trace
2026-01-26 23:57:53,284 | INFO | orchestrator | ‚úÖ Safety passed
2026-01-26 23:57:53,284 | INFO | orchestrator | üß≠ Router agent called
2026-01-26 23:57:53,284 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_952761ec61514f8bb67fb57219fc146b
2026-01-26 23:57:53,284 | DEBUG | openai.agents | Setting current trace: trace_952761ec61514f8bb67fb57219fc146b
2026-01-26 23:57:53,284 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000002120407EB60> with id None
2026-01-26 23:57:53,284 | DEBUG | openai.agents | Running agent RouterAgent (turn 1)
2026-01-26 23:57:53,284 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000021204079E40> with id None
2026-01-26 23:57:53,284 | DEBUG | openai.agents | Calling LLM
2026-01-26 23:57:53,284 | DEBUG | LiteLLM | 

2026-01-26 23:57:53,284 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-26 23:57:53,284 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'hi'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.5'}, api_key=None, base_url=None)[0m
2026-01-26 23:57:53,284 | DEBUG | LiteLLM | 

2026-01-26 23:57:53,284 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-26 23:57:53,284 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-26 23:57:53,284 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-26 23:57:53,284 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-26 23:57:53,284 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-26 23:57:53,284 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'hi'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-26 23:57:53,284 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-26 23:57:53,284 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-26 23:57:53,284 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-26 23:57:53,284 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:53,284 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:53,299 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.5' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'hi'}]}], 'system_instruction': {'parts': [{'text': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n'}]}, 'generationConfig': {}}'
[0m

2026-01-26 23:57:54,216 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Mon, 26 Jan 2026 18:27:54 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_9158b41ff0ae6afde29a6289477bd6b1'), (b'openai-processing-ms', b'605'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'user-u2cbhcbswv5bt4bmhvzdmhwd'), (b'openai-project', b'proj_oIIqE3fW3A3IDZXBAVOtc4gK'), (b'x-envoy-upstream-service-time', b'612'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c42111e39bbb232-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-26 23:57:54,217 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-26 23:57:54,217 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-26 23:57:54,217 | DEBUG | httpcore.http11 | response_closed.started
2026-01-26 23:57:54,217 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-26 23:57:54,217 | DEBUG | openai.agents | Exported 1 items
2026-01-26 23:57:54,217 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-26 23:57:54,217 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-26 23:57:54,217 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-26 23:57:54,217 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-26 23:57:54,217 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-26 23:57:54,633 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Mon, 26 Jan 2026 18:27:54 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_d9673f6582e3136f2dd1b1a067280499'), (b'openai-processing-ms', b'116'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'user-u2cbhcbswv5bt4bmhvzdmhwd'), (b'openai-project', b'proj_oIIqE3fW3A3IDZXBAVOtc4gK'), (b'x-envoy-upstream-service-time', b'119'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4211278953b232-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-26 23:57:54,633 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-26 23:57:54,633 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-26 23:57:54,633 | DEBUG | httpcore.http11 | response_closed.started
2026-01-26 23:57:54,633 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-26 23:57:54,633 | DEBUG | openai.agents | Exported 3 items
2026-01-26 23:57:55,931 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\"use_memory\": false, \"use_tool\": false, \"tool_name\": \"none\", \"tool_arguments\": null, \"intent\": \"none\", \"memory_key\": null}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 486,
    "candidatesTokenCount": 42,
    "totalTokenCount": 624,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 486
      }
    ],
    "thoughtsTokenCount": 96
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "K7J3afn1K-GGg8UPk6TOsAY"
}



2026-01-26 23:57:55,931 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-26 23:57:55,931 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-26 23:57:55,931 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-26 23:57:55,931 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:55,931 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-26 23:57:55,931 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:55,931 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:55,947 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:57:55,947 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:55,949 | DEBUG | LiteLLM | response_cost: 0.0004908
2026-01-26 23:57:55,950 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:57:55,951 | DEBUG | openai.agents | Received model response
2026-01-26 23:57:55,951 | DEBUG | LiteLLM | response_cost: 0.0004908
2026-01-26 23:57:55,952 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x0000021204084400>>
2026-01-26 23:57:55,953 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-26 23:57:55,954 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-26 23:57:55,954 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:55,955 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-26 23:57:55,955 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-26 23:57:55,955 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:57:55,956 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-26 23:57:55,957 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-26 23:57:55,957 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-26 23:57:55,958 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-26 23:57:55,959 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:55,960 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-26 23:57:55,961 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:55,961 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:57:55,962 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:55,962 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:57:55,964 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:57:55,965 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-26 23:57:55,966 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0004908
2026-01-26 23:57:55,966 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-26 23:57:55,967 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:55,967 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:57:55,967 | DEBUG | openai.agents | Resetting current trace
2026-01-26 23:57:55,967 | INFO | orchestrator | üß≠ Router | tool=False memory=False intent=none
2026-01-26 23:57:55,970 | INFO | orchestrator | üß† Reasoning agent called
2026-01-26 23:57:55,970 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_1e62188d2de741f8b078fb3d32314a1e
2026-01-26 23:57:55,970 | DEBUG | openai.agents | Setting current trace: trace_1e62188d2de741f8b078fb3d32314a1e
2026-01-26 23:57:55,970 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000002120407F830> with id None
2026-01-26 23:57:55,970 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x00000212040781C0> with id None
2026-01-26 23:57:55,970 | DEBUG | openai.agents | Calling LLM
2026-01-26 23:57:55,970 | DEBUG | LiteLLM | 

2026-01-26 23:57:55,970 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-26 23:57:55,970 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.\n\nIf the user mentions vague locations like:\n- nearest city\n- near me\n- nearby\n- around here\n\nand user location is provided,\nrewrite the query using the nearest major city.\n\nOutput ONLY the rewritten query.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n\\nYou may answer ANY general knowledge question.\\n\\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n\\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n\\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "hi"}, {"role": "user", "content": "hi"}], "memory_action": {}, "memory_data": [], "tool_context": {}}'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=True, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.5'}, api_key=None, base_url=None)[0m
2026-01-26 23:57:55,970 | DEBUG | LiteLLM | 

2026-01-26 23:57:55,970 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-26 23:57:55,970 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-26 23:57:55,970 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-26 23:57:55,970 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-26 23:57:55,970 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-26 23:57:55,970 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.\n\nIf the user mentions vague locations like:\n- nearest city\n- near me\n- nearby\n- around here\n\nand user location is provided,\nrewrite the query using the nearest major city.\n\nOutput ONLY the rewritten query.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n\\nYou may answer ANY general knowledge question.\\n\\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n\\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n\\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "hi"}, {"role": "user", "content": "hi"}], "memory_action": {}, "memory_data": [], "tool_context": {}}'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-26 23:57:55,970 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': True}
2026-01-26 23:57:55,970 | DEBUG | LiteLLM | Final returned optional params: {'stream': True}
2026-01-26 23:57:55,983 | DEBUG | LiteLLM | self.optional_params: {'stream': True}
2026-01-26 23:57:55,983 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:55,983 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:55,983 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?key=*****=sse \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.5' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n\\nYou may answer ANY general knowledge question.\\n\\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n\\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n\\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "hi"}, {"role": "user", "content": "hi"}], "memory_action": {}, "memory_data": [], "tool_context": {}}'}]}], 'system_instruction': {'parts': [{'text': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.\n\nIf the user mentions vague locations like:\n- nearest city\n- near me\n- nearby\n- around here\n\nand user location is provided,\nrewrite the query using the nearest major city.\n\nOutput ONLY the rewritten query.'}]}, 'generationConfig': {}}'
[0m

2026-01-26 23:57:55,983 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-26 23:57:55,983 | DEBUG | LiteLLM | messages in token_counter: None, text in token_counter: 
2026-01-26 23:57:55,983 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:55,983 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:55,983 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:57:55,983 | DEBUG | LiteLLM | response_cost: 0.0
2026-01-26 23:57:57,099 | DEBUG | LiteLLM | RAW RESPONSE:
first stream response received


2026-01-26 23:57:57,099 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': 'Hi there! How can I help you today?'}], 'role': 'model'}, 'finishReason': 'STOP', 'index': 0}], 'usageMetadata': {'promptTokenCount': 302, 'candidatesTokenCount': 10, 'totalTokenCount': 332, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 302}], 'thoughtsTokenCount': 20}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'LLJ3aZ2IK9yBqfkPsv_g0A8'}
2026-01-26 23:57:57,099 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='LLJ3aZ2IK9yBqfkPsv_g0A8', created=1769452077, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='Hi there! How can I help you today?', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=30, prompt_tokens=302, total_tokens=332, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=20, rejected_prediction_tokens=None, text_tokens=10, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=302, image_tokens=None)))
2026-01-26 23:57:57,099 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content='Hi there! How can I help you today?', role='assistant', function_call=None, tool_calls=None, audio=None)
2026-01-26 23:57:57,099 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='LLJ3aZ2IK9yBqfkPsv_g0A8', created=1769452077, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Hi there! How can I help you today?', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=30, prompt_tokens=302, total_tokens=332, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=20, rejected_prediction_tokens=None, text_tokens=10, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=302, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-26 23:57:57,116 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=False
2026-01-26 23:57:57,116 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=False
2026-01-26 23:57:57,116 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-26 23:57:57,116 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-26 23:57:57,116 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-26 23:57:57,121 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-26 23:57:57,121 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:57,121 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:57,121 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:57,123 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:57,124 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:57:57,124 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:57:57,125 | DEBUG | LiteLLM | response_cost: 0.00016560000000000001
2026-01-26 23:57:57,125 | DEBUG | LiteLLM | response_cost: 0.00016560000000000001
2026-01-26 23:57:57,127 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-26 23:57:57,127 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.00016560000000000001
2026-01-26 23:57:57,127 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:57,127 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-26 23:57:57,127 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:57:57,127 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:57:57,127 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:57:57,132 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-26 23:57:57,132 | DEBUG | openai.agents | Resetting current trace
2026-01-26 23:57:57,137 | INFO | chat | ‚úÖ Stream complete | tokens=1 | time=5.12s
2026-01-26 23:57:59,830 | DEBUG | httpcore.connection | close.started
2026-01-26 23:57:59,831 | DEBUG | httpcore.connection | close.complete
2026-01-26 23:57:59,832 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-26 23:57:59,964 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000021203948E20>
2026-01-26 23:57:59,964 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x000002127D66B4C0> server_hostname='api.openai.com' timeout=5.0
2026-01-26 23:58:00,258 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000021203948B20>
2026-01-26 23:58:00,258 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-26 23:58:00,260 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-26 23:58:00,260 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-26 23:58:00,262 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-26 23:58:00,262 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-26 23:58:01,558 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Mon, 26 Jan 2026 18:28:01 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_f678b95e861cd4459e6d3db11f3427df'), (b'openai-processing-ms', b'140'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'user-u2cbhcbswv5bt4bmhvzdmhwd'), (b'openai-project', b'proj_oIIqE3fW3A3IDZXBAVOtc4gK'), (b'x-envoy-upstream-service-time', b'143'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c42114d4c8a550d-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-26 23:58:01,559 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-26 23:58:01,559 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-26 23:58:01,560 | DEBUG | httpcore.http11 | response_closed.started
2026-01-26 23:58:01,560 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-26 23:58:01,561 | DEBUG | openai.agents | Exported 5 items
2026-01-26 23:58:35,246 | INFO | chat | üì© Chat request | user_id=6 | request_id=e1f7ba64-f33f-437b-a65b-36df530e32aa
2026-01-26 23:58:35,247 | INFO | chat | üßë User message | hi
2026-01-26 23:58:35,254 | INFO | chat | üÜï New session created | 7a24ad4f-c531-4c1c-a792-617a5e931cb3
2026-01-26 23:58:35,261 | INFO | chat | ü§ñ Stream started | session_id=7a24ad4f-c531-4c1c-a792-617a5e931cb3 | request_id=e1f7ba64-f33f-437b-a65b-36df530e32aa
2026-01-26 23:58:35,262 | INFO | chat | ü§ñ Running digital human
2026-01-26 23:58:35,263 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-26 23:58:35,263 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-26 23:58:35,264 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_f4baa7dc7acf4a6b85ab49fc7c8abb2f
2026-01-26 23:58:35,265 | DEBUG | openai.agents | Setting current trace: trace_f4baa7dc7acf4a6b85ab49fc7c8abb2f
2026-01-26 23:58:35,265 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x00000212040B4860> with id None
2026-01-26 23:58:35,265 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-26 23:58:35,265 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000021203A3E3E0> with id None
2026-01-26 23:58:35,265 | DEBUG | openai.agents | Calling LLM
2026-01-26 23:58:35,265 | DEBUG | LiteLLM | 

2026-01-26 23:58:35,265 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-26 23:58:35,265 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n', 'role': 'system'}, {'role': 'user', 'content': 'hi'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.5'}, api_key=None, base_url=None)[0m
2026-01-26 23:58:35,265 | DEBUG | LiteLLM | 

2026-01-26 23:58:35,265 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-26 23:58:35,265 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-26 23:58:35,265 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-26 23:58:35,271 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-26 23:58:35,273 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-26 23:58:35,273 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n', 'role': 'system'}, {'role': 'user', 'content': 'hi'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-26 23:58:35,273 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-26 23:58:35,273 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-26 23:58:35,273 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-26 23:58:35,273 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:58:35,273 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:58:35,273 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.5' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'hi'}]}], 'system_instruction': {'parts': [{'text': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n'}]}, 'generationConfig': {}}'
[0m

2026-01-26 23:58:37,485 | DEBUG | httpcore.connection | close.started
2026-01-26 23:58:37,486 | DEBUG | httpcore.connection | close.complete
2026-01-26 23:58:37,486 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-26 23:58:37,511 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000212040849D0>
2026-01-26 23:58:37,511 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x000002127D66B4C0> server_hostname='api.openai.com' timeout=5.0
2026-01-26 23:58:37,520 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000021204086A10>
2026-01-26 23:58:37,520 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-26 23:58:37,521 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-26 23:58:37,521 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-26 23:58:37,522 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-26 23:58:37,522 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-26 23:58:38,252 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "SAFE"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 188,
    "candidatesTokenCount": 1,
    "totalTokenCount": 230,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 188
      }
    ],
    "thoughtsTokenCount": 41
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "VrJ3aYToAqr1g8UP27Sw6Qw"
}



2026-01-26 23:58:38,254 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-26 23:58:38,254 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-26 23:58:38,254 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-26 23:58:38,256 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:58:38,256 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-26 23:58:38,256 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:58:38,256 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:58:38,256 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:58:38,256 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:58:38,258 | DEBUG | LiteLLM | response_cost: 0.00016140000000000002
2026-01-26 23:58:38,258 | DEBUG | openai.agents | Received model response
2026-01-26 23:58:38,258 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:58:38,258 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x0000021203A9F2B0>>
2026-01-26 23:58:38,258 | DEBUG | LiteLLM | response_cost: 0.00016140000000000002
2026-01-26 23:58:38,260 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-26 23:58:38,260 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-26 23:58:38,260 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-26 23:58:38,260 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-26 23:58:38,261 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:58:38,261 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-26 23:58:38,261 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-26 23:58:38,263 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:58:38,264 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-26 23:58:38,264 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:58:38,265 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:58:38,267 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:58:38,265 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:58:38,264 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-26 23:58:38,268 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-26 23:58:38,268 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-26 23:58:38,268 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-26 23:58:38,268 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.00016140000000000002
2026-01-26 23:58:38,270 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-26 23:58:38,270 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:58:38,272 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-26 23:58:38,272 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:58:38,272 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:58:38,273 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:58:38,274 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:58:38,274 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:58:38,275 | DEBUG | openai.agents | Resetting current trace
2026-01-26 23:58:38,275 | INFO | orchestrator | ‚úÖ Safety passed
2026-01-26 23:58:38,276 | INFO | orchestrator | üß≠ Router agent called
2026-01-26 23:58:38,277 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_1966b7ffca1b4fe19d91d1b57ea3ed2a
2026-01-26 23:58:38,277 | DEBUG | openai.agents | Setting current trace: trace_1966b7ffca1b4fe19d91d1b57ea3ed2a
2026-01-26 23:58:38,277 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x00000212040B66B0> with id None
2026-01-26 23:58:38,277 | DEBUG | openai.agents | Running agent RouterAgent (turn 1)
2026-01-26 23:58:38,278 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x00000212040B9180> with id None
2026-01-26 23:58:38,278 | DEBUG | openai.agents | Calling LLM
2026-01-26 23:58:38,278 | DEBUG | LiteLLM | 

2026-01-26 23:58:38,278 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-26 23:58:38,279 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'hi'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.5'}, api_key=None, base_url=None)[0m
2026-01-26 23:58:38,279 | DEBUG | LiteLLM | 

2026-01-26 23:58:38,280 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-26 23:58:38,280 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-26 23:58:38,280 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-26 23:58:38,282 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-26 23:58:38,283 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-26 23:58:38,284 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'hi'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-26 23:58:38,284 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-26 23:58:38,284 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-26 23:58:38,284 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-26 23:58:38,284 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:58:38,284 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:58:38,284 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.5' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'hi'}]}], 'system_instruction': {'parts': [{'text': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n'}]}, 'generationConfig': {}}'
[0m

2026-01-26 23:58:38,532 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Mon, 26 Jan 2026 18:28:38 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_05363f350d2381b893fdaa842b36c48f'), (b'openai-processing-ms', b'129'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'user-u2cbhcbswv5bt4bmhvzdmhwd'), (b'openai-project', b'proj_oIIqE3fW3A3IDZXBAVOtc4gK'), (b'x-envoy-upstream-service-time', b'132'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4212362f1254d1-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-26 23:58:38,532 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-26 23:58:38,532 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-26 23:58:38,532 | DEBUG | httpcore.http11 | response_closed.started
2026-01-26 23:58:38,532 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-26 23:58:38,532 | DEBUG | openai.agents | Exported 1 items
2026-01-26 23:58:38,532 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-26 23:58:38,532 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-26 23:58:38,532 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-26 23:58:38,532 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-26 23:58:38,532 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-26 23:58:38,949 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Mon, 26 Jan 2026 18:28:38 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_0ab8f7d2d819dbc06a6838138fb3c0ca'), (b'openai-processing-ms', b'103'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'user-u2cbhcbswv5bt4bmhvzdmhwd'), (b'openai-project', b'proj_oIIqE3fW3A3IDZXBAVOtc4gK'), (b'x-envoy-upstream-service-time', b'106'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c42123c8b3a54d1-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-26 23:58:38,949 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-26 23:58:38,949 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-26 23:58:38,963 | DEBUG | httpcore.http11 | response_closed.started
2026-01-26 23:58:38,963 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-26 23:58:38,964 | DEBUG | openai.agents | Exported 3 items
2026-01-26 23:58:41,154 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\"use_memory\": false, \"use_tool\": false, \"tool_name\": \"none\", \"tool_arguments\": null, \"intent\": \"none\", \"memory_key\": null}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 486,
    "candidatesTokenCount": 42,
    "totalTokenCount": 584,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 486
      }
    ],
    "thoughtsTokenCount": 56
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "WLJ3afvAOrL_g8UP7uG-oAk"
}



2026-01-26 23:58:41,156 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-26 23:58:41,158 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-26 23:58:41,158 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-26 23:58:41,159 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:58:41,160 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-26 23:58:41,161 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:58:41,162 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:58:41,162 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:58:41,163 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:58:41,163 | DEBUG | LiteLLM | response_cost: 0.00039079999999999996
2026-01-26 23:58:41,165 | DEBUG | openai.agents | Received model response
2026-01-26 23:58:41,164 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:58:41,165 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x00000212040C0070>>
2026-01-26 23:58:41,166 | DEBUG | LiteLLM | response_cost: 0.00039079999999999996
2026-01-26 23:58:41,166 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-26 23:58:41,167 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-26 23:58:41,167 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-26 23:58:41,167 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-26 23:58:41,167 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:58:41,167 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-26 23:58:41,167 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-26 23:58:41,167 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:58:41,167 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:58:41,167 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-26 23:58:41,167 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-26 23:58:41,167 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:58:41,167 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:58:41,167 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-26 23:58:41,167 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:58:41,167 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:58:41,167 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-26 23:58:41,167 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:58:41,167 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.00039079999999999996
2026-01-26 23:58:41,167 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-26 23:58:41,167 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:58:41,167 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:58:41,167 | DEBUG | openai.agents | Resetting current trace
2026-01-26 23:58:41,167 | INFO | orchestrator | üß≠ Router | tool=False memory=False intent=none
2026-01-26 23:58:41,167 | INFO | orchestrator | üß† Reasoning agent called
2026-01-26 23:58:41,167 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_cbfcdce071254750bf3bee48f791eedc
2026-01-26 23:58:41,181 | DEBUG | openai.agents | Setting current trace: trace_cbfcdce071254750bf3bee48f791eedc
2026-01-26 23:58:41,181 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x00000212040B76A0> with id None
2026-01-26 23:58:41,182 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x00000212040B9AE0> with id None
2026-01-26 23:58:41,183 | DEBUG | openai.agents | Calling LLM
2026-01-26 23:58:41,183 | DEBUG | LiteLLM | 

2026-01-26 23:58:41,183 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-26 23:58:41,183 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.\n\nIf the user mentions vague locations like:\n- nearest city\n- near me\n- nearby\n- around here\n\nand user location is provided,\nrewrite the query using the nearest major city.\n\nOutput ONLY the rewritten query.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n\\nYou may answer ANY general knowledge question.\\n\\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n\\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n\\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "hi"}, {"role": "user", "content": "hi"}], "memory_action": {}, "memory_data": [], "tool_context": {}}'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=True, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.5'}, api_key=None, base_url=None)[0m
2026-01-26 23:58:41,183 | DEBUG | LiteLLM | 

2026-01-26 23:58:41,183 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-26 23:58:41,183 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-26 23:58:41,183 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-26 23:58:41,183 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-26 23:58:41,183 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-26 23:58:41,183 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.\n\nIf the user mentions vague locations like:\n- nearest city\n- near me\n- nearby\n- around here\n\nand user location is provided,\nrewrite the query using the nearest major city.\n\nOutput ONLY the rewritten query.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n\\nYou may answer ANY general knowledge question.\\n\\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n\\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n\\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "hi"}, {"role": "user", "content": "hi"}], "memory_action": {}, "memory_data": [], "tool_context": {}}'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-26 23:58:41,183 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': True}
2026-01-26 23:58:41,183 | DEBUG | LiteLLM | Final returned optional params: {'stream': True}
2026-01-26 23:58:41,183 | DEBUG | LiteLLM | self.optional_params: {'stream': True}
2026-01-26 23:58:41,183 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:58:41,183 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:58:41,183 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?key=*****=sse \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.5' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n\\nYou may answer ANY general knowledge question.\\n\\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n\\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n\\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "hi"}, {"role": "user", "content": "hi"}], "memory_action": {}, "memory_data": [], "tool_context": {}}'}]}], 'system_instruction': {'parts': [{'text': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.\n\nIf the user mentions vague locations like:\n- nearest city\n- near me\n- nearby\n- around here\n\nand user location is provided,\nrewrite the query using the nearest major city.\n\nOutput ONLY the rewritten query.'}]}, 'generationConfig': {}}'
[0m

2026-01-26 23:58:41,183 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-26 23:58:41,183 | DEBUG | LiteLLM | messages in token_counter: None, text in token_counter: 
2026-01-26 23:58:41,183 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:58:41,183 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:58:41,196 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:58:41,196 | DEBUG | LiteLLM | response_cost: 0.0
2026-01-26 23:58:42,311 | DEBUG | LiteLLM | RAW RESPONSE:
first stream response received


2026-01-26 23:58:42,312 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': 'Hi there! How can I help you today?'}], 'role': 'model'}, 'finishReason': 'STOP', 'index': 0}], 'usageMetadata': {'promptTokenCount': 302, 'candidatesTokenCount': 10, 'totalTokenCount': 345, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 302}], 'thoughtsTokenCount': 33}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'WbJ3afeNMZnXjuMP_IaI0Qo'}
2026-01-26 23:58:42,313 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='WbJ3afeNMZnXjuMP_IaI0Qo', created=1769452122, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='Hi there! How can I help you today?', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=43, prompt_tokens=302, total_tokens=345, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=33, rejected_prediction_tokens=None, text_tokens=10, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=302, image_tokens=None)))
2026-01-26 23:58:42,314 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content='Hi there! How can I help you today?', role='assistant', function_call=None, tool_calls=None, audio=None)
2026-01-26 23:58:42,314 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='WbJ3afeNMZnXjuMP_IaI0Qo', created=1769452122, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Hi there! How can I help you today?', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=43, prompt_tokens=302, total_tokens=345, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=33, rejected_prediction_tokens=None, text_tokens=10, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=302, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-26 23:58:42,317 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=False
2026-01-26 23:58:42,317 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=False
2026-01-26 23:58:42,317 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-26 23:58:42,318 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-26 23:58:42,318 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-26 23:58:42,318 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-26 23:58:42,319 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:58:42,319 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:58:42,320 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:58:42,320 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:58:42,321 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:58:42,321 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:58:42,321 | DEBUG | LiteLLM | response_cost: 0.0001981
2026-01-26 23:58:42,322 | DEBUG | LiteLLM | response_cost: 0.0001981
2026-01-26 23:58:42,323 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-26 23:58:42,323 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0001981
2026-01-26 23:58:42,324 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:58:42,324 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-26 23:58:42,325 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:58:42,325 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-26 23:58:42,327 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-26 23:58:42,328 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-26 23:58:42,328 | DEBUG | openai.agents | Resetting current trace
2026-01-26 23:58:42,332 | INFO | chat | ‚úÖ Stream complete | tokens=1 | time=7.09s
2026-01-26 23:58:44,159 | DEBUG | httpcore.connection | close.started
2026-01-26 23:58:44,159 | DEBUG | httpcore.connection | close.complete
2026-01-26 23:58:44,159 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-26 23:58:44,169 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000021204843E20>
2026-01-26 23:58:44,169 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x000002127D66B4C0> server_hostname='api.openai.com' timeout=5.0
2026-01-26 23:58:44,184 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000021203AB4C10>
2026-01-26 23:58:44,184 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-26 23:58:44,186 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-26 23:58:44,187 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-26 23:58:44,188 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-26 23:58:44,188 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-26 23:58:44,830 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Mon, 26 Jan 2026 18:28:44 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_b769667134ac565a6294faaf4d287caf'), (b'openai-processing-ms', b'136'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'user-u2cbhcbswv5bt4bmhvzdmhwd'), (b'openai-project', b'proj_oIIqE3fW3A3IDZXBAVOtc4gK'), (b'x-envoy-upstream-service-time', b'140'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c42125fc843197f-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-26 23:58:44,830 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-26 23:58:44,830 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-26 23:58:44,846 | DEBUG | httpcore.http11 | response_closed.started
2026-01-26 23:58:44,846 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-26 23:58:44,846 | DEBUG | openai.agents | Exported 5 items
2026-01-27 00:04:28,240 | DEBUG | LiteLLM | LoggingWorker cancelled during shutdown
2026-01-27 00:04:28,242 | DEBUG | LiteLLM | [LoggingWorker] atexit: Queue is empty
2026-01-27 00:04:28,243 | DEBUG | asyncio | Using proactor: IocpProactor
2026-01-27 00:04:28,244 | DEBUG | openai.agents | Shutting down trace provider
2026-01-27 00:04:28,244 | DEBUG | openai.agents | Shutting down trace processor <agents.tracing.processors.BatchTraceProcessor object at 0x000002127D698790>
2026-01-27 00:04:28,558 | DEBUG | httpcore.connection | close.started
2026-01-27 00:04:28,558 | DEBUG | httpcore.connection | close.complete
