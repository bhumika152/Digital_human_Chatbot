2026-01-27 00:04:35,897 | DEBUG | httpcore.connection | connect_tcp.started host='raw.githubusercontent.com' port=443 local_address=None timeout=5 socket_options=None
2026-01-27 00:04:35,959 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000277E23628C0>
2026-01-27 00:04:35,959 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x00000277E109E7C0> server_hostname='raw.githubusercontent.com' timeout=5
2026-01-27 00:04:35,993 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000277E2362890>
2026-01-27 00:04:35,993 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'GET']>
2026-01-27 00:04:35,993 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-27 00:04:35,993 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'GET']>
2026-01-27 00:04:36,005 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-27 00:04:36,005 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'GET']>
2026-01-27 00:04:36,036 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Connection', b'keep-alive'), (b'Content-Length', b'62322'), (b'Cache-Control', b'max-age=300'), (b'Content-Security-Policy', b"default-src 'none'; style-src 'unsafe-inline'; sandbox"), (b'Content-Type', b'text/plain; charset=utf-8'), (b'ETag', b'W/"5f0be0ed2fe5ecf561c4b4488d64c304c35b980a45012ef1d1253b4b5d438af1"'), (b'Strict-Transport-Security', b'max-age=31536000'), (b'X-Content-Type-Options', b'nosniff'), (b'X-Frame-Options', b'deny'), (b'X-XSS-Protection', b'1; mode=block'), (b'X-GitHub-Request-Id', b'415E:3865D1:30D7E6:7A896E:69753835'), (b'Content-Encoding', b'gzip'), (b'Accept-Ranges', b'bytes'), (b'Date', b'Mon, 26 Jan 2026 18:34:35 GMT'), (b'Via', b'1.1 varnish'), (b'X-Served-By', b'cache-bom-vanm7210025-BOM'), (b'X-Cache', b'HIT'), (b'X-Cache-Hits', b'4'), (b'X-Timer', b'S1769452476.979280,VS0,VE0'), (b'Vary', b'Authorization,Accept-Encoding'), (b'Access-Control-Allow-Origin', b'*'), (b'Cross-Origin-Resource-Policy', b'cross-origin'), (b'X-Fastly-Request-ID', b'99257041517fa5b229f7792ad8d3ac4f8f167b36'), (b'Expires', b'Mon, 26 Jan 2026 18:39:35 GMT'), (b'Source-Age', b'105')])
2026-01-27 00:04:36,038 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'GET']>
2026-01-27 00:04:36,075 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-27 00:04:36,076 | DEBUG | httpcore.http11 | response_closed.started
2026-01-27 00:04:36,076 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-27 00:04:36,077 | DEBUG | httpcore.connection | close.started
2026-01-27 00:04:36,078 | DEBUG | httpcore.connection | close.complete
2026-01-27 00:04:37,270 | DEBUG | LiteLLM | [Non-Blocking] Unable to import GenericAPILogger - LiteLLM Enterprise Feature - No module named 'litellm_enterprise'
2026-01-27 00:04:37,864 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 00:04:37,864 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 00:04:37,864 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 00:04:37,864 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 00:04:37,864 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 00:04:37,864 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 00:04:37,864 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 00:04:37,864 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 00:04:37,864 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 00:04:37,878 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 00:04:37,878 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 00:04:37,878 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 00:04:37,989 | INFO | main | üöÄ FastAPI application starting
2026-01-27 00:04:38,068 | INFO | main | üóÑÔ∏è Database tables ensured
2026-01-27 00:05:14,789 | DEBUG | asyncio | Using proactor: IocpProactor
2026-01-27 00:05:14,791 | DEBUG | openai.agents | Shutting down trace provider
2026-01-27 00:05:14,791 | DEBUG | openai.agents | Shutting down trace processor <agents.tracing.processors.BatchTraceProcessor object at 0x00000277DFF48880>
2026-01-27 00:05:18,872 | DEBUG | httpcore.connection | connect_tcp.started host='raw.githubusercontent.com' port=443 local_address=None timeout=5 socket_options=None
2026-01-27 00:05:18,936 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000208581D28C0>
2026-01-27 00:05:18,937 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000020856F0E7C0> server_hostname='raw.githubusercontent.com' timeout=5
2026-01-27 00:05:18,972 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000208581D2890>
2026-01-27 00:05:18,972 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'GET']>
2026-01-27 00:05:18,972 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-27 00:05:18,972 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'GET']>
2026-01-27 00:05:18,972 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-27 00:05:18,972 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'GET']>
2026-01-27 00:05:19,005 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Connection', b'keep-alive'), (b'Content-Length', b'62322'), (b'Cache-Control', b'max-age=300'), (b'Content-Security-Policy', b"default-src 'none'; style-src 'unsafe-inline'; sandbox"), (b'Content-Type', b'text/plain; charset=utf-8'), (b'ETag', b'W/"5f0be0ed2fe5ecf561c4b4488d64c304c35b980a45012ef1d1253b4b5d438af1"'), (b'Strict-Transport-Security', b'max-age=31536000'), (b'X-Content-Type-Options', b'nosniff'), (b'X-Frame-Options', b'deny'), (b'X-XSS-Protection', b'1; mode=block'), (b'X-GitHub-Request-Id', b'415E:3865D1:30D7E6:7A896E:69753835'), (b'Content-Encoding', b'gzip'), (b'Accept-Ranges', b'bytes'), (b'Date', b'Mon, 26 Jan 2026 18:35:18 GMT'), (b'Via', b'1.1 varnish'), (b'X-Served-By', b'cache-bom-vanm7210093-BOM'), (b'X-Cache', b'HIT'), (b'X-Cache-Hits', b'9'), (b'X-Timer', b'S1769452519.952992,VS0,VE0'), (b'Vary', b'Authorization,Accept-Encoding'), (b'Access-Control-Allow-Origin', b'*'), (b'Cross-Origin-Resource-Policy', b'cross-origin'), (b'X-Fastly-Request-ID', b'827c23bf3ad242e3250baef06d11f35612e5627d'), (b'Expires', b'Mon, 26 Jan 2026 18:40:18 GMT'), (b'Source-Age', b'148')])
2026-01-27 00:05:19,005 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'GET']>
2026-01-27 00:05:19,056 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-27 00:05:19,056 | DEBUG | httpcore.http11 | response_closed.started
2026-01-27 00:05:19,056 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-27 00:05:19,056 | DEBUG | httpcore.connection | close.started
2026-01-27 00:05:19,056 | DEBUG | httpcore.connection | close.complete
2026-01-27 00:05:20,065 | DEBUG | LiteLLM | [Non-Blocking] Unable to import GenericAPILogger - LiteLLM Enterprise Feature - No module named 'litellm_enterprise'
2026-01-27 00:05:20,680 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 00:05:20,680 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 00:05:20,680 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 00:05:20,695 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 00:05:20,695 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 00:05:20,695 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 00:05:20,695 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 00:05:20,695 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 00:05:20,695 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 00:05:20,695 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 00:05:20,695 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 00:05:20,695 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 00:05:20,805 | INFO | main | üöÄ FastAPI application starting
2026-01-27 00:05:20,857 | INFO | main | üóÑÔ∏è Database tables ensured
2026-01-27 00:05:20,872 | DEBUG | asyncio | Using proactor: IocpProactor
2026-01-27 00:05:20,872 | DEBUG | openai.agents | Shutting down trace provider
2026-01-27 00:05:20,872 | DEBUG | openai.agents | Shutting down trace processor <agents.tracing.processors.BatchTraceProcessor object at 0x0000020855DB8880>
2026-01-27 00:05:24,938 | DEBUG | httpcore.connection | connect_tcp.started host='raw.githubusercontent.com' port=443 local_address=None timeout=5 socket_options=None
2026-01-27 00:05:24,979 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000028209852950>
2026-01-27 00:05:24,981 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x000002820858E7C0> server_hostname='raw.githubusercontent.com' timeout=5
2026-01-27 00:05:25,007 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000028209852920>
2026-01-27 00:05:25,007 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'GET']>
2026-01-27 00:05:25,007 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-27 00:05:25,007 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'GET']>
2026-01-27 00:05:25,007 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-27 00:05:25,007 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'GET']>
2026-01-27 00:05:25,039 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Connection', b'keep-alive'), (b'Content-Length', b'62322'), (b'Cache-Control', b'max-age=300'), (b'Content-Security-Policy', b"default-src 'none'; style-src 'unsafe-inline'; sandbox"), (b'Content-Type', b'text/plain; charset=utf-8'), (b'ETag', b'W/"5f0be0ed2fe5ecf561c4b4488d64c304c35b980a45012ef1d1253b4b5d438af1"'), (b'Strict-Transport-Security', b'max-age=31536000'), (b'X-Content-Type-Options', b'nosniff'), (b'X-Frame-Options', b'deny'), (b'X-XSS-Protection', b'1; mode=block'), (b'X-GitHub-Request-Id', b'415E:3865D1:30D7E6:7A896E:69753835'), (b'Content-Encoding', b'gzip'), (b'Accept-Ranges', b'bytes'), (b'Date', b'Mon, 26 Jan 2026 18:35:24 GMT'), (b'Via', b'1.1 varnish'), (b'X-Served-By', b'cache-bom-vanm7210053-BOM'), (b'X-Cache', b'HIT'), (b'X-Cache-Hits', b'5'), (b'X-Timer', b'S1769452525.992827,VS0,VE0'), (b'Vary', b'Authorization,Accept-Encoding'), (b'Access-Control-Allow-Origin', b'*'), (b'Cross-Origin-Resource-Policy', b'cross-origin'), (b'X-Fastly-Request-ID', b'585d918675c76756c893edd0391ce7d0f7ebd2c2'), (b'Expires', b'Mon, 26 Jan 2026 18:40:24 GMT'), (b'Source-Age', b'154')])
2026-01-27 00:05:25,039 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'GET']>
2026-01-27 00:05:25,086 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-27 00:05:25,087 | DEBUG | httpcore.http11 | response_closed.started
2026-01-27 00:05:25,087 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-27 00:05:25,087 | DEBUG | httpcore.connection | close.started
2026-01-27 00:05:25,087 | DEBUG | httpcore.connection | close.complete
2026-01-27 00:05:26,354 | DEBUG | LiteLLM | [Non-Blocking] Unable to import GenericAPILogger - LiteLLM Enterprise Feature - No module named 'litellm_enterprise'
2026-01-27 00:05:27,002 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 00:05:27,003 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 00:05:27,003 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 00:05:27,013 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 00:05:27,014 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 00:05:27,015 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 00:05:27,015 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 00:05:27,016 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 00:05:27,016 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 00:05:27,018 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 00:05:27,018 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 00:05:27,019 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 00:05:27,126 | INFO | main | üöÄ FastAPI application starting
2026-01-27 00:05:27,189 | INFO | main | üóÑÔ∏è Database tables ensured
2026-01-27 00:05:59,719 | DEBUG | asyncio | Using proactor: IocpProactor
2026-01-27 00:05:59,722 | DEBUG | openai.agents | Shutting down trace provider
2026-01-27 00:05:59,722 | DEBUG | openai.agents | Shutting down trace processor <agents.tracing.processors.BatchTraceProcessor object at 0x000002827F438880>
2026-01-27 00:06:02,928 | DEBUG | httpcore.connection | connect_tcp.started host='raw.githubusercontent.com' port=443 local_address=None timeout=5 socket_options=None
2026-01-27 00:06:02,972 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000231BAE528F0>
2026-01-27 00:06:02,972 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x00000231BAB5E7C0> server_hostname='raw.githubusercontent.com' timeout=5
2026-01-27 00:06:03,052 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000231BAE528C0>
2026-01-27 00:06:03,052 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'GET']>
2026-01-27 00:06:03,052 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-27 00:06:03,052 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'GET']>
2026-01-27 00:06:03,052 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-27 00:06:03,052 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'GET']>
2026-01-27 00:06:03,084 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Connection', b'keep-alive'), (b'Content-Length', b'62322'), (b'Cache-Control', b'max-age=300'), (b'Content-Security-Policy', b"default-src 'none'; style-src 'unsafe-inline'; sandbox"), (b'Content-Type', b'text/plain; charset=utf-8'), (b'ETag', b'W/"5f0be0ed2fe5ecf561c4b4488d64c304c35b980a45012ef1d1253b4b5d438af1"'), (b'Strict-Transport-Security', b'max-age=31536000'), (b'X-Content-Type-Options', b'nosniff'), (b'X-Frame-Options', b'deny'), (b'X-XSS-Protection', b'1; mode=block'), (b'X-GitHub-Request-Id', b'415E:3865D1:30D7E6:7A896E:69753835'), (b'Content-Encoding', b'gzip'), (b'Accept-Ranges', b'bytes'), (b'Date', b'Mon, 26 Jan 2026 18:36:03 GMT'), (b'Via', b'1.1 varnish'), (b'X-Served-By', b'cache-bom-vanm7210094-BOM'), (b'X-Cache', b'HIT'), (b'X-Cache-Hits', b'8'), (b'X-Timer', b'S1769452563.028637,VS0,VE0'), (b'Vary', b'Authorization,Accept-Encoding'), (b'Access-Control-Allow-Origin', b'*'), (b'Cross-Origin-Resource-Policy', b'cross-origin'), (b'X-Fastly-Request-ID', b'f5b6a304e06dd5ea9f92204a6ea9ef425e1e2cda'), (b'Expires', b'Mon, 26 Jan 2026 18:41:03 GMT'), (b'Source-Age', b'192')])
2026-01-27 00:06:03,084 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'GET']>
2026-01-27 00:06:03,125 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-27 00:06:03,125 | DEBUG | httpcore.http11 | response_closed.started
2026-01-27 00:06:03,125 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-27 00:06:03,125 | DEBUG | httpcore.connection | close.started
2026-01-27 00:06:03,125 | DEBUG | httpcore.connection | close.complete
2026-01-27 00:06:03,746 | DEBUG | LiteLLM | [Non-Blocking] Unable to import GenericAPILogger - LiteLLM Enterprise Feature - No module named 'litellm_enterprise'
2026-01-27 00:06:04,126 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 00:06:04,126 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 00:06:04,126 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 00:06:04,126 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 00:06:04,126 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 00:06:04,126 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 00:06:04,137 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 00:06:04,137 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 00:06:04,138 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 00:06:04,138 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 00:06:04,138 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 00:06:04,139 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 00:06:04,204 | INFO | main | üöÄ FastAPI application starting
2026-01-27 00:06:04,287 | INFO | main | üóÑÔ∏è Database tables ensured
2026-01-27 00:06:29,168 | INFO | chat | üì© Chat request | user_id=6 | request_id=f5790c83-81d7-4e16-b7bf-360db2e394b3
2026-01-27 00:06:29,169 | INFO | chat | üßë User message | hi
2026-01-27 00:06:29,185 | INFO | chat | üÜï New session created | 42356d82-d23e-4eab-8ef8-7d395e86e6fe
2026-01-27 00:06:29,187 | INFO | chat | ü§ñ Stream started | session_id=42356d82-d23e-4eab-8ef8-7d395e86e6fe | request_id=f5790c83-81d7-4e16-b7bf-360db2e394b3
2026-01-27 00:06:29,187 | INFO | chat | ü§ñ Running digital human
2026-01-27 00:06:29,187 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-27 00:06:29,187 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-27 00:06:29,199 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_cc4e48531e934011869b2ce92e580d56
2026-01-27 00:06:29,199 | DEBUG | openai.agents | Setting current trace: trace_cc4e48531e934011869b2ce92e580d56
2026-01-27 00:06:29,201 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x00000231BEBBF3D0> with id None
2026-01-27 00:06:29,201 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-27 00:06:29,210 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x00000231BEBC8A00> with id None
2026-01-27 00:06:29,210 | DEBUG | openai.agents | Calling LLM
2026-01-27 00:06:29,210 | DEBUG | LiteLLM | 

2026-01-27 00:06:29,210 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-27 00:06:29,210 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n', 'role': 'system'}, {'role': 'user', 'content': 'hi'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.5'}, api_key=None, base_url=None)[0m
2026-01-27 00:06:29,210 | DEBUG | LiteLLM | 

2026-01-27 00:06:29,210 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-27 00:06:29,210 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-27 00:06:29,210 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-27 00:06:29,237 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 00:06:29,237 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-27 00:06:29,242 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n', 'role': 'system'}, {'role': 'user', 'content': 'hi'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-27 00:06:29,242 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-27 00:06:29,245 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-27 00:06:29,245 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-27 00:06:29,245 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:06:29,440 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:06:29,442 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.5' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'hi'}]}], 'system_instruction': {'parts': [{'text': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n'}]}, 'generationConfig': {}}'
[0m

2026-01-27 00:06:32,000 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "SAFE"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 188,
    "candidatesTokenCount": 1,
    "totalTokenCount": 223,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 188
      }
    ],
    "thoughtsTokenCount": 34
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "L7R3aYf1K9Wq4-EP3e622Ak"
}



2026-01-27 00:06:32,005 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-27 00:06:32,005 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-27 00:06:32,005 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 00:06:32,005 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 00:06:32,012 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:06:32,012 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:06:32,012 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:06:32,012 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:06:32,015 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:06:32,015 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:06:32,015 | DEBUG | LiteLLM | response_cost: 0.0001439
2026-01-27 00:06:32,015 | DEBUG | openai.agents | Received model response
2026-01-27 00:06:32,015 | DEBUG | LiteLLM | response_cost: 0.0001439
2026-01-27 00:06:32,027 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x00000231BEC444C0>>
2026-01-27 00:06:32,027 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 00:06:32,035 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-27 00:06:32,036 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:06:32,036 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:06:32,038 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-27 00:06:32,036 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-27 00:06:32,038 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-27 00:06:32,039 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-27 00:06:32,040 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 00:06:32,040 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 00:06:32,040 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 00:06:32,040 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:06:32,040 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:06:32,040 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:06:32,040 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:06:32,040 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:06:32,040 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:06:32,040 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-27 00:06:32,040 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0001439
2026-01-27 00:06:32,040 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 00:06:32,040 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:06:32,040 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:06:32,040 | DEBUG | openai.agents | Resetting current trace
2026-01-27 00:06:32,040 | INFO | orchestrator | ‚úÖ Safety passed
2026-01-27 00:06:32,040 | INFO | orchestrator | üß≠ Router agent called
2026-01-27 00:06:32,040 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_ad20060ca5354bbcba10c943f9282f36
2026-01-27 00:06:32,040 | DEBUG | openai.agents | Setting current trace: trace_ad20060ca5354bbcba10c943f9282f36
2026-01-27 00:06:32,051 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x00000231BEB27BA0> with id None
2026-01-27 00:06:32,051 | DEBUG | openai.agents | Running agent RouterAgent (turn 1)
2026-01-27 00:06:32,052 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x00000231BF998940> with id None
2026-01-27 00:06:32,052 | DEBUG | openai.agents | Calling LLM
2026-01-27 00:06:32,052 | DEBUG | LiteLLM | 

2026-01-27 00:06:32,052 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-27 00:06:32,052 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'hi'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.5'}, api_key=None, base_url=None)[0m
2026-01-27 00:06:32,054 | DEBUG | LiteLLM | 

2026-01-27 00:06:32,054 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-27 00:06:32,055 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-27 00:06:32,055 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-27 00:06:32,056 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 00:06:32,057 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-27 00:06:32,058 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'hi'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-27 00:06:32,059 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-27 00:06:32,059 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-27 00:06:32,060 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-27 00:06:32,060 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:06:32,061 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:06:32,061 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.5' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'hi'}]}], 'system_instruction': {'parts': [{'text': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n'}]}, 'generationConfig': {}}'
[0m

2026-01-27 00:06:33,701 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\"use_memory\": false, \"use_tool\": false, \"tool_name\": \"none\", \"tool_arguments\": null, \"intent\": \"none\", \"memory_key\": null}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 486,
    "candidatesTokenCount": 42,
    "totalTokenCount": 583,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 486
      }
    ],
    "thoughtsTokenCount": 55
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "MbR3aZa2HYbtg8UPq7DcuA0"
}



2026-01-27 00:06:33,701 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-27 00:06:33,701 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 00:06:33,701 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-27 00:06:33,701 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:06:33,707 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 00:06:33,707 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:06:33,709 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:06:33,709 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:06:33,709 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:06:33,709 | DEBUG | LiteLLM | response_cost: 0.0003883
2026-01-27 00:06:33,709 | DEBUG | openai.agents | Received model response
2026-01-27 00:06:33,709 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:06:33,709 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x00000231B841AFE0>>
2026-01-27 00:06:33,709 | DEBUG | LiteLLM | response_cost: 0.0003883
2026-01-27 00:06:33,716 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-27 00:06:33,718 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 00:06:33,718 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-27 00:06:33,719 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-27 00:06:33,719 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:06:33,721 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 00:06:33,721 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-27 00:06:33,721 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:06:33,721 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:06:33,721 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 00:06:33,725 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-27 00:06:33,725 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:06:33,725 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:06:33,725 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 00:06:33,725 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:06:33,725 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:06:33,725 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-27 00:06:33,725 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:06:33,732 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0003883
2026-01-27 00:06:33,732 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 00:06:33,732 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:06:33,735 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:06:33,736 | DEBUG | openai.agents | Resetting current trace
2026-01-27 00:06:33,737 | INFO | orchestrator | üß≠ Router | tool=False memory=False intent=none
2026-01-27 00:06:33,737 | INFO | orchestrator | üß† Reasoning agent called
2026-01-27 00:06:33,738 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_0dfac7c378a1432c96029aa586adde35
2026-01-27 00:06:33,738 | DEBUG | openai.agents | Setting current trace: trace_0dfac7c378a1432c96029aa586adde35
2026-01-27 00:06:33,738 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x00000231BE165D00> with id None
2026-01-27 00:06:33,738 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x00000231BF9994E0> with id None
2026-01-27 00:06:33,738 | DEBUG | openai.agents | Calling LLM
2026-01-27 00:06:33,738 | DEBUG | LiteLLM | 

2026-01-27 00:06:33,738 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-27 00:06:33,738 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.\n\nIf the user mentions vague locations like:\n- nearest city\n- near me\n- nearby\n- around here\n\nand user location is provided,\nrewrite the query using the nearest major city.\n\nOutput ONLY the rewritten query.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n\\nYou may answer ANY general knowledge question.\\n\\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n\\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n\\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "hi"}, {"role": "user", "content": "hi"}], "memory_action": {}, "memory_data": [], "tool_context": {}}'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=True, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.5'}, api_key=None, base_url=None)[0m
2026-01-27 00:06:33,738 | DEBUG | LiteLLM | 

2026-01-27 00:06:33,738 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-27 00:06:33,738 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-27 00:06:33,738 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-27 00:06:33,738 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 00:06:33,738 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-27 00:06:33,738 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.\n\nIf the user mentions vague locations like:\n- nearest city\n- near me\n- nearby\n- around here\n\nand user location is provided,\nrewrite the query using the nearest major city.\n\nOutput ONLY the rewritten query.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n\\nYou may answer ANY general knowledge question.\\n\\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n\\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n\\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "hi"}, {"role": "user", "content": "hi"}], "memory_action": {}, "memory_data": [], "tool_context": {}}'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-27 00:06:33,738 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': True}
2026-01-27 00:06:33,738 | DEBUG | LiteLLM | Final returned optional params: {'stream': True}
2026-01-27 00:06:33,738 | DEBUG | LiteLLM | self.optional_params: {'stream': True}
2026-01-27 00:06:33,738 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:06:33,751 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:06:33,751 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?key=*****=sse \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.5' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n\\nYou may answer ANY general knowledge question.\\n\\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n\\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n\\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "hi"}, {"role": "user", "content": "hi"}], "memory_action": {}, "memory_data": [], "tool_context": {}}'}]}], 'system_instruction': {'parts': [{'text': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.\n\nIf the user mentions vague locations like:\n- nearest city\n- near me\n- nearby\n- around here\n\nand user location is provided,\nrewrite the query using the nearest major city.\n\nOutput ONLY the rewritten query.'}]}, 'generationConfig': {}}'
[0m

2026-01-27 00:06:33,752 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 00:06:33,754 | DEBUG | LiteLLM | messages in token_counter: None, text in token_counter: 
2026-01-27 00:06:33,756 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:06:33,756 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:06:33,756 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:06:33,756 | DEBUG | LiteLLM | response_cost: 0.0
2026-01-27 00:06:33,916 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 00:06:33,916 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 00:06:33,916 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 00:06:34,315 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-27 00:06:34,366 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000231BF364DF0>
2026-01-27 00:06:34,366 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x00000231B8A0B4C0> server_hostname='api.openai.com' timeout=5.0
2026-01-27 00:06:34,379 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000231BF364FD0>
2026-01-27 00:06:34,395 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-27 00:06:34,395 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-27 00:06:34,395 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-27 00:06:34,395 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-27 00:06:34,395 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-27 00:06:35,412 | DEBUG | LiteLLM | RAW RESPONSE:
first stream response received


2026-01-27 00:06:35,412 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': 'Hi there! How can I help you today?'}], 'role': 'model'}, 'finishReason': 'STOP', 'index': 0}], 'usageMetadata': {'promptTokenCount': 302, 'candidatesTokenCount': 10, 'totalTokenCount': 342, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 302}], 'thoughtsTokenCount': 30}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'MrR3aZvsOv2X4-EP54eI0Ak'}
2026-01-27 00:06:35,437 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='MrR3aZvsOv2X4-EP54eI0Ak', created=1769452595, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='Hi there! How can I help you today?', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=40, prompt_tokens=302, total_tokens=342, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=30, rejected_prediction_tokens=None, text_tokens=10, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=302, image_tokens=None)))
2026-01-27 00:06:35,441 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content='Hi there! How can I help you today?', role='assistant', function_call=None, tool_calls=None, audio=None)
2026-01-27 00:06:35,443 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='MrR3aZvsOv2X4-EP54eI0Ak', created=1769452595, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Hi there! How can I help you today?', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=40, prompt_tokens=302, total_tokens=342, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=30, rejected_prediction_tokens=None, text_tokens=10, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=302, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-27 00:06:35,503 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=False
2026-01-27 00:06:35,524 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=False
2026-01-27 00:06:35,524 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-27 00:06:35,524 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-27 00:06:35,524 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 00:06:35,524 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 00:06:35,524 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:06:35,524 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:06:35,524 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:06:35,524 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:06:35,524 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:06:35,535 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:06:35,536 | DEBUG | LiteLLM | response_cost: 0.0001906
2026-01-27 00:06:35,536 | DEBUG | LiteLLM | response_cost: 0.0001906
2026-01-27 00:06:35,536 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 00:06:35,536 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0001906
2026-01-27 00:06:35,536 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:06:35,536 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 00:06:35,536 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:06:35,536 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:06:35,536 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:06:35,536 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-27 00:06:35,536 | DEBUG | openai.agents | Resetting current trace
2026-01-27 00:06:35,555 | INFO | chat | ‚úÖ Stream complete | tokens=1 | time=6.39s
2026-01-27 00:06:36,103 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Mon, 26 Jan 2026 18:36:36 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_eee6e8f43bf3234b8a2e4effc19d63cd'), (b'openai-processing-ms', b'534'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'user-u2cbhcbswv5bt4bmhvzdmhwd'), (b'openai-project', b'proj_oIIqE3fW3A3IDZXBAVOtc4gK'), (b'x-envoy-upstream-service-time', b'537'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=nbUyWJ4rHfxYWwlxxfGC5ym40iCC5wwXjxRxenBIVFI-1769452596-1.0.1.1-kFxj1Zq_r8j4Ie5SZTr1Xeq9kkARm99ZS7yvEBpXmIvNw5uIF3ZT109Sv.14uKZnKzEg_ia1WwyqR75619P3AkBBw.YtebpLlTnKIVUDL4Y; path=/; expires=Mon, 26-Jan-26 19:06:36 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=Bj5Jiv0APW3SFKM2pYG77EybIaQpKuUrdWd3hj0JmxI-1769452596029-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c421dda9ecc7819-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-27 00:06:36,103 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-27 00:06:36,103 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-27 00:06:36,103 | DEBUG | httpcore.http11 | response_closed.started
2026-01-27 00:06:36,103 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-27 00:06:36,103 | DEBUG | openai.agents | Exported 7 items
2026-01-27 00:06:36,103 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-27 00:06:36,103 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-27 00:06:36,103 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-27 00:06:36,103 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-27 00:06:36,103 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-27 00:06:37,228 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Mon, 26 Jan 2026 18:36:37 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_9365b946e8a2a14833e661fc7252e83e'), (b'openai-processing-ms', b'218'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'user-u2cbhcbswv5bt4bmhvzdmhwd'), (b'openai-project', b'proj_oIIqE3fW3A3IDZXBAVOtc4gK'), (b'x-envoy-upstream-service-time', b'220'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c421de559787819-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-27 00:06:37,228 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-27 00:06:37,228 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-27 00:06:37,228 | DEBUG | httpcore.http11 | response_closed.started
2026-01-27 00:06:37,228 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-27 00:06:37,228 | DEBUG | openai.agents | Exported 2 items
2026-01-27 00:09:30,263 | DEBUG | LiteLLM | LoggingWorker cancelled during shutdown
2026-01-27 00:09:30,267 | DEBUG | LiteLLM | [LoggingWorker] atexit: Queue is empty
2026-01-27 00:09:30,267 | DEBUG | asyncio | Using proactor: IocpProactor
2026-01-27 00:09:30,267 | DEBUG | openai.agents | Shutting down trace provider
2026-01-27 00:09:30,267 | DEBUG | openai.agents | Shutting down trace processor <agents.tracing.processors.BatchTraceProcessor object at 0x00000231B8A38790>
2026-01-27 00:09:30,414 | ERROR | asyncio | Fatal error on SSL transport
protocol: <asyncio.sslproto.SSLProtocol object at 0x00000231BAD14B50>
transport: <_SelectorSocketTransport closing fd=2220>
Traceback (most recent call last):
  File "C:\Users\dell\AppData\Local\Programs\Python\Python310\lib\asyncio\selector_events.py", line 924, in write
    n = self._sock.send(data)
OSError: [WinError 10038] An operation was attempted on something that is not a socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\dell\AppData\Local\Programs\Python\Python310\lib\asyncio\sslproto.py", line 690, in _process_write_backlog
    self._transport.write(chunk)
  File "C:\Users\dell\AppData\Local\Programs\Python\Python310\lib\asyncio\selector_events.py", line 930, in write
    self._fatal_error(exc, 'Fatal write error on socket transport')
  File "C:\Users\dell\AppData\Local\Programs\Python\Python310\lib\asyncio\selector_events.py", line 725, in _fatal_error
    self._force_close(exc)
  File "C:\Users\dell\AppData\Local\Programs\Python\Python310\lib\asyncio\selector_events.py", line 737, in _force_close
    self._loop.call_soon(self._call_connection_lost, exc)
  File "C:\Users\dell\AppData\Local\Programs\Python\Python310\lib\asyncio\base_events.py", line 753, in call_soon
    self._check_closed()
  File "C:\Users\dell\AppData\Local\Programs\Python\Python310\lib\asyncio\base_events.py", line 515, in _check_closed
    raise RuntimeError('Event loop is closed')
RuntimeError: Event loop is closed
2026-01-27 00:10:13,961 | DEBUG | httpcore.connection | connect_tcp.started host='raw.githubusercontent.com' port=443 local_address=None timeout=5 socket_options=None
2026-01-27 00:10:14,012 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000232015228C0>
2026-01-27 00:10:14,013 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x000002320025E7C0> server_hostname='raw.githubusercontent.com' timeout=5
2026-01-27 00:10:14,051 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023201522890>
2026-01-27 00:10:14,051 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'GET']>
2026-01-27 00:10:14,051 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-27 00:10:14,051 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'GET']>
2026-01-27 00:10:14,057 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-27 00:10:14,057 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'GET']>
2026-01-27 00:10:14,099 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Connection', b'keep-alive'), (b'Content-Length', b'62322'), (b'Cache-Control', b'max-age=300'), (b'Content-Security-Policy', b"default-src 'none'; style-src 'unsafe-inline'; sandbox"), (b'Content-Type', b'text/plain; charset=utf-8'), (b'ETag', b'W/"5f0be0ed2fe5ecf561c4b4488d64c304c35b980a45012ef1d1253b4b5d438af1"'), (b'Strict-Transport-Security', b'max-age=31536000'), (b'X-Content-Type-Options', b'nosniff'), (b'X-Frame-Options', b'deny'), (b'X-XSS-Protection', b'1; mode=block'), (b'X-GitHub-Request-Id', b'415E:3865D1:30D7E6:7A896E:69753835'), (b'Content-Encoding', b'gzip'), (b'Accept-Ranges', b'bytes'), (b'Date', b'Mon, 26 Jan 2026 18:40:14 GMT'), (b'Via', b'1.1 varnish'), (b'X-Served-By', b'cache-bom-vanm7210083-BOM'), (b'X-Cache', b'HIT'), (b'X-Cache-Hits', b'10'), (b'X-Timer', b'S1769452814.028552,VS0,VE0'), (b'Vary', b'Authorization,Accept-Encoding'), (b'Access-Control-Allow-Origin', b'*'), (b'Cross-Origin-Resource-Policy', b'cross-origin'), (b'X-Fastly-Request-ID', b'c0566cb81662eb6d37ae8c440ea617fb8782e6d7'), (b'Expires', b'Mon, 26 Jan 2026 18:45:14 GMT'), (b'Source-Age', b'143')])
2026-01-27 00:10:14,099 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'GET']>
2026-01-27 00:10:14,129 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-27 00:10:14,129 | DEBUG | httpcore.http11 | response_closed.started
2026-01-27 00:10:14,129 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-27 00:10:14,129 | DEBUG | httpcore.connection | close.started
2026-01-27 00:10:14,145 | DEBUG | httpcore.connection | close.complete
2026-01-27 00:10:15,136 | DEBUG | LiteLLM | [Non-Blocking] Unable to import GenericAPILogger - LiteLLM Enterprise Feature - No module named 'litellm_enterprise'
2026-01-27 00:10:15,673 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 00:10:15,673 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 00:10:15,673 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 00:10:15,682 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 00:10:15,682 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 00:10:15,682 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 00:10:15,684 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 00:10:15,684 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 00:10:15,685 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 00:10:15,685 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 00:10:15,686 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 00:10:15,686 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 00:10:15,762 | INFO | main | üöÄ FastAPI application starting
2026-01-27 00:10:15,836 | INFO | main | üóÑÔ∏è Database tables ensured
2026-01-27 00:10:16,494 | DEBUG | asyncio | Using proactor: IocpProactor
2026-01-27 00:10:16,494 | DEBUG | openai.agents | Shutting down trace provider
2026-01-27 00:10:16,494 | DEBUG | openai.agents | Shutting down trace processor <agents.tracing.processors.BatchTraceProcessor object at 0x000002327F038880>
2026-01-27 00:11:49,860 | DEBUG | httpcore.connection | connect_tcp.started host='raw.githubusercontent.com' port=443 local_address=None timeout=5 socket_options=None
2026-01-27 00:11:49,924 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001CB69972830>
2026-01-27 00:11:49,924 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x000001CB686AE740> server_hostname='raw.githubusercontent.com' timeout=5
2026-01-27 00:11:49,960 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001CB69972800>
2026-01-27 00:11:49,960 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'GET']>
2026-01-27 00:11:49,961 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-27 00:11:49,961 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'GET']>
2026-01-27 00:11:49,961 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-27 00:11:49,961 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'GET']>
2026-01-27 00:11:50,004 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Connection', b'keep-alive'), (b'Content-Length', b'62322'), (b'Cache-Control', b'max-age=300'), (b'Content-Security-Policy', b"default-src 'none'; style-src 'unsafe-inline'; sandbox"), (b'Content-Type', b'text/plain; charset=utf-8'), (b'ETag', b'W/"5f0be0ed2fe5ecf561c4b4488d64c304c35b980a45012ef1d1253b4b5d438af1"'), (b'Strict-Transport-Security', b'max-age=31536000'), (b'X-Content-Type-Options', b'nosniff'), (b'X-Frame-Options', b'deny'), (b'X-XSS-Protection', b'1; mode=block'), (b'X-GitHub-Request-Id', b'415E:3865D1:30D7E6:7A896E:69753835'), (b'Content-Encoding', b'gzip'), (b'Accept-Ranges', b'bytes'), (b'Date', b'Mon, 26 Jan 2026 18:41:49 GMT'), (b'Via', b'1.1 varnish'), (b'X-Served-By', b'cache-bom-vanm7210029-BOM'), (b'X-Cache', b'HIT'), (b'X-Cache-Hits', b'12'), (b'X-Timer', b'S1769452910.943829,VS0,VE0'), (b'Vary', b'Authorization,Accept-Encoding'), (b'Access-Control-Allow-Origin', b'*'), (b'Cross-Origin-Resource-Policy', b'cross-origin'), (b'X-Fastly-Request-ID', b'f8c2c0e02a6d036f5d9c1d9174af5a612aff921b'), (b'Expires', b'Mon, 26 Jan 2026 18:46:49 GMT'), (b'Source-Age', b'238')])
2026-01-27 00:11:50,005 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'GET']>
2026-01-27 00:11:50,041 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-27 00:11:50,041 | DEBUG | httpcore.http11 | response_closed.started
2026-01-27 00:11:50,041 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-27 00:11:50,041 | DEBUG | httpcore.connection | close.started
2026-01-27 00:11:50,041 | DEBUG | httpcore.connection | close.complete
2026-01-27 00:11:51,365 | DEBUG | LiteLLM | [Non-Blocking] Unable to import GenericAPILogger - LiteLLM Enterprise Feature - No module named 'litellm_enterprise'
2026-01-27 00:11:52,103 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 00:11:52,104 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 00:11:52,104 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 00:11:52,114 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 00:11:52,114 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 00:11:52,115 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 00:11:52,116 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 00:11:52,116 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 00:11:52,116 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 00:11:52,117 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 00:11:52,118 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 00:11:52,118 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 00:11:52,193 | INFO | main | üöÄ FastAPI application starting
2026-01-27 00:11:52,245 | INFO | main | üóÑÔ∏è Database tables ensured
2026-01-27 00:12:06,724 | DEBUG | asyncio | Using proactor: IocpProactor
2026-01-27 00:12:06,726 | DEBUG | openai.agents | Shutting down trace provider
2026-01-27 00:12:06,726 | DEBUG | openai.agents | Shutting down trace processor <agents.tracing.processors.BatchTraceProcessor object at 0x000001CB67558880>
2026-01-27 00:12:20,610 | DEBUG | httpcore.connection | connect_tcp.started host='raw.githubusercontent.com' port=443 local_address=None timeout=5 socket_options=None
2026-01-27 00:12:20,672 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001F001F22920>
2026-01-27 00:12:20,673 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x000001F000C5E740> server_hostname='raw.githubusercontent.com' timeout=5
2026-01-27 00:12:20,707 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001F001F228F0>
2026-01-27 00:12:20,707 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'GET']>
2026-01-27 00:12:20,707 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-27 00:12:20,707 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'GET']>
2026-01-27 00:12:20,707 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-27 00:12:20,707 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'GET']>
2026-01-27 00:12:20,787 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Connection', b'keep-alive'), (b'Content-Length', b'62322'), (b'Cache-Control', b'max-age=300'), (b'Content-Security-Policy', b"default-src 'none'; style-src 'unsafe-inline'; sandbox"), (b'Content-Type', b'text/plain; charset=utf-8'), (b'ETag', b'W/"5f0be0ed2fe5ecf561c4b4488d64c304c35b980a45012ef1d1253b4b5d438af1"'), (b'Strict-Transport-Security', b'max-age=31536000'), (b'X-Content-Type-Options', b'nosniff'), (b'X-Frame-Options', b'deny'), (b'X-XSS-Protection', b'1; mode=block'), (b'X-GitHub-Request-Id', b'415E:3865D1:30D7E6:7A896E:69753835'), (b'Content-Encoding', b'gzip'), (b'Accept-Ranges', b'bytes'), (b'Date', b'Mon, 26 Jan 2026 18:42:20 GMT'), (b'Via', b'1.1 varnish'), (b'X-Served-By', b'cache-bom-vanm7210094-BOM'), (b'X-Cache', b'HIT'), (b'X-Cache-Hits', b'10'), (b'X-Timer', b'S1769452941.688869,VS0,VE0'), (b'Vary', b'Authorization,Accept-Encoding'), (b'Access-Control-Allow-Origin', b'*'), (b'Cross-Origin-Resource-Policy', b'cross-origin'), (b'X-Fastly-Request-ID', b'2d76742d5e0fbe64a9fc7398147335d86c625edb'), (b'Expires', b'Mon, 26 Jan 2026 18:47:20 GMT'), (b'Source-Age', b'269')])
2026-01-27 00:12:20,787 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'GET']>
2026-01-27 00:12:20,836 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-27 00:12:20,836 | DEBUG | httpcore.http11 | response_closed.started
2026-01-27 00:12:20,836 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-27 00:12:20,837 | DEBUG | httpcore.connection | close.started
2026-01-27 00:12:20,837 | DEBUG | httpcore.connection | close.complete
2026-01-27 00:12:21,498 | DEBUG | LiteLLM | [Non-Blocking] Unable to import GenericAPILogger - LiteLLM Enterprise Feature - No module named 'litellm_enterprise'
2026-01-27 00:12:21,920 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 00:12:21,920 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 00:12:21,920 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 00:12:21,920 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 00:12:21,930 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 00:12:21,930 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 00:12:21,931 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 00:12:21,931 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 00:12:21,931 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 00:12:21,933 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 00:12:21,933 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 00:12:21,933 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 00:12:22,016 | INFO | main | üöÄ FastAPI application starting
2026-01-27 00:12:22,086 | INFO | main | üóÑÔ∏è Database tables ensured
2026-01-27 00:12:52,693 | INFO | chat | üì© Chat request | user_id=6 | request_id=decef134-b84b-4e56-adaa-78d61f68fd72
2026-01-27 00:12:52,697 | INFO | chat | üßë User message | hi
2026-01-27 00:12:52,706 | INFO | chat | üÜï New session created | 1892a976-bd53-4512-a7bc-d11eadaa106f
2026-01-27 00:12:52,722 | INFO | chat | ü§ñ Stream started | session_id=1892a976-bd53-4512-a7bc-d11eadaa106f | request_id=decef134-b84b-4e56-adaa-78d61f68fd72
2026-01-27 00:12:52,723 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-27 00:12:52,724 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-27 00:12:52,725 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_b6047d5b491f46ca95dbf3f3178fd255
2026-01-27 00:12:52,725 | DEBUG | openai.agents | Setting current trace: trace_b6047d5b491f46ca95dbf3f3178fd255
2026-01-27 00:12:52,725 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000001F005C7B010> with id None
2026-01-27 00:12:52,728 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-27 00:12:52,732 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000001F005A877C0> with id None
2026-01-27 00:12:52,732 | DEBUG | openai.agents | Calling LLM
2026-01-27 00:12:52,732 | DEBUG | LiteLLM | 

2026-01-27 00:12:52,734 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-27 00:12:52,734 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n', 'role': 'system'}, {'role': 'user', 'content': 'hi'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.5'}, api_key=None, base_url=None)[0m
2026-01-27 00:12:52,734 | DEBUG | LiteLLM | 

2026-01-27 00:12:52,734 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-27 00:12:52,736 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-27 00:12:52,736 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-27 00:12:52,743 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 00:12:52,743 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-27 00:12:52,743 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n', 'role': 'system'}, {'role': 'user', 'content': 'hi'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-27 00:12:52,743 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-27 00:12:52,757 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-27 00:12:52,759 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-27 00:12:52,759 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:12:53,020 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:12:53,020 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.5' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'hi'}]}], 'system_instruction': {'parts': [{'text': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n'}]}, 'generationConfig': {}}'
[0m

2026-01-27 00:12:54,335 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "SAFE"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 188,
    "candidatesTokenCount": 1,
    "totalTokenCount": 230,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 188
      }
    ],
    "thoughtsTokenCount": 41
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "rrV3acrhB5Wzg8UP7ZGZuQ0"
}



2026-01-27 00:12:54,343 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-27 00:12:54,343 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-27 00:12:54,343 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 00:12:54,346 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 00:12:54,346 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:12:54,347 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:12:54,347 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:12:54,349 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:12:54,349 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:12:54,349 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:12:54,349 | DEBUG | LiteLLM | response_cost: 0.00016140000000000002
2026-01-27 00:12:54,349 | DEBUG | LiteLLM | response_cost: 0.00016140000000000002
2026-01-27 00:12:54,349 | DEBUG | openai.agents | Received model response
2026-01-27 00:12:54,349 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 00:12:54,370 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001F005B62350>>
2026-01-27 00:12:54,370 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:12:54,372 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:12:54,372 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-27 00:12:54,374 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-27 00:12:54,375 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-27 00:12:54,376 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-27 00:12:54,376 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 00:12:54,376 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 00:12:54,376 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-27 00:12:54,376 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:12:54,376 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:12:54,376 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 00:12:54,376 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:12:54,376 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:12:54,376 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:12:54,376 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:12:54,376 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-27 00:12:54,376 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.00016140000000000002
2026-01-27 00:12:54,376 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 00:12:54,376 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:12:54,376 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:12:54,392 | DEBUG | openai.agents | Resetting current trace
2026-01-27 00:12:54,392 | INFO | orchestrator | ‚úÖ Safety passed
2026-01-27 00:12:54,393 | INFO | orchestrator | üß≠ Router agent called
2026-01-27 00:12:54,394 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_478788ad1714463795103eacd81e5f01
2026-01-27 00:12:54,394 | DEBUG | openai.agents | Setting current trace: trace_478788ad1714463795103eacd81e5f01
2026-01-27 00:12:54,394 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000001F005BDB2E0> with id None
2026-01-27 00:12:54,394 | DEBUG | openai.agents | Running agent RouterAgent (turn 1)
2026-01-27 00:12:54,394 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000001F005CB6CE0> with id None
2026-01-27 00:12:54,394 | DEBUG | openai.agents | Calling LLM
2026-01-27 00:12:54,394 | DEBUG | LiteLLM | 

2026-01-27 00:12:54,394 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-27 00:12:54,394 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'hi'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.5'}, api_key=None, base_url=None)[0m
2026-01-27 00:12:54,394 | DEBUG | LiteLLM | 

2026-01-27 00:12:54,394 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-27 00:12:54,394 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-27 00:12:54,394 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-27 00:12:54,394 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 00:12:54,394 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-27 00:12:54,394 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'hi'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-27 00:12:54,394 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-27 00:12:54,394 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-27 00:12:54,394 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-27 00:12:54,394 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:12:54,408 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:12:54,409 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.5' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'hi'}]}], 'system_instruction': {'parts': [{'text': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n'}]}, 'generationConfig': {}}'
[0m

2026-01-27 00:12:56,186 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\n  \"use_memory\": false,\n  \"use_tool\": false,\n  \"tool_name\": \"none\",\n  \"tool_arguments\": {},\n  \"intent\": \"none\",\n  \"memory_key\": null\n}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 486,
    "candidatesTokenCount": 55,
    "totalTokenCount": 621,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 486
      }
    ],
    "thoughtsTokenCount": 80
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "r7V3aev7O7nVqfkPtOfuqQE"
}



2026-01-27 00:12:56,186 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-27 00:12:56,202 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 00:12:56,202 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-27 00:12:56,202 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:12:56,202 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 00:12:56,202 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:12:56,202 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:12:56,202 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:12:56,202 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:12:56,202 | DEBUG | LiteLLM | response_cost: 0.0004833
2026-01-27 00:12:56,202 | DEBUG | openai.agents | Received model response
2026-01-27 00:12:56,202 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:12:56,202 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001F07F48B580>>
2026-01-27 00:12:56,209 | DEBUG | LiteLLM | response_cost: 0.0004833
2026-01-27 00:12:56,209 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-27 00:12:56,211 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 00:12:56,211 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-27 00:12:56,211 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-27 00:12:56,211 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:12:56,211 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-27 00:12:56,216 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 00:12:56,216 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:12:56,217 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 00:12:56,217 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:12:56,217 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-27 00:12:56,217 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:12:56,217 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:12:56,217 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 00:12:56,217 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:12:56,217 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:12:56,217 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-27 00:12:56,217 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:12:56,225 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0004833
2026-01-27 00:12:56,225 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 00:12:56,225 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:12:56,225 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:12:56,225 | DEBUG | openai.agents | Resetting current trace
2026-01-27 00:12:56,225 | INFO | orchestrator | üß≠ Router | tool=False memory=False intent=none
2026-01-27 00:12:56,225 | INFO | orchestrator | üß† Reasoning agent called
2026-01-27 00:12:56,225 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_c0a80633f8dd45669909dfe8e6e0113e
2026-01-27 00:12:56,225 | DEBUG | openai.agents | Setting current trace: trace_c0a80633f8dd45669909dfe8e6e0113e
2026-01-27 00:12:56,225 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000001F005ABFB50> with id None
2026-01-27 00:12:56,225 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000001F00666CFA0> with id None
2026-01-27 00:12:56,233 | DEBUG | openai.agents | Calling LLM
2026-01-27 00:12:56,233 | DEBUG | LiteLLM | 

2026-01-27 00:12:56,233 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-27 00:12:56,233 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.\n\nIf the user mentions vague locations like:\n- nearest city\n- near me\n- nearby\n- around here\n\nand user location is provided,\nrewrite the query using the nearest major city.\n\nOutput ONLY the rewritten query.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n\\nYou may answer ANY general knowledge question.\\n\\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n\\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n\\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "hi"}, {"role": "user", "content": "hi"}], "memory_action": {}, "memory_data": [], "tool_context": {}}'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=True, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.5'}, api_key=None, base_url=None)[0m
2026-01-27 00:12:56,233 | DEBUG | LiteLLM | 

2026-01-27 00:12:56,233 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-27 00:12:56,233 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-27 00:12:56,233 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-27 00:12:56,233 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 00:12:56,233 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-27 00:12:56,233 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.\n\nIf the user mentions vague locations like:\n- nearest city\n- near me\n- nearby\n- around here\n\nand user location is provided,\nrewrite the query using the nearest major city.\n\nOutput ONLY the rewritten query.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n\\nYou may answer ANY general knowledge question.\\n\\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n\\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n\\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "hi"}, {"role": "user", "content": "hi"}], "memory_action": {}, "memory_data": [], "tool_context": {}}'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-27 00:12:56,233 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': True}
2026-01-27 00:12:56,233 | DEBUG | LiteLLM | Final returned optional params: {'stream': True}
2026-01-27 00:12:56,233 | DEBUG | LiteLLM | self.optional_params: {'stream': True}
2026-01-27 00:12:56,233 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:12:56,233 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:12:56,233 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?key=*****=sse \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.5' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n\\nYou may answer ANY general knowledge question.\\n\\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n\\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n\\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "hi"}, {"role": "user", "content": "hi"}], "memory_action": {}, "memory_data": [], "tool_context": {}}'}]}], 'system_instruction': {'parts': [{'text': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.\n\nIf the user mentions vague locations like:\n- nearest city\n- near me\n- nearby\n- around here\n\nand user location is provided,\nrewrite the query using the nearest major city.\n\nOutput ONLY the rewritten query.'}]}, 'generationConfig': {}}'
[0m

2026-01-27 00:12:56,233 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 00:12:56,245 | DEBUG | LiteLLM | messages in token_counter: None, text in token_counter: 
2026-01-27 00:12:56,245 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:12:56,245 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:12:56,247 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:12:56,249 | DEBUG | LiteLLM | response_cost: 0.0
2026-01-27 00:12:56,462 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 00:12:56,462 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 00:12:56,464 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 00:12:57,857 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-27 00:12:57,939 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001F006415330>
2026-01-27 00:12:57,940 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x000001F07FA7B440> server_hostname='api.openai.com' timeout=5.0
2026-01-27 00:12:57,941 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001F006415300>
2026-01-27 00:12:57,941 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-27 00:12:57,941 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-27 00:12:57,941 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-27 00:12:57,941 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-27 00:12:57,941 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-27 00:12:58,825 | DEBUG | LiteLLM | RAW RESPONSE:
first stream response received


2026-01-27 00:12:58,825 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': 'Hi there! How can I help you today?'}], 'role': 'model'}, 'finishReason': 'STOP', 'index': 0}], 'usageMetadata': {'promptTokenCount': 302, 'candidatesTokenCount': 10, 'totalTokenCount': 332, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 302}], 'thoughtsTokenCount': 20}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'srV3adTiF7-L4-EP5ouuQQ'}
2026-01-27 00:12:58,825 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='srV3adTiF7-L4-EP5ouuQQ', created=1769452978, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='Hi there! How can I help you today?', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=30, prompt_tokens=302, total_tokens=332, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=20, rejected_prediction_tokens=None, text_tokens=10, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=302, image_tokens=None)))
2026-01-27 00:12:58,825 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content='Hi there! How can I help you today?', role='assistant', function_call=None, tool_calls=None, audio=None)
2026-01-27 00:12:58,825 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='srV3adTiF7-L4-EP5ouuQQ', created=1769452978, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Hi there! How can I help you today?', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=30, prompt_tokens=302, total_tokens=332, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=20, rejected_prediction_tokens=None, text_tokens=10, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=302, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-27 00:12:58,857 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=False
2026-01-27 00:12:58,864 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=False
2026-01-27 00:12:58,864 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-27 00:12:58,864 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-27 00:12:58,864 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 00:12:58,868 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 00:12:58,868 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:12:58,869 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:12:58,869 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:12:58,870 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:12:58,870 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:12:58,870 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:12:58,871 | DEBUG | LiteLLM | response_cost: 0.00016560000000000001
2026-01-27 00:12:58,871 | DEBUG | LiteLLM | response_cost: 0.00016560000000000001
2026-01-27 00:12:58,872 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 00:12:58,872 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.00016560000000000001
2026-01-27 00:12:58,872 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:12:58,873 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 00:12:58,874 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:12:58,874 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:12:58,877 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:12:58,877 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-27 00:12:58,877 | DEBUG | openai.agents | Resetting current trace
2026-01-27 00:12:58,881 | INFO | chat | ‚úÖ Stream complete | tokens=1 | time=6.19s
2026-01-27 00:12:59,292 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Mon, 26 Jan 2026 18:42:59 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_4e7df2253e2491b47e5171bc75138628'), (b'openai-processing-ms', b'429'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'user-u2cbhcbswv5bt4bmhvzdmhwd'), (b'openai-project', b'proj_oIIqE3fW3A3IDZXBAVOtc4gK'), (b'x-envoy-upstream-service-time', b'432'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=JUr_NQGI_b1ABciwpX6P1FEHr1J6uz6m8WINxyjpIQo-1769452979-1.0.1.1-_ff9QsRk_2PvikR.YWO53pptclrxQeq8_tbbF4KC.xz0OImheCDyysuzWox7R4vae_SefIDivX.4iw0GFzGFSmdBp5p0oDPLkjfIAtJnQqo; path=/; expires=Mon, 26-Jan-26 19:12:59 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=lRuREmYSvTpX0ybpVUwRkp1Mrg830jAJCKd2TX7C_e8-1769452979220-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c422737bba759c9-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-27 00:12:59,292 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-27 00:12:59,292 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-27 00:12:59,292 | DEBUG | httpcore.http11 | response_closed.started
2026-01-27 00:12:59,292 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-27 00:12:59,292 | DEBUG | openai.agents | Exported 7 items
2026-01-27 00:12:59,292 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-27 00:12:59,292 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-27 00:12:59,303 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-27 00:12:59,303 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-27 00:12:59,303 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-27 00:13:00,333 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Mon, 26 Jan 2026 18:43:00 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_674befcd1a8f9d4b968cae5bc8ddbae1'), (b'openai-processing-ms', b'132'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'user-u2cbhcbswv5bt4bmhvzdmhwd'), (b'openai-project', b'proj_oIIqE3fW3A3IDZXBAVOtc4gK'), (b'x-envoy-upstream-service-time', b'135'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4227403ff059c9-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-27 00:13:00,333 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-27 00:13:00,333 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-27 00:13:00,333 | DEBUG | httpcore.http11 | response_closed.started
2026-01-27 00:13:00,333 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-27 00:13:00,333 | DEBUG | openai.agents | Exported 2 items
2026-01-27 09:45:07,017 | DEBUG | LiteLLM | LoggingWorker cancelled during shutdown
2026-01-27 09:45:07,017 | DEBUG | LiteLLM | [LoggingWorker] atexit: Queue is empty
2026-01-27 09:45:07,017 | DEBUG | asyncio | Using proactor: IocpProactor
2026-01-27 09:45:07,017 | DEBUG | openai.agents | Shutting down trace provider
2026-01-27 09:45:07,017 | DEBUG | openai.agents | Shutting down trace processor <agents.tracing.processors.BatchTraceProcessor object at 0x000001F07FAA8790>
2026-01-27 09:50:47,760 | DEBUG | httpcore.connection | connect_tcp.started host='raw.githubusercontent.com' port=443 local_address=None timeout=5 socket_options=None
2026-01-27 09:50:47,834 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001AFAFEE28C0>
2026-01-27 09:50:47,834 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x000001AFAEC1E740> server_hostname='raw.githubusercontent.com' timeout=5
2026-01-27 09:50:47,863 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001AFAFEE2890>
2026-01-27 09:50:47,863 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'GET']>
2026-01-27 09:50:47,865 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-27 09:50:47,865 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'GET']>
2026-01-27 09:50:47,865 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-27 09:50:47,865 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'GET']>
2026-01-27 09:50:47,890 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Connection', b'keep-alive'), (b'Content-Length', b'62322'), (b'Cache-Control', b'max-age=300'), (b'Content-Security-Policy', b"default-src 'none'; style-src 'unsafe-inline'; sandbox"), (b'Content-Type', b'text/plain; charset=utf-8'), (b'ETag', b'W/"5f0be0ed2fe5ecf561c4b4488d64c304c35b980a45012ef1d1253b4b5d438af1"'), (b'Strict-Transport-Security', b'max-age=31536000'), (b'X-Content-Type-Options', b'nosniff'), (b'X-Frame-Options', b'deny'), (b'X-XSS-Protection', b'1; mode=block'), (b'X-GitHub-Request-Id', b'415E:3865D1:30D7E6:7A896E:69753835'), (b'Content-Encoding', b'gzip'), (b'Accept-Ranges', b'bytes'), (b'Date', b'Tue, 27 Jan 2026 04:20:48 GMT'), (b'Via', b'1.1 varnish'), (b'X-Served-By', b'cache-bom-vanm7210045-BOM'), (b'X-Cache', b'HIT'), (b'X-Cache-Hits', b'62073'), (b'X-Timer', b'S1769487648.129327,VS0,VE0'), (b'Vary', b'Authorization,Accept-Encoding'), (b'Access-Control-Allow-Origin', b'*'), (b'Cross-Origin-Resource-Policy', b'cross-origin'), (b'X-Fastly-Request-ID', b'abfd7d7ce8e633096c166a579956a4ef4b538a1d'), (b'Expires', b'Tue, 27 Jan 2026 04:25:48 GMT'), (b'Source-Age', b'100')])
2026-01-27 09:50:47,894 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'GET']>
2026-01-27 09:50:47,928 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-27 09:50:47,928 | DEBUG | httpcore.http11 | response_closed.started
2026-01-27 09:50:47,928 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-27 09:50:47,928 | DEBUG | httpcore.connection | close.started
2026-01-27 09:50:47,928 | DEBUG | httpcore.connection | close.complete
2026-01-27 09:50:48,641 | DEBUG | LiteLLM | [Non-Blocking] Unable to import GenericAPILogger - LiteLLM Enterprise Feature - No module named 'litellm_enterprise'
2026-01-27 09:50:49,187 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 09:50:49,188 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 09:50:49,188 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 09:50:49,197 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 09:50:49,197 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 09:50:49,197 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 09:50:49,198 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 09:50:49,198 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 09:50:49,198 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 09:50:49,199 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 09:50:49,199 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 09:50:49,199 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 09:50:49,321 | INFO | main | üöÄ FastAPI application starting
2026-01-27 09:50:49,370 | INFO | main | üóÑÔ∏è Database tables ensured
