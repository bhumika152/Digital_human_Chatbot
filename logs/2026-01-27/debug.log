2026-01-27 00:04:35,897 | DEBUG | httpcore.connection | connect_tcp.started host='raw.githubusercontent.com' port=443 local_address=None timeout=5 socket_options=None
2026-01-27 00:04:35,959 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000277E23628C0>
2026-01-27 00:04:35,959 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x00000277E109E7C0> server_hostname='raw.githubusercontent.com' timeout=5
2026-01-27 00:04:35,993 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000277E2362890>
2026-01-27 00:04:35,993 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'GET']>
2026-01-27 00:04:35,993 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-27 00:04:35,993 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'GET']>
2026-01-27 00:04:36,005 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-27 00:04:36,005 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'GET']>
2026-01-27 00:04:36,036 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Connection', b'keep-alive'), (b'Content-Length', b'62322'), (b'Cache-Control', b'max-age=300'), (b'Content-Security-Policy', b"default-src 'none'; style-src 'unsafe-inline'; sandbox"), (b'Content-Type', b'text/plain; charset=utf-8'), (b'ETag', b'W/"5f0be0ed2fe5ecf561c4b4488d64c304c35b980a45012ef1d1253b4b5d438af1"'), (b'Strict-Transport-Security', b'max-age=31536000'), (b'X-Content-Type-Options', b'nosniff'), (b'X-Frame-Options', b'deny'), (b'X-XSS-Protection', b'1; mode=block'), (b'X-GitHub-Request-Id', b'415E:3865D1:30D7E6:7A896E:69753835'), (b'Content-Encoding', b'gzip'), (b'Accept-Ranges', b'bytes'), (b'Date', b'Mon, 26 Jan 2026 18:34:35 GMT'), (b'Via', b'1.1 varnish'), (b'X-Served-By', b'cache-bom-vanm7210025-BOM'), (b'X-Cache', b'HIT'), (b'X-Cache-Hits', b'4'), (b'X-Timer', b'S1769452476.979280,VS0,VE0'), (b'Vary', b'Authorization,Accept-Encoding'), (b'Access-Control-Allow-Origin', b'*'), (b'Cross-Origin-Resource-Policy', b'cross-origin'), (b'X-Fastly-Request-ID', b'99257041517fa5b229f7792ad8d3ac4f8f167b36'), (b'Expires', b'Mon, 26 Jan 2026 18:39:35 GMT'), (b'Source-Age', b'105')])
2026-01-27 00:04:36,038 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'GET']>
2026-01-27 00:04:36,075 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-27 00:04:36,076 | DEBUG | httpcore.http11 | response_closed.started
2026-01-27 00:04:36,076 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-27 00:04:36,077 | DEBUG | httpcore.connection | close.started
2026-01-27 00:04:36,078 | DEBUG | httpcore.connection | close.complete
2026-01-27 00:04:37,270 | DEBUG | LiteLLM | [Non-Blocking] Unable to import GenericAPILogger - LiteLLM Enterprise Feature - No module named 'litellm_enterprise'
2026-01-27 00:04:37,864 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 00:04:37,864 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 00:04:37,864 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 00:04:37,864 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 00:04:37,864 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 00:04:37,864 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 00:04:37,864 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 00:04:37,864 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 00:04:37,864 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 00:04:37,878 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 00:04:37,878 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 00:04:37,878 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 00:04:37,989 | INFO | main | üöÄ FastAPI application starting
2026-01-27 00:04:38,068 | INFO | main | üóÑÔ∏è Database tables ensured
2026-01-27 00:05:14,789 | DEBUG | asyncio | Using proactor: IocpProactor
2026-01-27 00:05:14,791 | DEBUG | openai.agents | Shutting down trace provider
2026-01-27 00:05:14,791 | DEBUG | openai.agents | Shutting down trace processor <agents.tracing.processors.BatchTraceProcessor object at 0x00000277DFF48880>
2026-01-27 00:05:18,872 | DEBUG | httpcore.connection | connect_tcp.started host='raw.githubusercontent.com' port=443 local_address=None timeout=5 socket_options=None
2026-01-27 00:05:18,936 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000208581D28C0>
2026-01-27 00:05:18,937 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000020856F0E7C0> server_hostname='raw.githubusercontent.com' timeout=5
2026-01-27 00:05:18,972 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000208581D2890>
2026-01-27 00:05:18,972 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'GET']>
2026-01-27 00:05:18,972 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-27 00:05:18,972 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'GET']>
2026-01-27 00:05:18,972 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-27 00:05:18,972 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'GET']>
2026-01-27 00:05:19,005 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Connection', b'keep-alive'), (b'Content-Length', b'62322'), (b'Cache-Control', b'max-age=300'), (b'Content-Security-Policy', b"default-src 'none'; style-src 'unsafe-inline'; sandbox"), (b'Content-Type', b'text/plain; charset=utf-8'), (b'ETag', b'W/"5f0be0ed2fe5ecf561c4b4488d64c304c35b980a45012ef1d1253b4b5d438af1"'), (b'Strict-Transport-Security', b'max-age=31536000'), (b'X-Content-Type-Options', b'nosniff'), (b'X-Frame-Options', b'deny'), (b'X-XSS-Protection', b'1; mode=block'), (b'X-GitHub-Request-Id', b'415E:3865D1:30D7E6:7A896E:69753835'), (b'Content-Encoding', b'gzip'), (b'Accept-Ranges', b'bytes'), (b'Date', b'Mon, 26 Jan 2026 18:35:18 GMT'), (b'Via', b'1.1 varnish'), (b'X-Served-By', b'cache-bom-vanm7210093-BOM'), (b'X-Cache', b'HIT'), (b'X-Cache-Hits', b'9'), (b'X-Timer', b'S1769452519.952992,VS0,VE0'), (b'Vary', b'Authorization,Accept-Encoding'), (b'Access-Control-Allow-Origin', b'*'), (b'Cross-Origin-Resource-Policy', b'cross-origin'), (b'X-Fastly-Request-ID', b'827c23bf3ad242e3250baef06d11f35612e5627d'), (b'Expires', b'Mon, 26 Jan 2026 18:40:18 GMT'), (b'Source-Age', b'148')])
2026-01-27 00:05:19,005 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'GET']>
2026-01-27 00:05:19,056 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-27 00:05:19,056 | DEBUG | httpcore.http11 | response_closed.started
2026-01-27 00:05:19,056 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-27 00:05:19,056 | DEBUG | httpcore.connection | close.started
2026-01-27 00:05:19,056 | DEBUG | httpcore.connection | close.complete
2026-01-27 00:05:20,065 | DEBUG | LiteLLM | [Non-Blocking] Unable to import GenericAPILogger - LiteLLM Enterprise Feature - No module named 'litellm_enterprise'
2026-01-27 00:05:20,680 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 00:05:20,680 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 00:05:20,680 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 00:05:20,695 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 00:05:20,695 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 00:05:20,695 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 00:05:20,695 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 00:05:20,695 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 00:05:20,695 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 00:05:20,695 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 00:05:20,695 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 00:05:20,695 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 00:05:20,805 | INFO | main | üöÄ FastAPI application starting
2026-01-27 00:05:20,857 | INFO | main | üóÑÔ∏è Database tables ensured
2026-01-27 00:05:20,872 | DEBUG | asyncio | Using proactor: IocpProactor
2026-01-27 00:05:20,872 | DEBUG | openai.agents | Shutting down trace provider
2026-01-27 00:05:20,872 | DEBUG | openai.agents | Shutting down trace processor <agents.tracing.processors.BatchTraceProcessor object at 0x0000020855DB8880>
2026-01-27 00:05:24,938 | DEBUG | httpcore.connection | connect_tcp.started host='raw.githubusercontent.com' port=443 local_address=None timeout=5 socket_options=None
2026-01-27 00:05:24,979 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000028209852950>
2026-01-27 00:05:24,981 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x000002820858E7C0> server_hostname='raw.githubusercontent.com' timeout=5
2026-01-27 00:05:25,007 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000028209852920>
2026-01-27 00:05:25,007 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'GET']>
2026-01-27 00:05:25,007 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-27 00:05:25,007 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'GET']>
2026-01-27 00:05:25,007 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-27 00:05:25,007 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'GET']>
2026-01-27 00:05:25,039 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Connection', b'keep-alive'), (b'Content-Length', b'62322'), (b'Cache-Control', b'max-age=300'), (b'Content-Security-Policy', b"default-src 'none'; style-src 'unsafe-inline'; sandbox"), (b'Content-Type', b'text/plain; charset=utf-8'), (b'ETag', b'W/"5f0be0ed2fe5ecf561c4b4488d64c304c35b980a45012ef1d1253b4b5d438af1"'), (b'Strict-Transport-Security', b'max-age=31536000'), (b'X-Content-Type-Options', b'nosniff'), (b'X-Frame-Options', b'deny'), (b'X-XSS-Protection', b'1; mode=block'), (b'X-GitHub-Request-Id', b'415E:3865D1:30D7E6:7A896E:69753835'), (b'Content-Encoding', b'gzip'), (b'Accept-Ranges', b'bytes'), (b'Date', b'Mon, 26 Jan 2026 18:35:24 GMT'), (b'Via', b'1.1 varnish'), (b'X-Served-By', b'cache-bom-vanm7210053-BOM'), (b'X-Cache', b'HIT'), (b'X-Cache-Hits', b'5'), (b'X-Timer', b'S1769452525.992827,VS0,VE0'), (b'Vary', b'Authorization,Accept-Encoding'), (b'Access-Control-Allow-Origin', b'*'), (b'Cross-Origin-Resource-Policy', b'cross-origin'), (b'X-Fastly-Request-ID', b'585d918675c76756c893edd0391ce7d0f7ebd2c2'), (b'Expires', b'Mon, 26 Jan 2026 18:40:24 GMT'), (b'Source-Age', b'154')])
2026-01-27 00:05:25,039 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'GET']>
2026-01-27 00:05:25,086 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-27 00:05:25,087 | DEBUG | httpcore.http11 | response_closed.started
2026-01-27 00:05:25,087 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-27 00:05:25,087 | DEBUG | httpcore.connection | close.started
2026-01-27 00:05:25,087 | DEBUG | httpcore.connection | close.complete
2026-01-27 00:05:26,354 | DEBUG | LiteLLM | [Non-Blocking] Unable to import GenericAPILogger - LiteLLM Enterprise Feature - No module named 'litellm_enterprise'
2026-01-27 00:05:27,002 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 00:05:27,003 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 00:05:27,003 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 00:05:27,013 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 00:05:27,014 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 00:05:27,015 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 00:05:27,015 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 00:05:27,016 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 00:05:27,016 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 00:05:27,018 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 00:05:27,018 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 00:05:27,019 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 00:05:27,126 | INFO | main | üöÄ FastAPI application starting
2026-01-27 00:05:27,189 | INFO | main | üóÑÔ∏è Database tables ensured
2026-01-27 00:05:59,719 | DEBUG | asyncio | Using proactor: IocpProactor
2026-01-27 00:05:59,722 | DEBUG | openai.agents | Shutting down trace provider
2026-01-27 00:05:59,722 | DEBUG | openai.agents | Shutting down trace processor <agents.tracing.processors.BatchTraceProcessor object at 0x000002827F438880>
2026-01-27 00:06:02,928 | DEBUG | httpcore.connection | connect_tcp.started host='raw.githubusercontent.com' port=443 local_address=None timeout=5 socket_options=None
2026-01-27 00:06:02,972 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000231BAE528F0>
2026-01-27 00:06:02,972 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x00000231BAB5E7C0> server_hostname='raw.githubusercontent.com' timeout=5
2026-01-27 00:06:03,052 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000231BAE528C0>
2026-01-27 00:06:03,052 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'GET']>
2026-01-27 00:06:03,052 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-27 00:06:03,052 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'GET']>
2026-01-27 00:06:03,052 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-27 00:06:03,052 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'GET']>
2026-01-27 00:06:03,084 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Connection', b'keep-alive'), (b'Content-Length', b'62322'), (b'Cache-Control', b'max-age=300'), (b'Content-Security-Policy', b"default-src 'none'; style-src 'unsafe-inline'; sandbox"), (b'Content-Type', b'text/plain; charset=utf-8'), (b'ETag', b'W/"5f0be0ed2fe5ecf561c4b4488d64c304c35b980a45012ef1d1253b4b5d438af1"'), (b'Strict-Transport-Security', b'max-age=31536000'), (b'X-Content-Type-Options', b'nosniff'), (b'X-Frame-Options', b'deny'), (b'X-XSS-Protection', b'1; mode=block'), (b'X-GitHub-Request-Id', b'415E:3865D1:30D7E6:7A896E:69753835'), (b'Content-Encoding', b'gzip'), (b'Accept-Ranges', b'bytes'), (b'Date', b'Mon, 26 Jan 2026 18:36:03 GMT'), (b'Via', b'1.1 varnish'), (b'X-Served-By', b'cache-bom-vanm7210094-BOM'), (b'X-Cache', b'HIT'), (b'X-Cache-Hits', b'8'), (b'X-Timer', b'S1769452563.028637,VS0,VE0'), (b'Vary', b'Authorization,Accept-Encoding'), (b'Access-Control-Allow-Origin', b'*'), (b'Cross-Origin-Resource-Policy', b'cross-origin'), (b'X-Fastly-Request-ID', b'f5b6a304e06dd5ea9f92204a6ea9ef425e1e2cda'), (b'Expires', b'Mon, 26 Jan 2026 18:41:03 GMT'), (b'Source-Age', b'192')])
2026-01-27 00:06:03,084 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'GET']>
2026-01-27 00:06:03,125 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-27 00:06:03,125 | DEBUG | httpcore.http11 | response_closed.started
2026-01-27 00:06:03,125 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-27 00:06:03,125 | DEBUG | httpcore.connection | close.started
2026-01-27 00:06:03,125 | DEBUG | httpcore.connection | close.complete
2026-01-27 00:06:03,746 | DEBUG | LiteLLM | [Non-Blocking] Unable to import GenericAPILogger - LiteLLM Enterprise Feature - No module named 'litellm_enterprise'
2026-01-27 00:06:04,126 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 00:06:04,126 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 00:06:04,126 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 00:06:04,126 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 00:06:04,126 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 00:06:04,126 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 00:06:04,137 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 00:06:04,137 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 00:06:04,138 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 00:06:04,138 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 00:06:04,138 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 00:06:04,139 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 00:06:04,204 | INFO | main | üöÄ FastAPI application starting
2026-01-27 00:06:04,287 | INFO | main | üóÑÔ∏è Database tables ensured
2026-01-27 00:06:29,168 | INFO | chat | üì© Chat request | user_id=6 | request_id=f5790c83-81d7-4e16-b7bf-360db2e394b3
2026-01-27 00:06:29,169 | INFO | chat | üßë User message | hi
2026-01-27 00:06:29,185 | INFO | chat | üÜï New session created | 42356d82-d23e-4eab-8ef8-7d395e86e6fe
2026-01-27 00:06:29,187 | INFO | chat | ü§ñ Stream started | session_id=42356d82-d23e-4eab-8ef8-7d395e86e6fe | request_id=f5790c83-81d7-4e16-b7bf-360db2e394b3
2026-01-27 00:06:29,187 | INFO | chat | ü§ñ Running digital human
2026-01-27 00:06:29,187 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-27 00:06:29,187 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-27 00:06:29,199 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_cc4e48531e934011869b2ce92e580d56
2026-01-27 00:06:29,199 | DEBUG | openai.agents | Setting current trace: trace_cc4e48531e934011869b2ce92e580d56
2026-01-27 00:06:29,201 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x00000231BEBBF3D0> with id None
2026-01-27 00:06:29,201 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-27 00:06:29,210 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x00000231BEBC8A00> with id None
2026-01-27 00:06:29,210 | DEBUG | openai.agents | Calling LLM
2026-01-27 00:06:29,210 | DEBUG | LiteLLM | 

2026-01-27 00:06:29,210 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-27 00:06:29,210 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n', 'role': 'system'}, {'role': 'user', 'content': 'hi'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.5'}, api_key=None, base_url=None)[0m
2026-01-27 00:06:29,210 | DEBUG | LiteLLM | 

2026-01-27 00:06:29,210 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-27 00:06:29,210 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-27 00:06:29,210 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-27 00:06:29,237 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 00:06:29,237 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-27 00:06:29,242 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n', 'role': 'system'}, {'role': 'user', 'content': 'hi'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-27 00:06:29,242 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-27 00:06:29,245 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-27 00:06:29,245 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-27 00:06:29,245 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:06:29,440 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:06:29,442 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.5' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'hi'}]}], 'system_instruction': {'parts': [{'text': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n'}]}, 'generationConfig': {}}'
[0m

2026-01-27 00:06:32,000 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "SAFE"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 188,
    "candidatesTokenCount": 1,
    "totalTokenCount": 223,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 188
      }
    ],
    "thoughtsTokenCount": 34
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "L7R3aYf1K9Wq4-EP3e622Ak"
}



2026-01-27 00:06:32,005 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-27 00:06:32,005 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-27 00:06:32,005 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 00:06:32,005 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 00:06:32,012 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:06:32,012 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:06:32,012 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:06:32,012 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:06:32,015 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:06:32,015 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:06:32,015 | DEBUG | LiteLLM | response_cost: 0.0001439
2026-01-27 00:06:32,015 | DEBUG | openai.agents | Received model response
2026-01-27 00:06:32,015 | DEBUG | LiteLLM | response_cost: 0.0001439
2026-01-27 00:06:32,027 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x00000231BEC444C0>>
2026-01-27 00:06:32,027 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 00:06:32,035 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-27 00:06:32,036 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:06:32,036 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:06:32,038 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-27 00:06:32,036 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-27 00:06:32,038 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-27 00:06:32,039 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-27 00:06:32,040 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 00:06:32,040 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 00:06:32,040 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 00:06:32,040 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:06:32,040 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:06:32,040 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:06:32,040 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:06:32,040 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:06:32,040 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:06:32,040 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-27 00:06:32,040 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0001439
2026-01-27 00:06:32,040 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 00:06:32,040 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:06:32,040 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:06:32,040 | DEBUG | openai.agents | Resetting current trace
2026-01-27 00:06:32,040 | INFO | orchestrator | ‚úÖ Safety passed
2026-01-27 00:06:32,040 | INFO | orchestrator | üß≠ Router agent called
2026-01-27 00:06:32,040 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_ad20060ca5354bbcba10c943f9282f36
2026-01-27 00:06:32,040 | DEBUG | openai.agents | Setting current trace: trace_ad20060ca5354bbcba10c943f9282f36
2026-01-27 00:06:32,051 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x00000231BEB27BA0> with id None
2026-01-27 00:06:32,051 | DEBUG | openai.agents | Running agent RouterAgent (turn 1)
2026-01-27 00:06:32,052 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x00000231BF998940> with id None
2026-01-27 00:06:32,052 | DEBUG | openai.agents | Calling LLM
2026-01-27 00:06:32,052 | DEBUG | LiteLLM | 

2026-01-27 00:06:32,052 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-27 00:06:32,052 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'hi'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.5'}, api_key=None, base_url=None)[0m
2026-01-27 00:06:32,054 | DEBUG | LiteLLM | 

2026-01-27 00:06:32,054 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-27 00:06:32,055 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-27 00:06:32,055 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-27 00:06:32,056 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 00:06:32,057 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-27 00:06:32,058 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'hi'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-27 00:06:32,059 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-27 00:06:32,059 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-27 00:06:32,060 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-27 00:06:32,060 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:06:32,061 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:06:32,061 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.5' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'hi'}]}], 'system_instruction': {'parts': [{'text': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n'}]}, 'generationConfig': {}}'
[0m

2026-01-27 00:06:33,701 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\"use_memory\": false, \"use_tool\": false, \"tool_name\": \"none\", \"tool_arguments\": null, \"intent\": \"none\", \"memory_key\": null}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 486,
    "candidatesTokenCount": 42,
    "totalTokenCount": 583,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 486
      }
    ],
    "thoughtsTokenCount": 55
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "MbR3aZa2HYbtg8UPq7DcuA0"
}



2026-01-27 00:06:33,701 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-27 00:06:33,701 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 00:06:33,701 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-27 00:06:33,701 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:06:33,707 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 00:06:33,707 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:06:33,709 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:06:33,709 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:06:33,709 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:06:33,709 | DEBUG | LiteLLM | response_cost: 0.0003883
2026-01-27 00:06:33,709 | DEBUG | openai.agents | Received model response
2026-01-27 00:06:33,709 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:06:33,709 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x00000231B841AFE0>>
2026-01-27 00:06:33,709 | DEBUG | LiteLLM | response_cost: 0.0003883
2026-01-27 00:06:33,716 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-27 00:06:33,718 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 00:06:33,718 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-27 00:06:33,719 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-27 00:06:33,719 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:06:33,721 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 00:06:33,721 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-27 00:06:33,721 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:06:33,721 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:06:33,721 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 00:06:33,725 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-27 00:06:33,725 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:06:33,725 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:06:33,725 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 00:06:33,725 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:06:33,725 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:06:33,725 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-27 00:06:33,725 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:06:33,732 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0003883
2026-01-27 00:06:33,732 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 00:06:33,732 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:06:33,735 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:06:33,736 | DEBUG | openai.agents | Resetting current trace
2026-01-27 00:06:33,737 | INFO | orchestrator | üß≠ Router | tool=False memory=False intent=none
2026-01-27 00:06:33,737 | INFO | orchestrator | üß† Reasoning agent called
2026-01-27 00:06:33,738 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_0dfac7c378a1432c96029aa586adde35
2026-01-27 00:06:33,738 | DEBUG | openai.agents | Setting current trace: trace_0dfac7c378a1432c96029aa586adde35
2026-01-27 00:06:33,738 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x00000231BE165D00> with id None
2026-01-27 00:06:33,738 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x00000231BF9994E0> with id None
2026-01-27 00:06:33,738 | DEBUG | openai.agents | Calling LLM
2026-01-27 00:06:33,738 | DEBUG | LiteLLM | 

2026-01-27 00:06:33,738 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-27 00:06:33,738 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.\n\nIf the user mentions vague locations like:\n- nearest city\n- near me\n- nearby\n- around here\n\nand user location is provided,\nrewrite the query using the nearest major city.\n\nOutput ONLY the rewritten query.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n\\nYou may answer ANY general knowledge question.\\n\\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n\\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n\\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "hi"}, {"role": "user", "content": "hi"}], "memory_action": {}, "memory_data": [], "tool_context": {}}'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=True, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.5'}, api_key=None, base_url=None)[0m
2026-01-27 00:06:33,738 | DEBUG | LiteLLM | 

2026-01-27 00:06:33,738 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-27 00:06:33,738 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-27 00:06:33,738 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-27 00:06:33,738 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 00:06:33,738 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-27 00:06:33,738 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.\n\nIf the user mentions vague locations like:\n- nearest city\n- near me\n- nearby\n- around here\n\nand user location is provided,\nrewrite the query using the nearest major city.\n\nOutput ONLY the rewritten query.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n\\nYou may answer ANY general knowledge question.\\n\\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n\\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n\\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "hi"}, {"role": "user", "content": "hi"}], "memory_action": {}, "memory_data": [], "tool_context": {}}'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-27 00:06:33,738 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': True}
2026-01-27 00:06:33,738 | DEBUG | LiteLLM | Final returned optional params: {'stream': True}
2026-01-27 00:06:33,738 | DEBUG | LiteLLM | self.optional_params: {'stream': True}
2026-01-27 00:06:33,738 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:06:33,751 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:06:33,751 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?key=*****=sse \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.5' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n\\nYou may answer ANY general knowledge question.\\n\\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n\\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n\\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "hi"}, {"role": "user", "content": "hi"}], "memory_action": {}, "memory_data": [], "tool_context": {}}'}]}], 'system_instruction': {'parts': [{'text': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.\n\nIf the user mentions vague locations like:\n- nearest city\n- near me\n- nearby\n- around here\n\nand user location is provided,\nrewrite the query using the nearest major city.\n\nOutput ONLY the rewritten query.'}]}, 'generationConfig': {}}'
[0m

2026-01-27 00:06:33,752 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 00:06:33,754 | DEBUG | LiteLLM | messages in token_counter: None, text in token_counter: 
2026-01-27 00:06:33,756 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:06:33,756 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:06:33,756 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:06:33,756 | DEBUG | LiteLLM | response_cost: 0.0
2026-01-27 00:06:33,916 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 00:06:33,916 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 00:06:33,916 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 00:06:34,315 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-27 00:06:34,366 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000231BF364DF0>
2026-01-27 00:06:34,366 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x00000231B8A0B4C0> server_hostname='api.openai.com' timeout=5.0
2026-01-27 00:06:34,379 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000231BF364FD0>
2026-01-27 00:06:34,395 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-27 00:06:34,395 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-27 00:06:34,395 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-27 00:06:34,395 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-27 00:06:34,395 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-27 00:06:35,412 | DEBUG | LiteLLM | RAW RESPONSE:
first stream response received


2026-01-27 00:06:35,412 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': 'Hi there! How can I help you today?'}], 'role': 'model'}, 'finishReason': 'STOP', 'index': 0}], 'usageMetadata': {'promptTokenCount': 302, 'candidatesTokenCount': 10, 'totalTokenCount': 342, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 302}], 'thoughtsTokenCount': 30}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'MrR3aZvsOv2X4-EP54eI0Ak'}
2026-01-27 00:06:35,437 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='MrR3aZvsOv2X4-EP54eI0Ak', created=1769452595, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='Hi there! How can I help you today?', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=40, prompt_tokens=302, total_tokens=342, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=30, rejected_prediction_tokens=None, text_tokens=10, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=302, image_tokens=None)))
2026-01-27 00:06:35,441 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content='Hi there! How can I help you today?', role='assistant', function_call=None, tool_calls=None, audio=None)
2026-01-27 00:06:35,443 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='MrR3aZvsOv2X4-EP54eI0Ak', created=1769452595, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Hi there! How can I help you today?', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=40, prompt_tokens=302, total_tokens=342, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=30, rejected_prediction_tokens=None, text_tokens=10, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=302, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-27 00:06:35,503 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=False
2026-01-27 00:06:35,524 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=False
2026-01-27 00:06:35,524 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-27 00:06:35,524 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-27 00:06:35,524 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 00:06:35,524 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 00:06:35,524 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:06:35,524 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:06:35,524 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:06:35,524 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:06:35,524 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:06:35,535 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:06:35,536 | DEBUG | LiteLLM | response_cost: 0.0001906
2026-01-27 00:06:35,536 | DEBUG | LiteLLM | response_cost: 0.0001906
2026-01-27 00:06:35,536 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 00:06:35,536 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0001906
2026-01-27 00:06:35,536 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:06:35,536 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 00:06:35,536 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:06:35,536 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:06:35,536 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:06:35,536 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-27 00:06:35,536 | DEBUG | openai.agents | Resetting current trace
2026-01-27 00:06:35,555 | INFO | chat | ‚úÖ Stream complete | tokens=1 | time=6.39s
2026-01-27 00:06:36,103 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Mon, 26 Jan 2026 18:36:36 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_eee6e8f43bf3234b8a2e4effc19d63cd'), (b'openai-processing-ms', b'534'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'user-u2cbhcbswv5bt4bmhvzdmhwd'), (b'openai-project', b'proj_oIIqE3fW3A3IDZXBAVOtc4gK'), (b'x-envoy-upstream-service-time', b'537'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=nbUyWJ4rHfxYWwlxxfGC5ym40iCC5wwXjxRxenBIVFI-1769452596-1.0.1.1-kFxj1Zq_r8j4Ie5SZTr1Xeq9kkARm99ZS7yvEBpXmIvNw5uIF3ZT109Sv.14uKZnKzEg_ia1WwyqR75619P3AkBBw.YtebpLlTnKIVUDL4Y; path=/; expires=Mon, 26-Jan-26 19:06:36 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=Bj5Jiv0APW3SFKM2pYG77EybIaQpKuUrdWd3hj0JmxI-1769452596029-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c421dda9ecc7819-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-27 00:06:36,103 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-27 00:06:36,103 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-27 00:06:36,103 | DEBUG | httpcore.http11 | response_closed.started
2026-01-27 00:06:36,103 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-27 00:06:36,103 | DEBUG | openai.agents | Exported 7 items
2026-01-27 00:06:36,103 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-27 00:06:36,103 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-27 00:06:36,103 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-27 00:06:36,103 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-27 00:06:36,103 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-27 00:06:37,228 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Mon, 26 Jan 2026 18:36:37 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_9365b946e8a2a14833e661fc7252e83e'), (b'openai-processing-ms', b'218'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'user-u2cbhcbswv5bt4bmhvzdmhwd'), (b'openai-project', b'proj_oIIqE3fW3A3IDZXBAVOtc4gK'), (b'x-envoy-upstream-service-time', b'220'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c421de559787819-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-27 00:06:37,228 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-27 00:06:37,228 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-27 00:06:37,228 | DEBUG | httpcore.http11 | response_closed.started
2026-01-27 00:06:37,228 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-27 00:06:37,228 | DEBUG | openai.agents | Exported 2 items
2026-01-27 00:09:30,263 | DEBUG | LiteLLM | LoggingWorker cancelled during shutdown
2026-01-27 00:09:30,267 | DEBUG | LiteLLM | [LoggingWorker] atexit: Queue is empty
2026-01-27 00:09:30,267 | DEBUG | asyncio | Using proactor: IocpProactor
2026-01-27 00:09:30,267 | DEBUG | openai.agents | Shutting down trace provider
2026-01-27 00:09:30,267 | DEBUG | openai.agents | Shutting down trace processor <agents.tracing.processors.BatchTraceProcessor object at 0x00000231B8A38790>
2026-01-27 00:09:30,414 | ERROR | asyncio | Fatal error on SSL transport
protocol: <asyncio.sslproto.SSLProtocol object at 0x00000231BAD14B50>
transport: <_SelectorSocketTransport closing fd=2220>
Traceback (most recent call last):
  File "C:\Users\dell\AppData\Local\Programs\Python\Python310\lib\asyncio\selector_events.py", line 924, in write
    n = self._sock.send(data)
OSError: [WinError 10038] An operation was attempted on something that is not a socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\dell\AppData\Local\Programs\Python\Python310\lib\asyncio\sslproto.py", line 690, in _process_write_backlog
    self._transport.write(chunk)
  File "C:\Users\dell\AppData\Local\Programs\Python\Python310\lib\asyncio\selector_events.py", line 930, in write
    self._fatal_error(exc, 'Fatal write error on socket transport')
  File "C:\Users\dell\AppData\Local\Programs\Python\Python310\lib\asyncio\selector_events.py", line 725, in _fatal_error
    self._force_close(exc)
  File "C:\Users\dell\AppData\Local\Programs\Python\Python310\lib\asyncio\selector_events.py", line 737, in _force_close
    self._loop.call_soon(self._call_connection_lost, exc)
  File "C:\Users\dell\AppData\Local\Programs\Python\Python310\lib\asyncio\base_events.py", line 753, in call_soon
    self._check_closed()
  File "C:\Users\dell\AppData\Local\Programs\Python\Python310\lib\asyncio\base_events.py", line 515, in _check_closed
    raise RuntimeError('Event loop is closed')
RuntimeError: Event loop is closed
2026-01-27 00:10:13,961 | DEBUG | httpcore.connection | connect_tcp.started host='raw.githubusercontent.com' port=443 local_address=None timeout=5 socket_options=None
2026-01-27 00:10:14,012 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000232015228C0>
2026-01-27 00:10:14,013 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x000002320025E7C0> server_hostname='raw.githubusercontent.com' timeout=5
2026-01-27 00:10:14,051 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023201522890>
2026-01-27 00:10:14,051 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'GET']>
2026-01-27 00:10:14,051 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-27 00:10:14,051 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'GET']>
2026-01-27 00:10:14,057 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-27 00:10:14,057 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'GET']>
2026-01-27 00:10:14,099 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Connection', b'keep-alive'), (b'Content-Length', b'62322'), (b'Cache-Control', b'max-age=300'), (b'Content-Security-Policy', b"default-src 'none'; style-src 'unsafe-inline'; sandbox"), (b'Content-Type', b'text/plain; charset=utf-8'), (b'ETag', b'W/"5f0be0ed2fe5ecf561c4b4488d64c304c35b980a45012ef1d1253b4b5d438af1"'), (b'Strict-Transport-Security', b'max-age=31536000'), (b'X-Content-Type-Options', b'nosniff'), (b'X-Frame-Options', b'deny'), (b'X-XSS-Protection', b'1; mode=block'), (b'X-GitHub-Request-Id', b'415E:3865D1:30D7E6:7A896E:69753835'), (b'Content-Encoding', b'gzip'), (b'Accept-Ranges', b'bytes'), (b'Date', b'Mon, 26 Jan 2026 18:40:14 GMT'), (b'Via', b'1.1 varnish'), (b'X-Served-By', b'cache-bom-vanm7210083-BOM'), (b'X-Cache', b'HIT'), (b'X-Cache-Hits', b'10'), (b'X-Timer', b'S1769452814.028552,VS0,VE0'), (b'Vary', b'Authorization,Accept-Encoding'), (b'Access-Control-Allow-Origin', b'*'), (b'Cross-Origin-Resource-Policy', b'cross-origin'), (b'X-Fastly-Request-ID', b'c0566cb81662eb6d37ae8c440ea617fb8782e6d7'), (b'Expires', b'Mon, 26 Jan 2026 18:45:14 GMT'), (b'Source-Age', b'143')])
2026-01-27 00:10:14,099 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'GET']>
2026-01-27 00:10:14,129 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-27 00:10:14,129 | DEBUG | httpcore.http11 | response_closed.started
2026-01-27 00:10:14,129 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-27 00:10:14,129 | DEBUG | httpcore.connection | close.started
2026-01-27 00:10:14,145 | DEBUG | httpcore.connection | close.complete
2026-01-27 00:10:15,136 | DEBUG | LiteLLM | [Non-Blocking] Unable to import GenericAPILogger - LiteLLM Enterprise Feature - No module named 'litellm_enterprise'
2026-01-27 00:10:15,673 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 00:10:15,673 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 00:10:15,673 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 00:10:15,682 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 00:10:15,682 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 00:10:15,682 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 00:10:15,684 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 00:10:15,684 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 00:10:15,685 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 00:10:15,685 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 00:10:15,686 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 00:10:15,686 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 00:10:15,762 | INFO | main | üöÄ FastAPI application starting
2026-01-27 00:10:15,836 | INFO | main | üóÑÔ∏è Database tables ensured
2026-01-27 00:10:16,494 | DEBUG | asyncio | Using proactor: IocpProactor
2026-01-27 00:10:16,494 | DEBUG | openai.agents | Shutting down trace provider
2026-01-27 00:10:16,494 | DEBUG | openai.agents | Shutting down trace processor <agents.tracing.processors.BatchTraceProcessor object at 0x000002327F038880>
2026-01-27 00:11:49,860 | DEBUG | httpcore.connection | connect_tcp.started host='raw.githubusercontent.com' port=443 local_address=None timeout=5 socket_options=None
2026-01-27 00:11:49,924 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001CB69972830>
2026-01-27 00:11:49,924 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x000001CB686AE740> server_hostname='raw.githubusercontent.com' timeout=5
2026-01-27 00:11:49,960 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001CB69972800>
2026-01-27 00:11:49,960 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'GET']>
2026-01-27 00:11:49,961 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-27 00:11:49,961 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'GET']>
2026-01-27 00:11:49,961 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-27 00:11:49,961 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'GET']>
2026-01-27 00:11:50,004 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Connection', b'keep-alive'), (b'Content-Length', b'62322'), (b'Cache-Control', b'max-age=300'), (b'Content-Security-Policy', b"default-src 'none'; style-src 'unsafe-inline'; sandbox"), (b'Content-Type', b'text/plain; charset=utf-8'), (b'ETag', b'W/"5f0be0ed2fe5ecf561c4b4488d64c304c35b980a45012ef1d1253b4b5d438af1"'), (b'Strict-Transport-Security', b'max-age=31536000'), (b'X-Content-Type-Options', b'nosniff'), (b'X-Frame-Options', b'deny'), (b'X-XSS-Protection', b'1; mode=block'), (b'X-GitHub-Request-Id', b'415E:3865D1:30D7E6:7A896E:69753835'), (b'Content-Encoding', b'gzip'), (b'Accept-Ranges', b'bytes'), (b'Date', b'Mon, 26 Jan 2026 18:41:49 GMT'), (b'Via', b'1.1 varnish'), (b'X-Served-By', b'cache-bom-vanm7210029-BOM'), (b'X-Cache', b'HIT'), (b'X-Cache-Hits', b'12'), (b'X-Timer', b'S1769452910.943829,VS0,VE0'), (b'Vary', b'Authorization,Accept-Encoding'), (b'Access-Control-Allow-Origin', b'*'), (b'Cross-Origin-Resource-Policy', b'cross-origin'), (b'X-Fastly-Request-ID', b'f8c2c0e02a6d036f5d9c1d9174af5a612aff921b'), (b'Expires', b'Mon, 26 Jan 2026 18:46:49 GMT'), (b'Source-Age', b'238')])
2026-01-27 00:11:50,005 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'GET']>
2026-01-27 00:11:50,041 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-27 00:11:50,041 | DEBUG | httpcore.http11 | response_closed.started
2026-01-27 00:11:50,041 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-27 00:11:50,041 | DEBUG | httpcore.connection | close.started
2026-01-27 00:11:50,041 | DEBUG | httpcore.connection | close.complete
2026-01-27 00:11:51,365 | DEBUG | LiteLLM | [Non-Blocking] Unable to import GenericAPILogger - LiteLLM Enterprise Feature - No module named 'litellm_enterprise'
2026-01-27 00:11:52,103 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 00:11:52,104 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 00:11:52,104 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 00:11:52,114 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 00:11:52,114 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 00:11:52,115 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 00:11:52,116 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 00:11:52,116 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 00:11:52,116 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 00:11:52,117 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 00:11:52,118 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 00:11:52,118 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 00:11:52,193 | INFO | main | üöÄ FastAPI application starting
2026-01-27 00:11:52,245 | INFO | main | üóÑÔ∏è Database tables ensured
2026-01-27 00:12:06,724 | DEBUG | asyncio | Using proactor: IocpProactor
2026-01-27 00:12:06,726 | DEBUG | openai.agents | Shutting down trace provider
2026-01-27 00:12:06,726 | DEBUG | openai.agents | Shutting down trace processor <agents.tracing.processors.BatchTraceProcessor object at 0x000001CB67558880>
2026-01-27 00:12:20,610 | DEBUG | httpcore.connection | connect_tcp.started host='raw.githubusercontent.com' port=443 local_address=None timeout=5 socket_options=None
2026-01-27 00:12:20,672 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001F001F22920>
2026-01-27 00:12:20,673 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x000001F000C5E740> server_hostname='raw.githubusercontent.com' timeout=5
2026-01-27 00:12:20,707 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001F001F228F0>
2026-01-27 00:12:20,707 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'GET']>
2026-01-27 00:12:20,707 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-27 00:12:20,707 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'GET']>
2026-01-27 00:12:20,707 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-27 00:12:20,707 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'GET']>
2026-01-27 00:12:20,787 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Connection', b'keep-alive'), (b'Content-Length', b'62322'), (b'Cache-Control', b'max-age=300'), (b'Content-Security-Policy', b"default-src 'none'; style-src 'unsafe-inline'; sandbox"), (b'Content-Type', b'text/plain; charset=utf-8'), (b'ETag', b'W/"5f0be0ed2fe5ecf561c4b4488d64c304c35b980a45012ef1d1253b4b5d438af1"'), (b'Strict-Transport-Security', b'max-age=31536000'), (b'X-Content-Type-Options', b'nosniff'), (b'X-Frame-Options', b'deny'), (b'X-XSS-Protection', b'1; mode=block'), (b'X-GitHub-Request-Id', b'415E:3865D1:30D7E6:7A896E:69753835'), (b'Content-Encoding', b'gzip'), (b'Accept-Ranges', b'bytes'), (b'Date', b'Mon, 26 Jan 2026 18:42:20 GMT'), (b'Via', b'1.1 varnish'), (b'X-Served-By', b'cache-bom-vanm7210094-BOM'), (b'X-Cache', b'HIT'), (b'X-Cache-Hits', b'10'), (b'X-Timer', b'S1769452941.688869,VS0,VE0'), (b'Vary', b'Authorization,Accept-Encoding'), (b'Access-Control-Allow-Origin', b'*'), (b'Cross-Origin-Resource-Policy', b'cross-origin'), (b'X-Fastly-Request-ID', b'2d76742d5e0fbe64a9fc7398147335d86c625edb'), (b'Expires', b'Mon, 26 Jan 2026 18:47:20 GMT'), (b'Source-Age', b'269')])
2026-01-27 00:12:20,787 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'GET']>
2026-01-27 00:12:20,836 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-27 00:12:20,836 | DEBUG | httpcore.http11 | response_closed.started
2026-01-27 00:12:20,836 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-27 00:12:20,837 | DEBUG | httpcore.connection | close.started
2026-01-27 00:12:20,837 | DEBUG | httpcore.connection | close.complete
2026-01-27 00:12:21,498 | DEBUG | LiteLLM | [Non-Blocking] Unable to import GenericAPILogger - LiteLLM Enterprise Feature - No module named 'litellm_enterprise'
2026-01-27 00:12:21,920 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 00:12:21,920 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 00:12:21,920 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 00:12:21,920 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 00:12:21,930 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 00:12:21,930 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 00:12:21,931 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 00:12:21,931 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 00:12:21,931 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 00:12:21,933 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 00:12:21,933 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 00:12:21,933 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 00:12:22,016 | INFO | main | üöÄ FastAPI application starting
2026-01-27 00:12:22,086 | INFO | main | üóÑÔ∏è Database tables ensured
2026-01-27 00:12:52,693 | INFO | chat | üì© Chat request | user_id=6 | request_id=decef134-b84b-4e56-adaa-78d61f68fd72
2026-01-27 00:12:52,697 | INFO | chat | üßë User message | hi
2026-01-27 00:12:52,706 | INFO | chat | üÜï New session created | 1892a976-bd53-4512-a7bc-d11eadaa106f
2026-01-27 00:12:52,722 | INFO | chat | ü§ñ Stream started | session_id=1892a976-bd53-4512-a7bc-d11eadaa106f | request_id=decef134-b84b-4e56-adaa-78d61f68fd72
2026-01-27 00:12:52,723 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-27 00:12:52,724 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-27 00:12:52,725 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_b6047d5b491f46ca95dbf3f3178fd255
2026-01-27 00:12:52,725 | DEBUG | openai.agents | Setting current trace: trace_b6047d5b491f46ca95dbf3f3178fd255
2026-01-27 00:12:52,725 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000001F005C7B010> with id None
2026-01-27 00:12:52,728 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-27 00:12:52,732 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000001F005A877C0> with id None
2026-01-27 00:12:52,732 | DEBUG | openai.agents | Calling LLM
2026-01-27 00:12:52,732 | DEBUG | LiteLLM | 

2026-01-27 00:12:52,734 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-27 00:12:52,734 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n', 'role': 'system'}, {'role': 'user', 'content': 'hi'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.5'}, api_key=None, base_url=None)[0m
2026-01-27 00:12:52,734 | DEBUG | LiteLLM | 

2026-01-27 00:12:52,734 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-27 00:12:52,736 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-27 00:12:52,736 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-27 00:12:52,743 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 00:12:52,743 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-27 00:12:52,743 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n', 'role': 'system'}, {'role': 'user', 'content': 'hi'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-27 00:12:52,743 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-27 00:12:52,757 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-27 00:12:52,759 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-27 00:12:52,759 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:12:53,020 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:12:53,020 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.5' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'hi'}]}], 'system_instruction': {'parts': [{'text': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n'}]}, 'generationConfig': {}}'
[0m

2026-01-27 00:12:54,335 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "SAFE"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 188,
    "candidatesTokenCount": 1,
    "totalTokenCount": 230,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 188
      }
    ],
    "thoughtsTokenCount": 41
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "rrV3acrhB5Wzg8UP7ZGZuQ0"
}



2026-01-27 00:12:54,343 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-27 00:12:54,343 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-27 00:12:54,343 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 00:12:54,346 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 00:12:54,346 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:12:54,347 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:12:54,347 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:12:54,349 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:12:54,349 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:12:54,349 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:12:54,349 | DEBUG | LiteLLM | response_cost: 0.00016140000000000002
2026-01-27 00:12:54,349 | DEBUG | LiteLLM | response_cost: 0.00016140000000000002
2026-01-27 00:12:54,349 | DEBUG | openai.agents | Received model response
2026-01-27 00:12:54,349 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 00:12:54,370 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001F005B62350>>
2026-01-27 00:12:54,370 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:12:54,372 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:12:54,372 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-27 00:12:54,374 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-27 00:12:54,375 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-27 00:12:54,376 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-27 00:12:54,376 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 00:12:54,376 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 00:12:54,376 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-27 00:12:54,376 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:12:54,376 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:12:54,376 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 00:12:54,376 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:12:54,376 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:12:54,376 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:12:54,376 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:12:54,376 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-27 00:12:54,376 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.00016140000000000002
2026-01-27 00:12:54,376 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 00:12:54,376 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:12:54,376 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:12:54,392 | DEBUG | openai.agents | Resetting current trace
2026-01-27 00:12:54,392 | INFO | orchestrator | ‚úÖ Safety passed
2026-01-27 00:12:54,393 | INFO | orchestrator | üß≠ Router agent called
2026-01-27 00:12:54,394 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_478788ad1714463795103eacd81e5f01
2026-01-27 00:12:54,394 | DEBUG | openai.agents | Setting current trace: trace_478788ad1714463795103eacd81e5f01
2026-01-27 00:12:54,394 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000001F005BDB2E0> with id None
2026-01-27 00:12:54,394 | DEBUG | openai.agents | Running agent RouterAgent (turn 1)
2026-01-27 00:12:54,394 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000001F005CB6CE0> with id None
2026-01-27 00:12:54,394 | DEBUG | openai.agents | Calling LLM
2026-01-27 00:12:54,394 | DEBUG | LiteLLM | 

2026-01-27 00:12:54,394 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-27 00:12:54,394 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'hi'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.5'}, api_key=None, base_url=None)[0m
2026-01-27 00:12:54,394 | DEBUG | LiteLLM | 

2026-01-27 00:12:54,394 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-27 00:12:54,394 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-27 00:12:54,394 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-27 00:12:54,394 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 00:12:54,394 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-27 00:12:54,394 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'hi'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-27 00:12:54,394 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-27 00:12:54,394 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-27 00:12:54,394 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-27 00:12:54,394 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:12:54,408 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:12:54,409 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.5' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'hi'}]}], 'system_instruction': {'parts': [{'text': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n'}]}, 'generationConfig': {}}'
[0m

2026-01-27 00:12:56,186 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\n  \"use_memory\": false,\n  \"use_tool\": false,\n  \"tool_name\": \"none\",\n  \"tool_arguments\": {},\n  \"intent\": \"none\",\n  \"memory_key\": null\n}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 486,
    "candidatesTokenCount": 55,
    "totalTokenCount": 621,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 486
      }
    ],
    "thoughtsTokenCount": 80
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "r7V3aev7O7nVqfkPtOfuqQE"
}



2026-01-27 00:12:56,186 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-27 00:12:56,202 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 00:12:56,202 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-27 00:12:56,202 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:12:56,202 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 00:12:56,202 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:12:56,202 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:12:56,202 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:12:56,202 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:12:56,202 | DEBUG | LiteLLM | response_cost: 0.0004833
2026-01-27 00:12:56,202 | DEBUG | openai.agents | Received model response
2026-01-27 00:12:56,202 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:12:56,202 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001F07F48B580>>
2026-01-27 00:12:56,209 | DEBUG | LiteLLM | response_cost: 0.0004833
2026-01-27 00:12:56,209 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-27 00:12:56,211 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 00:12:56,211 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-27 00:12:56,211 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-27 00:12:56,211 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:12:56,211 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-27 00:12:56,216 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 00:12:56,216 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:12:56,217 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 00:12:56,217 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:12:56,217 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-27 00:12:56,217 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:12:56,217 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:12:56,217 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 00:12:56,217 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:12:56,217 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:12:56,217 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-27 00:12:56,217 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:12:56,225 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0004833
2026-01-27 00:12:56,225 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 00:12:56,225 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:12:56,225 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:12:56,225 | DEBUG | openai.agents | Resetting current trace
2026-01-27 00:12:56,225 | INFO | orchestrator | üß≠ Router | tool=False memory=False intent=none
2026-01-27 00:12:56,225 | INFO | orchestrator | üß† Reasoning agent called
2026-01-27 00:12:56,225 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_c0a80633f8dd45669909dfe8e6e0113e
2026-01-27 00:12:56,225 | DEBUG | openai.agents | Setting current trace: trace_c0a80633f8dd45669909dfe8e6e0113e
2026-01-27 00:12:56,225 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000001F005ABFB50> with id None
2026-01-27 00:12:56,225 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000001F00666CFA0> with id None
2026-01-27 00:12:56,233 | DEBUG | openai.agents | Calling LLM
2026-01-27 00:12:56,233 | DEBUG | LiteLLM | 

2026-01-27 00:12:56,233 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-27 00:12:56,233 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.\n\nIf the user mentions vague locations like:\n- nearest city\n- near me\n- nearby\n- around here\n\nand user location is provided,\nrewrite the query using the nearest major city.\n\nOutput ONLY the rewritten query.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n\\nYou may answer ANY general knowledge question.\\n\\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n\\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n\\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "hi"}, {"role": "user", "content": "hi"}], "memory_action": {}, "memory_data": [], "tool_context": {}}'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=True, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.5'}, api_key=None, base_url=None)[0m
2026-01-27 00:12:56,233 | DEBUG | LiteLLM | 

2026-01-27 00:12:56,233 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-27 00:12:56,233 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-27 00:12:56,233 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-27 00:12:56,233 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 00:12:56,233 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-27 00:12:56,233 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.\n\nIf the user mentions vague locations like:\n- nearest city\n- near me\n- nearby\n- around here\n\nand user location is provided,\nrewrite the query using the nearest major city.\n\nOutput ONLY the rewritten query.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n\\nYou may answer ANY general knowledge question.\\n\\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n\\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n\\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "hi"}, {"role": "user", "content": "hi"}], "memory_action": {}, "memory_data": [], "tool_context": {}}'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-27 00:12:56,233 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': True}
2026-01-27 00:12:56,233 | DEBUG | LiteLLM | Final returned optional params: {'stream': True}
2026-01-27 00:12:56,233 | DEBUG | LiteLLM | self.optional_params: {'stream': True}
2026-01-27 00:12:56,233 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:12:56,233 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:12:56,233 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?key=*****=sse \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.5' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n\\nYou may answer ANY general knowledge question.\\n\\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n\\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n\\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "hi"}, {"role": "user", "content": "hi"}], "memory_action": {}, "memory_data": [], "tool_context": {}}'}]}], 'system_instruction': {'parts': [{'text': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.\n\nIf the user mentions vague locations like:\n- nearest city\n- near me\n- nearby\n- around here\n\nand user location is provided,\nrewrite the query using the nearest major city.\n\nOutput ONLY the rewritten query.'}]}, 'generationConfig': {}}'
[0m

2026-01-27 00:12:56,233 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 00:12:56,245 | DEBUG | LiteLLM | messages in token_counter: None, text in token_counter: 
2026-01-27 00:12:56,245 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:12:56,245 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:12:56,247 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:12:56,249 | DEBUG | LiteLLM | response_cost: 0.0
2026-01-27 00:12:56,462 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 00:12:56,462 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 00:12:56,464 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 00:12:57,857 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-27 00:12:57,939 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001F006415330>
2026-01-27 00:12:57,940 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x000001F07FA7B440> server_hostname='api.openai.com' timeout=5.0
2026-01-27 00:12:57,941 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001F006415300>
2026-01-27 00:12:57,941 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-27 00:12:57,941 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-27 00:12:57,941 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-27 00:12:57,941 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-27 00:12:57,941 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-27 00:12:58,825 | DEBUG | LiteLLM | RAW RESPONSE:
first stream response received


2026-01-27 00:12:58,825 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': 'Hi there! How can I help you today?'}], 'role': 'model'}, 'finishReason': 'STOP', 'index': 0}], 'usageMetadata': {'promptTokenCount': 302, 'candidatesTokenCount': 10, 'totalTokenCount': 332, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 302}], 'thoughtsTokenCount': 20}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'srV3adTiF7-L4-EP5ouuQQ'}
2026-01-27 00:12:58,825 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='srV3adTiF7-L4-EP5ouuQQ', created=1769452978, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='Hi there! How can I help you today?', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=30, prompt_tokens=302, total_tokens=332, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=20, rejected_prediction_tokens=None, text_tokens=10, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=302, image_tokens=None)))
2026-01-27 00:12:58,825 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content='Hi there! How can I help you today?', role='assistant', function_call=None, tool_calls=None, audio=None)
2026-01-27 00:12:58,825 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='srV3adTiF7-L4-EP5ouuQQ', created=1769452978, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Hi there! How can I help you today?', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=30, prompt_tokens=302, total_tokens=332, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=20, rejected_prediction_tokens=None, text_tokens=10, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=302, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-27 00:12:58,857 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=False
2026-01-27 00:12:58,864 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=False
2026-01-27 00:12:58,864 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-27 00:12:58,864 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-27 00:12:58,864 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 00:12:58,868 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 00:12:58,868 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:12:58,869 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:12:58,869 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:12:58,870 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:12:58,870 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:12:58,870 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:12:58,871 | DEBUG | LiteLLM | response_cost: 0.00016560000000000001
2026-01-27 00:12:58,871 | DEBUG | LiteLLM | response_cost: 0.00016560000000000001
2026-01-27 00:12:58,872 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 00:12:58,872 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.00016560000000000001
2026-01-27 00:12:58,872 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:12:58,873 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 00:12:58,874 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:12:58,874 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 00:12:58,877 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 00:12:58,877 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-27 00:12:58,877 | DEBUG | openai.agents | Resetting current trace
2026-01-27 00:12:58,881 | INFO | chat | ‚úÖ Stream complete | tokens=1 | time=6.19s
2026-01-27 00:12:59,292 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Mon, 26 Jan 2026 18:42:59 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_4e7df2253e2491b47e5171bc75138628'), (b'openai-processing-ms', b'429'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'user-u2cbhcbswv5bt4bmhvzdmhwd'), (b'openai-project', b'proj_oIIqE3fW3A3IDZXBAVOtc4gK'), (b'x-envoy-upstream-service-time', b'432'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=JUr_NQGI_b1ABciwpX6P1FEHr1J6uz6m8WINxyjpIQo-1769452979-1.0.1.1-_ff9QsRk_2PvikR.YWO53pptclrxQeq8_tbbF4KC.xz0OImheCDyysuzWox7R4vae_SefIDivX.4iw0GFzGFSmdBp5p0oDPLkjfIAtJnQqo; path=/; expires=Mon, 26-Jan-26 19:12:59 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=lRuREmYSvTpX0ybpVUwRkp1Mrg830jAJCKd2TX7C_e8-1769452979220-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c422737bba759c9-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-27 00:12:59,292 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-27 00:12:59,292 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-27 00:12:59,292 | DEBUG | httpcore.http11 | response_closed.started
2026-01-27 00:12:59,292 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-27 00:12:59,292 | DEBUG | openai.agents | Exported 7 items
2026-01-27 00:12:59,292 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-27 00:12:59,292 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-27 00:12:59,303 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-27 00:12:59,303 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-27 00:12:59,303 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-27 00:13:00,333 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Mon, 26 Jan 2026 18:43:00 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_674befcd1a8f9d4b968cae5bc8ddbae1'), (b'openai-processing-ms', b'132'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'user-u2cbhcbswv5bt4bmhvzdmhwd'), (b'openai-project', b'proj_oIIqE3fW3A3IDZXBAVOtc4gK'), (b'x-envoy-upstream-service-time', b'135'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4227403ff059c9-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-27 00:13:00,333 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-27 00:13:00,333 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-27 00:13:00,333 | DEBUG | httpcore.http11 | response_closed.started
2026-01-27 00:13:00,333 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-27 00:13:00,333 | DEBUG | openai.agents | Exported 2 items
2026-01-27 09:45:07,017 | DEBUG | LiteLLM | LoggingWorker cancelled during shutdown
2026-01-27 09:45:07,017 | DEBUG | LiteLLM | [LoggingWorker] atexit: Queue is empty
2026-01-27 09:45:07,017 | DEBUG | asyncio | Using proactor: IocpProactor
2026-01-27 09:45:07,017 | DEBUG | openai.agents | Shutting down trace provider
2026-01-27 09:45:07,017 | DEBUG | openai.agents | Shutting down trace processor <agents.tracing.processors.BatchTraceProcessor object at 0x000001F07FAA8790>
2026-01-27 09:50:47,760 | DEBUG | httpcore.connection | connect_tcp.started host='raw.githubusercontent.com' port=443 local_address=None timeout=5 socket_options=None
2026-01-27 09:50:47,834 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001AFAFEE28C0>
2026-01-27 09:50:47,834 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x000001AFAEC1E740> server_hostname='raw.githubusercontent.com' timeout=5
2026-01-27 09:50:47,863 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001AFAFEE2890>
2026-01-27 09:50:47,863 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'GET']>
2026-01-27 09:50:47,865 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-27 09:50:47,865 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'GET']>
2026-01-27 09:50:47,865 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-27 09:50:47,865 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'GET']>
2026-01-27 09:50:47,890 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Connection', b'keep-alive'), (b'Content-Length', b'62322'), (b'Cache-Control', b'max-age=300'), (b'Content-Security-Policy', b"default-src 'none'; style-src 'unsafe-inline'; sandbox"), (b'Content-Type', b'text/plain; charset=utf-8'), (b'ETag', b'W/"5f0be0ed2fe5ecf561c4b4488d64c304c35b980a45012ef1d1253b4b5d438af1"'), (b'Strict-Transport-Security', b'max-age=31536000'), (b'X-Content-Type-Options', b'nosniff'), (b'X-Frame-Options', b'deny'), (b'X-XSS-Protection', b'1; mode=block'), (b'X-GitHub-Request-Id', b'415E:3865D1:30D7E6:7A896E:69753835'), (b'Content-Encoding', b'gzip'), (b'Accept-Ranges', b'bytes'), (b'Date', b'Tue, 27 Jan 2026 04:20:48 GMT'), (b'Via', b'1.1 varnish'), (b'X-Served-By', b'cache-bom-vanm7210045-BOM'), (b'X-Cache', b'HIT'), (b'X-Cache-Hits', b'62073'), (b'X-Timer', b'S1769487648.129327,VS0,VE0'), (b'Vary', b'Authorization,Accept-Encoding'), (b'Access-Control-Allow-Origin', b'*'), (b'Cross-Origin-Resource-Policy', b'cross-origin'), (b'X-Fastly-Request-ID', b'abfd7d7ce8e633096c166a579956a4ef4b538a1d'), (b'Expires', b'Tue, 27 Jan 2026 04:25:48 GMT'), (b'Source-Age', b'100')])
2026-01-27 09:50:47,894 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'GET']>
2026-01-27 09:50:47,928 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-27 09:50:47,928 | DEBUG | httpcore.http11 | response_closed.started
2026-01-27 09:50:47,928 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-27 09:50:47,928 | DEBUG | httpcore.connection | close.started
2026-01-27 09:50:47,928 | DEBUG | httpcore.connection | close.complete
2026-01-27 09:50:48,641 | DEBUG | LiteLLM | [Non-Blocking] Unable to import GenericAPILogger - LiteLLM Enterprise Feature - No module named 'litellm_enterprise'
2026-01-27 09:50:49,187 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 09:50:49,188 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 09:50:49,188 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 09:50:49,197 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 09:50:49,197 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 09:50:49,197 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 09:50:49,198 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 09:50:49,198 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 09:50:49,198 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 09:50:49,199 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 09:50:49,199 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 09:50:49,199 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 09:50:49,321 | INFO | main | üöÄ FastAPI application starting
2026-01-27 09:50:49,370 | INFO | main | üóÑÔ∏è Database tables ensured
2026-01-27 12:32:38,527 | DEBUG | asyncio | Using proactor: IocpProactor
2026-01-27 12:32:38,528 | DEBUG | openai.agents | Shutting down trace provider
2026-01-27 12:32:38,529 | DEBUG | openai.agents | Shutting down trace processor <agents.tracing.processors.BatchTraceProcessor object at 0x0000016F49885A20>
2026-01-27 12:33:16,888 | DEBUG | httpcore.connection | connect_tcp.started host='raw.githubusercontent.com' port=443 local_address=None timeout=5 socket_options=None
2026-01-27 12:33:16,936 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001491F517D90>
2026-01-27 12:33:16,937 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x000001491E25A7C0> server_hostname='raw.githubusercontent.com' timeout=5
2026-01-27 12:33:16,988 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001491F517D60>
2026-01-27 12:33:16,989 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'GET']>
2026-01-27 12:33:16,989 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-27 12:33:16,991 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'GET']>
2026-01-27 12:33:16,991 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-27 12:33:16,991 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'GET']>
2026-01-27 12:33:17,034 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Connection', b'keep-alive'), (b'Content-Length', b'62322'), (b'Cache-Control', b'max-age=300'), (b'Content-Security-Policy', b"default-src 'none'; style-src 'unsafe-inline'; sandbox"), (b'Content-Type', b'text/plain; charset=utf-8'), (b'ETag', b'W/"5f0be0ed2fe5ecf561c4b4488d64c304c35b980a45012ef1d1253b4b5d438af1"'), (b'Strict-Transport-Security', b'max-age=31536000'), (b'X-Content-Type-Options', b'nosniff'), (b'X-Frame-Options', b'deny'), (b'X-XSS-Protection', b'1; mode=block'), (b'X-GitHub-Request-Id', b'415E:3865D1:30D7E6:7A896E:69753835'), (b'Content-Encoding', b'gzip'), (b'Accept-Ranges', b'bytes'), (b'Date', b'Tue, 27 Jan 2026 07:03:16 GMT'), (b'Via', b'1.1 varnish'), (b'X-Served-By', b'cache-bom-vanm7210067-BOM'), (b'X-Cache', b'HIT'), (b'X-Cache-Hits', b'43'), (b'X-Timer', b'S1769497397.926682,VS0,VE0'), (b'Vary', b'Authorization,Accept-Encoding'), (b'Access-Control-Allow-Origin', b'*'), (b'Cross-Origin-Resource-Policy', b'cross-origin'), (b'X-Fastly-Request-ID', b'a7f5d0ca3005c443d201b28a569fccde863c8084'), (b'Expires', b'Tue, 27 Jan 2026 07:08:16 GMT'), (b'Source-Age', b'235')])
2026-01-27 12:33:17,035 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'GET']>
2026-01-27 12:33:17,071 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-27 12:33:17,072 | DEBUG | httpcore.http11 | response_closed.started
2026-01-27 12:33:17,072 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-27 12:33:17,073 | DEBUG | httpcore.connection | close.started
2026-01-27 12:33:17,073 | DEBUG | httpcore.connection | close.complete
2026-01-27 12:33:18,201 | DEBUG | LiteLLM | [Non-Blocking] Unable to import GenericAPILogger - LiteLLM Enterprise Feature - No module named 'litellm_enterprise'
2026-01-27 12:33:18,680 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 12:33:18,680 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 12:33:18,681 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 12:33:18,687 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 12:33:18,687 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 12:33:18,687 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 12:33:18,689 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 12:33:18,690 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 12:33:18,690 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 12:33:18,690 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 12:33:18,691 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 12:33:18,691 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 12:33:18,848 | INFO | main | üöÄ FastAPI application starting
2026-01-27 12:33:32,890 | INFO | chat | üì© Chat request | user_id=3 | request_id=a180176a-84c7-4bb0-8806-9d4ee7da7a9e
2026-01-27 12:33:32,905 | INFO | chat | üßë User message | hello
2026-01-27 12:33:32,935 | INFO | chat | üÜï New session created | 28c9dd48-f74c-4801-be14-d38892a91067
2026-01-27 12:36:32,238 | DEBUG | asyncio | Using proactor: IocpProactor
2026-01-27 12:36:32,240 | DEBUG | openai.agents | Shutting down trace provider
2026-01-27 12:36:32,242 | DEBUG | openai.agents | Shutting down trace processor <agents.tracing.processors.BatchTraceProcessor object at 0x000001491D0E9F60>
2026-01-27 12:36:40,938 | DEBUG | httpcore.connection | connect_tcp.started host='raw.githubusercontent.com' port=443 local_address=None timeout=5 socket_options=None
2026-01-27 12:36:40,992 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000207DEC73E20>
2026-01-27 12:36:40,993 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x00000207DD9BE7C0> server_hostname='raw.githubusercontent.com' timeout=5
2026-01-27 12:36:41,027 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000207DEC73DF0>
2026-01-27 12:36:41,028 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'GET']>
2026-01-27 12:36:41,029 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-27 12:36:41,030 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'GET']>
2026-01-27 12:36:41,032 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-27 12:36:41,032 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'GET']>
2026-01-27 12:36:41,058 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Connection', b'keep-alive'), (b'Content-Length', b'62322'), (b'Cache-Control', b'max-age=300'), (b'Content-Security-Policy', b"default-src 'none'; style-src 'unsafe-inline'; sandbox"), (b'Content-Type', b'text/plain; charset=utf-8'), (b'ETag', b'W/"5f0be0ed2fe5ecf561c4b4488d64c304c35b980a45012ef1d1253b4b5d438af1"'), (b'Strict-Transport-Security', b'max-age=31536000'), (b'X-Content-Type-Options', b'nosniff'), (b'X-Frame-Options', b'deny'), (b'X-XSS-Protection', b'1; mode=block'), (b'X-GitHub-Request-Id', b'415E:3865D1:30D7E6:7A896E:69753835'), (b'Content-Encoding', b'gzip'), (b'Accept-Ranges', b'bytes'), (b'Date', b'Tue, 27 Jan 2026 07:06:40 GMT'), (b'Via', b'1.1 varnish'), (b'X-Served-By', b'cache-bom-vanm7210043-BOM'), (b'X-Cache', b'HIT'), (b'X-Cache-Hits', b'13'), (b'X-Timer', b'S1769497601.964756,VS0,VE0'), (b'Vary', b'Authorization,Accept-Encoding'), (b'Access-Control-Allow-Origin', b'*'), (b'Cross-Origin-Resource-Policy', b'cross-origin'), (b'X-Fastly-Request-ID', b'7eb9859471d9a5acf771d36574bb5ed915a215d9'), (b'Expires', b'Tue, 27 Jan 2026 07:11:40 GMT'), (b'Source-Age', b'139')])
2026-01-27 12:36:41,060 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'GET']>
2026-01-27 12:36:41,095 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-27 12:36:41,096 | DEBUG | httpcore.http11 | response_closed.started
2026-01-27 12:36:41,097 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-27 12:36:41,098 | DEBUG | httpcore.connection | close.started
2026-01-27 12:36:41,099 | DEBUG | httpcore.connection | close.complete
2026-01-27 12:36:44,137 | DEBUG | LiteLLM | [Non-Blocking] Unable to import GenericAPILogger - LiteLLM Enterprise Feature - No module named 'litellm_enterprise'
2026-01-27 12:36:46,080 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 12:36:46,082 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 12:36:46,083 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 12:36:46,113 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 12:36:46,115 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 12:36:46,118 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 12:36:46,120 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 12:36:46,121 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 12:36:46,122 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 12:36:46,125 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 12:36:46,127 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 12:36:46,129 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 12:36:46,484 | INFO | main | üöÄ FastAPI application starting
2026-01-27 12:38:59,195 | DEBUG | asyncio | Using proactor: IocpProactor
2026-01-27 12:38:59,196 | DEBUG | openai.agents | Shutting down trace provider
2026-01-27 12:38:59,197 | DEBUG | openai.agents | Shutting down trace processor <agents.tracing.processors.BatchTraceProcessor object at 0x00000207DC849FC0>
2026-01-27 12:39:04,051 | DEBUG | httpcore.connection | connect_tcp.started host='raw.githubusercontent.com' port=443 local_address=None timeout=5 socket_options=None
2026-01-27 12:39:04,126 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000020E9A1C3EB0>
2026-01-27 12:39:04,127 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000020E98F0E7C0> server_hostname='raw.githubusercontent.com' timeout=5
2026-01-27 12:39:04,161 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000020E9A1C3E80>
2026-01-27 12:39:04,161 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'GET']>
2026-01-27 12:39:04,162 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-27 12:39:04,162 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'GET']>
2026-01-27 12:39:04,162 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-27 12:39:04,163 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'GET']>
2026-01-27 12:39:04,192 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Connection', b'keep-alive'), (b'Content-Length', b'62322'), (b'Cache-Control', b'max-age=300'), (b'Content-Security-Policy', b"default-src 'none'; style-src 'unsafe-inline'; sandbox"), (b'Content-Type', b'text/plain; charset=utf-8'), (b'ETag', b'W/"5f0be0ed2fe5ecf561c4b4488d64c304c35b980a45012ef1d1253b4b5d438af1"'), (b'Strict-Transport-Security', b'max-age=31536000'), (b'X-Content-Type-Options', b'nosniff'), (b'X-Frame-Options', b'deny'), (b'X-XSS-Protection', b'1; mode=block'), (b'X-GitHub-Request-Id', b'415E:3865D1:30D7E6:7A896E:69753835'), (b'Content-Encoding', b'gzip'), (b'Accept-Ranges', b'bytes'), (b'Date', b'Tue, 27 Jan 2026 07:09:04 GMT'), (b'Via', b'1.1 varnish'), (b'X-Served-By', b'cache-bom-vanm7210066-BOM'), (b'X-Cache', b'HIT'), (b'X-Cache-Hits', b'40'), (b'X-Timer', b'S1769497744.098086,VS0,VE0'), (b'Vary', b'Authorization,Accept-Encoding'), (b'Access-Control-Allow-Origin', b'*'), (b'Cross-Origin-Resource-Policy', b'cross-origin'), (b'X-Fastly-Request-ID', b'84665a74697e00cd7a87a17ef782ad7d32713be7'), (b'Expires', b'Tue, 27 Jan 2026 07:14:04 GMT'), (b'Source-Age', b'282')])
2026-01-27 12:39:04,194 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'GET']>
2026-01-27 12:39:04,236 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-27 12:39:04,236 | DEBUG | httpcore.http11 | response_closed.started
2026-01-27 12:39:04,237 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-27 12:39:04,238 | DEBUG | httpcore.connection | close.started
2026-01-27 12:39:04,238 | DEBUG | httpcore.connection | close.complete
2026-01-27 12:39:06,497 | DEBUG | LiteLLM | [Non-Blocking] Unable to import GenericAPILogger - LiteLLM Enterprise Feature - No module named 'litellm_enterprise'
2026-01-27 12:39:08,109 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 12:39:08,111 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 12:39:08,112 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 12:39:08,136 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 12:39:08,138 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 12:39:08,139 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 12:39:08,141 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 12:39:08,142 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 12:39:08,145 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 12:39:08,147 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 12:39:08,147 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 12:39:08,148 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 12:39:08,509 | INFO | main | üöÄ FastAPI application starting
2026-01-27 12:39:09,926 | INFO | chat | üì© Chat request | user_id=3 | request_id=5941c3ef-1a92-41a6-b1f9-124733c80f7e
2026-01-27 12:39:10,027 | INFO | chat | üßë User message | hello
2026-01-27 12:39:10,052 | INFO | chat | üÜï New session created | 7eb29f18-be52-4614-997c-ecd4ba3cc0ec
2026-01-27 12:40:18,419 | DEBUG | asyncio | Using proactor: IocpProactor
2026-01-27 12:40:18,421 | DEBUG | openai.agents | Shutting down trace provider
2026-01-27 12:40:18,421 | DEBUG | openai.agents | Shutting down trace processor <agents.tracing.processors.BatchTraceProcessor object at 0x0000020E97D99FC0>
2026-01-27 12:55:37,606 | INFO | sentence_transformers.SentenceTransformer | Use pytorch device_name: cpu
2026-01-27 12:55:37,607 | INFO | sentence_transformers.SentenceTransformer | Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2026-01-27 12:55:37,610 | DEBUG | urllib3.connectionpool | Starting new HTTPS connection (1): huggingface.co:443
2026-01-27 12:55:37,901 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-27 12:55:37,915 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-27 12:55:38,161 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-27 12:55:38,175 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-27 12:55:38,417 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-27 12:55:38,430 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-27 12:55:38,665 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md HTTP/1.1" 307 0
2026-01-27 12:55:38,677 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/README.md HTTP/1.1" 200 0
2026-01-27 12:55:38,926 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-27 12:55:38,941 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-27 12:55:39,189 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json HTTP/1.1" 307 0
2026-01-27 12:55:39,207 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/sentence_bert_config.json HTTP/1.1" 200 0
2026-01-27 12:55:39,444 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json HTTP/1.1" 404 0
2026-01-27 12:55:39,701 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1" 307 0
2026-01-27 12:55:39,711 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json HTTP/1.1" 200 0
2026-01-27 12:55:40,021 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/1.1" 307 0
2026-01-27 12:55:40,032 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json HTTP/1.1" 200 0
2026-01-27 12:55:40,283 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1" 404 64
2026-01-27 12:55:40,536 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main?recursive=True&expand=False HTTP/1.1" 200 6465
2026-01-27 12:55:40,794 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json HTTP/1.1" 307 0
2026-01-27 12:55:40,807 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json HTTP/1.1" 200 0
2026-01-27 12:55:41,044 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2 HTTP/1.1" 200 6825
2026-01-27 12:55:42,098 | DEBUG | httpcore.connection | connect_tcp.started host='raw.githubusercontent.com' port=443 local_address=None timeout=5 socket_options=None
2026-01-27 12:55:42,154 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000255087213F0>
2026-01-27 12:55:42,154 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000025508565AC0> server_hostname='raw.githubusercontent.com' timeout=5
2026-01-27 12:55:42,188 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000255087213C0>
2026-01-27 12:55:42,188 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'GET']>
2026-01-27 12:55:42,189 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-27 12:55:42,189 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'GET']>
2026-01-27 12:55:42,189 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-27 12:55:42,189 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'GET']>
2026-01-27 12:55:42,221 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Connection', b'keep-alive'), (b'Content-Length', b'62322'), (b'Cache-Control', b'max-age=300'), (b'Content-Security-Policy', b"default-src 'none'; style-src 'unsafe-inline'; sandbox"), (b'Content-Type', b'text/plain; charset=utf-8'), (b'ETag', b'W/"5f0be0ed2fe5ecf561c4b4488d64c304c35b980a45012ef1d1253b4b5d438af1"'), (b'Strict-Transport-Security', b'max-age=31536000'), (b'X-Content-Type-Options', b'nosniff'), (b'X-Frame-Options', b'deny'), (b'X-XSS-Protection', b'1; mode=block'), (b'X-GitHub-Request-Id', b'415E:3865D1:30D7E6:7A896E:69753835'), (b'Content-Encoding', b'gzip'), (b'Accept-Ranges', b'bytes'), (b'Date', b'Tue, 27 Jan 2026 07:25:42 GMT'), (b'Via', b'1.1 varnish'), (b'X-Served-By', b'cache-bom-vanm7210071-BOM'), (b'X-Cache', b'HIT'), (b'X-Cache-Hits', b'11'), (b'X-Timer', b'S1769498742.124446,VS0,VE0'), (b'Vary', b'Authorization,Accept-Encoding'), (b'Access-Control-Allow-Origin', b'*'), (b'Cross-Origin-Resource-Policy', b'cross-origin'), (b'X-Fastly-Request-ID', b'bb590926fad2fad88191944e3c439268037e47b2'), (b'Expires', b'Tue, 27 Jan 2026 07:30:42 GMT'), (b'Source-Age', b'78')])
2026-01-27 12:55:42,222 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'GET']>
2026-01-27 12:55:42,261 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-27 12:55:42,261 | DEBUG | httpcore.http11 | response_closed.started
2026-01-27 12:55:42,261 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-27 12:55:42,262 | DEBUG | httpcore.connection | close.started
2026-01-27 12:55:42,263 | DEBUG | httpcore.connection | close.complete
2026-01-27 12:55:42,876 | DEBUG | LiteLLM | [Non-Blocking] Unable to import GenericAPILogger - LiteLLM Enterprise Feature - No module named 'litellm_enterprise'
2026-01-27 12:55:43,284 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 12:55:43,286 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 12:55:43,286 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 12:55:43,291 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 12:55:43,292 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 12:55:43,292 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 12:55:43,293 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 12:55:43,293 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 12:55:43,293 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 12:55:43,294 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 12:55:43,294 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 12:55:43,294 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 12:55:43,333 | INFO | main | üöÄ FastAPI application starting
2026-01-27 12:55:53,070 | INFO | chat | üì© Chat request | user_id=3 | request_id=f5a1fa0d-d5a2-420b-984f-fc9797225d11
2026-01-27 12:55:53,079 | INFO | chat | üßë User message | hello
2026-01-27 12:55:53,106 | INFO | chat | üÜï New session created | 0ba7a5e8-8a18-47df-9ea1-9282cf4c76ae
2026-01-27 12:55:53,147 | INFO | chat | ü§ñ Stream started | session_id=0ba7a5e8-8a18-47df-9ea1-9282cf4c76ae | request_id=f5a1fa0d-d5a2-420b-984f-fc9797225d11
2026-01-27 12:55:53,149 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-27 12:55:53,157 | INFO | orchestrator | üßë USER_INPUT | hello
2026-01-27 12:55:53,162 | INFO | orchestrator | üß© CONTEXT | user_id=3 | enable_memory=True | db=None
2026-01-27 12:55:53,167 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-27 12:55:53,175 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_1c421abc7d4248d886da7fd9ff2a7500
2026-01-27 12:55:53,182 | DEBUG | openai.agents | Setting current trace: trace_1c421abc7d4248d886da7fd9ff2a7500
2026-01-27 12:55:53,187 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000002551182A8E0> with id None
2026-01-27 12:55:53,188 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-27 12:55:53,222 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000002551185B940> with id None
2026-01-27 12:55:53,223 | DEBUG | openai.agents | Calling LLM
2026-01-27 12:55:53,225 | DEBUG | LiteLLM | 

2026-01-27 12:55:53,232 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-27 12:55:53,237 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n', 'role': 'system'}, {'role': 'user', 'content': 'hello'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-27 12:55:53,241 | DEBUG | LiteLLM | 

2026-01-27 12:55:53,242 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-27 12:55:53,244 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-27 12:55:53,244 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-27 12:55:53,272 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 12:55:53,279 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-27 12:55:53,286 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n', 'role': 'system'}, {'role': 'user', 'content': 'hello'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-27 12:55:53,287 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-27 12:55:53,290 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-27 12:55:53,299 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-27 12:55:53,305 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:55:53,704 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:55:53,706 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'hello'}]}], 'system_instruction': {'parts': [{'text': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n'}]}, 'generationConfig': {}}'
[0m

2026-01-27 12:55:54,913 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "SAFE"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 188,
    "candidatesTokenCount": 1,
    "totalTokenCount": 223,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 188
      }
    ],
    "thoughtsTokenCount": 34
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "gmh4acfUIrrR4-EPzLOd4QI"
}



2026-01-27 12:55:54,920 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-27 12:55:54,921 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-27 12:55:54,922 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 12:55:54,923 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 12:55:54,925 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:55:54,926 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:55:54,927 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:55:54,928 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:55:54,928 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 12:55:54,929 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 12:55:54,930 | DEBUG | LiteLLM | response_cost: 0.0001439
2026-01-27 12:55:54,931 | DEBUG | LiteLLM | response_cost: 0.0001439
2026-01-27 12:55:54,932 | DEBUG | openai.agents | Received model response
2026-01-27 12:55:54,935 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 12:55:54,966 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x0000025511988130>>
2026-01-27 12:55:54,966 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:55:54,969 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 12:55:54,970 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-27 12:55:54,971 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-27 12:55:54,976 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-27 12:55:54,975 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 12:55:54,975 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-27 12:55:54,977 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-27 12:55:54,977 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:55:54,978 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 12:55:54,981 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 12:55:54,982 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 12:55:54,982 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:55:54,984 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:55:54,985 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 12:55:54,985 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 12:55:54,986 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-27 12:55:54,987 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0001439
2026-01-27 12:55:54,991 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 12:55:54,992 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:55:54,992 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 12:55:54,993 | DEBUG | openai.agents | Resetting current trace
2026-01-27 12:55:54,994 | ERROR | digital_human_sdk.app.intelligence.utils.json_utils | safe_json_loads failed
2026-01-27 12:55:54,995 | INFO | orchestrator | ‚úÖ Safety passed
2026-01-27 12:55:54,997 | INFO | orchestrator | üì• Fetching semantic memory (via MemoryService)
2026-01-27 12:55:55,071 | INFO | orchestrator | üß† MEMORY_RESULT | found=False | count=0
2026-01-27 12:55:55,072 | INFO | orchestrator | üß≠ Router agent called
2026-01-27 12:55:55,074 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_8e196575314f4af8a609a8ef955c706e
2026-01-27 12:55:55,074 | DEBUG | openai.agents | Setting current trace: trace_8e196575314f4af8a609a8ef955c706e
2026-01-27 12:55:55,074 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x0000025511F54130> with id None
2026-01-27 12:55:55,076 | DEBUG | openai.agents | Running agent RouterAgent (turn 1)
2026-01-27 12:55:55,078 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000025511886200> with id None
2026-01-27 12:55:55,078 | DEBUG | openai.agents | Calling LLM
2026-01-27 12:55:55,079 | DEBUG | LiteLLM | 

2026-01-27 12:55:55,080 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-27 12:55:55,082 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'hello'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-27 12:55:55,084 | DEBUG | LiteLLM | 

2026-01-27 12:55:55,086 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-27 12:55:55,086 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-27 12:55:55,087 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-27 12:55:55,089 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 12:55:55,089 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-27 12:55:55,090 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'hello'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-27 12:55:55,090 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-27 12:55:55,091 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-27 12:55:55,091 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-27 12:55:55,092 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:55:55,093 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:55:55,097 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'hello'}]}], 'system_instruction': {'parts': [{'text': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n'}]}, 'generationConfig': {}}'
[0m

2026-01-27 12:55:57,165 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\"use_memory\": false, \"use_tool\": false, \"tool_name\": \"none\", \"tool_arguments\": null, \"intent\": \"none\", \"memory_key\": null}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 486,
    "candidatesTokenCount": 42,
    "totalTokenCount": 586,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 486
      }
    ],
    "thoughtsTokenCount": 58
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "hGh4acjUM76E4-EPs6fIkQ4"
}



2026-01-27 12:55:57,168 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-27 12:55:57,168 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 12:55:57,168 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-27 12:55:57,169 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:55:57,169 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 12:55:57,169 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:55:57,170 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:55:57,170 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 12:55:57,170 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:55:57,171 | DEBUG | LiteLLM | response_cost: 0.00039579999999999997
2026-01-27 12:55:57,171 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 12:55:57,171 | DEBUG | openai.agents | Received model response
2026-01-27 12:55:57,171 | DEBUG | LiteLLM | response_cost: 0.00039579999999999997
2026-01-27 12:55:57,172 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x0000025511989CF0>>
2026-01-27 12:55:57,172 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 12:55:57,174 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-27 12:55:57,174 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:55:57,175 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-27 12:55:57,174 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-27 12:55:57,175 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 12:55:57,175 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-27 12:55:57,176 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 12:55:57,176 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-27 12:55:57,176 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 12:55:57,176 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:55:57,177 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 12:55:57,177 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:55:57,178 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 12:55:57,178 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:55:57,178 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 12:55:57,179 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 12:55:57,181 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-27 12:55:57,183 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.00039579999999999997
2026-01-27 12:55:57,185 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 12:55:57,185 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:55:57,185 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 12:55:57,186 | DEBUG | openai.agents | Resetting current trace
2026-01-27 12:55:57,186 | INFO | orchestrator | üß≠ ROUTER_DECISION | tool=False | memory=False | intent=none
2026-01-27 12:55:57,189 | INFO | orchestrator | üß† REASONING_INPUT_SNAPSHOT
{
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful AI assistant.\n \nYou may answer ANY general knowledge question.\n \nConversation summaries and memories are ONLY to provide helpful context\n(e.g. preferences, ongoing plans).\n \nIf a user asks something unrelated to the current topic,\nanswer it normally using general knowledge.\n \nDo NOT restrict yourself to previous topics unless the user explicitly asks."
    },
    {
      "role": "user",
      "content": "hello"
    },
    {
      "role": "user",
      "content": "hello"
    }
  ],
  "safety": {
    "safe": true,
    "message": null
  },
  "memory_action": {},
  "memory_data": [],
  "memory_found": false,
  "tool_context": {}
}
2026-01-27 12:55:57,191 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_3797095eef9a406b95e6470d3c983354
2026-01-27 12:55:57,191 | DEBUG | openai.agents | Setting current trace: trace_3797095eef9a406b95e6470d3c983354
2026-01-27 12:55:57,191 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x0000025512437B50> with id None
2026-01-27 12:55:57,193 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000025512610F40> with id None
2026-01-27 12:55:57,193 | DEBUG | openai.agents | Calling LLM
2026-01-27 12:55:57,193 | DEBUG | LiteLLM | 

2026-01-27 12:55:57,197 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-27 12:55:57,197 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "hello"}, {"role": "user", "content": "hello"}], "safety": {"safe": true, "message": null}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=True, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-27 12:55:57,197 | DEBUG | LiteLLM | 

2026-01-27 12:55:57,198 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-27 12:55:57,198 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-27 12:55:57,198 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-27 12:55:57,200 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 12:55:57,201 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-27 12:55:57,202 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "hello"}, {"role": "user", "content": "hello"}], "safety": {"safe": true, "message": null}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-27 12:55:57,202 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': True}
2026-01-27 12:55:57,204 | DEBUG | LiteLLM | Final returned optional params: {'stream': True}
2026-01-27 12:55:57,205 | DEBUG | LiteLLM | self.optional_params: {'stream': True}
2026-01-27 12:55:57,205 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:55:57,206 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:55:57,206 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?key=*****=sse \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "hello"}, {"role": "user", "content": "hello"}], "safety": {"safe": true, "message": null}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}]}], 'system_instruction': {'parts': [{'text': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.'}]}, 'generationConfig': {}}'
[0m

2026-01-27 12:55:57,207 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 12:55:57,210 | DEBUG | LiteLLM | messages in token_counter: None, text in token_counter: 
2026-01-27 12:55:57,210 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:55:57,211 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:55:57,211 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 12:55:57,212 | DEBUG | LiteLLM | response_cost: 0.0
2026-01-27 12:55:57,243 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 12:55:57,243 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 12:55:57,244 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 12:55:58,190 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-27 12:55:58,221 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002551177ADA0>
2026-01-27 12:55:58,221 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x00000255064B9540> server_hostname='api.openai.com' timeout=5.0
2026-01-27 12:55:58,233 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002551177ACE0>
2026-01-27 12:55:58,234 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-27 12:55:58,234 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-27 12:55:58,234 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-27 12:55:58,234 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-27 12:55:58,235 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-27 12:55:58,556 | DEBUG | LiteLLM | RAW RESPONSE:
first stream response received


2026-01-27 12:55:58,557 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': 'Hello! How can I'}], 'role': 'model'}, 'index': 0}], 'usageMetadata': {'promptTokenCount': 274, 'candidatesTokenCount': 5, 'totalTokenCount': 293, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 274}], 'thoughtsTokenCount': 14}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'hmh4aY2QDNPLqfkPoIiMgA4'}
2026-01-27 12:55:58,559 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='hmh4aY2QDNPLqfkPoIiMgA4', created=1769498758, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='Hello! How can I', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=19, prompt_tokens=274, total_tokens=293, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=14, rejected_prediction_tokens=None, text_tokens=5, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=274, image_tokens=None)))
2026-01-27 12:55:58,561 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content='Hello! How can I', role='assistant', function_call=None, tool_calls=None, audio=None)
2026-01-27 12:55:58,562 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='hmh4aY2QDNPLqfkPoIiMgA4', created=1769498758, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Hello! How can I', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=19, prompt_tokens=274, total_tokens=293, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=14, rejected_prediction_tokens=None, text_tokens=5, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=274, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-27 12:55:58,578 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': ' help you today?'}], 'role': 'model'}, 'finishReason': 'STOP', 'index': 0}], 'usageMetadata': {'promptTokenCount': 274, 'candidatesTokenCount': 9, 'totalTokenCount': 297, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 274}], 'thoughtsTokenCount': 14}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'hmh4aY2QDNPLqfkPoIiMgA4'}
2026-01-27 12:55:58,579 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='hmh4aY2QDNPLqfkPoIiMgA4', created=1769498758, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content=' help you today?', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=23, prompt_tokens=274, total_tokens=297, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=14, rejected_prediction_tokens=None, text_tokens=9, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=274, image_tokens=None)))
2026-01-27 12:55:58,580 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content=' help you today?', role=None, function_call=None, tool_calls=None, audio=None)
2026-01-27 12:55:58,581 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='hmh4aY2QDNPLqfkPoIiMgA4', created=1769498758, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' help you today?', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=23, prompt_tokens=274, total_tokens=297, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=14, rejected_prediction_tokens=None, text_tokens=9, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=274, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-27 12:55:58,594 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=False
2026-01-27 12:55:58,598 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=False
2026-01-27 12:55:58,598 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-27 12:55:58,599 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-27 12:55:58,599 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 12:55:58,599 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 12:55:58,599 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:55:58,601 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:55:58,601 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:55:58,602 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:55:58,602 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 12:55:58,603 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 12:55:58,605 | DEBUG | LiteLLM | response_cost: 0.00013969999999999998
2026-01-27 12:55:58,605 | DEBUG | LiteLLM | response_cost: 0.00013969999999999998
2026-01-27 12:55:58,606 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 12:55:58,606 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.00013969999999999998
2026-01-27 12:55:58,607 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:55:58,607 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 12:55:58,608 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 12:55:58,608 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:55:58,609 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 12:55:58,609 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-27 12:55:58,609 | DEBUG | openai.agents | Resetting current trace
2026-01-27 12:55:58,616 | INFO | session_summary | üß† SUMMARY_START | session_id=0ba7a5e8-8a18-47df-9ea1-9282cf4c76ae
2026-01-27 12:55:58,626 | INFO | session_summary | üìä Unsummarized messages = 2
2026-01-27 12:55:58,627 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-27 12:55:58,627 | INFO | chat | ‚úÖ Stream complete | tokens=2 | time=5.56s
2026-01-27 12:56:00,214 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Tue, 27 Jan 2026 07:26:00 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_a569813fd47e9fb319b0318c94de10fe'), (b'openai-processing-ms', b'809'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'812'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=sApYiPz7PkLRjgmYFeS3.qMwiCoS0Q5x5EutIxYUDnc-1769498760-1.0.1.1-VgrngSrSEPIisaLCz8V5F89DzpEbF0rVrH22O_bwg7qezn27PpIwF3c1S8RJUvoUmuu38diqovUfTdIA6IJyqWK4xqhFA90SRvSxUvQwA0U; path=/; expires=Tue, 27-Jan-26 07:56:00 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=c3c5HGpwb7a_5v8Q2.FMDgTXlx82cTBEDZcGCbHnpf8-1769498760107-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4684e65b2c91ca-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-27 12:56:00,216 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-27 12:56:00,216 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-27 12:56:00,216 | DEBUG | httpcore.http11 | response_closed.started
2026-01-27 12:56:00,217 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-27 12:56:00,217 | DEBUG | openai.agents | Exported 7 items
2026-01-27 12:56:00,217 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-27 12:56:00,218 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-27 12:56:00,218 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-27 12:56:00,218 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-27 12:56:00,218 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-27 12:56:01,444 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Tue, 27 Jan 2026 07:26:01 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_e063ddd5c430712f43589f36b098cda2'), (b'openai-processing-ms', b'306'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'310'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4684f2db8791ca-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-27 12:56:01,445 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-27 12:56:01,445 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-27 12:56:01,445 | DEBUG | httpcore.http11 | response_closed.started
2026-01-27 12:56:01,445 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-27 12:56:01,446 | DEBUG | openai.agents | Exported 2 items
2026-01-27 12:56:11,255 | INFO | chat | üì© Chat request | user_id=3 | request_id=37e20afb-db17-4bc4-b71d-e3f65a2985da
2026-01-27 12:56:11,256 | INFO | chat | üßë User message | what is my favourite color?
2026-01-27 12:56:11,282 | INFO | chat | ü§ñ Stream started | session_id=0ba7a5e8-8a18-47df-9ea1-9282cf4c76ae | request_id=37e20afb-db17-4bc4-b71d-e3f65a2985da
2026-01-27 12:56:11,286 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-27 12:56:11,289 | INFO | orchestrator | üßë USER_INPUT | what is my favourite color?
2026-01-27 12:56:11,290 | INFO | orchestrator | üß© CONTEXT | user_id=3 | enable_memory=True | db=None
2026-01-27 12:56:11,291 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-27 12:56:11,294 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_fafd3bb795364ad690b4d09d5f674b03
2026-01-27 12:56:11,294 | DEBUG | openai.agents | Setting current trace: trace_fafd3bb795364ad690b4d09d5f674b03
2026-01-27 12:56:11,295 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x0000025511E201D0> with id None
2026-01-27 12:56:11,295 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-27 12:56:11,296 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000025511711600> with id None
2026-01-27 12:56:11,296 | DEBUG | openai.agents | Calling LLM
2026-01-27 12:56:11,297 | DEBUG | LiteLLM | 

2026-01-27 12:56:11,297 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-27 12:56:11,300 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n', 'role': 'system'}, {'role': 'user', 'content': 'what is my favourite color?'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-27 12:56:11,300 | DEBUG | LiteLLM | 

2026-01-27 12:56:11,302 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-27 12:56:11,305 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-27 12:56:11,306 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-27 12:56:11,308 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 12:56:11,309 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-27 12:56:11,310 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n', 'role': 'system'}, {'role': 'user', 'content': 'what is my favourite color?'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-27 12:56:11,312 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-27 12:56:11,313 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-27 12:56:11,313 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-27 12:56:11,314 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:11,315 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:11,316 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'what is my favourite color?'}]}], 'system_instruction': {'parts': [{'text': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n'}]}, 'generationConfig': {}}'
[0m

2026-01-27 12:56:11,580 | DEBUG | httpcore.connection | close.started
2026-01-27 12:56:11,581 | DEBUG | httpcore.connection | close.complete
2026-01-27 12:56:11,581 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-27 12:56:11,610 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000255120B3070>
2026-01-27 12:56:11,610 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x00000255064B9540> server_hostname='api.openai.com' timeout=5.0
2026-01-27 12:56:11,627 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000255120B2CE0>
2026-01-27 12:56:11,628 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-27 12:56:11,628 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-27 12:56:11,629 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-27 12:56:11,629 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-27 12:56:11,629 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-27 12:56:12,487 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "SAFE"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 193,
    "candidatesTokenCount": 1,
    "totalTokenCount": 245,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 193
      }
    ],
    "thoughtsTokenCount": 51
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "lGh4abGUDaG14-EPgIeNqQ4"
}



2026-01-27 12:56:12,491 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-27 12:56:12,492 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 12:56:12,492 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-27 12:56:12,493 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:12,493 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 12:56:12,494 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:12,495 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:12,495 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 12:56:12,496 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:12,497 | DEBUG | LiteLLM | response_cost: 0.00018790000000000001
2026-01-27 12:56:12,497 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 12:56:12,498 | DEBUG | openai.agents | Received model response
2026-01-27 12:56:12,498 | DEBUG | LiteLLM | response_cost: 0.00018790000000000001
2026-01-27 12:56:12,499 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x00000255126C84C0>>
2026-01-27 12:56:12,501 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 12:56:12,502 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-27 12:56:12,503 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:12,504 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-27 12:56:12,504 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-27 12:56:12,504 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 12:56:12,505 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-27 12:56:12,508 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 12:56:12,509 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-27 12:56:12,510 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 12:56:12,511 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:12,516 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 12:56:12,517 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:12,518 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 12:56:12,518 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:12,518 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 12:56:12,519 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 12:56:12,521 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-27 12:56:12,525 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.00018790000000000001
2026-01-27 12:56:12,527 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 12:56:12,527 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:12,528 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 12:56:12,529 | DEBUG | openai.agents | Resetting current trace
2026-01-27 12:56:12,530 | ERROR | digital_human_sdk.app.intelligence.utils.json_utils | safe_json_loads failed
2026-01-27 12:56:12,531 | INFO | orchestrator | ‚úÖ Safety passed
2026-01-27 12:56:12,532 | INFO | orchestrator | üì• Fetching semantic memory (via MemoryService)
2026-01-27 12:56:12,570 | INFO | orchestrator | üß† MEMORY_RESULT | found=True | count=2
2026-01-27 12:56:12,571 | INFO | orchestrator | üß≠ Router agent called
2026-01-27 12:56:12,572 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_e67390b0ed0e4160b4c26d897805281c
2026-01-27 12:56:12,572 | DEBUG | openai.agents | Setting current trace: trace_e67390b0ed0e4160b4c26d897805281c
2026-01-27 12:56:12,573 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000002551182A8E0> with id None
2026-01-27 12:56:12,573 | DEBUG | openai.agents | Running agent RouterAgent (turn 1)
2026-01-27 12:56:12,575 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000025512613400> with id None
2026-01-27 12:56:12,576 | DEBUG | openai.agents | Calling LLM
2026-01-27 12:56:12,576 | DEBUG | LiteLLM | 

2026-01-27 12:56:12,577 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-27 12:56:12,578 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'what is my favourite color?'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-27 12:56:12,579 | DEBUG | LiteLLM | 

2026-01-27 12:56:12,580 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-27 12:56:12,581 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-27 12:56:12,581 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-27 12:56:12,583 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 12:56:12,584 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-27 12:56:12,588 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'what is my favourite color?'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-27 12:56:12,590 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-27 12:56:12,590 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-27 12:56:12,591 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-27 12:56:12,592 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:12,594 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:12,594 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'what is my favourite color?'}]}], 'system_instruction': {'parts': [{'text': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n'}]}, 'generationConfig': {}}'
[0m

2026-01-27 12:56:13,136 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Tue, 27 Jan 2026 07:26:13 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_d1db45d5c9afd00822081127c4bb986a'), (b'openai-processing-ms', b'411'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'414'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c46853a0a6f5a0c-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-27 12:56:13,137 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-27 12:56:13,138 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-27 12:56:13,138 | DEBUG | httpcore.http11 | response_closed.started
2026-01-27 12:56:13,138 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-27 12:56:13,138 | DEBUG | openai.agents | Exported 1 items
2026-01-27 12:56:13,139 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-27 12:56:13,140 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-27 12:56:13,140 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-27 12:56:13,140 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-27 12:56:13,141 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-27 12:56:13,894 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\n  \"use_memory\": true,\n  \"use_tool\": false,\n  \"tool_name\": \"none\",\n  \"tool_arguments\": null,\n  \"intent\": \"read\",\n  \"memory_key\": \"favourite_color\"\n}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 491,
    "candidatesTokenCount": 61,
    "totalTokenCount": 601,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 491
      }
    ],
    "thoughtsTokenCount": 49
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "lWh4af2sJN_njuMPy_aVoA0"
}



2026-01-27 12:56:13,894 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-27 12:56:13,895 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 12:56:13,895 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-27 12:56:13,896 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:13,897 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 12:56:13,897 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:13,897 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:13,897 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 12:56:13,898 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:13,898 | DEBUG | LiteLLM | response_cost: 0.0004223000000000001
2026-01-27 12:56:13,899 | DEBUG | openai.agents | Received model response
2026-01-27 12:56:13,898 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 12:56:13,899 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x00000255126C99C0>>
2026-01-27 12:56:13,900 | DEBUG | LiteLLM | response_cost: 0.0004223000000000001
2026-01-27 12:56:13,900 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-27 12:56:13,900 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 12:56:13,901 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-27 12:56:13,901 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-27 12:56:13,901 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:13,901 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-27 12:56:13,902 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 12:56:13,902 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 12:56:13,903 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 12:56:13,903 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:13,904 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-27 12:56:13,904 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:13,904 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 12:56:13,906 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 12:56:13,906 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 12:56:13,907 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:13,909 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-27 12:56:13,910 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 12:56:13,910 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0004223000000000001
2026-01-27 12:56:13,912 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 12:56:13,913 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:13,913 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 12:56:13,914 | DEBUG | openai.agents | Resetting current trace
2026-01-27 12:56:13,914 | INFO | orchestrator | üß≠ ROUTER_DECISION | tool=False | memory=True | intent=read
2026-01-27 12:56:13,915 | INFO | orchestrator | üß† REASONING_INPUT_SNAPSHOT
{
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful AI assistant.\n \nYou may answer ANY general knowledge question.\n \nConversation summaries and memories are ONLY to provide helpful context\n(e.g. preferences, ongoing plans).\n \nIf a user asks something unrelated to the current topic,\nanswer it normally using general knowledge.\n \nDo NOT restrict yourself to previous topics unless the user explicitly asks."
    },
    {
      "role": "user",
      "content": "hello"
    },
    {
      "role": "assistant",
      "content": "Hello! How can I help you today?"
    },
    {
      "role": "user",
      "content": "what is my favourite color?"
    },
    {
      "role": "user",
      "content": "what is my favourite color?"
    }
  ],
  "safety": {
    "safe": true,
    "message": null
  },
  "memory_action": {},
  "memory_data": [
    {
      "memory_id": 6,
      "user_id": 3,
      "text": "My favourite color is yellow.",
      "confidence": 0.85,
      "created_at": "2026-01-26T21:50:52.573607+05:30",
      "expires_at": "2026-02-25T21:50:52.427804+05:30",
      "score": 0.7962166666984558
    },
    {
      "memory_id": 7,
      "user_id": 3,
      "text": "my name is bhumika raheja",
      "confidence": 0.95,
      "created_at": "2026-01-26T22:31:33.870462+05:30",
      "expires_at": "2026-02-25T22:31:33.867621+05:30",
      "score": 0.35538041591644287
    }
  ],
  "memory_found": true,
  "tool_context": {}
}
2026-01-27 12:56:13,918 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_00306a763506446aa2ef0a544df31fcf
2026-01-27 12:56:13,920 | DEBUG | openai.agents | Setting current trace: trace_00306a763506446aa2ef0a544df31fcf
2026-01-27 12:56:13,920 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x0000025511CAA570> with id None
2026-01-27 12:56:13,921 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000025512612E60> with id None
2026-01-27 12:56:13,921 | DEBUG | openai.agents | Calling LLM
2026-01-27 12:56:13,921 | DEBUG | LiteLLM | 

2026-01-27 12:56:13,921 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-27 12:56:13,922 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "hello"}, {"role": "assistant", "content": "Hello! How can I help you today?"}, {"role": "user", "content": "what is my favourite color?"}, {"role": "user", "content": "what is my favourite color?"}], "safety": {"safe": true, "message": null}, "memory_action": {}, "memory_data": [{"memory_id": 6, "user_id": 3, "text": "My favourite color is yellow.", "confidence": 0.85, "created_at": "2026-01-26T21:50:52.573607+05:30", "expires_at": "2026-02-25T21:50:52.427804+05:30", "score": 0.7962166666984558}, {"memory_id": 7, "user_id": 3, "text": "my name is bhumika raheja", "confidence": 0.95, "created_at": "2026-01-26T22:31:33.870462+05:30", "expires_at": "2026-02-25T22:31:33.867621+05:30", "score": 0.35538041591644287}], "memory_found": true, "tool_context": {}}'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=True, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-27 12:56:13,922 | DEBUG | LiteLLM | 

2026-01-27 12:56:13,923 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-27 12:56:13,923 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-27 12:56:13,923 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-27 12:56:13,924 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 12:56:13,925 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-27 12:56:13,926 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "hello"}, {"role": "assistant", "content": "Hello! How can I help you today?"}, {"role": "user", "content": "what is my favourite color?"}, {"role": "user", "content": "what is my favourite color?"}], "safety": {"safe": true, "message": null}, "memory_action": {}, "memory_data": [{"memory_id": 6, "user_id": 3, "text": "My favourite color is yellow.", "confidence": 0.85, "created_at": "2026-01-26T21:50:52.573607+05:30", "expires_at": "2026-02-25T21:50:52.427804+05:30", "score": 0.7962166666984558}, {"memory_id": 7, "user_id": 3, "text": "my name is bhumika raheja", "confidence": 0.95, "created_at": "2026-01-26T22:31:33.870462+05:30", "expires_at": "2026-02-25T22:31:33.867621+05:30", "score": 0.35538041591644287}], "memory_found": true, "tool_context": {}}'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-27 12:56:13,926 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': True}
2026-01-27 12:56:13,926 | DEBUG | LiteLLM | Final returned optional params: {'stream': True}
2026-01-27 12:56:13,927 | DEBUG | LiteLLM | self.optional_params: {'stream': True}
2026-01-27 12:56:13,927 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:13,928 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:13,928 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?key=*****=sse \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "hello"}, {"role": "assistant", "content": "Hello! How can I help you today?"}, {"role": "user", "content": "what is my favourite color?"}, {"role": "user", "content": "what is my favourite color?"}], "safety": {"safe": true, "message": null}, "memory_action": {}, "memory_data": [{"memory_id": 6, "user_id": 3, "text": "My favourite color is yellow.", "confidence": 0.85, "created_at": "2026-01-26T21:50:52.573607+05:30", "expires_at": "2026-02-25T21:50:52.427804+05:30", "score": 0.7962166666984558}, {"memory_id": 7, "user_id": 3, "text": "my name is bhumika raheja", "confidence": 0.95, "created_at": "2026-01-26T22:31:33.870462+05:30", "expires_at": "2026-02-25T22:31:33.867621+05:30", "score": 0.35538041591644287}], "memory_found": true, "tool_context": {}}'}]}], 'system_instruction': {'parts': [{'text': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.'}]}, 'generationConfig': {}}'
[0m

2026-01-27 12:56:13,930 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 12:56:13,930 | DEBUG | LiteLLM | messages in token_counter: None, text in token_counter: 
2026-01-27 12:56:13,931 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:13,931 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:13,931 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 12:56:13,932 | DEBUG | LiteLLM | response_cost: 0.0
2026-01-27 12:56:14,225 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Tue, 27 Jan 2026 07:26:14 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_283d2398a44b1e6ff2e9766264325a68'), (b'openai-processing-ms', b'209'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'212'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c46854379a95a0c-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-27 12:56:14,228 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-27 12:56:14,228 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-27 12:56:14,228 | DEBUG | httpcore.http11 | response_closed.started
2026-01-27 12:56:14,229 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-27 12:56:14,230 | DEBUG | openai.agents | Exported 3 items
2026-01-27 12:56:14,231 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-27 12:56:14,232 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-27 12:56:14,232 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-27 12:56:14,233 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-27 12:56:14,233 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-27 12:56:15,076 | DEBUG | LiteLLM | RAW RESPONSE:
first stream response received


2026-01-27 12:56:15,077 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': 'Your favorite color is yellow.'}], 'role': 'model'}, 'finishReason': 'STOP', 'index': 0}], 'usageMetadata': {'promptTokenCount': 590, 'candidatesTokenCount': 6, 'totalTokenCount': 629, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 590}], 'thoughtsTokenCount': 33}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'lmh4aYPVJKi8qfkPo9K5iQE'}
2026-01-27 12:56:15,078 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='lmh4aYPVJKi8qfkPo9K5iQE', created=1769498775, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='Your favorite color is yellow.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=39, prompt_tokens=590, total_tokens=629, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=33, rejected_prediction_tokens=None, text_tokens=6, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=590, image_tokens=None)))
2026-01-27 12:56:15,079 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content='Your favorite color is yellow.', role='assistant', function_call=None, tool_calls=None, audio=None)
2026-01-27 12:56:15,079 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='lmh4aYPVJKi8qfkPo9K5iQE', created=1769498775, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Your favorite color is yellow.', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=39, prompt_tokens=590, total_tokens=629, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=33, rejected_prediction_tokens=None, text_tokens=6, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=590, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-27 12:56:15,081 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=False
2026-01-27 12:56:15,081 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=False
2026-01-27 12:56:15,081 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-27 12:56:15,083 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-27 12:56:15,083 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 12:56:15,083 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 12:56:15,084 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:15,084 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:15,085 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:15,085 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:15,085 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 12:56:15,085 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 12:56:15,086 | DEBUG | LiteLLM | response_cost: 0.0002745
2026-01-27 12:56:15,086 | DEBUG | LiteLLM | response_cost: 0.0002745
2026-01-27 12:56:15,086 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0002745
2026-01-27 12:56:15,087 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 12:56:15,087 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 12:56:15,088 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:15,088 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:15,088 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 12:56:15,088 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 12:56:15,089 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-27 12:56:15,090 | DEBUG | openai.agents | Resetting current trace
2026-01-27 12:56:15,106 | INFO | session_summary | üß† SUMMARY_START | session_id=0ba7a5e8-8a18-47df-9ea1-9282cf4c76ae
2026-01-27 12:56:15,110 | INFO | session_summary | üìä Unsummarized messages = 4
2026-01-27 12:56:15,110 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-27 12:56:15,111 | INFO | chat | ‚úÖ Stream complete | tokens=1 | time=3.86s
2026-01-27 12:56:15,209 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Tue, 27 Jan 2026 07:26:15 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_d427ba16629ed7356e227e41e068c5de'), (b'openai-processing-ms', b'81'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'84'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c46854a4e5d5a0c-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-27 12:56:15,210 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-27 12:56:15,210 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-27 12:56:15,210 | DEBUG | httpcore.http11 | response_closed.started
2026-01-27 12:56:15,210 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-27 12:56:15,211 | DEBUG | openai.agents | Exported 3 items
2026-01-27 12:56:15,211 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-27 12:56:15,212 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-27 12:56:15,212 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-27 12:56:15,212 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-27 12:56:15,212 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-27 12:56:15,630 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Tue, 27 Jan 2026 07:26:15 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_9fe168564f82e5f8861bfff283f9375c'), (b'openai-processing-ms', b'102'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'106'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4685506bd35a0c-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-27 12:56:15,630 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-27 12:56:15,631 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-27 12:56:15,631 | DEBUG | httpcore.http11 | response_closed.started
2026-01-27 12:56:15,631 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-27 12:56:15,631 | DEBUG | openai.agents | Exported 2 items
2026-01-27 12:56:32,016 | INFO | chat | üì© Chat request | user_id=3 | request_id=d6c1aa7f-a6d6-470c-a27c-c9b5e92a6542
2026-01-27 12:56:32,018 | INFO | chat | üßë User message | what is weather in delhi now?
2026-01-27 12:56:32,037 | INFO | chat | ü§ñ Stream started | session_id=0ba7a5e8-8a18-47df-9ea1-9282cf4c76ae | request_id=d6c1aa7f-a6d6-470c-a27c-c9b5e92a6542
2026-01-27 12:56:32,038 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-27 12:56:32,039 | INFO | orchestrator | üßë USER_INPUT | what is weather in delhi now?
2026-01-27 12:56:32,042 | INFO | orchestrator | üß© CONTEXT | user_id=3 | enable_memory=True | db=None
2026-01-27 12:56:32,045 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-27 12:56:32,046 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_559e7fbfdb984798808d22a972a58532
2026-01-27 12:56:32,047 | DEBUG | openai.agents | Setting current trace: trace_559e7fbfdb984798808d22a972a58532
2026-01-27 12:56:32,047 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000002551196E8E0> with id None
2026-01-27 12:56:32,048 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-27 12:56:32,049 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000025512612AA0> with id None
2026-01-27 12:56:32,049 | DEBUG | openai.agents | Calling LLM
2026-01-27 12:56:32,050 | DEBUG | LiteLLM | 

2026-01-27 12:56:32,050 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-27 12:56:32,050 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n', 'role': 'system'}, {'role': 'user', 'content': 'what is weather in delhi now?'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-27 12:56:32,051 | DEBUG | LiteLLM | 

2026-01-27 12:56:32,053 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-27 12:56:32,053 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-27 12:56:32,054 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-27 12:56:32,056 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 12:56:32,056 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-27 12:56:32,058 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n', 'role': 'system'}, {'role': 'user', 'content': 'what is weather in delhi now?'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-27 12:56:32,058 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-27 12:56:32,059 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-27 12:56:32,059 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-27 12:56:32,061 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:32,063 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:32,064 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'what is weather in delhi now?'}]}], 'system_instruction': {'parts': [{'text': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n'}]}, 'generationConfig': {}}'
[0m

2026-01-27 12:56:33,105 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "SAFE"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 194,
    "candidatesTokenCount": 1,
    "totalTokenCount": 235,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 194
      }
    ],
    "thoughtsTokenCount": 40
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "qGh4adn4MtSHqfkPu9HTiA4"
}



2026-01-27 12:56:33,107 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-27 12:56:33,108 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 12:56:33,109 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-27 12:56:33,110 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:33,111 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 12:56:33,111 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:33,112 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:33,114 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 12:56:33,123 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:33,124 | DEBUG | LiteLLM | response_cost: 0.0001607
2026-01-27 12:56:33,125 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 12:56:33,126 | DEBUG | openai.agents | Received model response
2026-01-27 12:56:33,127 | DEBUG | LiteLLM | response_cost: 0.0001607
2026-01-27 12:56:33,129 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x0000025511E54580>>
2026-01-27 12:56:33,132 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 12:56:33,133 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-27 12:56:33,134 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:33,136 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-27 12:56:33,136 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-27 12:56:33,137 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 12:56:33,138 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-27 12:56:33,139 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 12:56:33,140 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-27 12:56:33,141 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 12:56:33,143 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:33,144 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 12:56:33,145 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:33,146 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 12:56:33,147 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:33,147 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 12:56:33,151 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 12:56:33,152 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-27 12:56:33,154 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0001607
2026-01-27 12:56:33,155 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 12:56:33,155 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:33,156 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 12:56:33,159 | DEBUG | openai.agents | Resetting current trace
2026-01-27 12:56:33,159 | ERROR | digital_human_sdk.app.intelligence.utils.json_utils | safe_json_loads failed
2026-01-27 12:56:33,161 | INFO | orchestrator | ‚úÖ Safety passed
2026-01-27 12:56:33,161 | INFO | orchestrator | üì• Fetching semantic memory (via MemoryService)
2026-01-27 12:56:33,201 | INFO | orchestrator | üß† MEMORY_RESULT | found=False | count=0
2026-01-27 12:56:33,202 | INFO | orchestrator | üß≠ Router agent called
2026-01-27 12:56:33,204 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_71d175720066406699e50e3f662a6e43
2026-01-27 12:56:33,204 | DEBUG | openai.agents | Setting current trace: trace_71d175720066406699e50e3f662a6e43
2026-01-27 12:56:33,204 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x0000025511E21E40> with id None
2026-01-27 12:56:33,205 | DEBUG | openai.agents | Running agent RouterAgent (turn 1)
2026-01-27 12:56:33,205 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000025511E588E0> with id None
2026-01-27 12:56:33,206 | DEBUG | openai.agents | Calling LLM
2026-01-27 12:56:33,206 | DEBUG | LiteLLM | 

2026-01-27 12:56:33,207 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-27 12:56:33,207 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'what is weather in delhi now?'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-27 12:56:33,209 | DEBUG | LiteLLM | 

2026-01-27 12:56:33,210 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-27 12:56:33,211 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-27 12:56:33,211 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-27 12:56:33,214 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 12:56:33,216 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-27 12:56:33,220 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'what is weather in delhi now?'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-27 12:56:33,220 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-27 12:56:33,221 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-27 12:56:33,221 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-27 12:56:33,222 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:33,224 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:33,224 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'what is weather in delhi now?'}]}], 'system_instruction': {'parts': [{'text': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n'}]}, 'generationConfig': {}}'
[0m

2026-01-27 12:56:34,939 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\"use_memory\": false, \"use_tool\": true, \"tool_name\": \"weather\", \"tool_arguments\": {\"location\": \"delhi\"}, \"intent\": \"none\", \"memory_key\": null}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 492,
    "candidatesTokenCount": 47,
    "totalTokenCount": 615,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 492
      }
    ],
    "thoughtsTokenCount": 76
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "qmh4acnnJezZjuMPu9DLsAE"
}



2026-01-27 12:56:34,941 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-27 12:56:34,943 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 12:56:34,943 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-27 12:56:34,944 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:34,945 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 12:56:34,945 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:34,946 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:34,947 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 12:56:34,948 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:34,949 | DEBUG | LiteLLM | response_cost: 0.00045510000000000006
2026-01-27 12:56:34,949 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 12:56:34,950 | DEBUG | openai.agents | Received model response
2026-01-27 12:56:34,951 | DEBUG | LiteLLM | response_cost: 0.00045510000000000006
2026-01-27 12:56:34,952 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x0000025511E56170>>
2026-01-27 12:56:34,955 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 12:56:34,956 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-27 12:56:34,956 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:34,957 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-27 12:56:34,958 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-27 12:56:34,958 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 12:56:34,959 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-27 12:56:34,963 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 12:56:34,965 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-27 12:56:34,966 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 12:56:34,967 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:34,968 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 12:56:34,970 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:34,970 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 12:56:34,970 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:34,970 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 12:56:34,971 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 12:56:34,972 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-27 12:56:34,972 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.00045510000000000006
2026-01-27 12:56:34,973 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 12:56:34,983 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:34,984 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 12:56:34,985 | DEBUG | openai.agents | Resetting current trace
2026-01-27 12:56:34,985 | INFO | orchestrator | üß≠ ROUTER_DECISION | tool=True | memory=False | intent=none
2026-01-27 12:56:34,985 | INFO | orchestrator | üõ†Ô∏è Tool agent called
2026-01-27 12:56:34,986 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_884bfac40e37451bbd8fe68421f19dd6
2026-01-27 12:56:34,986 | DEBUG | openai.agents | Setting current trace: trace_884bfac40e37451bbd8fe68421f19dd6
2026-01-27 12:56:34,986 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x0000025511CA8CC0> with id None
2026-01-27 12:56:34,986 | DEBUG | openai.agents | Running agent ToolAgent (turn 1)
2026-01-27 12:56:34,987 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x00000255117114E0> with id None
2026-01-27 12:56:34,987 | DEBUG | openai.agents | Calling LLM
2026-01-27 12:56:34,987 | DEBUG | LiteLLM | 

2026-01-27 12:56:34,987 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-27 12:56:34,988 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a Tool Agent.\n \nYour task is to decide whether the user\'s query requires a tool.\n \nAvailable tools:\n- weather ‚Üí for weather-related queries (city-based)\n- calculator ‚Üí for mathematical calculations\n- web_search ‚Üí for latest information, prices, news, current events\n- browser ‚Üí for visiting, fetching, or summarizing a webpage\n \nRules:\n- You MUST output valid JSON only.\n- If a tool is required, return:\n  { "tool": "<tool_name>", "arguments": { ... } }\n- If no tool is required, return:\n  { "tool": "none", "arguments": {} }\n \nSTRICT RULES:\n- Do NOT answer the user.\n- Do NOT explain.\n- Do NOT include markdown.\n- Do NOT include extra keys.\n- Do NOT wrap JSON in text.\n- Output JSON ONLY.\n \nExamples:\n \nUser: What is the weather in Delhi today?\nOutput:\n{ "tool": "weather", "arguments": { "city": "Delhi" } }\n \nUser: Calculate (10 + 5) * 2\nOutput:\n{ "tool": "calculator", "arguments": { "expression": "(10 + 5) * 2" } }\n \nUser: Explain machine learning\nOutput:\n{ "tool": "none", "arguments": {} }\n ', 'role': 'system'}, {'role': 'user', 'content': 'what is weather in delhi now?'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-27 12:56:34,988 | DEBUG | LiteLLM | 

2026-01-27 12:56:34,988 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-27 12:56:34,990 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-27 12:56:34,990 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-27 12:56:34,991 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 12:56:34,991 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-27 12:56:34,992 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Tool Agent.\n \nYour task is to decide whether the user\'s query requires a tool.\n \nAvailable tools:\n- weather ‚Üí for weather-related queries (city-based)\n- calculator ‚Üí for mathematical calculations\n- web_search ‚Üí for latest information, prices, news, current events\n- browser ‚Üí for visiting, fetching, or summarizing a webpage\n \nRules:\n- You MUST output valid JSON only.\n- If a tool is required, return:\n  { "tool": "<tool_name>", "arguments": { ... } }\n- If no tool is required, return:\n  { "tool": "none", "arguments": {} }\n \nSTRICT RULES:\n- Do NOT answer the user.\n- Do NOT explain.\n- Do NOT include markdown.\n- Do NOT include extra keys.\n- Do NOT wrap JSON in text.\n- Output JSON ONLY.\n \nExamples:\n \nUser: What is the weather in Delhi today?\nOutput:\n{ "tool": "weather", "arguments": { "city": "Delhi" } }\n \nUser: Calculate (10 + 5) * 2\nOutput:\n{ "tool": "calculator", "arguments": { "expression": "(10 + 5) * 2" } }\n \nUser: Explain machine learning\nOutput:\n{ "tool": "none", "arguments": {} }\n ', 'role': 'system'}, {'role': 'user', 'content': 'what is weather in delhi now?'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-27 12:56:34,992 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-27 12:56:34,992 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-27 12:56:34,993 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-27 12:56:34,993 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:34,997 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:34,997 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'what is weather in delhi now?'}]}], 'system_instruction': {'parts': [{'text': 'You are a Tool Agent.\n \nYour task is to decide whether the user\'s query requires a tool.\n \nAvailable tools:\n- weather ‚Üí for weather-related queries (city-based)\n- calculator ‚Üí for mathematical calculations\n- web_search ‚Üí for latest information, prices, news, current events\n- browser ‚Üí for visiting, fetching, or summarizing a webpage\n \nRules:\n- You MUST output valid JSON only.\n- If a tool is required, return:\n  { "tool": "<tool_name>", "arguments": { ... } }\n- If no tool is required, return:\n  { "tool": "none", "arguments": {} }\n \nSTRICT RULES:\n- Do NOT answer the user.\n- Do NOT explain.\n- Do NOT include markdown.\n- Do NOT include extra keys.\n- Do NOT wrap JSON in text.\n- Output JSON ONLY.\n \nExamples:\n \nUser: What is the weather in Delhi today?\nOutput:\n{ "tool": "weather", "arguments": { "city": "Delhi" } }\n \nUser: Calculate (10 + 5) * 2\nOutput:\n{ "tool": "calculator", "arguments": { "expression": "(10 + 5) * 2" } }\n \nUser: Explain machine learning\nOutput:\n{ "tool": "none", "arguments": {} }\n '}]}, 'generationConfig': {}}'
[0m

2026-01-27 12:56:35,930 | DEBUG | httpcore.connection | close.started
2026-01-27 12:56:35,931 | DEBUG | httpcore.connection | close.complete
2026-01-27 12:56:35,931 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-27 12:56:35,960 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000025511779E40>
2026-01-27 12:56:35,960 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x00000255064B9540> server_hostname='api.openai.com' timeout=5.0
2026-01-27 12:56:35,971 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002551177A5C0>
2026-01-27 12:56:35,971 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-27 12:56:35,971 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-27 12:56:35,971 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-27 12:56:35,972 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-27 12:56:35,972 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-27 12:56:36,238 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\"tool\": \"weather\", \"arguments\": {\"city\": \"delhi\"}}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 319,
    "candidatesTokenCount": 16,
    "totalTokenCount": 385,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 319
      }
    ],
    "thoughtsTokenCount": 50
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "rGh4aa4C8IyDxQ_H59S5Dg"
}



2026-01-27 12:56:36,239 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-27 12:56:36,239 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 12:56:36,239 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-27 12:56:36,241 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:36,241 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 12:56:36,242 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:36,242 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:36,242 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 12:56:36,243 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:36,243 | DEBUG | LiteLLM | response_cost: 0.0002607
2026-01-27 12:56:36,244 | DEBUG | openai.agents | Received model response
2026-01-27 12:56:36,244 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 12:56:36,244 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x000002551198A500>>
2026-01-27 12:56:36,245 | DEBUG | LiteLLM | response_cost: 0.0002607
2026-01-27 12:56:36,245 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-27 12:56:36,245 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 12:56:36,246 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-27 12:56:36,246 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-27 12:56:36,246 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:36,246 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-27 12:56:36,248 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 12:56:36,248 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 12:56:36,248 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 12:56:36,248 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:36,249 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-27 12:56:36,249 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:36,249 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 12:56:36,250 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 12:56:36,250 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 12:56:36,251 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:36,251 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-27 12:56:36,252 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 12:56:36,252 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0002607
2026-01-27 12:56:36,252 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 12:56:36,253 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:36,253 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 12:56:36,255 | DEBUG | openai.agents | Resetting current trace
2026-01-27 12:56:36,255 | INFO | orchestrator | üõ†Ô∏è TOOL_EXEC | name=weather | args={'city': 'delhi'}
2026-01-27 12:56:36,257 | DEBUG | urllib3.connectionpool | Starting new HTTPS connection (1): api.openweathermap.org:443
2026-01-27 12:56:36,477 | DEBUG | urllib3.connectionpool | https://api.openweathermap.org:443 "GET /data/2.5/weather?q=delhi&appid=1b9bd663a5cceef8c4355bfe45209f86&units=metric HTTP/1.1" 200 497
2026-01-27 12:56:36,480 | INFO | orchestrator | üß† REASONING_INPUT_SNAPSHOT
{
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful AI assistant.\n \nYou may answer ANY general knowledge question.\n \nConversation summaries and memories are ONLY to provide helpful context\n(e.g. preferences, ongoing plans).\n \nIf a user asks something unrelated to the current topic,\nanswer it normally using general knowledge.\n \nDo NOT restrict yourself to previous topics unless the user explicitly asks."
    },
    {
      "role": "user",
      "content": "hello"
    },
    {
      "role": "assistant",
      "content": "Hello! How can I help you today?"
    },
    {
      "role": "user",
      "content": "what is my favourite color?"
    },
    {
      "role": "assistant",
      "content": "Your favorite color is yellow."
    },
    {
      "role": "user",
      "content": "what is weather in delhi now?"
    },
    {
      "role": "user",
      "content": "what is weather in delhi now?"
    }
  ],
  "safety": {
    "safe": true,
    "message": null
  },
  "memory_action": {},
  "memory_data": [],
  "memory_found": false,
  "tool_context": {
    "coord": {
      "lon": 77.2167,
      "lat": 28.6667
    },
    "weather": [
      {
        "id": 301,
        "main": "Drizzle",
        "description": "drizzle",
        "icon": "09d"
      }
    ],
    "base": "stations",
    "main": {
      "temp": 14.05,
      "feels_like": 13.97,
      "temp_min": 14.05,
      "temp_max": 14.05,
      "pressure": 1018,
      "humidity": 94,
      "sea_level": 1018,
      "grnd_level": 992
    },
    "visibility": 1500,
    "wind": {
      "speed": 5.66,
      "deg": 90
    },
    "clouds": {
      "all": 100
    },
    "dt": 1769498640,
    "sys": {
      "type": 1,
      "id": 9165,
      "country": "IN",
      "sunrise": 1769478112,
      "sunset": 1769516730
    },
    "timezone": 19800,
    "id": 1273294,
    "name": "Delhi",
    "cod": 200
  }
}
2026-01-27 12:56:36,481 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_7678629059614bb3af40a1d40abf829e
2026-01-27 12:56:36,481 | DEBUG | openai.agents | Setting current trace: trace_7678629059614bb3af40a1d40abf829e
2026-01-27 12:56:36,481 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000002551182B6A0> with id None
2026-01-27 12:56:36,482 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000025512613580> with id None
2026-01-27 12:56:36,482 | DEBUG | openai.agents | Calling LLM
2026-01-27 12:56:36,482 | DEBUG | LiteLLM | 

2026-01-27 12:56:36,482 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-27 12:56:36,483 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "hello"}, {"role": "assistant", "content": "Hello! How can I help you today?"}, {"role": "user", "content": "what is my favourite color?"}, {"role": "assistant", "content": "Your favorite color is yellow."}, {"role": "user", "content": "what is weather in delhi now?"}, {"role": "user", "content": "what is weather in delhi now?"}], "safety": {"safe": true, "message": null}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {"coord": {"lon": 77.2167, "lat": 28.6667}, "weather": [{"id": 301, "main": "Drizzle", "description": "drizzle", "icon": "09d"}], "base": "stations", "main": {"temp": 14.05, "feels_like": 13.97, "temp_min": 14.05, "temp_max": 14.05, "pressure": 1018, "humidity": 94, "sea_level": 1018, "grnd_level": 992}, "visibility": 1500, "wind": {"speed": 5.66, "deg": 90}, "clouds": {"all": 100}, "dt": 1769498640, "sys": {"type": 1, "id": 9165, "country": "IN", "sunrise": 1769478112, "sunset": 1769516730}, "timezone": 19800, "id": 1273294, "name": "Delhi", "cod": 200}}'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=True, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-27 12:56:36,483 | DEBUG | LiteLLM | 

2026-01-27 12:56:36,483 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-27 12:56:36,484 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-27 12:56:36,484 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-27 12:56:36,485 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 12:56:36,485 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-27 12:56:36,486 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "hello"}, {"role": "assistant", "content": "Hello! How can I help you today?"}, {"role": "user", "content": "what is my favourite color?"}, {"role": "assistant", "content": "Your favorite color is yellow."}, {"role": "user", "content": "what is weather in delhi now?"}, {"role": "user", "content": "what is weather in delhi now?"}], "safety": {"safe": true, "message": null}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {"coord": {"lon": 77.2167, "lat": 28.6667}, "weather": [{"id": 301, "main": "Drizzle", "description": "drizzle", "icon": "09d"}], "base": "stations", "main": {"temp": 14.05, "feels_like": 13.97, "temp_min": 14.05, "temp_max": 14.05, "pressure": 1018, "humidity": 94, "sea_level": 1018, "grnd_level": 992}, "visibility": 1500, "wind": {"speed": 5.66, "deg": 90}, "clouds": {"all": 100}, "dt": 1769498640, "sys": {"type": 1, "id": 9165, "country": "IN", "sunrise": 1769478112, "sunset": 1769516730}, "timezone": 19800, "id": 1273294, "name": "Delhi", "cod": 200}}'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-27 12:56:36,486 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': True}
2026-01-27 12:56:36,487 | DEBUG | LiteLLM | Final returned optional params: {'stream': True}
2026-01-27 12:56:36,487 | DEBUG | LiteLLM | self.optional_params: {'stream': True}
2026-01-27 12:56:36,488 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:36,488 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:36,488 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?key=*****=sse \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "hello"}, {"role": "assistant", "content": "Hello! How can I help you today?"}, {"role": "user", "content": "what is my favourite color?"}, {"role": "assistant", "content": "Your favorite color is yellow."}, {"role": "user", "content": "what is weather in delhi now?"}, {"role": "user", "content": "what is weather in delhi now?"}], "safety": {"safe": true, "message": null}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {"coord": {"lon": 77.2167, "lat": 28.6667}, "weather": [{"id": 301, "main": "Drizzle", "description": "drizzle", "icon": "09d"}], "base": "stations", "main": {"temp": 14.05, "feels_like": 13.97, "temp_min": 14.05, "temp_max": 14.05, "pressure": 1018, "humidity": 94, "sea_level": 1018, "grnd_level": 992}, "visibility": 1500, "wind": {"speed": 5.66, "deg": 90}, "clouds": {"all": 100}, "dt": 1769498640, "sys": {"type": 1, "id": 9165, "country": "IN", "sunrise": 1769478112, "sunset": 1769516730}, "timezone": 19800, "id": 1273294, "name": "Delhi", "cod": 200}}'}]}], 'system_instruction': {'parts': [{'text': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.'}]}, 'generationConfig': {}}'
[0m

2026-01-27 12:56:36,493 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 12:56:36,494 | DEBUG | LiteLLM | messages in token_counter: None, text in token_counter: 
2026-01-27 12:56:36,495 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:36,495 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:36,495 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 12:56:36,496 | DEBUG | LiteLLM | response_cost: 0.0
2026-01-27 12:56:36,535 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Tue, 27 Jan 2026 07:26:36 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_262e01bd22453f53189f0ef0f6165297'), (b'openai-processing-ms', b'225'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'229'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4685d24a01d04b-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-27 12:56:36,536 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-27 12:56:36,536 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-27 12:56:36,536 | DEBUG | httpcore.http11 | response_closed.started
2026-01-27 12:56:36,538 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-27 12:56:36,538 | DEBUG | openai.agents | Exported 7 items
2026-01-27 12:56:36,538 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-27 12:56:36,539 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-27 12:56:36,539 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-27 12:56:36,539 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-27 12:56:36,539 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-27 12:56:36,985 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Tue, 27 Jan 2026 07:26:36 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_9277b7c37f4bf4f5e74e0fba51fc5ebd'), (b'openai-processing-ms', b'115'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'118'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4685d5c9ffd04b-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-27 12:56:36,986 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-27 12:56:36,986 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-27 12:56:36,986 | DEBUG | httpcore.http11 | response_closed.started
2026-01-27 12:56:36,986 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-27 12:56:36,987 | DEBUG | openai.agents | Exported 3 items
2026-01-27 12:56:38,289 | DEBUG | LiteLLM | RAW RESPONSE:
first stream response received


2026-01-27 12:56:38,290 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': 'In Delhi, the current weather is drizzle with a temperature of 14.05¬∞C.'}], 'role': 'model'}, 'index': 0}], 'usageMetadata': {'promptTokenCount': 646, 'candidatesTokenCount': 21, 'totalTokenCount': 813, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 646}], 'thoughtsTokenCount': 146}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'rWh4afbWDof5juMP4vSJ8Q8'}
2026-01-27 12:56:38,291 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='rWh4afbWDof5juMP4vSJ8Q8', created=1769498798, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='In Delhi, the current weather is drizzle with a temperature of 14.05¬∞C.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=167, prompt_tokens=646, total_tokens=813, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=146, rejected_prediction_tokens=None, text_tokens=21, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=646, image_tokens=None)))
2026-01-27 12:56:38,292 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content='In Delhi, the current weather is drizzle with a temperature of 14.05¬∞C.', role='assistant', function_call=None, tool_calls=None, audio=None)
2026-01-27 12:56:38,292 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='rWh4afbWDof5juMP4vSJ8Q8', created=1769498798, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='In Delhi, the current weather is drizzle with a temperature of 14.05¬∞C.', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=167, prompt_tokens=646, total_tokens=813, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=146, rejected_prediction_tokens=None, text_tokens=21, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=646, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-27 12:56:38,294 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': ' The humidity is 94% and the wind speed is 5.66 m/s.'}], 'role': 'model'}, 'finishReason': 'STOP', 'index': 0}], 'usageMetadata': {'promptTokenCount': 646, 'candidatesTokenCount': 42, 'totalTokenCount': 834, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 646}], 'thoughtsTokenCount': 146}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'rWh4afbWDof5juMP4vSJ8Q8'}
2026-01-27 12:56:38,295 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='rWh4afbWDof5juMP4vSJ8Q8', created=1769498798, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content=' The humidity is 94% and the wind speed is 5.66 m/s.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=188, prompt_tokens=646, total_tokens=834, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=146, rejected_prediction_tokens=None, text_tokens=42, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=646, image_tokens=None)))
2026-01-27 12:56:38,296 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content=' The humidity is 94% and the wind speed is 5.66 m/s.', role=None, function_call=None, tool_calls=None, audio=None)
2026-01-27 12:56:38,296 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='rWh4afbWDof5juMP4vSJ8Q8', created=1769498798, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' The humidity is 94% and the wind speed is 5.66 m/s.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=188, prompt_tokens=646, total_tokens=834, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=146, rejected_prediction_tokens=None, text_tokens=42, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=646, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-27 12:56:38,300 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=False
2026-01-27 12:56:38,300 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=False
2026-01-27 12:56:38,300 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-27 12:56:38,301 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-27 12:56:38,301 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 12:56:38,301 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 12:56:38,302 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:38,302 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:38,302 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:38,303 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:38,303 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 12:56:38,304 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 12:56:38,305 | DEBUG | LiteLLM | response_cost: 0.0006638
2026-01-27 12:56:38,307 | DEBUG | LiteLLM | response_cost: 0.0006638
2026-01-27 12:56:38,309 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 12:56:38,310 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0006638
2026-01-27 12:56:38,310 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:38,312 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 12:56:38,312 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 12:56:38,313 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 12:56:38,315 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 12:56:38,317 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-27 12:56:38,317 | DEBUG | openai.agents | Resetting current trace
2026-01-27 12:56:38,331 | INFO | session_summary | üß† SUMMARY_START | session_id=0ba7a5e8-8a18-47df-9ea1-9282cf4c76ae
2026-01-27 12:56:38,335 | INFO | session_summary | üìä Unsummarized messages = 6
2026-01-27 12:56:38,335 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-27 12:56:38,336 | INFO | chat | ‚úÖ Stream complete | tokens=2 | time=6.32s
2026-01-27 12:56:42,062 | DEBUG | httpcore.connection | close.started
2026-01-27 12:56:42,063 | DEBUG | httpcore.connection | close.complete
2026-01-27 12:56:42,063 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-27 12:56:42,108 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000255120B0940>
2026-01-27 12:56:42,109 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x00000255064B9540> server_hostname='api.openai.com' timeout=5.0
2026-01-27 12:56:42,138 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000255120B0850>
2026-01-27 12:56:42,138 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-27 12:56:42,139 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-27 12:56:42,139 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-27 12:56:42,139 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-27 12:56:42,139 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-27 12:56:43,157 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Tue, 27 Jan 2026 07:26:43 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_fb981a59a00dd035c19f32fe63ce0f73'), (b'openai-processing-ms', b'124'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'127'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4685f8dd93087a-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-27 12:56:43,157 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-27 12:56:43,158 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-27 12:56:43,158 | DEBUG | httpcore.http11 | response_closed.started
2026-01-27 12:56:43,158 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-27 12:56:43,158 | DEBUG | openai.agents | Exported 2 items
2026-01-27 13:06:11,304 | DEBUG | LiteLLM | LoggingWorker cancelled during shutdown
2026-01-27 13:06:11,306 | DEBUG | LiteLLM | [LoggingWorker] atexit: Queue is empty
2026-01-27 13:06:11,306 | DEBUG | asyncio | Using proactor: IocpProactor
2026-01-27 13:06:11,307 | DEBUG | openai.agents | Shutting down trace provider
2026-01-27 13:06:11,307 | DEBUG | openai.agents | Shutting down trace processor <agents.tracing.processors.BatchTraceProcessor object at 0x00000255065041C0>
2026-01-27 13:06:21,806 | INFO | sentence_transformers.SentenceTransformer | Use pytorch device_name: cpu
2026-01-27 13:06:21,808 | INFO | sentence_transformers.SentenceTransformer | Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2026-01-27 13:06:21,811 | DEBUG | urllib3.connectionpool | Starting new HTTPS connection (1): huggingface.co:443
2026-01-27 13:06:22,132 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-27 13:06:22,143 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-27 13:06:22,379 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-27 13:06:22,390 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-27 13:06:22,640 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-27 13:06:22,651 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-27 13:06:22,911 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md HTTP/1.1" 307 0
2026-01-27 13:06:22,920 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/README.md HTTP/1.1" 200 0
2026-01-27 13:06:23,182 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-27 13:06:23,194 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-27 13:06:23,427 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json HTTP/1.1" 307 0
2026-01-27 13:06:23,446 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/sentence_bert_config.json HTTP/1.1" 200 0
2026-01-27 13:06:23,717 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json HTTP/1.1" 404 0
2026-01-27 13:06:23,971 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1" 307 0
2026-01-27 13:06:23,987 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json HTTP/1.1" 200 0
2026-01-27 13:06:24,290 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/1.1" 307 0
2026-01-27 13:06:24,303 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json HTTP/1.1" 200 0
2026-01-27 13:06:24,550 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1" 404 64
2026-01-27 13:06:24,779 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main?recursive=True&expand=False HTTP/1.1" 200 6465
2026-01-27 13:06:25,070 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json HTTP/1.1" 307 0
2026-01-27 13:06:25,082 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json HTTP/1.1" 200 0
2026-01-27 13:06:25,333 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2 HTTP/1.1" 200 6825
2026-01-27 13:06:26,494 | DEBUG | httpcore.connection | connect_tcp.started host='raw.githubusercontent.com' port=443 local_address=None timeout=5 socket_options=None
2026-01-27 13:06:26,525 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002388F125450>
2026-01-27 13:06:26,525 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x000002388EF5DC40> server_hostname='raw.githubusercontent.com' timeout=5
2026-01-27 13:06:26,563 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002388F125420>
2026-01-27 13:06:26,564 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'GET']>
2026-01-27 13:06:26,564 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-27 13:06:26,565 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'GET']>
2026-01-27 13:06:26,565 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-27 13:06:26,565 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'GET']>
2026-01-27 13:06:26,598 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Connection', b'keep-alive'), (b'Content-Length', b'62322'), (b'Cache-Control', b'max-age=300'), (b'Content-Security-Policy', b"default-src 'none'; style-src 'unsafe-inline'; sandbox"), (b'Content-Type', b'text/plain; charset=utf-8'), (b'ETag', b'W/"5f0be0ed2fe5ecf561c4b4488d64c304c35b980a45012ef1d1253b4b5d438af1"'), (b'Strict-Transport-Security', b'max-age=31536000'), (b'X-Content-Type-Options', b'nosniff'), (b'X-Frame-Options', b'deny'), (b'X-XSS-Protection', b'1; mode=block'), (b'X-GitHub-Request-Id', b'415E:3865D1:30D7E6:7A896E:69753835'), (b'Content-Encoding', b'gzip'), (b'Accept-Ranges', b'bytes'), (b'Date', b'Tue, 27 Jan 2026 07:36:26 GMT'), (b'Via', b'1.1 varnish'), (b'X-Served-By', b'cache-bom-vanm7210061-BOM'), (b'X-Cache', b'HIT'), (b'X-Cache-Hits', b'17'), (b'X-Timer', b'S1769499387.501459,VS0,VE0'), (b'Vary', b'Authorization,Accept-Encoding'), (b'Access-Control-Allow-Origin', b'*'), (b'Cross-Origin-Resource-Policy', b'cross-origin'), (b'X-Fastly-Request-ID', b'ca5016a922b97c29a3761d6bf7e29e6aad743f57'), (b'Expires', b'Tue, 27 Jan 2026 07:41:26 GMT'), (b'Source-Age', b'122')])
2026-01-27 13:06:26,601 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'GET']>
2026-01-27 13:06:26,642 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-27 13:06:26,643 | DEBUG | httpcore.http11 | response_closed.started
2026-01-27 13:06:26,643 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-27 13:06:26,643 | DEBUG | httpcore.connection | close.started
2026-01-27 13:06:26,643 | DEBUG | httpcore.connection | close.complete
2026-01-27 13:06:27,804 | DEBUG | LiteLLM | [Non-Blocking] Unable to import GenericAPILogger - LiteLLM Enterprise Feature - No module named 'litellm_enterprise'
2026-01-27 13:06:28,237 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 13:06:28,237 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 13:06:28,238 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 13:06:28,245 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 13:06:28,246 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 13:06:28,246 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 13:06:28,246 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 13:06:28,247 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 13:06:28,247 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 13:06:28,248 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 13:06:28,248 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 13:06:28,248 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 13:06:28,288 | INFO | main | üöÄ FastAPI application starting
2026-01-27 13:06:36,457 | INFO | chat | üì© Chat request | user_id=3 | request_id=124cf192-c1bd-4ae6-bf0f-4f1acdee5edd
2026-01-27 13:06:36,508 | INFO | chat | üßë User message | hi
2026-01-27 13:06:36,815 | INFO | chat | ü§ñ Stream started | session_id=0ba7a5e8-8a18-47df-9ea1-9282cf4c76ae | request_id=124cf192-c1bd-4ae6-bf0f-4f1acdee5edd
2026-01-27 13:06:36,816 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-27 13:06:36,816 | INFO | orchestrator | üßë USER_INPUT | hi
2026-01-27 13:06:36,818 | INFO | orchestrator | üß© CONTEXT | user_id=3 | enable_memory=True | db=None
2026-01-27 13:06:36,819 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-27 13:06:36,820 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_6d2ab67dd0444b38be2047be53ce169d
2026-01-27 13:06:36,821 | DEBUG | openai.agents | Setting current trace: trace_6d2ab67dd0444b38be2047be53ce169d
2026-01-27 13:06:36,823 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x00000238982AA1B0> with id None
2026-01-27 13:06:36,823 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-27 13:06:36,833 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000002389823BE20> with id None
2026-01-27 13:06:36,834 | DEBUG | openai.agents | Calling LLM
2026-01-27 13:06:36,834 | DEBUG | LiteLLM | 

2026-01-27 13:06:36,836 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-27 13:06:36,836 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n', 'role': 'system'}, {'role': 'user', 'content': 'hi'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-27 13:06:36,837 | DEBUG | LiteLLM | 

2026-01-27 13:06:36,838 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-27 13:06:36,839 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-27 13:06:36,840 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-27 13:06:36,868 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:06:36,870 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-27 13:06:36,872 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n', 'role': 'system'}, {'role': 'user', 'content': 'hi'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-27 13:06:36,874 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-27 13:06:36,875 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-27 13:06:36,876 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-27 13:06:36,877 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:06:37,286 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:06:37,287 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'hi'}]}], 'system_instruction': {'parts': [{'text': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n'}]}, 'generationConfig': {}}'
[0m

2026-01-27 13:06:38,081 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "SAFE"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 188,
    "candidatesTokenCount": 1,
    "totalTokenCount": 215,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 188
      }
    ],
    "thoughtsTokenCount": 26
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "BWt4acaANM2O4-EP_cWxsQ4"
}



2026-01-27 13:06:38,085 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-27 13:06:38,087 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-27 13:06:38,088 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 13:06:38,089 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 13:06:38,090 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:06:38,090 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:06:38,091 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:06:38,092 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:06:38,092 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:06:38,094 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:06:38,094 | DEBUG | LiteLLM | response_cost: 0.0001239
2026-01-27 13:06:38,095 | DEBUG | LiteLLM | response_cost: 0.0001239
2026-01-27 13:06:38,096 | DEBUG | openai.agents | Received model response
2026-01-27 13:06:38,098 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:06:38,122 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x0000023898358280>>
2026-01-27 13:06:38,123 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:06:38,124 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:06:38,126 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-27 13:06:38,127 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-27 13:06:38,129 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:06:38,129 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-27 13:06:38,130 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-27 13:06:38,130 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:06:38,134 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:06:38,136 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-27 13:06:38,137 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:06:38,137 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:06:38,138 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:06:38,139 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:06:38,140 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:06:38,141 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:06:38,143 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-27 13:06:38,144 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0001239
2026-01-27 13:06:38,145 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:06:38,145 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:06:38,146 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:06:38,147 | DEBUG | openai.agents | Resetting current trace
2026-01-27 13:06:38,148 | ERROR | digital_human_sdk.app.intelligence.utils.json_utils | safe_json_loads failed
2026-01-27 13:06:38,151 | INFO | orchestrator | ‚úÖ Safety passed
2026-01-27 13:06:38,154 | INFO | orchestrator | üì• Fetching semantic memory (via MemoryService)
2026-01-27 13:06:38,272 | INFO | orchestrator | üß† MEMORY_RESULT | found=False | count=0
2026-01-27 13:06:38,273 | INFO | orchestrator | üß≠ Router agent called
2026-01-27 13:06:38,274 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_c43862f79c124c15883c51222acf92e0
2026-01-27 13:06:38,274 | DEBUG | openai.agents | Setting current trace: trace_c43862f79c124c15883c51222acf92e0
2026-01-27 13:06:38,275 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x00000238982D5C60> with id None
2026-01-27 13:06:38,275 | DEBUG | openai.agents | Running agent RouterAgent (turn 1)
2026-01-27 13:06:38,276 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000002389823A680> with id None
2026-01-27 13:06:38,277 | DEBUG | openai.agents | Calling LLM
2026-01-27 13:06:38,277 | DEBUG | LiteLLM | 

2026-01-27 13:06:38,277 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-27 13:06:38,279 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'hi'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-27 13:06:38,279 | DEBUG | LiteLLM | 

2026-01-27 13:06:38,280 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-27 13:06:38,281 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-27 13:06:38,281 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-27 13:06:38,284 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:06:38,284 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-27 13:06:38,285 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'hi'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-27 13:06:38,286 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-27 13:06:38,287 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-27 13:06:38,288 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-27 13:06:38,289 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:06:38,290 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:06:38,292 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'hi'}]}], 'system_instruction': {'parts': [{'text': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n'}]}, 'generationConfig': {}}'
[0m

2026-01-27 13:06:40,568 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\n  \"use_memory\": false,\n  \"use_tool\": false,\n  \"tool_name\": \"none\",\n  \"tool_arguments\": {},\n  \"intent\": \"none\",\n  \"memory_key\": null\n}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 486,
    "candidatesTokenCount": 55,
    "totalTokenCount": 596,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 486
      }
    ],
    "thoughtsTokenCount": 55
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "CGt4abeiD6LL4-EPvKC3gAw"
}



2026-01-27 13:06:40,570 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-27 13:06:40,572 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 13:06:40,572 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-27 13:06:40,573 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:06:40,575 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 13:06:40,575 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:06:40,576 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:06:40,577 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:06:40,578 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:06:40,579 | DEBUG | LiteLLM | response_cost: 0.00042080000000000004
2026-01-27 13:06:40,580 | DEBUG | openai.agents | Received model response
2026-01-27 13:06:40,580 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:06:40,581 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x0000023898A85AE0>>
2026-01-27 13:06:40,582 | DEBUG | LiteLLM | response_cost: 0.00042080000000000004
2026-01-27 13:06:40,582 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-27 13:06:40,584 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:06:40,585 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-27 13:06:40,585 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-27 13:06:40,586 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:06:40,586 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-27 13:06:40,587 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:06:40,589 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:06:40,590 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:06:40,591 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:06:40,592 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-27 13:06:40,592 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:06:40,592 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:06:40,594 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:06:40,594 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:06:40,596 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:06:40,596 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-27 13:06:40,597 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:06:40,597 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.00042080000000000004
2026-01-27 13:06:40,599 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:06:40,600 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:06:40,600 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:06:40,602 | DEBUG | openai.agents | Resetting current trace
2026-01-27 13:06:40,602 | INFO | orchestrator | üß≠ ROUTER_DECISION | tool=False | memory=False | intent=none
2026-01-27 13:06:40,604 | INFO | orchestrator | üß† REASONING_INPUT_SNAPSHOT
{
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful AI assistant.\n \nYou may answer ANY general knowledge question.\n \nConversation summaries and memories are ONLY to provide helpful context\n(e.g. preferences, ongoing plans).\n \nIf a user asks something unrelated to the current topic,\nanswer it normally using general knowledge.\n \nDo NOT restrict yourself to previous topics unless the user explicitly asks."
    },
    {
      "role": "assistant",
      "content": "Hello! How can I help you today?"
    },
    {
      "role": "user",
      "content": "what is my favourite color?"
    },
    {
      "role": "assistant",
      "content": "Your favorite color is yellow."
    },
    {
      "role": "user",
      "content": "what is weather in delhi now?"
    },
    {
      "role": "assistant",
      "content": "In Delhi, the current weather is drizzle with a temperature of 14.05\u00b0C. The humidity is 94% and the wind speed is 5.66 m/s."
    },
    {
      "role": "user",
      "content": "hi"
    },
    {
      "role": "user",
      "content": "hi"
    }
  ],
  "safety": {
    "safe": true,
    "message": null
  },
  "memory_action": {},
  "memory_data": [],
  "memory_found": false,
  "tool_context": {}
}
2026-01-27 13:06:40,607 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_77745410de944ff3953b052149ec578e
2026-01-27 13:06:40,608 | DEBUG | openai.agents | Setting current trace: trace_77745410de944ff3953b052149ec578e
2026-01-27 13:06:40,608 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x00000238983C5B70> with id None
2026-01-27 13:06:40,610 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000023898239600> with id None
2026-01-27 13:06:40,610 | DEBUG | openai.agents | Calling LLM
2026-01-27 13:06:40,611 | DEBUG | LiteLLM | 

2026-01-27 13:06:40,612 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-27 13:06:40,612 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "assistant", "content": "Hello! How can I help you today?"}, {"role": "user", "content": "what is my favourite color?"}, {"role": "assistant", "content": "Your favorite color is yellow."}, {"role": "user", "content": "what is weather in delhi now?"}, {"role": "assistant", "content": "In Delhi, the current weather is drizzle with a temperature of 14.05\\u00b0C. The humidity is 94% and the wind speed is 5.66 m/s."}, {"role": "user", "content": "hi"}, {"role": "user", "content": "hi"}], "safety": {"safe": true, "message": null}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=True, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-27 13:06:40,614 | DEBUG | LiteLLM | 

2026-01-27 13:06:40,615 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-27 13:06:40,616 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-27 13:06:40,616 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-27 13:06:40,618 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:06:40,619 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-27 13:06:40,620 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "assistant", "content": "Hello! How can I help you today?"}, {"role": "user", "content": "what is my favourite color?"}, {"role": "assistant", "content": "Your favorite color is yellow."}, {"role": "user", "content": "what is weather in delhi now?"}, {"role": "assistant", "content": "In Delhi, the current weather is drizzle with a temperature of 14.05\\u00b0C. The humidity is 94% and the wind speed is 5.66 m/s."}, {"role": "user", "content": "hi"}, {"role": "user", "content": "hi"}], "safety": {"safe": true, "message": null}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-27 13:06:40,621 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': True}
2026-01-27 13:06:40,622 | DEBUG | LiteLLM | Final returned optional params: {'stream': True}
2026-01-27 13:06:40,622 | DEBUG | LiteLLM | self.optional_params: {'stream': True}
2026-01-27 13:06:40,623 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:06:40,624 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:06:40,626 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?key=*****=sse \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "assistant", "content": "Hello! How can I help you today?"}, {"role": "user", "content": "what is my favourite color?"}, {"role": "assistant", "content": "Your favorite color is yellow."}, {"role": "user", "content": "what is weather in delhi now?"}, {"role": "assistant", "content": "In Delhi, the current weather is drizzle with a temperature of 14.05\\u00b0C. The humidity is 94% and the wind speed is 5.66 m/s."}, {"role": "user", "content": "hi"}, {"role": "user", "content": "hi"}], "safety": {"safe": true, "message": null}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}]}], 'system_instruction': {'parts': [{'text': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.'}]}, 'generationConfig': {}}'
[0m

2026-01-27 13:06:40,628 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 13:06:40,631 | DEBUG | LiteLLM | messages in token_counter: None, text in token_counter: 
2026-01-27 13:06:40,632 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:06:40,633 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:06:40,634 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:06:40,635 | DEBUG | LiteLLM | response_cost: 0.0
2026-01-27 13:06:40,734 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 13:06:40,734 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 13:06:40,735 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 13:06:41,538 | DEBUG | LiteLLM | RAW RESPONSE:
first stream response received


2026-01-27 13:06:41,539 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': 'Hi'}], 'role': 'model'}, 'index': 0}], 'usageMetadata': {'promptTokenCount': 402, 'candidatesTokenCount': 1, 'totalTokenCount': 419, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 402}], 'thoughtsTokenCount': 16}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'CWt4adO_CKG14-EPgIeNqQ4'}
2026-01-27 13:06:41,549 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='CWt4adO_CKG14-EPgIeNqQ4', created=1769499401, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='Hi', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=17, prompt_tokens=402, total_tokens=419, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=16, rejected_prediction_tokens=None, text_tokens=1, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=402, image_tokens=None)))
2026-01-27 13:06:41,552 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content='Hi', role='assistant', function_call=None, tool_calls=None, audio=None)
2026-01-27 13:06:41,553 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='CWt4adO_CKG14-EPgIeNqQ4', created=1769499401, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Hi', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=17, prompt_tokens=402, total_tokens=419, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=16, rejected_prediction_tokens=None, text_tokens=1, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=402, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-27 13:06:41,604 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': ' there! How can I help you?'}], 'role': 'model'}, 'finishReason': 'STOP', 'index': 0}], 'usageMetadata': {'promptTokenCount': 402, 'candidatesTokenCount': 9, 'totalTokenCount': 427, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 402}], 'thoughtsTokenCount': 16}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'CWt4adO_CKG14-EPgIeNqQ4'}
2026-01-27 13:06:41,607 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='CWt4adO_CKG14-EPgIeNqQ4', created=1769499401, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content=' there! How can I help you?', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=25, prompt_tokens=402, total_tokens=427, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=16, rejected_prediction_tokens=None, text_tokens=9, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=402, image_tokens=None)))
2026-01-27 13:06:41,609 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content=' there! How can I help you?', role=None, function_call=None, tool_calls=None, audio=None)
2026-01-27 13:06:41,610 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='CWt4adO_CKG14-EPgIeNqQ4', created=1769499401, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' there! How can I help you?', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=25, prompt_tokens=402, total_tokens=427, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=16, rejected_prediction_tokens=None, text_tokens=9, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=402, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-27 13:06:41,634 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=False
2026-01-27 13:06:41,669 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=False
2026-01-27 13:06:41,670 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-27 13:06:41,671 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-27 13:06:41,672 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 13:06:41,672 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 13:06:41,674 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:06:41,675 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:06:41,676 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:06:41,676 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:06:41,677 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:06:41,678 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:06:41,678 | DEBUG | LiteLLM | response_cost: 0.0001831
2026-01-27 13:06:41,679 | DEBUG | LiteLLM | response_cost: 0.0001831
2026-01-27 13:06:41,681 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:06:41,682 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0001831
2026-01-27 13:06:41,683 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:06:41,684 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:06:41,685 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:06:41,685 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:06:41,687 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:06:41,689 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-27 13:06:41,690 | DEBUG | openai.agents | Resetting current trace
2026-01-27 13:06:41,704 | INFO | session_summary | üß† SUMMARY_START | session_id=0ba7a5e8-8a18-47df-9ea1-9282cf4c76ae
2026-01-27 13:06:41,714 | INFO | session_summary | üìä Unsummarized messages = 8
2026-01-27 13:06:41,715 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-27 13:06:41,716 | INFO | chat | ‚úÖ Stream complete | tokens=2 | time=5.26s
2026-01-27 13:06:41,978 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-27 13:06:42,008 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000238981B5AE0>
2026-01-27 13:06:42,008 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x000002388CE956C0> server_hostname='api.openai.com' timeout=5.0
2026-01-27 13:06:42,029 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000238981B5390>
2026-01-27 13:06:42,029 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-27 13:06:42,030 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-27 13:06:42,030 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-27 13:06:42,031 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-27 13:06:42,031 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-27 13:06:43,520 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Tue, 27 Jan 2026 07:36:43 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_6851cb3f0e49a1c725a9b186da0143d4'), (b'openai-processing-ms', b'327'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'330'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=o10qwMplwI7Oos82_gZCQKOCLKejWSS2g8vNmOVjkHo-1769499403-1.0.1.1-2F42JMavKoVhLO8hQpqPUwMjrW9iv_FMwKzw_jz3IEjweQuHYDeX6OIXm23J8PXwwjuMTc7pky5APeyLdA7batcsmiVt6DUajjwV9g.dkiE; path=/; expires=Tue, 27-Jan-26 08:06:43 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=8VZYeEeUisvHDcH1Y8u72BS9l9cUcTv02rOGhagxpNo-1769499403413-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c46949e0ad68e7a-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-27 13:06:43,521 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-27 13:06:43,522 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-27 13:06:43,522 | DEBUG | httpcore.http11 | response_closed.started
2026-01-27 13:06:43,523 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-27 13:06:43,523 | DEBUG | openai.agents | Exported 9 items
2026-01-27 13:06:53,889 | DEBUG | LiteLLM | LoggingWorker cancelled during shutdown
2026-01-27 13:06:53,890 | DEBUG | LiteLLM | [LoggingWorker] atexit: Queue is empty
2026-01-27 13:06:53,891 | DEBUG | asyncio | Using proactor: IocpProactor
2026-01-27 13:06:53,892 | DEBUG | openai.agents | Shutting down trace provider
2026-01-27 13:06:53,892 | DEBUG | openai.agents | Shutting down trace processor <agents.tracing.processors.BatchTraceProcessor object at 0x000002388CF04220>
2026-01-27 13:06:54,108 | ERROR | asyncio | Fatal error on SSL transport
protocol: <asyncio.sslproto.SSLProtocol object at 0x000002389835A6B0>
transport: <_SelectorSocketTransport closing fd=776>
Traceback (most recent call last):
  File "C:\Users\DELL\AppData\Local\Programs\Python\Python310\lib\asyncio\selector_events.py", line 924, in write
    n = self._sock.send(data)
OSError: [WinError 10038] An operation was attempted on something that is not a socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\DELL\AppData\Local\Programs\Python\Python310\lib\asyncio\sslproto.py", line 690, in _process_write_backlog
    self._transport.write(chunk)
  File "C:\Users\DELL\AppData\Local\Programs\Python\Python310\lib\asyncio\selector_events.py", line 930, in write
    self._fatal_error(exc, 'Fatal write error on socket transport')
  File "C:\Users\DELL\AppData\Local\Programs\Python\Python310\lib\asyncio\selector_events.py", line 725, in _fatal_error
    self._force_close(exc)
  File "C:\Users\DELL\AppData\Local\Programs\Python\Python310\lib\asyncio\selector_events.py", line 737, in _force_close
    self._loop.call_soon(self._call_connection_lost, exc)
  File "C:\Users\DELL\AppData\Local\Programs\Python\Python310\lib\asyncio\base_events.py", line 753, in call_soon
    self._check_closed()
  File "C:\Users\DELL\AppData\Local\Programs\Python\Python310\lib\asyncio\base_events.py", line 515, in _check_closed
    raise RuntimeError('Event loop is closed')
RuntimeError: Event loop is closed
2026-01-27 13:07:02,164 | INFO | sentence_transformers.SentenceTransformer | Use pytorch device_name: cpu
2026-01-27 13:07:02,165 | INFO | sentence_transformers.SentenceTransformer | Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2026-01-27 13:07:02,167 | DEBUG | urllib3.connectionpool | Starting new HTTPS connection (1): huggingface.co:443
2026-01-27 13:07:02,453 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-27 13:07:02,474 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-27 13:07:02,714 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-27 13:07:02,725 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-27 13:07:02,980 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-27 13:07:02,992 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-27 13:07:03,259 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md HTTP/1.1" 307 0
2026-01-27 13:07:03,270 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/README.md HTTP/1.1" 200 0
2026-01-27 13:07:03,534 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-27 13:07:03,546 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-27 13:07:03,792 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json HTTP/1.1" 307 0
2026-01-27 13:07:03,803 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/sentence_bert_config.json HTTP/1.1" 200 0
2026-01-27 13:07:04,054 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json HTTP/1.1" 404 0
2026-01-27 13:07:04,348 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1" 307 0
2026-01-27 13:07:04,362 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json HTTP/1.1" 200 0
2026-01-27 13:07:04,802 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/1.1" 307 0
2026-01-27 13:07:04,819 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json HTTP/1.1" 200 0
2026-01-27 13:07:05,067 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1" 404 64
2026-01-27 13:07:05,327 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main?recursive=True&expand=False HTTP/1.1" 200 6465
2026-01-27 13:07:05,665 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json HTTP/1.1" 307 0
2026-01-27 13:07:05,681 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json HTTP/1.1" 200 0
2026-01-27 13:07:05,922 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2 HTTP/1.1" 200 6825
2026-01-27 13:07:09,192 | DEBUG | httpcore.connection | connect_tcp.started host='raw.githubusercontent.com' port=443 local_address=None timeout=5 socket_options=None
2026-01-27 13:07:09,245 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001DCDE635420>
2026-01-27 13:07:09,245 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x000001DCDE475AC0> server_hostname='raw.githubusercontent.com' timeout=5
2026-01-27 13:07:09,278 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001DCDE6353F0>
2026-01-27 13:07:09,278 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'GET']>
2026-01-27 13:07:09,279 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-27 13:07:09,280 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'GET']>
2026-01-27 13:07:09,280 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-27 13:07:09,280 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'GET']>
2026-01-27 13:07:09,311 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Connection', b'keep-alive'), (b'Content-Length', b'62322'), (b'Cache-Control', b'max-age=300'), (b'Content-Security-Policy', b"default-src 'none'; style-src 'unsafe-inline'; sandbox"), (b'Content-Type', b'text/plain; charset=utf-8'), (b'ETag', b'W/"5f0be0ed2fe5ecf561c4b4488d64c304c35b980a45012ef1d1253b4b5d438af1"'), (b'Strict-Transport-Security', b'max-age=31536000'), (b'X-Content-Type-Options', b'nosniff'), (b'X-Frame-Options', b'deny'), (b'X-XSS-Protection', b'1; mode=block'), (b'X-GitHub-Request-Id', b'415E:3865D1:30D7E6:7A896E:69753835'), (b'Content-Encoding', b'gzip'), (b'Accept-Ranges', b'bytes'), (b'Date', b'Tue, 27 Jan 2026 07:37:09 GMT'), (b'Via', b'1.1 varnish'), (b'X-Served-By', b'cache-bom-vanm7210093-BOM'), (b'X-Cache', b'HIT'), (b'X-Cache-Hits', b'23'), (b'X-Timer', b'S1769499429.215597,VS0,VE0'), (b'Vary', b'Authorization,Accept-Encoding'), (b'Access-Control-Allow-Origin', b'*'), (b'Cross-Origin-Resource-Policy', b'cross-origin'), (b'X-Fastly-Request-ID', b'15dbcee62efbb5d3c7d7598f29adf305fffee97e'), (b'Expires', b'Tue, 27 Jan 2026 07:42:09 GMT'), (b'Source-Age', b'165')])
2026-01-27 13:07:09,313 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'GET']>
2026-01-27 13:07:09,350 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-27 13:07:09,351 | DEBUG | httpcore.http11 | response_closed.started
2026-01-27 13:07:09,352 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-27 13:07:09,353 | DEBUG | httpcore.connection | close.started
2026-01-27 13:07:09,353 | DEBUG | httpcore.connection | close.complete
2026-01-27 13:07:10,946 | DEBUG | LiteLLM | [Non-Blocking] Unable to import GenericAPILogger - LiteLLM Enterprise Feature - No module named 'litellm_enterprise'
2026-01-27 13:07:11,778 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 13:07:11,779 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 13:07:11,780 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 13:07:11,790 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 13:07:11,791 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 13:07:11,791 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 13:07:11,792 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 13:07:11,793 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 13:07:11,793 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 13:07:11,794 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 13:07:11,795 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 13:07:11,795 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 13:07:11,855 | INFO | main | üöÄ FastAPI application starting
2026-01-27 13:21:06,067 | DEBUG | asyncio | Using proactor: IocpProactor
2026-01-27 13:21:06,068 | DEBUG | openai.agents | Shutting down trace provider
2026-01-27 13:21:06,068 | DEBUG | openai.agents | Shutting down trace processor <agents.tracing.processors.BatchTraceProcessor object at 0x000001DCDC4241C0>
2026-01-27 13:21:15,042 | INFO | sentence_transformers.SentenceTransformer | Use pytorch device_name: cpu
2026-01-27 13:21:15,043 | INFO | sentence_transformers.SentenceTransformer | Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2026-01-27 13:21:15,046 | DEBUG | urllib3.connectionpool | Starting new HTTPS connection (1): huggingface.co:443
2026-01-27 13:21:15,332 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-27 13:21:15,342 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-27 13:21:15,980 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-27 13:21:15,994 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-27 13:21:16,223 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-27 13:21:16,234 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-27 13:21:16,485 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md HTTP/1.1" 307 0
2026-01-27 13:21:16,500 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/README.md HTTP/1.1" 200 0
2026-01-27 13:21:16,732 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-27 13:21:16,747 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-27 13:21:16,974 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json HTTP/1.1" 307 0
2026-01-27 13:21:16,991 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/sentence_bert_config.json HTTP/1.1" 200 0
2026-01-27 13:21:17,234 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json HTTP/1.1" 404 0
2026-01-27 13:21:17,464 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1" 307 0
2026-01-27 13:21:17,478 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json HTTP/1.1" 200 0
2026-01-27 13:21:17,788 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/1.1" 307 0
2026-01-27 13:21:17,798 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json HTTP/1.1" 200 0
2026-01-27 13:21:18,035 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1" 404 64
2026-01-27 13:21:18,288 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main?recursive=True&expand=False HTTP/1.1" 200 6465
2026-01-27 13:21:18,553 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json HTTP/1.1" 307 0
2026-01-27 13:21:18,564 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json HTTP/1.1" 200 0
2026-01-27 13:21:18,798 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2 HTTP/1.1" 200 6825
2026-01-27 13:21:19,866 | DEBUG | httpcore.connection | connect_tcp.started host='raw.githubusercontent.com' port=443 local_address=None timeout=5 socket_options=None
2026-01-27 13:21:19,898 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000020DA26D53F0>
2026-01-27 13:21:19,898 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000020DA2515AC0> server_hostname='raw.githubusercontent.com' timeout=5
2026-01-27 13:21:19,933 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000020DA26D53C0>
2026-01-27 13:21:19,934 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'GET']>
2026-01-27 13:21:19,934 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-27 13:21:19,934 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'GET']>
2026-01-27 13:21:19,935 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-27 13:21:19,935 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'GET']>
2026-01-27 13:21:19,966 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Connection', b'keep-alive'), (b'Content-Length', b'62322'), (b'Cache-Control', b'max-age=300'), (b'Content-Security-Policy', b"default-src 'none'; style-src 'unsafe-inline'; sandbox"), (b'Content-Type', b'text/plain; charset=utf-8'), (b'ETag', b'W/"5f0be0ed2fe5ecf561c4b4488d64c304c35b980a45012ef1d1253b4b5d438af1"'), (b'Strict-Transport-Security', b'max-age=31536000'), (b'X-Content-Type-Options', b'nosniff'), (b'X-Frame-Options', b'deny'), (b'X-XSS-Protection', b'1; mode=block'), (b'X-GitHub-Request-Id', b'415E:3865D1:30D7E6:7A896E:69753835'), (b'Content-Encoding', b'gzip'), (b'Accept-Ranges', b'bytes'), (b'Date', b'Tue, 27 Jan 2026 07:51:19 GMT'), (b'Via', b'1.1 varnish'), (b'X-Served-By', b'cache-bom-vanm7210067-BOM'), (b'X-Cache', b'HIT'), (b'X-Cache-Hits', b'22'), (b'X-Timer', b'S1769500280.869439,VS0,VE0'), (b'Vary', b'Authorization,Accept-Encoding'), (b'Access-Control-Allow-Origin', b'*'), (b'Cross-Origin-Resource-Policy', b'cross-origin'), (b'X-Fastly-Request-ID', b'bad2b71a976b29763061a18a036d5159841cbd4a'), (b'Expires', b'Tue, 27 Jan 2026 07:56:19 GMT'), (b'Source-Age', b'114')])
2026-01-27 13:21:19,967 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'GET']>
2026-01-27 13:21:20,008 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-27 13:21:20,008 | DEBUG | httpcore.http11 | response_closed.started
2026-01-27 13:21:20,008 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-27 13:21:20,009 | DEBUG | httpcore.connection | close.started
2026-01-27 13:21:20,009 | DEBUG | httpcore.connection | close.complete
2026-01-27 13:21:20,674 | DEBUG | LiteLLM | [Non-Blocking] Unable to import GenericAPILogger - LiteLLM Enterprise Feature - No module named 'litellm_enterprise'
2026-01-27 13:21:21,035 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 13:21:21,036 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 13:21:21,036 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 13:21:21,041 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 13:21:21,041 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 13:21:21,042 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 13:21:21,042 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 13:21:21,042 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 13:21:21,042 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 13:21:21,043 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 13:21:21,043 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 13:21:21,043 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 13:21:21,075 | INFO | main | üöÄ FastAPI application starting
2026-01-27 13:22:19,171 | INFO | chat | üì© Chat request | user_id=4 | request_id=2db4b9a2-3a8d-4ac2-bc19-fea685d41bb4
2026-01-27 13:22:19,179 | INFO | chat | üßë User message | hi 
2026-01-27 13:22:19,192 | INFO | chat | üÜï New session created | 27be17d2-4892-42ed-80d2-74ba6e13ea5a
2026-01-27 13:22:19,209 | INFO | chat | ü§ñ Stream started | session_id=27be17d2-4892-42ed-80d2-74ba6e13ea5a | request_id=2db4b9a2-3a8d-4ac2-bc19-fea685d41bb4
2026-01-27 13:22:19,210 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-27 13:22:19,211 | INFO | orchestrator | üßë USER_INPUT | hi 
2026-01-27 13:22:19,212 | INFO | orchestrator | üß© CONTEXT | user_id=4 | enable_memory=True | db=None
2026-01-27 13:22:19,213 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-27 13:22:19,214 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_5e5d9de1092b4bda82cb7c51b8e3d80d
2026-01-27 13:22:19,215 | DEBUG | openai.agents | Setting current trace: trace_5e5d9de1092b4bda82cb7c51b8e3d80d
2026-01-27 13:22:19,216 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x0000020DAB86DE40> with id None
2026-01-27 13:22:19,217 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-27 13:22:19,224 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000020DAB85E200> with id None
2026-01-27 13:22:19,225 | DEBUG | openai.agents | Calling LLM
2026-01-27 13:22:19,225 | DEBUG | LiteLLM | 

2026-01-27 13:22:19,226 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-27 13:22:19,227 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n', 'role': 'system'}, {'role': 'user', 'content': 'hi '}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-27 13:22:19,227 | DEBUG | LiteLLM | 

2026-01-27 13:22:19,228 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-27 13:22:19,228 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-27 13:22:19,229 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-27 13:22:19,247 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:22:19,249 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-27 13:22:19,257 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n', 'role': 'system'}, {'role': 'user', 'content': 'hi '}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-27 13:22:19,259 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-27 13:22:19,259 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-27 13:22:19,260 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-27 13:22:19,261 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:22:19,632 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:22:19,634 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'hi '}]}], 'system_instruction': {'parts': [{'text': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n'}]}, 'generationConfig': {}}'
[0m

2026-01-27 13:22:20,826 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "SAFE"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 189,
    "candidatesTokenCount": 1,
    "totalTokenCount": 214,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 189
      }
    ],
    "thoughtsTokenCount": 24
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "tG54abObI6ir4-EPoMPkkQ4"
}



2026-01-27 13:22:20,837 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-27 13:22:20,839 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-27 13:22:20,840 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 13:22:20,841 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 13:22:20,842 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:22:20,843 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:22:20,843 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:22:20,844 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:22:20,844 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:22:20,846 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:22:20,847 | DEBUG | LiteLLM | response_cost: 0.00011920000000000001
2026-01-27 13:22:20,848 | DEBUG | LiteLLM | response_cost: 0.00011920000000000001
2026-01-27 13:22:20,849 | DEBUG | openai.agents | Received model response
2026-01-27 13:22:20,850 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:22:20,868 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x0000020DAB9051E0>>
2026-01-27 13:22:20,868 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:22:20,870 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:22:20,871 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-27 13:22:20,872 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-27 13:22:20,876 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-27 13:22:20,874 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:22:20,875 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-27 13:22:20,877 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-27 13:22:20,878 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:22:20,879 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:22:20,880 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:22:20,881 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:22:20,882 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:22:20,882 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:22:20,884 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:22:20,884 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:22:20,887 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-27 13:22:20,888 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.00011920000000000001
2026-01-27 13:22:20,889 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:22:20,890 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:22:20,890 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:22:20,892 | DEBUG | openai.agents | Resetting current trace
2026-01-27 13:22:20,892 | ERROR | digital_human_sdk.app.intelligence.utils.json_utils | safe_json_loads failed
2026-01-27 13:22:20,894 | INFO | orchestrator | ‚úÖ Safety passed
2026-01-27 13:22:20,895 | INFO | orchestrator | üì• Fetching semantic memory (via MemoryService)
2026-01-27 13:22:20,943 | INFO | orchestrator | üß† MEMORY_RESULT | found=False | count=0
2026-01-27 13:22:20,944 | INFO | orchestrator | üß≠ Router agent called
2026-01-27 13:22:20,946 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_6e387ff0970e440ca8c72acb70c4c41b
2026-01-27 13:22:20,946 | DEBUG | openai.agents | Setting current trace: trace_6e387ff0970e440ca8c72acb70c4c41b
2026-01-27 13:22:20,947 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x0000020DAB976E80> with id None
2026-01-27 13:22:20,947 | DEBUG | openai.agents | Running agent RouterAgent (turn 1)
2026-01-27 13:22:20,948 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000020DABE53580> with id None
2026-01-27 13:22:20,949 | DEBUG | openai.agents | Calling LLM
2026-01-27 13:22:20,949 | DEBUG | LiteLLM | 

2026-01-27 13:22:20,950 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-27 13:22:20,950 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'hi '}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-27 13:22:20,952 | DEBUG | LiteLLM | 

2026-01-27 13:22:20,953 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-27 13:22:20,954 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-27 13:22:20,954 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-27 13:22:20,956 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:22:20,957 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-27 13:22:20,958 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'hi '}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-27 13:22:20,958 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-27 13:22:20,958 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-27 13:22:20,960 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-27 13:22:20,960 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:22:20,961 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:22:20,962 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'hi '}]}], 'system_instruction': {'parts': [{'text': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n'}]}, 'generationConfig': {}}'
[0m

2026-01-27 13:22:22,771 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\"use_memory\": false, \"use_tool\": false, \"tool_name\": \"none\", \"tool_arguments\": null, \"intent\": \"none\", \"memory_key\": null}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 487,
    "candidatesTokenCount": 42,
    "totalTokenCount": 613,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 487
      }
    ],
    "thoughtsTokenCount": 84
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "tm54af29HeS_juMPhrnq-As"
}



2026-01-27 13:22:22,773 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-27 13:22:22,774 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 13:22:22,774 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-27 13:22:22,775 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:22:22,776 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 13:22:22,777 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:22:22,777 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:22:22,779 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:22:22,779 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:22:22,780 | DEBUG | LiteLLM | response_cost: 0.0004611
2026-01-27 13:22:22,781 | DEBUG | openai.agents | Received model response
2026-01-27 13:22:22,780 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:22:22,782 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x0000020DAC6665F0>>
2026-01-27 13:22:22,782 | DEBUG | LiteLLM | response_cost: 0.0004611
2026-01-27 13:22:22,783 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-27 13:22:22,784 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:22:22,785 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-27 13:22:22,785 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-27 13:22:22,786 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:22:22,787 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-27 13:22:22,789 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:22:22,789 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:22:22,791 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:22:22,791 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:22:22,792 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-27 13:22:22,792 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:22:22,793 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:22:22,794 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:22:22,794 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:22:22,796 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:22:22,797 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-27 13:22:22,798 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:22:22,798 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0004611
2026-01-27 13:22:22,799 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:22:22,799 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:22:22,801 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:22:22,802 | DEBUG | openai.agents | Resetting current trace
2026-01-27 13:22:22,803 | INFO | orchestrator | üß≠ ROUTER_DECISION | tool=False | memory=False | intent=none
2026-01-27 13:22:22,803 | INFO | orchestrator | üß† REASONING_INPUT_SNAPSHOT
{
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful AI assistant.\n \nYou may answer ANY general knowledge question.\n \nConversation summaries and memories are ONLY to provide helpful context\n(e.g. preferences, ongoing plans).\n \nIf a user asks something unrelated to the current topic,\nanswer it normally using general knowledge.\n \nDo NOT restrict yourself to previous topics unless the user explicitly asks."
    },
    {
      "role": "user",
      "content": "hi "
    },
    {
      "role": "user",
      "content": "hi "
    }
  ],
  "safety": {
    "safe": true,
    "message": null
  },
  "memory_action": {},
  "memory_data": [],
  "memory_found": false,
  "tool_context": {}
}
2026-01-27 13:22:22,806 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_270ae35e596d4ec19e337db9b521eda0
2026-01-27 13:22:22,806 | DEBUG | openai.agents | Setting current trace: trace_270ae35e596d4ec19e337db9b521eda0
2026-01-27 13:22:22,806 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x0000020DAB842390> with id None
2026-01-27 13:22:22,809 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000020DABE7C160> with id None
2026-01-27 13:22:22,809 | DEBUG | openai.agents | Calling LLM
2026-01-27 13:22:22,810 | DEBUG | LiteLLM | 

2026-01-27 13:22:22,810 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-27 13:22:22,811 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "hi "}, {"role": "user", "content": "hi "}], "safety": {"safe": true, "message": null}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=True, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-27 13:22:22,811 | DEBUG | LiteLLM | 

2026-01-27 13:22:22,812 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-27 13:22:22,814 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-27 13:22:22,814 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-27 13:22:22,816 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:22:22,817 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-27 13:22:22,819 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "hi "}, {"role": "user", "content": "hi "}], "safety": {"safe": true, "message": null}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-27 13:22:22,820 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': True}
2026-01-27 13:22:22,820 | DEBUG | LiteLLM | Final returned optional params: {'stream': True}
2026-01-27 13:22:22,821 | DEBUG | LiteLLM | self.optional_params: {'stream': True}
2026-01-27 13:22:22,821 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:22:22,822 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:22:22,823 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?key=*****=sse \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "hi "}, {"role": "user", "content": "hi "}], "safety": {"safe": true, "message": null}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}]}], 'system_instruction': {'parts': [{'text': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.'}]}, 'generationConfig': {}}'
[0m

2026-01-27 13:22:22,825 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 13:22:22,828 | DEBUG | LiteLLM | messages in token_counter: None, text in token_counter: 
2026-01-27 13:22:22,830 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:22:22,830 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:22:22,831 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:22:22,832 | DEBUG | LiteLLM | response_cost: 0.0
2026-01-27 13:22:22,905 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 13:22:22,906 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 13:22:22,907 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 13:22:24,063 | DEBUG | LiteLLM | RAW RESPONSE:
first stream response received


2026-01-27 13:22:24,065 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': 'Hello!'}], 'role': 'model'}, 'index': 0}], 'usageMetadata': {'promptTokenCount': 275, 'candidatesTokenCount': 2, 'totalTokenCount': 293, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 275}], 'thoughtsTokenCount': 16}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 't254aY_YKZO4juMPo-SzuA4'}
2026-01-27 13:22:24,074 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='t254aY_YKZO4juMPo-SzuA4', created=1769500344, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='Hello!', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=18, prompt_tokens=275, total_tokens=293, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=16, rejected_prediction_tokens=None, text_tokens=2, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=275, image_tokens=None)))
2026-01-27 13:22:24,077 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content='Hello!', role='assistant', function_call=None, tool_calls=None, audio=None)
2026-01-27 13:22:24,078 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='t254aY_YKZO4juMPo-SzuA4', created=1769500344, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Hello!', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=18, prompt_tokens=275, total_tokens=293, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=16, rejected_prediction_tokens=None, text_tokens=2, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=275, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-27 13:22:24,116 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': ' How can I help you today?'}], 'role': 'model'}, 'finishReason': 'STOP', 'index': 0}], 'usageMetadata': {'promptTokenCount': 275, 'candidatesTokenCount': 9, 'totalTokenCount': 300, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 275}], 'thoughtsTokenCount': 16}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 't254aY_YKZO4juMPo-SzuA4'}
2026-01-27 13:22:24,117 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='t254aY_YKZO4juMPo-SzuA4', created=1769500344, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content=' How can I help you today?', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=25, prompt_tokens=275, total_tokens=300, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=16, rejected_prediction_tokens=None, text_tokens=9, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=275, image_tokens=None)))
2026-01-27 13:22:24,119 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content=' How can I help you today?', role=None, function_call=None, tool_calls=None, audio=None)
2026-01-27 13:22:24,120 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='t254aY_YKZO4juMPo-SzuA4', created=1769500344, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' How can I help you today?', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=25, prompt_tokens=275, total_tokens=300, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=16, rejected_prediction_tokens=None, text_tokens=9, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=275, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-27 13:22:24,134 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=False
2026-01-27 13:22:24,161 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=False
2026-01-27 13:22:24,162 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-27 13:22:24,162 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-27 13:22:24,163 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 13:22:24,164 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 13:22:24,166 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:22:24,167 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:22:24,167 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:22:24,168 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:22:24,169 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:22:24,170 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:22:24,172 | DEBUG | LiteLLM | response_cost: 0.000145
2026-01-27 13:22:24,173 | DEBUG | LiteLLM | response_cost: 0.000145
2026-01-27 13:22:24,174 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:22:24,174 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.000145
2026-01-27 13:22:24,175 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:22:24,175 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:22:24,176 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:22:24,177 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:22:24,179 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:22:24,180 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-27 13:22:24,182 | DEBUG | openai.agents | Resetting current trace
2026-01-27 13:22:24,193 | INFO | session_summary | üß† SUMMARY_START | session_id=27be17d2-4892-42ed-80d2-74ba6e13ea5a
2026-01-27 13:22:24,206 | INFO | session_summary | üìä Unsummarized messages = 2
2026-01-27 13:22:24,207 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-27 13:22:24,208 | INFO | chat | ‚úÖ Stream complete | tokens=2 | time=5.04s
2026-01-27 13:22:24,323 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-27 13:22:24,351 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000020DAB73E710>
2026-01-27 13:22:24,352 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000020DA0481540> server_hostname='api.openai.com' timeout=5.0
2026-01-27 13:22:24,368 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000020DAB7C9600>
2026-01-27 13:22:24,369 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-27 13:22:24,371 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-27 13:22:24,371 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-27 13:22:24,372 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-27 13:22:24,373 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-27 13:22:25,025 | INFO | chat | üì© Chat request | user_id=4 | request_id=8831ad9e-29c5-45c8-840a-2c91ba8550f0
2026-01-27 13:22:25,026 | INFO | chat | üßë User message | hi 
2026-01-27 13:22:25,039 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Tue, 27 Jan 2026 07:52:24 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_2843ba8966a587cf22e4c87f9f043e8e'), (b'openai-processing-ms', b'320'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'323'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=OHKht_6r1TKN7tP92U1sKLodkxkvZ.s6AijSuPJTJeE-1769500344-1.0.1.1-PMx0mWS9rgHyTql4ltYGbC8M4qyLH.F7ZuHnZwUuqP7LiRcDuKIgjthcDKp8GI4KSgYvFFaEhv2ElX6_F0bAyPXQ0Q4pOuN5L6Odkm4LoVo; path=/; expires=Tue, 27-Jan-26 08:22:24 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=hRLxQvjOoJcCG_WZQ0prJDZSEnBkcEpngqcVRsc.MoE-1769500344917-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c46ab9fae1c59e2-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-27 13:22:25,043 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-27 13:22:25,045 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-27 13:22:25,047 | DEBUG | httpcore.http11 | response_closed.started
2026-01-27 13:22:25,048 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-27 13:22:25,047 | INFO | chat | ü§ñ Stream started | session_id=27be17d2-4892-42ed-80d2-74ba6e13ea5a | request_id=8831ad9e-29c5-45c8-840a-2c91ba8550f0
2026-01-27 13:22:25,048 | DEBUG | openai.agents | Exported 9 items
2026-01-27 13:22:25,048 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-27 13:22:25,050 | INFO | orchestrator | üßë USER_INPUT | hi 
2026-01-27 13:22:25,051 | INFO | orchestrator | üß© CONTEXT | user_id=4 | enable_memory=True | db=None
2026-01-27 13:22:25,053 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-27 13:22:25,054 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_f29e0a4214f049ffbd2404eb2a09a6a6
2026-01-27 13:22:25,055 | DEBUG | openai.agents | Setting current trace: trace_f29e0a4214f049ffbd2404eb2a09a6a6
2026-01-27 13:22:25,055 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x0000020DAB86DE40> with id None
2026-01-27 13:22:25,055 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-27 13:22:25,057 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000020DAB89A500> with id None
2026-01-27 13:22:25,057 | DEBUG | openai.agents | Calling LLM
2026-01-27 13:22:25,058 | DEBUG | LiteLLM | 

2026-01-27 13:22:25,058 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-27 13:22:25,059 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n', 'role': 'system'}, {'role': 'user', 'content': 'hi '}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-27 13:22:25,059 | DEBUG | LiteLLM | 

2026-01-27 13:22:25,060 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-27 13:22:25,060 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-27 13:22:25,061 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-27 13:22:25,062 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:22:25,063 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-27 13:22:25,064 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n', 'role': 'system'}, {'role': 'user', 'content': 'hi '}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-27 13:22:25,065 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-27 13:22:25,065 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-27 13:22:25,066 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-27 13:22:25,067 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:22:25,068 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:22:25,069 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'hi '}]}], 'system_instruction': {'parts': [{'text': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n'}]}, 'generationConfig': {}}'
[0m

2026-01-27 13:22:27,965 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "SAFE"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 189,
    "candidatesTokenCount": 1,
    "totalTokenCount": 223,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 189
      }
    ],
    "thoughtsTokenCount": 33
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "u254advEKqzeg8UP8f-MkQI"
}



2026-01-27 13:22:27,967 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-27 13:22:27,968 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 13:22:27,968 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-27 13:22:27,970 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:22:27,971 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 13:22:27,972 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:22:27,973 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:22:27,974 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:22:27,974 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:22:27,975 | DEBUG | LiteLLM | response_cost: 0.00014170000000000003
2026-01-27 13:22:27,976 | DEBUG | openai.agents | Received model response
2026-01-27 13:22:27,976 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:22:27,977 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x0000020DABE783D0>>
2026-01-27 13:22:27,978 | DEBUG | LiteLLM | response_cost: 0.00014170000000000003
2026-01-27 13:22:27,978 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-27 13:22:27,981 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:22:27,982 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-27 13:22:27,981 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-27 13:22:27,982 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:22:27,983 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-27 13:22:27,985 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:22:27,986 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:22:27,987 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:22:27,987 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:22:27,988 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-27 13:22:27,989 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:22:27,989 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:22:27,991 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:22:27,991 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:22:27,993 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:22:27,994 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-27 13:22:27,994 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:22:27,995 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.00014170000000000003
2026-01-27 13:22:27,996 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:22:27,998 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:22:27,999 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:22:28,002 | DEBUG | openai.agents | Resetting current trace
2026-01-27 13:22:28,003 | ERROR | digital_human_sdk.app.intelligence.utils.json_utils | safe_json_loads failed
2026-01-27 13:22:28,006 | INFO | orchestrator | ‚úÖ Safety passed
2026-01-27 13:22:28,008 | INFO | orchestrator | üì• Fetching semantic memory (via MemoryService)
2026-01-27 13:22:28,036 | INFO | orchestrator | üß† MEMORY_RESULT | found=False | count=0
2026-01-27 13:22:28,037 | INFO | orchestrator | üß≠ Router agent called
2026-01-27 13:22:28,038 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_57364ccbd1564fd19a0430ecaef8db92
2026-01-27 13:22:28,038 | DEBUG | openai.agents | Setting current trace: trace_57364ccbd1564fd19a0430ecaef8db92
2026-01-27 13:22:28,039 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x0000020DAB893C40> with id None
2026-01-27 13:22:28,039 | DEBUG | openai.agents | Running agent RouterAgent (turn 1)
2026-01-27 13:22:28,039 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000020DABE7D720> with id None
2026-01-27 13:22:28,040 | DEBUG | openai.agents | Calling LLM
2026-01-27 13:22:28,040 | DEBUG | LiteLLM | 

2026-01-27 13:22:28,042 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-27 13:22:28,042 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'hi '}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-27 13:22:28,043 | DEBUG | LiteLLM | 

2026-01-27 13:22:28,044 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-27 13:22:28,044 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-27 13:22:28,045 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-27 13:22:28,046 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:22:28,048 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-27 13:22:28,050 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'hi '}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-27 13:22:28,052 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-27 13:22:28,054 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-27 13:22:28,055 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-27 13:22:28,055 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:22:28,056 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:22:28,057 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'hi '}]}], 'system_instruction': {'parts': [{'text': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n'}]}, 'generationConfig': {}}'
[0m

2026-01-27 13:22:29,796 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\"use_memory\": false, \"use_tool\": false, \"tool_name\": \"none\", \"tool_arguments\": null, \"intent\": \"none\", \"memory_key\": null}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 487,
    "candidatesTokenCount": 42,
    "totalTokenCount": 589,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 487
      }
    ],
    "thoughtsTokenCount": 60
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "vW54aY22Hbieg8UP173HyA4"
}



2026-01-27 13:22:29,799 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-27 13:22:29,800 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 13:22:29,801 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-27 13:22:29,802 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:22:29,802 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 13:22:29,803 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:22:29,804 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:22:29,804 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:22:29,805 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:22:29,806 | DEBUG | LiteLLM | response_cost: 0.00040110000000000005
2026-01-27 13:22:29,807 | DEBUG | openai.agents | Received model response
2026-01-27 13:22:29,807 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:22:29,809 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x0000020DABE7BC40>>
2026-01-27 13:22:29,810 | DEBUG | LiteLLM | response_cost: 0.00040110000000000005
2026-01-27 13:22:29,810 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-27 13:22:29,811 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:22:29,812 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-27 13:22:29,812 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-27 13:22:29,813 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:22:29,814 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-27 13:22:29,816 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:22:29,817 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:22:29,818 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:22:29,818 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:22:29,819 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-27 13:22:29,821 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:22:29,822 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:22:29,823 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:22:29,823 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:22:29,824 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:22:29,825 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-27 13:22:29,826 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:22:29,827 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.00040110000000000005
2026-01-27 13:22:29,831 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:22:29,831 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:22:29,833 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:22:29,835 | DEBUG | openai.agents | Resetting current trace
2026-01-27 13:22:29,835 | INFO | orchestrator | üß≠ ROUTER_DECISION | tool=False | memory=False | intent=none
2026-01-27 13:22:29,836 | INFO | orchestrator | üß† REASONING_INPUT_SNAPSHOT
{
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful AI assistant.\n \nYou may answer ANY general knowledge question.\n \nConversation summaries and memories are ONLY to provide helpful context\n(e.g. preferences, ongoing plans).\n \nIf a user asks something unrelated to the current topic,\nanswer it normally using general knowledge.\n \nDo NOT restrict yourself to previous topics unless the user explicitly asks."
    },
    {
      "role": "user",
      "content": "hi "
    },
    {
      "role": "assistant",
      "content": "Hello! How can I help you today?"
    },
    {
      "role": "user",
      "content": "hi "
    },
    {
      "role": "user",
      "content": "hi "
    }
  ],
  "safety": {
    "safe": true,
    "message": null
  },
  "memory_action": {},
  "memory_data": [],
  "memory_found": false,
  "tool_context": {}
}
2026-01-27 13:22:29,838 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_1e77786150cb473f902a3e7006bbbe29
2026-01-27 13:22:29,839 | DEBUG | openai.agents | Setting current trace: trace_1e77786150cb473f902a3e7006bbbe29
2026-01-27 13:22:29,839 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x0000020DAB842110> with id None
2026-01-27 13:22:29,841 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000020DABE7D960> with id None
2026-01-27 13:22:29,841 | DEBUG | openai.agents | Calling LLM
2026-01-27 13:22:29,842 | DEBUG | LiteLLM | 

2026-01-27 13:22:29,842 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-27 13:22:29,843 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "hi "}, {"role": "assistant", "content": "Hello! How can I help you today?"}, {"role": "user", "content": "hi "}, {"role": "user", "content": "hi "}], "safety": {"safe": true, "message": null}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=True, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-27 13:22:29,843 | DEBUG | LiteLLM | 

2026-01-27 13:22:29,844 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-27 13:22:29,845 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-27 13:22:29,845 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-27 13:22:29,848 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:22:29,849 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-27 13:22:29,850 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "hi "}, {"role": "assistant", "content": "Hello! How can I help you today?"}, {"role": "user", "content": "hi "}, {"role": "user", "content": "hi "}], "safety": {"safe": true, "message": null}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-27 13:22:29,850 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': True}
2026-01-27 13:22:29,851 | DEBUG | LiteLLM | Final returned optional params: {'stream': True}
2026-01-27 13:22:29,851 | DEBUG | LiteLLM | self.optional_params: {'stream': True}
2026-01-27 13:22:29,853 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:22:29,854 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:22:29,855 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?key=*****=sse \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "hi "}, {"role": "assistant", "content": "Hello! How can I help you today?"}, {"role": "user", "content": "hi "}, {"role": "user", "content": "hi "}], "safety": {"safe": true, "message": null}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}]}], 'system_instruction': {'parts': [{'text': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.'}]}, 'generationConfig': {}}'
[0m

2026-01-27 13:22:29,856 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 13:22:29,857 | DEBUG | LiteLLM | messages in token_counter: None, text in token_counter: 
2026-01-27 13:22:29,858 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:22:29,858 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:22:29,859 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:22:29,861 | DEBUG | LiteLLM | response_cost: 0.0
2026-01-27 13:22:30,131 | DEBUG | httpcore.connection | close.started
2026-01-27 13:22:30,132 | DEBUG | httpcore.connection | close.complete
2026-01-27 13:22:30,132 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-27 13:22:30,191 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000020DABFED510>
2026-01-27 13:22:30,191 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000020DA0481540> server_hostname='api.openai.com' timeout=5.0
2026-01-27 13:22:30,207 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000020DABFECEB0>
2026-01-27 13:22:30,208 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-27 13:22:30,208 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-27 13:22:30,209 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-27 13:22:30,209 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-27 13:22:30,209 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-27 13:22:30,681 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Tue, 27 Jan 2026 07:52:30 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_fe99c05d959d6be5cca284b781aa7cc7'), (b'openai-processing-ms', b'123'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'127'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c46abc429e774f6-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-27 13:22:30,682 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-27 13:22:30,682 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-27 13:22:30,682 | DEBUG | httpcore.http11 | response_closed.started
2026-01-27 13:22:30,683 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-27 13:22:30,683 | DEBUG | openai.agents | Exported 7 items
2026-01-27 13:22:31,317 | DEBUG | LiteLLM | RAW RESPONSE:
first stream response received


2026-01-27 13:22:31,322 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': 'Hello! How can I help you today?'}], 'role': 'model'}, 'finishReason': 'STOP', 'index': 0}], 'usageMetadata': {'promptTokenCount': 308, 'candidatesTokenCount': 9, 'totalTokenCount': 347, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 308}], 'thoughtsTokenCount': 30}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'vm54aejhLIiq4-EPwtiTsQ4'}
2026-01-27 13:22:31,324 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='vm54aejhLIiq4-EPwtiTsQ4', created=1769500351, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='Hello! How can I help you today?', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=39, prompt_tokens=308, total_tokens=347, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=30, rejected_prediction_tokens=None, text_tokens=9, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=308, image_tokens=None)))
2026-01-27 13:22:31,326 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content='Hello! How can I help you today?', role='assistant', function_call=None, tool_calls=None, audio=None)
2026-01-27 13:22:31,327 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='vm54aejhLIiq4-EPwtiTsQ4', created=1769500351, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Hello! How can I help you today?', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=39, prompt_tokens=308, total_tokens=347, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=30, rejected_prediction_tokens=None, text_tokens=9, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=308, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-27 13:22:31,332 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=False
2026-01-27 13:22:31,333 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=False
2026-01-27 13:22:31,333 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-27 13:22:31,334 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-27 13:22:31,334 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 13:22:31,336 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 13:22:31,337 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:22:31,338 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:22:31,338 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:22:31,339 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:22:31,340 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:22:31,340 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:22:31,341 | DEBUG | LiteLLM | response_cost: 0.0001899
2026-01-27 13:22:31,342 | DEBUG | LiteLLM | response_cost: 0.0001899
2026-01-27 13:22:31,344 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:22:31,345 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0001899
2026-01-27 13:22:31,345 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:22:31,346 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:22:31,347 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:22:31,348 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:22:31,349 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:22:31,351 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-27 13:22:31,352 | DEBUG | openai.agents | Resetting current trace
2026-01-27 13:22:31,356 | INFO | session_summary | üß† SUMMARY_START | session_id=27be17d2-4892-42ed-80d2-74ba6e13ea5a
2026-01-27 13:22:31,360 | INFO | session_summary | üìä Unsummarized messages = 4
2026-01-27 13:22:31,361 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-27 13:22:31,363 | INFO | chat | ‚úÖ Stream complete | tokens=1 | time=6.34s
2026-01-27 13:22:35,758 | DEBUG | httpcore.connection | close.started
2026-01-27 13:22:35,759 | DEBUG | httpcore.connection | close.complete
2026-01-27 13:22:35,759 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-27 13:22:35,880 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000020DAB73D060>
2026-01-27 13:22:35,880 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000020DA0481540> server_hostname='api.openai.com' timeout=5.0
2026-01-27 13:22:35,969 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000020DAB788040>
2026-01-27 13:22:35,970 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-27 13:22:35,971 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-27 13:22:35,972 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-27 13:22:35,972 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-27 13:22:35,973 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-27 13:22:36,742 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Tue, 27 Jan 2026 07:52:36 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_2a7d3a58141653df1bdf8e73faa9cc44'), (b'openai-processing-ms', b'451'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'454'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c46abe83c415982-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-27 13:22:36,743 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-27 13:22:36,744 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-27 13:22:36,744 | DEBUG | httpcore.http11 | response_closed.started
2026-01-27 13:22:36,745 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-27 13:22:36,746 | DEBUG | openai.agents | Exported 2 items
2026-01-27 13:23:00,183 | INFO | chat | üì© Chat request | user_id=4 | request_id=28ee8b43-14fb-4580-bf65-c2b98e955f5e
2026-01-27 13:23:00,185 | INFO | chat | üßë User message | what is my favourite PIZZA
2026-01-27 13:23:00,211 | INFO | chat | ü§ñ Stream started | session_id=27be17d2-4892-42ed-80d2-74ba6e13ea5a | request_id=28ee8b43-14fb-4580-bf65-c2b98e955f5e
2026-01-27 13:23:00,212 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-27 13:23:00,215 | INFO | orchestrator | üßë USER_INPUT | what is my favourite PIZZA
2026-01-27 13:23:00,216 | INFO | orchestrator | üß© CONTEXT | user_id=4 | enable_memory=True | db=None
2026-01-27 13:23:00,218 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-27 13:23:00,219 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_6740da73b4cd4a41b6f166bf9942e320
2026-01-27 13:23:00,219 | DEBUG | openai.agents | Setting current trace: trace_6740da73b4cd4a41b6f166bf9942e320
2026-01-27 13:23:00,220 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x0000020DABFCAB60> with id None
2026-01-27 13:23:00,221 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-27 13:23:00,221 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000020DABE7D300> with id None
2026-01-27 13:23:00,222 | DEBUG | openai.agents | Calling LLM
2026-01-27 13:23:00,222 | DEBUG | LiteLLM | 

2026-01-27 13:23:00,222 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-27 13:23:00,223 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n', 'role': 'system'}, {'role': 'user', 'content': 'what is my favourite PIZZA'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-27 13:23:00,224 | DEBUG | LiteLLM | 

2026-01-27 13:23:00,225 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-27 13:23:00,226 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-27 13:23:00,226 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-27 13:23:00,228 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:23:00,229 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-27 13:23:00,231 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n', 'role': 'system'}, {'role': 'user', 'content': 'what is my favourite PIZZA'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-27 13:23:00,231 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-27 13:23:00,232 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-27 13:23:00,233 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-27 13:23:00,233 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:00,234 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:00,235 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'what is my favourite PIZZA'}]}], 'system_instruction': {'parts': [{'text': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n'}]}, 'generationConfig': {}}'
[0m

2026-01-27 13:23:01,280 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "SAFE"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 194,
    "candidatesTokenCount": 1,
    "totalTokenCount": 252,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 194
      }
    ],
    "thoughtsTokenCount": 57
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "3G54ab-LPL6E4-EPs6fIkQ4"
}



2026-01-27 13:23:01,282 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-27 13:23:01,282 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 13:23:01,282 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-27 13:23:01,284 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:01,285 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 13:23:01,286 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:01,286 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:01,287 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:23:01,287 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:01,288 | DEBUG | LiteLLM | response_cost: 0.00020320000000000003
2026-01-27 13:23:01,289 | DEBUG | openai.agents | Received model response
2026-01-27 13:23:01,289 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:23:01,290 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x0000020DABFEEDD0>>
2026-01-27 13:23:01,291 | DEBUG | LiteLLM | response_cost: 0.00020320000000000003
2026-01-27 13:23:01,292 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-27 13:23:01,293 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:23:01,294 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-27 13:23:01,294 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-27 13:23:01,297 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:01,298 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-27 13:23:01,299 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:23:01,300 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:23:01,301 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:23:01,301 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:01,303 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-27 13:23:01,304 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:01,304 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:23:01,306 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:23:01,306 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:23:01,308 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:01,308 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-27 13:23:01,309 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:23:01,309 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.00020320000000000003
2026-01-27 13:23:01,312 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:23:01,313 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:01,314 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:23:01,316 | DEBUG | openai.agents | Resetting current trace
2026-01-27 13:23:01,316 | ERROR | digital_human_sdk.app.intelligence.utils.json_utils | safe_json_loads failed
2026-01-27 13:23:01,317 | INFO | orchestrator | ‚úÖ Safety passed
2026-01-27 13:23:01,318 | INFO | orchestrator | üì• Fetching semantic memory (via MemoryService)
2026-01-27 13:23:01,340 | INFO | orchestrator | üß† MEMORY_RESULT | found=False | count=0
2026-01-27 13:23:01,341 | INFO | orchestrator | üß≠ Router agent called
2026-01-27 13:23:01,343 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_116ecc6df3fe437587b83d067dce1c8b
2026-01-27 13:23:01,343 | DEBUG | openai.agents | Setting current trace: trace_116ecc6df3fe437587b83d067dce1c8b
2026-01-27 13:23:01,343 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x0000020DABFCBD80> with id None
2026-01-27 13:23:01,344 | DEBUG | openai.agents | Running agent RouterAgent (turn 1)
2026-01-27 13:23:01,344 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000020DABE7EEC0> with id None
2026-01-27 13:23:01,346 | DEBUG | openai.agents | Calling LLM
2026-01-27 13:23:01,347 | DEBUG | LiteLLM | 

2026-01-27 13:23:01,347 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-27 13:23:01,348 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'what is my favourite PIZZA'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-27 13:23:01,349 | DEBUG | LiteLLM | 

2026-01-27 13:23:01,350 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-27 13:23:01,351 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-27 13:23:01,351 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-27 13:23:01,353 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:23:01,355 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-27 13:23:01,357 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'what is my favourite PIZZA'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-27 13:23:01,358 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-27 13:23:01,359 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-27 13:23:01,361 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-27 13:23:01,361 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:01,363 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:01,364 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'what is my favourite PIZZA'}]}], 'system_instruction': {'parts': [{'text': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n'}]}, 'generationConfig': {}}'
[0m

2026-01-27 13:23:02,090 | DEBUG | httpcore.connection | close.started
2026-01-27 13:23:02,091 | DEBUG | httpcore.connection | close.complete
2026-01-27 13:23:02,091 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-27 13:23:02,133 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000020DABFEC6D0>
2026-01-27 13:23:02,133 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000020DA0481540> server_hostname='api.openai.com' timeout=5.0
2026-01-27 13:23:02,146 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000020DABFEEBF0>
2026-01-27 13:23:02,147 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-27 13:23:02,148 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-27 13:23:02,149 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-27 13:23:02,149 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-27 13:23:02,150 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-27 13:23:02,704 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Tue, 27 Jan 2026 07:53:02 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_cb641f3cc2e0ccfc246a3b8290eb7371'), (b'openai-processing-ms', b'208'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'211'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c46ac8bcc30ad7b-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-27 13:23:02,705 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-27 13:23:02,706 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-27 13:23:02,706 | DEBUG | httpcore.http11 | response_closed.started
2026-01-27 13:23:02,707 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-27 13:23:02,707 | DEBUG | openai.agents | Exported 4 items
2026-01-27 13:23:03,765 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\n  \"use_memory\": true,\n  \"use_tool\": false,\n  \"tool_name\": \"none\",\n  \"tool_arguments\": null,\n  \"intent\": \"read\",\n  \"memory_key\": \"food_preference\"\n}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 492,
    "candidatesTokenCount": 60,
    "totalTokenCount": 628,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 492
      }
    ],
    "thoughtsTokenCount": 76
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "3254aeinHaCyjuMP-u-NyQ4"
}



2026-01-27 13:23:03,768 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-27 13:23:03,769 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 13:23:03,769 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-27 13:23:03,770 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:03,770 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 13:23:03,771 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:03,772 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:03,773 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:23:03,773 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:03,774 | DEBUG | LiteLLM | response_cost: 0.00048760000000000003
2026-01-27 13:23:03,776 | DEBUG | openai.agents | Received model response
2026-01-27 13:23:03,774 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:23:03,777 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x0000020DABFEF0D0>>
2026-01-27 13:23:03,778 | DEBUG | LiteLLM | response_cost: 0.00048760000000000003
2026-01-27 13:23:03,778 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-27 13:23:03,779 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:23:03,780 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-27 13:23:03,780 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-27 13:23:03,781 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:03,781 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-27 13:23:03,782 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:23:03,783 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:23:03,784 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:23:03,784 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:03,785 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-27 13:23:03,785 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:03,785 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:23:03,787 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:23:03,787 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:23:03,790 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:03,791 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-27 13:23:03,791 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:23:03,792 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.00048760000000000003
2026-01-27 13:23:03,794 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:23:03,795 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:03,795 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:23:03,798 | DEBUG | openai.agents | Resetting current trace
2026-01-27 13:23:03,798 | INFO | orchestrator | üß≠ ROUTER_DECISION | tool=False | memory=True | intent=read
2026-01-27 13:23:03,799 | INFO | orchestrator | üß† REASONING_INPUT_SNAPSHOT
{
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful AI assistant.\n \nYou may answer ANY general knowledge question.\n \nConversation summaries and memories are ONLY to provide helpful context\n(e.g. preferences, ongoing plans).\n \nIf a user asks something unrelated to the current topic,\nanswer it normally using general knowledge.\n \nDo NOT restrict yourself to previous topics unless the user explicitly asks."
    },
    {
      "role": "user",
      "content": "hi "
    },
    {
      "role": "assistant",
      "content": "Hello! How can I help you today?"
    },
    {
      "role": "user",
      "content": "hi "
    },
    {
      "role": "assistant",
      "content": "Hello! How can I help you today?"
    },
    {
      "role": "user",
      "content": "what is my favourite PIZZA"
    },
    {
      "role": "user",
      "content": "what is my favourite PIZZA"
    }
  ],
  "safety": {
    "safe": true,
    "message": null
  },
  "memory_action": {},
  "memory_data": [],
  "memory_found": false,
  "tool_context": {}
}
2026-01-27 13:23:03,801 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_9a26adc878aa4630b1c837f152b3eb36
2026-01-27 13:23:03,801 | DEBUG | openai.agents | Setting current trace: trace_9a26adc878aa4630b1c837f152b3eb36
2026-01-27 13:23:03,802 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x0000020DAB91D3A0> with id None
2026-01-27 13:23:03,803 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000020DAB85D5A0> with id None
2026-01-27 13:23:03,803 | DEBUG | openai.agents | Calling LLM
2026-01-27 13:23:03,804 | DEBUG | LiteLLM | 

2026-01-27 13:23:03,804 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-27 13:23:03,805 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "hi "}, {"role": "assistant", "content": "Hello! How can I help you today?"}, {"role": "user", "content": "hi "}, {"role": "assistant", "content": "Hello! How can I help you today?"}, {"role": "user", "content": "what is my favourite PIZZA"}, {"role": "user", "content": "what is my favourite PIZZA"}], "safety": {"safe": true, "message": null}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=True, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-27 13:23:03,806 | DEBUG | LiteLLM | 

2026-01-27 13:23:03,807 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-27 13:23:03,807 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-27 13:23:03,809 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-27 13:23:03,811 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:23:03,811 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-27 13:23:03,813 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "hi "}, {"role": "assistant", "content": "Hello! How can I help you today?"}, {"role": "user", "content": "hi "}, {"role": "assistant", "content": "Hello! How can I help you today?"}, {"role": "user", "content": "what is my favourite PIZZA"}, {"role": "user", "content": "what is my favourite PIZZA"}], "safety": {"safe": true, "message": null}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-27 13:23:03,813 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': True}
2026-01-27 13:23:03,814 | DEBUG | LiteLLM | Final returned optional params: {'stream': True}
2026-01-27 13:23:03,815 | DEBUG | LiteLLM | self.optional_params: {'stream': True}
2026-01-27 13:23:03,815 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:03,816 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:03,817 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?key=*****=sse \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "hi "}, {"role": "assistant", "content": "Hello! How can I help you today?"}, {"role": "user", "content": "hi "}, {"role": "assistant", "content": "Hello! How can I help you today?"}, {"role": "user", "content": "what is my favourite PIZZA"}, {"role": "user", "content": "what is my favourite PIZZA"}], "safety": {"safe": true, "message": null}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}]}], 'system_instruction': {'parts': [{'text': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.'}]}, 'generationConfig': {}}'
[0m

2026-01-27 13:23:03,820 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 13:23:03,821 | DEBUG | LiteLLM | messages in token_counter: None, text in token_counter: 
2026-01-27 13:23:03,821 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:03,822 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:03,823 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:23:03,824 | DEBUG | LiteLLM | response_cost: 0.0
2026-01-27 13:23:05,266 | DEBUG | LiteLLM | RAW RESPONSE:
first stream response received


2026-01-27 13:23:05,267 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': "I don't know what your favorite pizza is. I don't have access to personal information like that."}], 'role': 'model'}, 'finishReason': 'STOP', 'index': 0}], 'usageMetadata': {'promptTokenCount': 352, 'candidatesTokenCount': 23, 'totalTokenCount': 449, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 352}], 'thoughtsTokenCount': 74}, 'modelVersion': 'gemini-2.5-flash', 'responseId': '4G54adDaHqir4-EPoMPkkQ4'}
2026-01-27 13:23:05,268 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='4G54adDaHqir4-EPoMPkkQ4', created=1769500385, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content="I don't know what your favorite pizza is. I don't have access to personal information like that.", role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=97, prompt_tokens=352, total_tokens=449, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=74, rejected_prediction_tokens=None, text_tokens=23, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=352, image_tokens=None)))
2026-01-27 13:23:05,270 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content="I don't know what your favorite pizza is. I don't have access to personal information like that.", role='assistant', function_call=None, tool_calls=None, audio=None)
2026-01-27 13:23:05,271 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='4G54adDaHqir4-EPoMPkkQ4', created=1769500385, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content="I don't know what your favorite pizza is. I don't have access to personal information like that.", role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=97, prompt_tokens=352, total_tokens=449, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=74, rejected_prediction_tokens=None, text_tokens=23, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=352, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-27 13:23:05,274 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=False
2026-01-27 13:23:05,274 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=False
2026-01-27 13:23:05,275 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-27 13:23:05,275 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-27 13:23:05,275 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 13:23:05,276 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 13:23:05,278 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:05,278 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:05,279 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:05,279 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:05,280 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:23:05,281 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:23:05,282 | DEBUG | LiteLLM | response_cost: 0.00034810000000000006
2026-01-27 13:23:05,283 | DEBUG | LiteLLM | response_cost: 0.00034810000000000006
2026-01-27 13:23:05,284 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:23:05,284 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.00034810000000000006
2026-01-27 13:23:05,285 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:05,288 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:23:05,288 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:23:05,289 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:05,290 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:23:05,292 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-27 13:23:05,293 | DEBUG | openai.agents | Resetting current trace
2026-01-27 13:23:05,299 | INFO | session_summary | üß† SUMMARY_START | session_id=27be17d2-4892-42ed-80d2-74ba6e13ea5a
2026-01-27 13:23:05,307 | INFO | session_summary | üìä Unsummarized messages = 6
2026-01-27 13:23:05,310 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-27 13:23:05,313 | INFO | chat | ‚úÖ Stream complete | tokens=1 | time=5.13s
2026-01-27 13:23:07,795 | DEBUG | httpcore.connection | close.started
2026-01-27 13:23:07,796 | DEBUG | httpcore.connection | close.complete
2026-01-27 13:23:07,797 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-27 13:23:07,825 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000020DAB73E200>
2026-01-27 13:23:07,826 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000020DA0481540> server_hostname='api.openai.com' timeout=5.0
2026-01-27 13:23:07,841 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000020DAB73E0B0>
2026-01-27 13:23:07,842 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-27 13:23:07,843 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-27 13:23:07,843 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-27 13:23:07,844 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-27 13:23:07,844 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-27 13:23:08,366 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Tue, 27 Jan 2026 07:53:08 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_fa7208cd1eccbea7735dd9cb7e1744dd'), (b'openai-processing-ms', b'189'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'192'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c46acaf5ee40cf5-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-27 13:23:08,367 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-27 13:23:08,368 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-27 13:23:08,368 | DEBUG | httpcore.http11 | response_closed.started
2026-01-27 13:23:08,369 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-27 13:23:08,369 | DEBUG | openai.agents | Exported 5 items
2026-01-27 13:23:26,401 | INFO | chat | üì© Chat request | user_id=3 | request_id=a49183b5-30bc-4926-a598-611fdcc19b96
2026-01-27 13:23:26,405 | INFO | chat | üßë User message | WHAT IS MY FAVOURITE PIZZA
2026-01-27 13:23:26,421 | INFO | chat | üÜï New session created | d3ad5fa1-6363-4d3f-927e-12a05c40ee39
2026-01-27 13:23:26,431 | INFO | chat | ü§ñ Stream started | session_id=d3ad5fa1-6363-4d3f-927e-12a05c40ee39 | request_id=a49183b5-30bc-4926-a598-611fdcc19b96
2026-01-27 13:23:26,432 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-27 13:23:26,434 | INFO | orchestrator | üßë USER_INPUT | WHAT IS MY FAVOURITE PIZZA
2026-01-27 13:23:26,435 | INFO | orchestrator | üß© CONTEXT | user_id=3 | enable_memory=True | db=None
2026-01-27 13:23:26,436 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-27 13:23:26,437 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_68331953fe8740d49e9d14a51c089f2a
2026-01-27 13:23:26,437 | DEBUG | openai.agents | Setting current trace: trace_68331953fe8740d49e9d14a51c089f2a
2026-01-27 13:23:26,439 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x0000020DABFF59E0> with id None
2026-01-27 13:23:26,439 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-27 13:23:26,440 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000020DABE7DA20> with id None
2026-01-27 13:23:26,440 | DEBUG | openai.agents | Calling LLM
2026-01-27 13:23:26,441 | DEBUG | LiteLLM | 

2026-01-27 13:23:26,441 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-27 13:23:26,442 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n', 'role': 'system'}, {'role': 'user', 'content': 'WHAT IS MY FAVOURITE PIZZA'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-27 13:23:26,442 | DEBUG | LiteLLM | 

2026-01-27 13:23:26,443 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-27 13:23:26,443 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-27 13:23:26,444 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-27 13:23:26,445 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:23:26,446 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-27 13:23:26,447 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n', 'role': 'system'}, {'role': 'user', 'content': 'WHAT IS MY FAVOURITE PIZZA'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-27 13:23:26,448 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-27 13:23:26,448 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-27 13:23:26,450 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-27 13:23:26,450 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:26,451 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:26,452 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'WHAT IS MY FAVOURITE PIZZA'}]}], 'system_instruction': {'parts': [{'text': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n'}]}, 'generationConfig': {}}'
[0m

2026-01-27 13:23:28,584 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "SAFE"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 196,
    "candidatesTokenCount": 1,
    "totalTokenCount": 393,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 196
      }
    ],
    "thoughtsTokenCount": 196
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "-G54ac-FE7GajuMPt7qYqA4"
}



2026-01-27 13:23:28,586 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-27 13:23:28,587 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 13:23:28,587 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-27 13:23:28,588 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:28,588 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 13:23:28,590 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:28,591 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:28,592 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:23:28,592 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:28,593 | DEBUG | LiteLLM | response_cost: 0.0005513000000000001
2026-01-27 13:23:28,595 | DEBUG | openai.agents | Received model response
2026-01-27 13:23:28,594 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:23:28,596 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x0000020DABFED2A0>>
2026-01-27 13:23:28,597 | DEBUG | LiteLLM | response_cost: 0.0005513000000000001
2026-01-27 13:23:28,597 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-27 13:23:28,599 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:23:28,601 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-27 13:23:28,601 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-27 13:23:28,601 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:28,602 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-27 13:23:28,603 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:23:28,604 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:23:28,605 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:23:28,605 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:28,606 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-27 13:23:28,608 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:28,608 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:23:28,609 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:23:28,610 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:23:28,611 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:28,612 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-27 13:23:28,613 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:23:28,614 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0005513000000000001
2026-01-27 13:23:28,616 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:23:28,617 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:28,617 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:23:28,619 | DEBUG | openai.agents | Resetting current trace
2026-01-27 13:23:28,620 | ERROR | digital_human_sdk.app.intelligence.utils.json_utils | safe_json_loads failed
2026-01-27 13:23:28,621 | INFO | orchestrator | ‚úÖ Safety passed
2026-01-27 13:23:28,622 | INFO | orchestrator | üì• Fetching semantic memory (via MemoryService)
2026-01-27 13:23:28,633 | DEBUG | httpcore.connection | close.started
2026-01-27 13:23:28,634 | DEBUG | httpcore.connection | close.complete
2026-01-27 13:23:28,636 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-27 13:23:28,650 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000020DABFEEA70>
2026-01-27 13:23:28,650 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000020DA0481540> server_hostname='api.openai.com' timeout=5.0
2026-01-27 13:23:28,657 | INFO | orchestrator | üß† MEMORY_RESULT | found=True | count=2
2026-01-27 13:23:28,658 | INFO | orchestrator | üß≠ Router agent called
2026-01-27 13:23:28,659 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_9872b87a3ea045fcbb0e511df2d72ddb
2026-01-27 13:23:28,660 | DEBUG | openai.agents | Setting current trace: trace_9872b87a3ea045fcbb0e511df2d72ddb
2026-01-27 13:23:28,660 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x0000020DABE422A0> with id None
2026-01-27 13:23:28,661 | DEBUG | openai.agents | Running agent RouterAgent (turn 1)
2026-01-27 13:23:28,661 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000020DABE7E5C0> with id None
2026-01-27 13:23:28,661 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000020DABFEC130>
2026-01-27 13:23:28,661 | DEBUG | openai.agents | Calling LLM
2026-01-27 13:23:28,663 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-27 13:23:28,663 | DEBUG | LiteLLM | 

2026-01-27 13:23:28,664 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-27 13:23:28,665 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-27 13:23:28,665 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-27 13:23:28,666 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'WHAT IS MY FAVOURITE PIZZA'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-27 13:23:28,666 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-27 13:23:28,666 | DEBUG | LiteLLM | 

2026-01-27 13:23:28,667 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-27 13:23:28,667 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-27 13:23:28,669 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-27 13:23:28,670 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-27 13:23:28,671 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:23:28,672 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-27 13:23:28,673 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'WHAT IS MY FAVOURITE PIZZA'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-27 13:23:28,674 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-27 13:23:28,675 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-27 13:23:28,676 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-27 13:23:28,676 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:28,677 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:28,678 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'WHAT IS MY FAVOURITE PIZZA'}]}], 'system_instruction': {'parts': [{'text': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n'}]}, 'generationConfig': {}}'
[0m

2026-01-27 13:23:29,161 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Tue, 27 Jan 2026 07:53:29 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_4aeb4de67b9308391137f0fab801a3b6'), (b'openai-processing-ms', b'188'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'191'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c46ad318bfb16e3-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-27 13:23:29,162 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-27 13:23:29,163 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-27 13:23:29,163 | DEBUG | httpcore.http11 | response_closed.started
2026-01-27 13:23:29,165 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-27 13:23:29,166 | DEBUG | openai.agents | Exported 3 items
2026-01-27 13:23:29,168 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-27 13:23:29,169 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-27 13:23:29,170 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-27 13:23:29,171 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-27 13:23:29,171 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-27 13:23:29,598 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Tue, 27 Jan 2026 07:53:29 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_dea1cbbbbf0db9bf8230050cfc338879'), (b'openai-processing-ms', b'93'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'97'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c46ad34aaaa16e3-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-27 13:23:29,599 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-27 13:23:29,600 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-27 13:23:29,600 | DEBUG | httpcore.http11 | response_closed.started
2026-01-27 13:23:29,601 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-27 13:23:29,601 | DEBUG | openai.agents | Exported 1 items
2026-01-27 13:23:30,324 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\n  \"use_memory\": true,\n  \"use_tool\": false,\n  \"tool_name\": \"none\",\n  \"tool_arguments\": null,\n  \"intent\": \"read\",\n  \"memory_key\": \"food_preference\"\n}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 494,
    "candidatesTokenCount": 60,
    "totalTokenCount": 595,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 494
      }
    ],
    "thoughtsTokenCount": 41
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "-m54aYX1ApyLjuMPv9-doQ0"
}



2026-01-27 13:23:30,327 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-27 13:23:30,328 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 13:23:30,328 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-27 13:23:30,329 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:30,331 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 13:23:30,332 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:30,332 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:30,332 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:23:30,333 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:30,334 | DEBUG | LiteLLM | response_cost: 0.00040070000000000004
2026-01-27 13:23:30,334 | DEBUG | openai.agents | Received model response
2026-01-27 13:23:30,334 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:23:30,336 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x0000020DABFEEA40>>
2026-01-27 13:23:30,337 | DEBUG | LiteLLM | response_cost: 0.00040070000000000004
2026-01-27 13:23:30,337 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-27 13:23:30,338 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:23:30,339 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-27 13:23:30,339 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-27 13:23:30,340 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:30,342 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-27 13:23:30,343 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:23:30,344 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:23:30,345 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:23:30,345 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:30,346 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-27 13:23:30,346 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:30,346 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:23:30,349 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:23:30,349 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:23:30,350 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:30,351 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-27 13:23:30,351 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:23:30,352 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.00040070000000000004
2026-01-27 13:23:30,355 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:23:30,356 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:30,356 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:23:30,358 | DEBUG | openai.agents | Resetting current trace
2026-01-27 13:23:30,359 | INFO | orchestrator | üß≠ ROUTER_DECISION | tool=False | memory=True | intent=read
2026-01-27 13:23:30,360 | INFO | orchestrator | üß† REASONING_INPUT_SNAPSHOT
{
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful AI assistant.\n \nYou may answer ANY general knowledge question.\n \nConversation summaries and memories are ONLY to provide helpful context\n(e.g. preferences, ongoing plans).\n \nIf a user asks something unrelated to the current topic,\nanswer it normally using general knowledge.\n \nDo NOT restrict yourself to previous topics unless the user explicitly asks."
    },
    {
      "role": "user",
      "content": "WHAT IS MY FAVOURITE PIZZA"
    },
    {
      "role": "user",
      "content": "WHAT IS MY FAVOURITE PIZZA"
    }
  ],
  "safety": {
    "safe": true,
    "message": null
  },
  "memory_action": {},
  "memory_data": [
    {
      "memory_id": 4,
      "user_id": 3,
      "text": "my favourite pizza is margneita.",
      "confidence": 0.85,
      "created_at": "2026-01-23T22:15:19.533318+05:30",
      "expires_at": "2026-02-26T10:09:28.217743+05:30",
      "score": 0.8125231862068176
    },
    {
      "memory_id": 6,
      "user_id": 3,
      "text": "My favourite color is yellow.",
      "confidence": 0.85,
      "created_at": "2026-01-26T21:50:52.573607+05:30",
      "expires_at": "2026-02-25T21:50:52.427804+05:30",
      "score": 0.3765886127948761
    }
  ],
  "memory_found": true,
  "tool_context": {}
}
2026-01-27 13:23:30,362 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_977917f193a848db993e1b643d4abf15
2026-01-27 13:23:30,362 | DEBUG | openai.agents | Setting current trace: trace_977917f193a848db993e1b643d4abf15
2026-01-27 13:23:30,363 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x0000020DABFF76A0> with id None
2026-01-27 13:23:30,364 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000020DABFFD480> with id None
2026-01-27 13:23:30,366 | DEBUG | openai.agents | Calling LLM
2026-01-27 13:23:30,366 | DEBUG | LiteLLM | 

2026-01-27 13:23:30,367 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-27 13:23:30,367 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "WHAT IS MY FAVOURITE PIZZA"}, {"role": "user", "content": "WHAT IS MY FAVOURITE PIZZA"}], "safety": {"safe": true, "message": null}, "memory_action": {}, "memory_data": [{"memory_id": 4, "user_id": 3, "text": "my favourite pizza is margneita.", "confidence": 0.85, "created_at": "2026-01-23T22:15:19.533318+05:30", "expires_at": "2026-02-26T10:09:28.217743+05:30", "score": 0.8125231862068176}, {"memory_id": 6, "user_id": 3, "text": "My favourite color is yellow.", "confidence": 0.85, "created_at": "2026-01-26T21:50:52.573607+05:30", "expires_at": "2026-02-25T21:50:52.427804+05:30", "score": 0.3765886127948761}], "memory_found": true, "tool_context": {}}'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=True, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-27 13:23:30,367 | DEBUG | LiteLLM | 

2026-01-27 13:23:30,368 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-27 13:23:30,369 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-27 13:23:30,369 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-27 13:23:30,371 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:23:30,373 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-27 13:23:30,373 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "WHAT IS MY FAVOURITE PIZZA"}, {"role": "user", "content": "WHAT IS MY FAVOURITE PIZZA"}], "safety": {"safe": true, "message": null}, "memory_action": {}, "memory_data": [{"memory_id": 4, "user_id": 3, "text": "my favourite pizza is margneita.", "confidence": 0.85, "created_at": "2026-01-23T22:15:19.533318+05:30", "expires_at": "2026-02-26T10:09:28.217743+05:30", "score": 0.8125231862068176}, {"memory_id": 6, "user_id": 3, "text": "My favourite color is yellow.", "confidence": 0.85, "created_at": "2026-01-26T21:50:52.573607+05:30", "expires_at": "2026-02-25T21:50:52.427804+05:30", "score": 0.3765886127948761}], "memory_found": true, "tool_context": {}}'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-27 13:23:30,374 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': True}
2026-01-27 13:23:30,375 | DEBUG | LiteLLM | Final returned optional params: {'stream': True}
2026-01-27 13:23:30,375 | DEBUG | LiteLLM | self.optional_params: {'stream': True}
2026-01-27 13:23:30,376 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:30,377 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:30,378 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?key=*****=sse \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "WHAT IS MY FAVOURITE PIZZA"}, {"role": "user", "content": "WHAT IS MY FAVOURITE PIZZA"}], "safety": {"safe": true, "message": null}, "memory_action": {}, "memory_data": [{"memory_id": 4, "user_id": 3, "text": "my favourite pizza is margneita.", "confidence": 0.85, "created_at": "2026-01-23T22:15:19.533318+05:30", "expires_at": "2026-02-26T10:09:28.217743+05:30", "score": 0.8125231862068176}, {"memory_id": 6, "user_id": 3, "text": "My favourite color is yellow.", "confidence": 0.85, "created_at": "2026-01-26T21:50:52.573607+05:30", "expires_at": "2026-02-25T21:50:52.427804+05:30", "score": 0.3765886127948761}], "memory_found": true, "tool_context": {}}'}]}], 'system_instruction': {'parts': [{'text': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.'}]}, 'generationConfig': {}}'
[0m

2026-01-27 13:23:30,379 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 13:23:30,380 | DEBUG | LiteLLM | messages in token_counter: None, text in token_counter: 
2026-01-27 13:23:30,382 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:30,382 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:30,383 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:23:30,384 | DEBUG | LiteLLM | response_cost: 0.0
2026-01-27 13:23:32,260 | DEBUG | LiteLLM | RAW RESPONSE:
first stream response received


2026-01-27 13:23:32,261 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': 'Your favorite pizza is margherita.'}], 'role': 'model'}, 'finishReason': 'STOP', 'index': 0}], 'usageMetadata': {'promptTokenCount': 563, 'candidatesTokenCount': 7, 'totalTokenCount': 744, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 563}], 'thoughtsTokenCount': 174}, 'modelVersion': 'gemini-2.5-flash', 'responseId': '-254ace6Arzcg8UPm9KpgQ4'}
2026-01-27 13:23:32,263 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='-254ace6Arzcg8UPm9KpgQ4', created=1769500412, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='Your favorite pizza is margherita.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=181, prompt_tokens=563, total_tokens=744, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=174, rejected_prediction_tokens=None, text_tokens=7, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=563, image_tokens=None)))
2026-01-27 13:23:32,266 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content='Your favorite pizza is margherita.', role='assistant', function_call=None, tool_calls=None, audio=None)
2026-01-27 13:23:32,267 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='-254ace6Arzcg8UPm9KpgQ4', created=1769500412, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Your favorite pizza is margherita.', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=181, prompt_tokens=563, total_tokens=744, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=174, rejected_prediction_tokens=None, text_tokens=7, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=563, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-27 13:23:32,270 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=False
2026-01-27 13:23:32,271 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=False
2026-01-27 13:23:32,271 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-27 13:23:32,271 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-27 13:23:32,273 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 13:23:32,274 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 13:23:32,274 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:32,275 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:32,275 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:32,277 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:32,278 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:23:32,278 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:23:32,279 | DEBUG | LiteLLM | response_cost: 0.0006214
2026-01-27 13:23:32,280 | DEBUG | LiteLLM | response_cost: 0.0006214
2026-01-27 13:23:32,281 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:23:32,283 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0006214
2026-01-27 13:23:32,284 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:32,284 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:23:32,285 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:23:32,285 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:32,286 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:23:32,289 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-27 13:23:32,290 | DEBUG | openai.agents | Resetting current trace
2026-01-27 13:23:32,302 | INFO | session_summary | üß† SUMMARY_START | session_id=d3ad5fa1-6363-4d3f-927e-12a05c40ee39
2026-01-27 13:23:32,305 | INFO | session_summary | üìä Unsummarized messages = 2
2026-01-27 13:23:32,307 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-27 13:23:32,309 | INFO | chat | ‚úÖ Stream complete | tokens=1 | time=5.91s
2026-01-27 13:23:33,278 | INFO | chat | üì© Chat request | user_id=3 | request_id=4b1f331c-f011-46ac-a226-f936a6ab4a78
2026-01-27 13:23:33,279 | INFO | chat | üßë User message | WHAT IS MY FAVOURITE PIZZA
2026-01-27 13:23:33,295 | INFO | chat | ü§ñ Stream started | session_id=d3ad5fa1-6363-4d3f-927e-12a05c40ee39 | request_id=4b1f331c-f011-46ac-a226-f936a6ab4a78
2026-01-27 13:23:33,296 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-27 13:23:33,297 | INFO | orchestrator | üßë USER_INPUT | WHAT IS MY FAVOURITE PIZZA
2026-01-27 13:23:33,300 | INFO | orchestrator | üß© CONTEXT | user_id=3 | enable_memory=True | db=None
2026-01-27 13:23:33,301 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-27 13:23:33,302 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_b109d3edab134c64a4f7754ccca800b0
2026-01-27 13:23:33,302 | DEBUG | openai.agents | Setting current trace: trace_b109d3edab134c64a4f7754ccca800b0
2026-01-27 13:23:33,303 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x0000020DABFCACF0> with id None
2026-01-27 13:23:33,303 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-27 13:23:33,304 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000020DABE7DF60> with id None
2026-01-27 13:23:33,305 | DEBUG | openai.agents | Calling LLM
2026-01-27 13:23:33,305 | DEBUG | LiteLLM | 

2026-01-27 13:23:33,306 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-27 13:23:33,306 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n', 'role': 'system'}, {'role': 'user', 'content': 'WHAT IS MY FAVOURITE PIZZA'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-27 13:23:33,307 | DEBUG | LiteLLM | 

2026-01-27 13:23:33,307 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-27 13:23:33,308 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-27 13:23:33,309 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-27 13:23:33,310 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:23:33,312 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-27 13:23:33,314 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n', 'role': 'system'}, {'role': 'user', 'content': 'WHAT IS MY FAVOURITE PIZZA'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-27 13:23:33,314 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-27 13:23:33,315 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-27 13:23:33,315 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-27 13:23:33,316 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:33,317 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:33,318 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'WHAT IS MY FAVOURITE PIZZA'}]}], 'system_instruction': {'parts': [{'text': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n'}]}, 'generationConfig': {}}'
[0m

2026-01-27 13:23:34,673 | DEBUG | httpcore.connection | close.started
2026-01-27 13:23:34,674 | DEBUG | httpcore.connection | close.complete
2026-01-27 13:23:34,675 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-27 13:23:34,703 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000020DABFF93F0>
2026-01-27 13:23:34,703 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000020DA0481540> server_hostname='api.openai.com' timeout=5.0
2026-01-27 13:23:34,718 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000020DABFF9390>
2026-01-27 13:23:34,718 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-27 13:23:34,719 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-27 13:23:34,720 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-27 13:23:34,720 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-27 13:23:34,721 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-27 13:23:35,200 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Tue, 27 Jan 2026 07:53:35 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_11489661fa158226d76b6471cec6ca46'), (b'openai-processing-ms', b'150'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'153'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c46ad576bb2d04b-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-27 13:23:35,201 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-27 13:23:35,202 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-27 13:23:35,202 | DEBUG | httpcore.http11 | response_closed.started
2026-01-27 13:23:35,203 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-27 13:23:35,203 | DEBUG | openai.agents | Exported 6 items
2026-01-27 13:23:35,290 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "SAFE"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 196,
    "candidatesTokenCount": 1,
    "totalTokenCount": 355,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 196
      }
    ],
    "thoughtsTokenCount": 158
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "_m54aY-xPNCrjuMPzMjI6Qs"
}



2026-01-27 13:23:35,292 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-27 13:23:35,294 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 13:23:35,294 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-27 13:23:35,295 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:35,295 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 13:23:35,296 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:35,297 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:35,298 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:23:35,299 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:35,300 | DEBUG | LiteLLM | response_cost: 0.00045630000000000003
2026-01-27 13:23:35,300 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:23:35,301 | DEBUG | openai.agents | Received model response
2026-01-27 13:23:35,302 | DEBUG | LiteLLM | response_cost: 0.00045630000000000003
2026-01-27 13:23:35,304 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x0000020DABE7A7D0>>
2026-01-27 13:23:35,305 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:23:35,306 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-27 13:23:35,306 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:35,308 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-27 13:23:35,307 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-27 13:23:35,308 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:23:35,309 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-27 13:23:35,312 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:23:35,313 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-27 13:23:35,314 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:23:35,315 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:35,317 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:23:35,317 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:35,318 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:23:35,319 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:35,319 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:23:35,320 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:23:35,322 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-27 13:23:35,322 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.00045630000000000003
2026-01-27 13:23:35,324 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:23:35,326 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:35,326 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:23:35,328 | DEBUG | openai.agents | Resetting current trace
2026-01-27 13:23:35,328 | ERROR | digital_human_sdk.app.intelligence.utils.json_utils | safe_json_loads failed
2026-01-27 13:23:35,330 | INFO | orchestrator | ‚úÖ Safety passed
2026-01-27 13:23:35,331 | INFO | orchestrator | üì• Fetching semantic memory (via MemoryService)
2026-01-27 13:23:35,359 | INFO | orchestrator | üß† MEMORY_RESULT | found=True | count=2
2026-01-27 13:23:35,360 | INFO | orchestrator | üß≠ Router agent called
2026-01-27 13:23:35,361 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_93c15a46b87241758662ba5591bb846c
2026-01-27 13:23:35,362 | DEBUG | openai.agents | Setting current trace: trace_93c15a46b87241758662ba5591bb846c
2026-01-27 13:23:35,362 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x0000020DABFF76A0> with id None
2026-01-27 13:23:35,363 | DEBUG | openai.agents | Running agent RouterAgent (turn 1)
2026-01-27 13:23:35,365 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000020DABFFDC00> with id None
2026-01-27 13:23:35,365 | DEBUG | openai.agents | Calling LLM
2026-01-27 13:23:35,365 | DEBUG | LiteLLM | 

2026-01-27 13:23:35,366 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-27 13:23:35,366 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'WHAT IS MY FAVOURITE PIZZA'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-27 13:23:35,367 | DEBUG | LiteLLM | 

2026-01-27 13:23:35,369 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-27 13:23:35,369 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-27 13:23:35,370 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-27 13:23:35,372 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:23:35,373 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-27 13:23:35,374 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'WHAT IS MY FAVOURITE PIZZA'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-27 13:23:35,375 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-27 13:23:35,376 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-27 13:23:35,377 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-27 13:23:35,377 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:35,378 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:35,381 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'WHAT IS MY FAVOURITE PIZZA'}]}], 'system_instruction': {'parts': [{'text': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n'}]}, 'generationConfig': {}}'
[0m

2026-01-27 13:23:37,015 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\n  \"use_memory\": true,\n  \"use_tool\": false,\n  \"tool_name\": \"none\",\n  \"tool_arguments\": null,\n  \"intent\": \"read\",\n  \"memory_key\": \"food_preference\"\n}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 494,
    "candidatesTokenCount": 60,
    "totalTokenCount": 604,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 494
      }
    ],
    "thoughtsTokenCount": 50
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "AG94aZj8LfafjuMPvJTV0Q0"
}



2026-01-27 13:23:37,018 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-27 13:23:37,020 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 13:23:37,020 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-27 13:23:37,021 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:37,021 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 13:23:37,022 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:37,023 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:37,024 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:23:37,024 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:37,025 | DEBUG | LiteLLM | response_cost: 0.0004232
2026-01-27 13:23:37,025 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:23:37,026 | DEBUG | openai.agents | Received model response
2026-01-27 13:23:37,027 | DEBUG | LiteLLM | response_cost: 0.0004232
2026-01-27 13:23:37,028 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x0000020DABFFA0B0>>
2026-01-27 13:23:37,029 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:23:37,029 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-27 13:23:37,029 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:37,030 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-27 13:23:37,030 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-27 13:23:37,030 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:23:37,033 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:23:37,034 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-27 13:23:37,035 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-27 13:23:37,035 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:37,037 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:23:37,039 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:23:37,039 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:23:37,040 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:37,040 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:37,043 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:23:37,043 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:23:37,044 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-27 13:23:37,045 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0004232
2026-01-27 13:23:37,047 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:23:37,048 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:37,049 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:23:37,052 | DEBUG | openai.agents | Resetting current trace
2026-01-27 13:23:37,052 | INFO | orchestrator | üß≠ ROUTER_DECISION | tool=False | memory=True | intent=read
2026-01-27 13:23:37,053 | INFO | orchestrator | üß† REASONING_INPUT_SNAPSHOT
{
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful AI assistant.\n \nYou may answer ANY general knowledge question.\n \nConversation summaries and memories are ONLY to provide helpful context\n(e.g. preferences, ongoing plans).\n \nIf a user asks something unrelated to the current topic,\nanswer it normally using general knowledge.\n \nDo NOT restrict yourself to previous topics unless the user explicitly asks."
    },
    {
      "role": "user",
      "content": "WHAT IS MY FAVOURITE PIZZA"
    },
    {
      "role": "assistant",
      "content": "Your favorite pizza is margherita."
    },
    {
      "role": "user",
      "content": "WHAT IS MY FAVOURITE PIZZA"
    },
    {
      "role": "user",
      "content": "WHAT IS MY FAVOURITE PIZZA"
    }
  ],
  "safety": {
    "safe": true,
    "message": null
  },
  "memory_action": {},
  "memory_data": [
    {
      "memory_id": 4,
      "user_id": 3,
      "text": "my favourite pizza is margneita.",
      "confidence": 0.85,
      "created_at": "2026-01-23T22:15:19.533318+05:30",
      "expires_at": "2026-02-26T10:09:28.217743+05:30",
      "score": 0.8125231862068176
    },
    {
      "memory_id": 6,
      "user_id": 3,
      "text": "My favourite color is yellow.",
      "confidence": 0.85,
      "created_at": "2026-01-26T21:50:52.573607+05:30",
      "expires_at": "2026-02-25T21:50:52.427804+05:30",
      "score": 0.3765886127948761
    }
  ],
  "memory_found": true,
  "tool_context": {}
}
2026-01-27 13:23:37,056 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_16c3efd69eed489f8a8f0b5fbdcbf45f
2026-01-27 13:23:37,057 | DEBUG | openai.agents | Setting current trace: trace_16c3efd69eed489f8a8f0b5fbdcbf45f
2026-01-27 13:23:37,057 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x0000020DABE422A0> with id None
2026-01-27 13:23:37,060 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000020DABE7FE80> with id None
2026-01-27 13:23:37,061 | DEBUG | openai.agents | Calling LLM
2026-01-27 13:23:37,061 | DEBUG | LiteLLM | 

2026-01-27 13:23:37,062 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-27 13:23:37,063 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "WHAT IS MY FAVOURITE PIZZA"}, {"role": "assistant", "content": "Your favorite pizza is margherita."}, {"role": "user", "content": "WHAT IS MY FAVOURITE PIZZA"}, {"role": "user", "content": "WHAT IS MY FAVOURITE PIZZA"}], "safety": {"safe": true, "message": null}, "memory_action": {}, "memory_data": [{"memory_id": 4, "user_id": 3, "text": "my favourite pizza is margneita.", "confidence": 0.85, "created_at": "2026-01-23T22:15:19.533318+05:30", "expires_at": "2026-02-26T10:09:28.217743+05:30", "score": 0.8125231862068176}, {"memory_id": 6, "user_id": 3, "text": "My favourite color is yellow.", "confidence": 0.85, "created_at": "2026-01-26T21:50:52.573607+05:30", "expires_at": "2026-02-25T21:50:52.427804+05:30", "score": 0.3765886127948761}], "memory_found": true, "tool_context": {}}'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=True, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-27 13:23:37,063 | DEBUG | LiteLLM | 

2026-01-27 13:23:37,064 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-27 13:23:37,065 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-27 13:23:37,066 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-27 13:23:37,068 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:23:37,068 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-27 13:23:37,069 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "WHAT IS MY FAVOURITE PIZZA"}, {"role": "assistant", "content": "Your favorite pizza is margherita."}, {"role": "user", "content": "WHAT IS MY FAVOURITE PIZZA"}, {"role": "user", "content": "WHAT IS MY FAVOURITE PIZZA"}], "safety": {"safe": true, "message": null}, "memory_action": {}, "memory_data": [{"memory_id": 4, "user_id": 3, "text": "my favourite pizza is margneita.", "confidence": 0.85, "created_at": "2026-01-23T22:15:19.533318+05:30", "expires_at": "2026-02-26T10:09:28.217743+05:30", "score": 0.8125231862068176}, {"memory_id": 6, "user_id": 3, "text": "My favourite color is yellow.", "confidence": 0.85, "created_at": "2026-01-26T21:50:52.573607+05:30", "expires_at": "2026-02-25T21:50:52.427804+05:30", "score": 0.3765886127948761}], "memory_found": true, "tool_context": {}}'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-27 13:23:37,071 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': True}
2026-01-27 13:23:37,071 | DEBUG | LiteLLM | Final returned optional params: {'stream': True}
2026-01-27 13:23:37,072 | DEBUG | LiteLLM | self.optional_params: {'stream': True}
2026-01-27 13:23:37,073 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:37,074 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:37,075 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?key=*****=sse \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "WHAT IS MY FAVOURITE PIZZA"}, {"role": "assistant", "content": "Your favorite pizza is margherita."}, {"role": "user", "content": "WHAT IS MY FAVOURITE PIZZA"}, {"role": "user", "content": "WHAT IS MY FAVOURITE PIZZA"}], "safety": {"safe": true, "message": null}, "memory_action": {}, "memory_data": [{"memory_id": 4, "user_id": 3, "text": "my favourite pizza is margneita.", "confidence": 0.85, "created_at": "2026-01-23T22:15:19.533318+05:30", "expires_at": "2026-02-26T10:09:28.217743+05:30", "score": 0.8125231862068176}, {"memory_id": 6, "user_id": 3, "text": "My favourite color is yellow.", "confidence": 0.85, "created_at": "2026-01-26T21:50:52.573607+05:30", "expires_at": "2026-02-25T21:50:52.427804+05:30", "score": 0.3765886127948761}], "memory_found": true, "tool_context": {}}'}]}], 'system_instruction': {'parts': [{'text': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.'}]}, 'generationConfig': {}}'
[0m

2026-01-27 13:23:37,077 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 13:23:37,077 | DEBUG | LiteLLM | messages in token_counter: None, text in token_counter: 
2026-01-27 13:23:37,078 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:37,079 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:37,080 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:23:37,081 | DEBUG | LiteLLM | response_cost: 0.0
2026-01-27 13:23:39,496 | DEBUG | LiteLLM | RAW RESPONSE:
first stream response received


2026-01-27 13:23:39,498 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': 'Your favorite pizza is margherita.'}], 'role': 'model'}, 'finishReason': 'STOP', 'index': 0}], 'usageMetadata': {'promptTokenCount': 600, 'candidatesTokenCount': 7, 'totalTokenCount': 897, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 600}], 'thoughtsTokenCount': 290}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'AW94aZrlLoOzjuMPkcScsAI'}
2026-01-27 13:23:39,499 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='AW94aZrlLoOzjuMPkcScsAI', created=1769500419, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='Your favorite pizza is margherita.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=297, prompt_tokens=600, total_tokens=897, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=290, rejected_prediction_tokens=None, text_tokens=7, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=600, image_tokens=None)))
2026-01-27 13:23:39,502 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content='Your favorite pizza is margherita.', role='assistant', function_call=None, tool_calls=None, audio=None)
2026-01-27 13:23:39,502 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='AW94aZrlLoOzjuMPkcScsAI', created=1769500419, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Your favorite pizza is margherita.', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=297, prompt_tokens=600, total_tokens=897, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=290, rejected_prediction_tokens=None, text_tokens=7, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=600, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-27 13:23:39,509 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=False
2026-01-27 13:23:39,510 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=False
2026-01-27 13:23:39,510 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-27 13:23:39,511 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-27 13:23:39,511 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 13:23:39,512 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 13:23:39,513 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:39,514 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:39,514 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:39,515 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:39,515 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:23:39,515 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:23:39,517 | DEBUG | LiteLLM | response_cost: 0.0009225000000000001
2026-01-27 13:23:39,518 | DEBUG | LiteLLM | response_cost: 0.0009225000000000001
2026-01-27 13:23:39,519 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:23:39,520 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0009225000000000001
2026-01-27 13:23:39,521 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:39,522 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:23:39,522 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:23:39,524 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:23:39,525 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:23:39,526 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-27 13:23:39,527 | DEBUG | openai.agents | Resetting current trace
2026-01-27 13:23:39,531 | INFO | session_summary | üß† SUMMARY_START | session_id=d3ad5fa1-6363-4d3f-927e-12a05c40ee39
2026-01-27 13:23:39,535 | INFO | session_summary | üìä Unsummarized messages = 4
2026-01-27 13:23:39,536 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-27 13:23:39,537 | INFO | chat | ‚úÖ Stream complete | tokens=1 | time=6.26s
2026-01-27 13:23:40,277 | DEBUG | httpcore.connection | close.started
2026-01-27 13:23:40,278 | DEBUG | httpcore.connection | close.complete
2026-01-27 13:23:40,279 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-27 13:23:40,338 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000020DAB788460>
2026-01-27 13:23:40,338 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000020DA0481540> server_hostname='api.openai.com' timeout=5.0
2026-01-27 13:23:40,354 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000020DAB788880>
2026-01-27 13:23:40,354 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-27 13:23:40,355 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-27 13:23:40,356 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-27 13:23:40,357 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-27 13:23:40,357 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-27 13:23:41,401 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Tue, 27 Jan 2026 07:53:41 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_273533f0fbb82c4a151dba6eb4f06c31'), (b'openai-processing-ms', b'127'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'130'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c46ad7a8a7b598e-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-27 13:23:41,402 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-27 13:23:41,403 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-27 13:23:41,403 | DEBUG | httpcore.http11 | response_closed.started
2026-01-27 13:23:41,404 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-27 13:23:41,404 | DEBUG | openai.agents | Exported 8 items
2026-01-27 13:28:37,863 | DEBUG | LiteLLM | LoggingWorker cancelled during shutdown
2026-01-27 13:28:37,866 | DEBUG | LiteLLM | [LoggingWorker] atexit: Queue is empty
2026-01-27 13:28:37,868 | DEBUG | asyncio | Using proactor: IocpProactor
2026-01-27 13:28:37,871 | DEBUG | openai.agents | Shutting down trace provider
2026-01-27 13:28:37,871 | DEBUG | openai.agents | Shutting down trace processor <agents.tracing.processors.BatchTraceProcessor object at 0x0000020DA04C81F0>
2026-01-27 13:28:45,352 | INFO | sentence_transformers.SentenceTransformer | Use pytorch device_name: cpu
2026-01-27 13:28:45,352 | INFO | sentence_transformers.SentenceTransformer | Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2026-01-27 13:28:45,355 | DEBUG | urllib3.connectionpool | Starting new HTTPS connection (1): huggingface.co:443
2026-01-27 13:28:45,651 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-27 13:28:45,667 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-27 13:28:45,901 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-27 13:28:45,912 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-27 13:28:46,145 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-27 13:28:46,156 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-27 13:28:46,410 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md HTTP/1.1" 307 0
2026-01-27 13:28:46,425 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/README.md HTTP/1.1" 200 0
2026-01-27 13:28:46,657 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-27 13:28:46,674 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-27 13:28:46,906 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json HTTP/1.1" 307 0
2026-01-27 13:28:46,923 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/sentence_bert_config.json HTTP/1.1" 200 0
2026-01-27 13:28:47,154 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json HTTP/1.1" 404 0
2026-01-27 13:28:47,412 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1" 307 0
2026-01-27 13:28:47,429 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json HTTP/1.1" 200 0
2026-01-27 13:28:47,757 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/1.1" 307 0
2026-01-27 13:28:47,774 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json HTTP/1.1" 200 0
2026-01-27 13:28:48,052 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1" 404 64
2026-01-27 13:28:48,290 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main?recursive=True&expand=False HTTP/1.1" 200 6465
2026-01-27 13:28:48,593 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json HTTP/1.1" 307 0
2026-01-27 13:28:48,609 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json HTTP/1.1" 200 0
2026-01-27 13:28:48,848 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2 HTTP/1.1" 200 6825
2026-01-27 13:28:50,117 | DEBUG | httpcore.connection | connect_tcp.started host='raw.githubusercontent.com' port=443 local_address=None timeout=5 socket_options=None
2026-01-27 13:28:50,151 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023502F85390>
2026-01-27 13:28:50,151 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000023502DC9AC0> server_hostname='raw.githubusercontent.com' timeout=5
2026-01-27 13:28:50,185 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023502F85360>
2026-01-27 13:28:50,185 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'GET']>
2026-01-27 13:28:50,186 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-27 13:28:50,186 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'GET']>
2026-01-27 13:28:50,186 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-27 13:28:50,186 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'GET']>
2026-01-27 13:28:50,217 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Connection', b'keep-alive'), (b'Content-Length', b'62322'), (b'Cache-Control', b'max-age=300'), (b'Content-Security-Policy', b"default-src 'none'; style-src 'unsafe-inline'; sandbox"), (b'Content-Type', b'text/plain; charset=utf-8'), (b'ETag', b'W/"5f0be0ed2fe5ecf561c4b4488d64c304c35b980a45012ef1d1253b4b5d438af1"'), (b'Strict-Transport-Security', b'max-age=31536000'), (b'X-Content-Type-Options', b'nosniff'), (b'X-Frame-Options', b'deny'), (b'X-XSS-Protection', b'1; mode=block'), (b'X-GitHub-Request-Id', b'415E:3865D1:30D7E6:7A896E:69753835'), (b'Content-Encoding', b'gzip'), (b'Accept-Ranges', b'bytes'), (b'Date', b'Tue, 27 Jan 2026 07:58:50 GMT'), (b'Via', b'1.1 varnish'), (b'X-Served-By', b'cache-bom-vanm7210064-BOM'), (b'X-Cache', b'HIT'), (b'X-Cache-Hits', b'42'), (b'X-Timer', b'S1769500730.116321,VS0,VE0'), (b'Vary', b'Authorization,Accept-Encoding'), (b'Access-Control-Allow-Origin', b'*'), (b'Cross-Origin-Resource-Policy', b'cross-origin'), (b'X-Fastly-Request-ID', b'9ba7dbc3dd208ae388da8d866540a1b3aae135dd'), (b'Expires', b'Tue, 27 Jan 2026 08:03:50 GMT'), (b'Source-Age', b'264')])
2026-01-27 13:28:50,220 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'GET']>
2026-01-27 13:28:50,257 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-27 13:28:50,257 | DEBUG | httpcore.http11 | response_closed.started
2026-01-27 13:28:50,257 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-27 13:28:50,258 | DEBUG | httpcore.connection | close.started
2026-01-27 13:28:50,258 | DEBUG | httpcore.connection | close.complete
2026-01-27 13:28:51,157 | DEBUG | LiteLLM | [Non-Blocking] Unable to import GenericAPILogger - LiteLLM Enterprise Feature - No module named 'litellm_enterprise'
2026-01-27 13:28:51,590 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 13:28:51,591 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 13:28:51,592 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 13:28:51,597 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 13:28:51,597 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 13:28:51,598 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 13:28:51,598 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 13:28:51,599 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 13:28:51,599 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 13:28:51,599 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 13:28:51,599 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 13:28:51,600 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 13:28:51,626 | INFO | main | üöÄ FastAPI application starting
2026-01-27 13:29:02,084 | INFO | chat | üì© Chat request | user_id=3 | request_id=f340af03-cf4d-4613-83fb-a2024ed23af7
2026-01-27 13:29:02,116 | INFO | chat | üßë User message | hello
2026-01-27 13:29:02,145 | INFO | chat | üÜï New session created | 29af8d62-64a8-4520-910a-d9462c3cddbc
2026-01-27 13:29:02,167 | INFO | chat | ü§ñ Stream started | session_id=29af8d62-64a8-4520-910a-d9462c3cddbc | request_id=f340af03-cf4d-4613-83fb-a2024ed23af7
2026-01-27 13:29:02,170 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-27 13:29:02,173 | INFO | orchestrator | üßë USER_INPUT | hello
2026-01-27 13:29:02,175 | INFO | orchestrator | üß© CONTEXT | user_id=3 | enable_memory=True | db=None
2026-01-27 13:29:02,176 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-27 13:29:02,177 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_3cffaea5d2354e42a00c52daa7656125
2026-01-27 13:29:02,178 | DEBUG | openai.agents | Setting current trace: trace_3cffaea5d2354e42a00c52daa7656125
2026-01-27 13:29:02,179 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000002350C0D2340> with id None
2026-01-27 13:29:02,180 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-27 13:29:02,190 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000002350C0964A0> with id None
2026-01-27 13:29:02,191 | DEBUG | openai.agents | Calling LLM
2026-01-27 13:29:02,191 | DEBUG | LiteLLM | 

2026-01-27 13:29:02,192 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-27 13:29:02,192 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n', 'role': 'system'}, {'role': 'user', 'content': 'hello'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-27 13:29:02,193 | DEBUG | LiteLLM | 

2026-01-27 13:29:02,194 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-27 13:29:02,194 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-27 13:29:02,195 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-27 13:29:02,220 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:29:02,221 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-27 13:29:02,222 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n', 'role': 'system'}, {'role': 'user', 'content': 'hello'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-27 13:29:02,224 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-27 13:29:02,224 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-27 13:29:02,225 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-27 13:29:02,227 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:02,611 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:02,613 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'hello'}]}], 'system_instruction': {'parts': [{'text': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n'}]}, 'generationConfig': {}}'
[0m

2026-01-27 13:29:03,786 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "SAFE"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 188,
    "candidatesTokenCount": 1,
    "totalTokenCount": 222,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 188
      }
    ],
    "thoughtsTokenCount": 33
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "R3B4abakH4LjjuMPw7XjgQ4"
}



2026-01-27 13:29:03,789 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-27 13:29:03,789 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-27 13:29:03,791 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 13:29:03,791 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 13:29:03,792 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:03,792 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:03,792 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:03,792 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:03,794 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:29:03,795 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:29:03,796 | DEBUG | LiteLLM | response_cost: 0.00014140000000000002
2026-01-27 13:29:03,798 | DEBUG | openai.agents | Received model response
2026-01-27 13:29:03,797 | DEBUG | LiteLLM | response_cost: 0.00014140000000000002
2026-01-27 13:29:03,807 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x000002350C1B0340>>
2026-01-27 13:29:03,809 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:29:03,811 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-27 13:29:03,813 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:03,815 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:29:03,816 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-27 13:29:03,817 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-27 13:29:03,818 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:29:03,816 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-27 13:29:03,819 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-27 13:29:03,819 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:03,820 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:29:03,821 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:29:03,821 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:29:03,822 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:03,822 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:03,823 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:29:03,823 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:29:03,823 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-27 13:29:03,824 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.00014140000000000002
2026-01-27 13:29:03,824 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:29:03,825 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:03,825 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:29:03,826 | DEBUG | openai.agents | Resetting current trace
2026-01-27 13:29:03,826 | ERROR | digital_human_sdk.app.intelligence.utils.json_utils | safe_json_loads failed
2026-01-27 13:29:03,828 | INFO | orchestrator | ‚úÖ Safety passed
2026-01-27 13:29:03,829 | INFO | orchestrator | üì• Fetching semantic memory (via MemoryService)
2026-01-27 13:29:03,875 | INFO | orchestrator | üß† MEMORY_RESULT | found=False | count=0
2026-01-27 13:29:03,875 | INFO | orchestrator | üß≠ Router agent called
2026-01-27 13:29:03,878 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_9ded7a7506db48b6b60e7597ce2db75c
2026-01-27 13:29:03,879 | DEBUG | openai.agents | Setting current trace: trace_9ded7a7506db48b6b60e7597ce2db75c
2026-01-27 13:29:03,880 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000002350C22E390> with id None
2026-01-27 13:29:03,881 | DEBUG | openai.agents | Running agent RouterAgent (turn 1)
2026-01-27 13:29:03,882 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000002350CACDFC0> with id None
2026-01-27 13:29:03,882 | DEBUG | openai.agents | Calling LLM
2026-01-27 13:29:03,883 | DEBUG | LiteLLM | 

2026-01-27 13:29:03,883 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-27 13:29:03,883 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'hello'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-27 13:29:03,884 | DEBUG | LiteLLM | 

2026-01-27 13:29:03,884 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-27 13:29:03,885 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-27 13:29:03,885 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-27 13:29:03,886 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:29:03,887 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-27 13:29:03,888 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'hello'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-27 13:29:03,888 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-27 13:29:03,889 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-27 13:29:03,889 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-27 13:29:03,890 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:03,890 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:03,891 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'hello'}]}], 'system_instruction': {'parts': [{'text': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n'}]}, 'generationConfig': {}}'
[0m

2026-01-27 13:29:05,625 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\n  \"use_memory\": false,\n  \"use_tool\": false,\n  \"tool_name\": \"none\",\n  \"tool_arguments\": null,\n  \"intent\": \"none\",\n  \"memory_key\": null\n}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 486,
    "candidatesTokenCount": 56,
    "totalTokenCount": 576,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 486
      }
    ],
    "thoughtsTokenCount": 34
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "SXB4aeiQGKzeg8UP8f-MkQI"
}



2026-01-27 13:29:05,626 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-27 13:29:05,627 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 13:29:05,627 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-27 13:29:05,627 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:05,627 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 13:29:05,629 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:05,629 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:05,629 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:29:05,630 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:05,630 | DEBUG | LiteLLM | response_cost: 0.0003708
2026-01-27 13:29:05,630 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:29:05,630 | DEBUG | openai.agents | Received model response
2026-01-27 13:29:05,631 | DEBUG | LiteLLM | response_cost: 0.0003708
2026-01-27 13:29:05,631 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x000002350C8E6020>>
2026-01-27 13:29:05,633 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:29:05,635 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-27 13:29:05,637 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:05,637 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-27 13:29:05,637 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-27 13:29:05,638 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:29:05,638 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-27 13:29:05,639 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:29:05,639 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-27 13:29:05,640 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:29:05,640 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:05,640 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:29:05,641 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:05,641 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:29:05,641 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:05,641 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:29:05,643 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:29:05,643 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-27 13:29:05,644 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0003708
2026-01-27 13:29:05,645 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:29:05,645 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:05,646 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:29:05,647 | DEBUG | openai.agents | Resetting current trace
2026-01-27 13:29:05,647 | INFO | orchestrator | üß≠ ROUTER_DECISION | tool=False | memory=False | intent=none
2026-01-27 13:29:05,647 | INFO | orchestrator | üß† REASONING_INPUT_SNAPSHOT
{
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful AI assistant.\n \nYou may answer ANY general knowledge question.\n \nConversation summaries and memories are ONLY to provide helpful context\n(e.g. preferences, ongoing plans).\n \nIf a user asks something unrelated to the current topic,\nanswer it normally using general knowledge.\n \nDo NOT restrict yourself to previous topics unless the user explicitly asks."
    },
    {
      "role": "user",
      "content": "hello"
    },
    {
      "role": "user",
      "content": "hello"
    }
  ],
  "safety": {
    "safe": true,
    "message": null
  },
  "memory_action": {},
  "memory_data": [],
  "memory_found": false,
  "tool_context": {}
}
2026-01-27 13:29:05,648 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_5c8f077e4f2c4966a08f3f2625bdd2fe
2026-01-27 13:29:05,648 | DEBUG | openai.agents | Setting current trace: trace_5c8f077e4f2c4966a08f3f2625bdd2fe
2026-01-27 13:29:05,649 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000002350C0A76A0> with id None
2026-01-27 13:29:05,650 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000002350CACF1C0> with id None
2026-01-27 13:29:05,650 | DEBUG | openai.agents | Calling LLM
2026-01-27 13:29:05,650 | DEBUG | LiteLLM | 

2026-01-27 13:29:05,652 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-27 13:29:05,654 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "hello"}, {"role": "user", "content": "hello"}], "safety": {"safe": true, "message": null}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=True, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-27 13:29:05,654 | DEBUG | LiteLLM | 

2026-01-27 13:29:05,655 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-27 13:29:05,655 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-27 13:29:05,656 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-27 13:29:05,656 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:29:05,657 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-27 13:29:05,657 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "hello"}, {"role": "user", "content": "hello"}], "safety": {"safe": true, "message": null}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-27 13:29:05,658 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': True}
2026-01-27 13:29:05,658 | DEBUG | LiteLLM | Final returned optional params: {'stream': True}
2026-01-27 13:29:05,658 | DEBUG | LiteLLM | self.optional_params: {'stream': True}
2026-01-27 13:29:05,658 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:05,660 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:05,661 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?key=*****=sse \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "hello"}, {"role": "user", "content": "hello"}], "safety": {"safe": true, "message": null}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}]}], 'system_instruction': {'parts': [{'text': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.'}]}, 'generationConfig': {}}'
[0m

2026-01-27 13:29:05,661 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 13:29:05,663 | DEBUG | LiteLLM | messages in token_counter: None, text in token_counter: 
2026-01-27 13:29:05,664 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:05,664 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:05,664 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:29:05,665 | DEBUG | LiteLLM | response_cost: 0.0
2026-01-27 13:29:05,701 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 13:29:05,702 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 13:29:05,702 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 13:29:06,704 | DEBUG | LiteLLM | RAW RESPONSE:
first stream response received


2026-01-27 13:29:06,704 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': 'Hello! How can I help you today?'}], 'role': 'model'}, 'finishReason': 'STOP', 'index': 0}], 'usageMetadata': {'promptTokenCount': 274, 'candidatesTokenCount': 9, 'totalTokenCount': 303, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 274}], 'thoughtsTokenCount': 20}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'SnB4aav8EL3Hg8UP9OK2iA4'}
2026-01-27 13:29:06,708 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='SnB4aav8EL3Hg8UP9OK2iA4', created=1769500746, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='Hello! How can I help you today?', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=29, prompt_tokens=274, total_tokens=303, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=20, rejected_prediction_tokens=None, text_tokens=9, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=274, image_tokens=None)))
2026-01-27 13:29:06,711 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content='Hello! How can I help you today?', role='assistant', function_call=None, tool_calls=None, audio=None)
2026-01-27 13:29:06,711 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='SnB4aav8EL3Hg8UP9OK2iA4', created=1769500746, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Hello! How can I help you today?', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=29, prompt_tokens=274, total_tokens=303, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=20, rejected_prediction_tokens=None, text_tokens=9, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=274, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-27 13:29:06,744 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=False
2026-01-27 13:29:06,751 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=False
2026-01-27 13:29:06,752 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-27 13:29:06,753 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-27 13:29:06,754 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 13:29:06,754 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 13:29:06,755 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:06,755 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:06,755 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:06,756 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:06,756 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:29:06,756 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:29:06,756 | DEBUG | LiteLLM | response_cost: 0.0001547
2026-01-27 13:29:06,757 | DEBUG | LiteLLM | response_cost: 0.0001547
2026-01-27 13:29:06,757 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:29:06,758 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0001547
2026-01-27 13:29:06,758 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:06,758 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:29:06,759 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:29:06,759 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:06,759 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:29:06,761 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-27 13:29:06,762 | DEBUG | openai.agents | Resetting current trace
2026-01-27 13:29:06,769 | INFO | session_summary | üß† SUMMARY_START | session_id=29af8d62-64a8-4520-910a-d9462c3cddbc
2026-01-27 13:29:06,774 | INFO | session_summary | üìä Unsummarized messages = 2
2026-01-27 13:29:06,775 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-27 13:29:06,776 | INFO | chat | ‚úÖ Stream complete | tokens=1 | time=4.69s
2026-01-27 13:29:07,272 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-27 13:29:07,288 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002350BFD9F90>
2026-01-27 13:29:07,288 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000023500D2D540> server_hostname='api.openai.com' timeout=5.0
2026-01-27 13:29:07,300 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002350BFD9F30>
2026-01-27 13:29:07,301 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-27 13:29:07,301 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-27 13:29:07,301 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-27 13:29:07,302 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-27 13:29:07,302 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-27 13:29:08,591 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Tue, 27 Jan 2026 07:59:08 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_729cbd3a0dfca2ad6d9e058cf62ed4ee'), (b'openai-processing-ms', b'126'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'129'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=KnFOxsvV1vIoSEEJLbyeYDQzyHsqmpzGCnrnV64UB9Y-1769500748-1.0.1.1-xys4b25x8uHelOldBTazErNOglQGZT9dItNKpfrjm8pxLWxynZdfTLf7YOspxKdI5lu6vmWtIQwuNG9.MOH3vJ7AFdsG4K9kieWnMTFSfss; path=/; expires=Tue, 27-Jan-26 08:29:08 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=HoK1n_lIasR9KTh6jNks5F4B6n3SwgtiMiIdUXDPlCg-1769500748479-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c46b575fcce5464-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-27 13:29:08,592 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-27 13:29:08,592 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-27 13:29:08,593 | DEBUG | httpcore.http11 | response_closed.started
2026-01-27 13:29:08,593 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-27 13:29:08,593 | DEBUG | openai.agents | Exported 9 items
2026-01-27 13:29:21,031 | INFO | chat | üì© Chat request | user_id=3 | request_id=0da7e334-440c-4a71-8e52-770c8d2d649e
2026-01-27 13:29:21,033 | INFO | chat | üßë User message | hello my name is bhumika raheja
2026-01-27 13:29:21,059 | INFO | chat | ü§ñ Stream started | session_id=29af8d62-64a8-4520-910a-d9462c3cddbc | request_id=0da7e334-440c-4a71-8e52-770c8d2d649e
2026-01-27 13:29:21,062 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-27 13:29:21,063 | INFO | orchestrator | üßë USER_INPUT | hello my name is bhumika raheja
2026-01-27 13:29:21,064 | INFO | orchestrator | üß© CONTEXT | user_id=3 | enable_memory=True | db=None
2026-01-27 13:29:21,065 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-27 13:29:21,067 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_2a1bad7b1e0a43fab023e53897ba9704
2026-01-27 13:29:21,067 | DEBUG | openai.agents | Setting current trace: trace_2a1bad7b1e0a43fab023e53897ba9704
2026-01-27 13:29:21,068 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000002350C869AD0> with id None
2026-01-27 13:29:21,068 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-27 13:29:21,069 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000002350CACFFA0> with id None
2026-01-27 13:29:21,070 | DEBUG | openai.agents | Calling LLM
2026-01-27 13:29:21,070 | DEBUG | LiteLLM | 

2026-01-27 13:29:21,071 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-27 13:29:21,072 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n', 'role': 'system'}, {'role': 'user', 'content': 'hello my name is bhumika raheja'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-27 13:29:21,072 | DEBUG | LiteLLM | 

2026-01-27 13:29:21,073 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-27 13:29:21,074 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-27 13:29:21,074 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-27 13:29:21,076 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:29:21,077 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-27 13:29:21,079 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n', 'role': 'system'}, {'role': 'user', 'content': 'hello my name is bhumika raheja'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-27 13:29:21,082 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-27 13:29:21,082 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-27 13:29:21,083 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-27 13:29:21,084 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:21,085 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:21,086 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'hello my name is bhumika raheja'}]}], 'system_instruction': {'parts': [{'text': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n'}]}, 'generationConfig': {}}'
[0m

2026-01-27 13:29:21,932 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "SAFE"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 196,
    "candidatesTokenCount": 1,
    "totalTokenCount": 224,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 196
      }
    ],
    "thoughtsTokenCount": 27
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "WXB4aYG-JfXGjuMPmLi3iA4"
}



2026-01-27 13:29:21,934 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-27 13:29:21,935 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 13:29:21,935 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-27 13:29:21,936 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:21,937 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 13:29:21,937 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:21,938 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:21,939 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:29:21,940 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:21,940 | DEBUG | LiteLLM | response_cost: 0.00012880000000000001
2026-01-27 13:29:21,942 | DEBUG | openai.agents | Received model response
2026-01-27 13:29:21,941 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:29:21,943 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x000002350CEF62F0>>
2026-01-27 13:29:21,944 | DEBUG | LiteLLM | response_cost: 0.00012880000000000001
2026-01-27 13:29:21,945 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-27 13:29:21,946 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:29:21,949 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-27 13:29:21,948 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-27 13:29:21,949 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:21,954 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-27 13:29:21,955 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:29:21,956 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:29:21,957 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:29:21,957 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:21,958 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-27 13:29:21,959 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:21,959 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:29:21,962 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:29:21,964 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:29:21,967 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:21,968 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-27 13:29:21,969 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:29:21,969 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.00012880000000000001
2026-01-27 13:29:21,970 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:29:21,972 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:21,973 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:29:21,974 | DEBUG | openai.agents | Resetting current trace
2026-01-27 13:29:21,975 | ERROR | digital_human_sdk.app.intelligence.utils.json_utils | safe_json_loads failed
2026-01-27 13:29:21,976 | INFO | orchestrator | ‚úÖ Safety passed
2026-01-27 13:29:21,977 | INFO | orchestrator | üì• Fetching semantic memory (via MemoryService)
2026-01-27 13:29:22,020 | INFO | orchestrator | üß† MEMORY_RESULT | found=True | count=1
2026-01-27 13:29:22,022 | INFO | orchestrator | üß≠ Router agent called
2026-01-27 13:29:22,023 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_9c68a8c6fe50415cb894587534383350
2026-01-27 13:29:22,023 | DEBUG | openai.agents | Setting current trace: trace_9c68a8c6fe50415cb894587534383350
2026-01-27 13:29:22,023 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x0000023500D620C0> with id None
2026-01-27 13:29:22,024 | DEBUG | openai.agents | Running agent RouterAgent (turn 1)
2026-01-27 13:29:22,026 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000002350CACCAC0> with id None
2026-01-27 13:29:22,026 | DEBUG | openai.agents | Calling LLM
2026-01-27 13:29:22,026 | DEBUG | LiteLLM | 

2026-01-27 13:29:22,027 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-27 13:29:22,028 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'hello my name is bhumika raheja'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-27 13:29:22,030 | DEBUG | LiteLLM | 

2026-01-27 13:29:22,033 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-27 13:29:22,033 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-27 13:29:22,034 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-27 13:29:22,035 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:29:22,036 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-27 13:29:22,037 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'hello my name is bhumika raheja'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-27 13:29:22,039 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-27 13:29:22,040 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-27 13:29:22,041 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-27 13:29:22,042 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:22,043 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:22,044 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'hello my name is bhumika raheja'}]}], 'system_instruction': {'parts': [{'text': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n'}]}, 'generationConfig': {}}'
[0m

2026-01-27 13:29:23,825 | DEBUG | httpcore.connection | close.started
2026-01-27 13:29:23,827 | DEBUG | httpcore.connection | close.complete
2026-01-27 13:29:23,828 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-27 13:29:23,856 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002350CEF5CF0>
2026-01-27 13:29:23,856 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000023500D2D540> server_hostname='api.openai.com' timeout=5.0
2026-01-27 13:29:23,872 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002350CEF6380>
2026-01-27 13:29:23,872 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-27 13:29:23,873 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-27 13:29:23,873 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-27 13:29:23,873 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-27 13:29:23,873 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-27 13:29:24,290 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Tue, 27 Jan 2026 07:59:24 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_596ad2110ee71922532f6e142bb1b632'), (b'openai-processing-ms', b'103'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'105'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c46b5dd8ab354ae-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-27 13:29:24,290 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-27 13:29:24,291 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-27 13:29:24,291 | DEBUG | httpcore.http11 | response_closed.started
2026-01-27 13:29:24,291 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-27 13:29:24,291 | DEBUG | openai.agents | Exported 4 items
2026-01-27 13:29:24,738 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\n  \"use_memory\": true,\n  \"use_tool\": false,\n  \"tool_name\": \"none\",\n  \"tool_arguments\": null,\n  \"intent\": \"write\",\n  \"memory_key\": \"name\"\n}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 494,
    "candidatesTokenCount": 58,
    "totalTokenCount": 662,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 494
      }
    ],
    "thoughtsTokenCount": 110
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "XHB4afPzHb3Hg8UP9OK2iA4"
}



2026-01-27 13:29:24,740 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-27 13:29:24,740 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 13:29:24,740 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-27 13:29:24,741 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:24,742 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 13:29:24,744 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:24,745 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:24,746 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:29:24,746 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:24,746 | DEBUG | LiteLLM | response_cost: 0.0005682
2026-01-27 13:29:24,747 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:29:24,747 | DEBUG | openai.agents | Received model response
2026-01-27 13:29:24,747 | DEBUG | LiteLLM | response_cost: 0.0005682
2026-01-27 13:29:24,747 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x000002350CE52020>>
2026-01-27 13:29:24,748 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:29:24,748 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-27 13:29:24,748 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:24,750 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-27 13:29:24,750 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-27 13:29:24,750 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:29:24,750 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-27 13:29:24,751 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:29:24,751 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-27 13:29:24,752 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:29:24,752 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:24,753 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:29:24,753 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:24,753 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:29:24,754 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:24,754 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:29:24,755 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:29:24,755 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-27 13:29:24,755 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0005682
2026-01-27 13:29:24,757 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:29:24,758 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:24,758 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:29:24,758 | DEBUG | openai.agents | Resetting current trace
2026-01-27 13:29:24,759 | INFO | orchestrator | üß≠ ROUTER_DECISION | tool=False | memory=True | intent=write
2026-01-27 13:29:24,759 | INFO | orchestrator | üß† Memory agent called (WRITE)
2026-01-27 13:29:24,760 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_15999f344b394aa59b73985ba9a9a582
2026-01-27 13:29:24,760 | DEBUG | openai.agents | Setting current trace: trace_15999f344b394aa59b73985ba9a9a582
2026-01-27 13:29:24,760 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000002350C7213A0> with id None
2026-01-27 13:29:24,761 | DEBUG | openai.agents | Running agent Memory Agent (turn 1)
2026-01-27 13:29:24,763 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000002350C872320> with id None
2026-01-27 13:29:24,765 | DEBUG | openai.agents | Calling LLM
2026-01-27 13:29:24,765 | DEBUG | LiteLLM | 

2026-01-27 13:29:24,766 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-27 13:29:24,766 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a Memory Extraction Agent.\n\nYour job is to extract ONLY long-term, personal, user-specific information\nfrom the user\'s message and decide the appropriate memory action.\n\nYou will be called ONLY when memory handling is required.\n\n========================\nWHAT TO EXTRACT\n========================\n- Stable personal facts (name, education, location, job)\n- Long-term preferences (food, language, habits)\n- Information the user would reasonably expect to be remembered\n- Facts that should be retrievable across sessions via semantic search\n\nStore memory as FULL, NATURAL-LANGUAGE TEXT.\nDO NOT create structured keys.\nDO NOT summarize aggressively.\nPreserve meaning exactly as stated.\n\n========================\nWHAT TO IGNORE\n========================\n- Questions\n- Small talk\n- Temporary states (mood, weather, current task)\n- Opinions about content\n- One-time or session-only context\n\n========================\nACTIONS (STRICT)\n========================\nUse exactly ONE action:\n\n- "save"\n  ‚Üí New long-term personal information stated for the first time\n\n- "update"\n  ‚Üí Existing long-term information is explicitly changed,\n    corrected, or overridden\n    (keywords: "now", "instead", "changed", "earlier", "previously")\n\n- "delete"\n  ‚Üí User explicitly asks to forget, remove, or delete information\n\nNEVER guess.\nNEVER infer deletion.\nNEVER revive deleted information.\nDeleted memory must be treated as non-existent.\n\n========================\nIMPORTANT RULES\n========================\n- Store memory as ONE complete sentence\n- Do NOT split into multiple memories\n- Do NOT invent information\n- Do NOT rely on database state\n- Backend will validate save vs update\n- If unsure, return "none"\n\n========================\nOUTPUT RULES (CRITICAL)\n========================\n- Return VALID JSON only\n- Do NOT explain anything\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Do NOT return multiple objects\n\n========================\nOUTPUT FORMAT (JSON ONLY)\n========================\n{\n  "action": "save | update | delete | none",\n  "memory_text": "full natural language memory text",\n  "confidence": 0.0\n}\n\n========================\nCONFIDENCE GUIDELINES\n========================\n- 0.9‚Äì1.0 ‚Üí Explicit personal fact (e.g., education, name, job)\n- 0.8‚Äì0.9 ‚Üí Clear long-term preference or habit\n- Below 0.7 ‚Üí Weak or ambiguous (avoid storing)\n\n========================\nNO MEMORY CASE\n========================\nIf no valid long-term memory is present, return:\n{\n  "action": "none",\n  "memory_text": "",\n  "confidence": 0.0\n}\n', 'role': 'system'}, {'role': 'user', 'content': 'hello my name is bhumika raheja'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-27 13:29:24,767 | DEBUG | LiteLLM | 

2026-01-27 13:29:24,767 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-27 13:29:24,767 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-27 13:29:24,768 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-27 13:29:24,768 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:29:24,770 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-27 13:29:24,770 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Memory Extraction Agent.\n\nYour job is to extract ONLY long-term, personal, user-specific information\nfrom the user\'s message and decide the appropriate memory action.\n\nYou will be called ONLY when memory handling is required.\n\n========================\nWHAT TO EXTRACT\n========================\n- Stable personal facts (name, education, location, job)\n- Long-term preferences (food, language, habits)\n- Information the user would reasonably expect to be remembered\n- Facts that should be retrievable across sessions via semantic search\n\nStore memory as FULL, NATURAL-LANGUAGE TEXT.\nDO NOT create structured keys.\nDO NOT summarize aggressively.\nPreserve meaning exactly as stated.\n\n========================\nWHAT TO IGNORE\n========================\n- Questions\n- Small talk\n- Temporary states (mood, weather, current task)\n- Opinions about content\n- One-time or session-only context\n\n========================\nACTIONS (STRICT)\n========================\nUse exactly ONE action:\n\n- "save"\n  ‚Üí New long-term personal information stated for the first time\n\n- "update"\n  ‚Üí Existing long-term information is explicitly changed,\n    corrected, or overridden\n    (keywords: "now", "instead", "changed", "earlier", "previously")\n\n- "delete"\n  ‚Üí User explicitly asks to forget, remove, or delete information\n\nNEVER guess.\nNEVER infer deletion.\nNEVER revive deleted information.\nDeleted memory must be treated as non-existent.\n\n========================\nIMPORTANT RULES\n========================\n- Store memory as ONE complete sentence\n- Do NOT split into multiple memories\n- Do NOT invent information\n- Do NOT rely on database state\n- Backend will validate save vs update\n- If unsure, return "none"\n\n========================\nOUTPUT RULES (CRITICAL)\n========================\n- Return VALID JSON only\n- Do NOT explain anything\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Do NOT return multiple objects\n\n========================\nOUTPUT FORMAT (JSON ONLY)\n========================\n{\n  "action": "save | update | delete | none",\n  "memory_text": "full natural language memory text",\n  "confidence": 0.0\n}\n\n========================\nCONFIDENCE GUIDELINES\n========================\n- 0.9‚Äì1.0 ‚Üí Explicit personal fact (e.g., education, name, job)\n- 0.8‚Äì0.9 ‚Üí Clear long-term preference or habit\n- Below 0.7 ‚Üí Weak or ambiguous (avoid storing)\n\n========================\nNO MEMORY CASE\n========================\nIf no valid long-term memory is present, return:\n{\n  "action": "none",\n  "memory_text": "",\n  "confidence": 0.0\n}\n', 'role': 'system'}, {'role': 'user', 'content': 'hello my name is bhumika raheja'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-27 13:29:24,771 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-27 13:29:24,771 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-27 13:29:24,771 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-27 13:29:24,772 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:24,772 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:24,773 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'hello my name is bhumika raheja'}]}], 'system_instruction': {'parts': [{'text': 'You are a Memory Extraction Agent.\n\nYour job is to extract ONLY long-term, personal, user-specific information\nfrom the user\'s message and decide the appropriate memory action.\n\nYou will be called ONLY when memory handling is required.\n\n========================\nWHAT TO EXTRACT\n========================\n- Stable personal facts (name, education, location, job)\n- Long-term preferences (food, language, habits)\n- Information the user would reasonably expect to be remembered\n- Facts that should be retrievable across sessions via semantic search\n\nStore memory as FULL, NATURAL-LANGUAGE TEXT.\nDO NOT create structured keys.\nDO NOT summarize aggressively.\nPreserve meaning exactly as stated.\n\n========================\nWHAT TO IGNORE\n========================\n- Questions\n- Small talk\n- Temporary states (mood, weather, current task)\n- Opinions about content\n- One-time or session-only context\n\n========================\nACTIONS (STRICT)\n========================\nUse exactly ONE action:\n\n- "save"\n  ‚Üí New long-term personal information stated for the first time\n\n- "update"\n  ‚Üí Existing long-term information is explicitly changed,\n    corrected, or overridden\n    (keywords: "now", "instead", "changed", "earlier", "previously")\n\n- "delete"\n  ‚Üí User explicitly asks to forget, remove, or delete information\n\nNEVER guess.\nNEVER infer deletion.\nNEVER revive deleted information.\nDeleted memory must be treated as non-existent.\n\n========================\nIMPORTANT RULES\n========================\n- Store memory as ONE complete sentence\n- Do NOT split into multiple memories\n- Do NOT invent information\n- Do NOT rely on database state\n- Backend will validate save vs update\n- If unsure, return "none"\n\n========================\nOUTPUT RULES (CRITICAL)\n========================\n- Return VALID JSON only\n- Do NOT explain anything\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Do NOT return multiple objects\n\n========================\nOUTPUT FORMAT (JSON ONLY)\n========================\n{\n  "action": "save | update | delete | none",\n  "memory_text": "full natural language memory text",\n  "confidence": 0.0\n}\n\n========================\nCONFIDENCE GUIDELINES\n========================\n- 0.9‚Äì1.0 ‚Üí Explicit personal fact (e.g., education, name, job)\n- 0.8‚Äì0.9 ‚Üí Clear long-term preference or habit\n- Below 0.7 ‚Üí Weak or ambiguous (avoid storing)\n\n========================\nNO MEMORY CASE\n========================\nIf no valid long-term memory is present, return:\n{\n  "action": "none",\n  "memory_text": "",\n  "confidence": 0.0\n}\n'}]}, 'generationConfig': {}}'
[0m

2026-01-27 13:29:25,708 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "```json\n{\n  \"action\": \"save\",\n  \"memory_text\": \"My name is Bhumika Raheja.\",\n  \"confidence\": 1.0\n}\n```"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 609,
    "candidatesTokenCount": 42,
    "totalTokenCount": 669,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 609
      }
    ],
    "thoughtsTokenCount": 18
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "XXB4ad3pGdCrjuMPzMjI6Qs"
}



2026-01-27 13:29:25,710 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-27 13:29:25,711 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 13:29:25,711 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-27 13:29:25,711 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:25,711 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 13:29:25,712 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:25,715 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:25,716 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:29:25,716 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:25,717 | DEBUG | LiteLLM | response_cost: 0.0003327
2026-01-27 13:29:25,718 | DEBUG | openai.agents | Received model response
2026-01-27 13:29:25,717 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:29:25,718 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x000002350CEF7850>>
2026-01-27 13:29:25,718 | DEBUG | LiteLLM | response_cost: 0.0003327
2026-01-27 13:29:25,720 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-27 13:29:25,721 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:29:25,721 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-27 13:29:25,721 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-27 13:29:25,721 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:25,722 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-27 13:29:25,722 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:29:25,723 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:29:25,723 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:29:25,724 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:25,724 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-27 13:29:25,724 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:25,724 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:29:25,725 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:29:25,725 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:29:25,726 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:25,726 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-27 13:29:25,727 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:29:25,727 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0003327
2026-01-27 13:29:25,729 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:29:25,729 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:25,729 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:29:25,731 | DEBUG | openai.agents | Resetting current trace
2026-01-27 13:29:25,731 | INFO | orchestrator | üß† MEMORY_DECISION | {
  "action": "save",
  "memory_text": "My name is Bhumika Raheja.",
  "confidence": 1.0
}
2026-01-27 13:29:25,732 | INFO | orchestrator | üß† REASONING_INPUT_SNAPSHOT
{
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful AI assistant.\n \nYou may answer ANY general knowledge question.\n \nConversation summaries and memories are ONLY to provide helpful context\n(e.g. preferences, ongoing plans).\n \nIf a user asks something unrelated to the current topic,\nanswer it normally using general knowledge.\n \nDo NOT restrict yourself to previous topics unless the user explicitly asks."
    },
    {
      "role": "user",
      "content": "hello"
    },
    {
      "role": "assistant",
      "content": "Hello! How can I help you today?"
    },
    {
      "role": "user",
      "content": "hello my name is bhumika raheja"
    },
    {
      "role": "user",
      "content": "hello my name is bhumika raheja"
    }
  ],
  "safety": {
    "safe": true,
    "message": null
  },
  "memory_action": {},
  "memory_data": [
    {
      "memory_id": 7,
      "user_id": 3,
      "text": "my name is bhumika raheja",
      "confidence": 0.95,
      "created_at": "2026-01-26T22:31:33.870462+05:30",
      "expires_at": "2026-02-25T22:31:33.867621+05:30",
      "score": 0.9172467589378357
    }
  ],
  "memory_found": true,
  "tool_context": {}
}
2026-01-27 13:29:25,733 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_43fd9a4165e847bc8cc0f0961f59276d
2026-01-27 13:29:25,734 | DEBUG | openai.agents | Setting current trace: trace_43fd9a4165e847bc8cc0f0961f59276d
2026-01-27 13:29:25,734 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000002350C7229D0> with id None
2026-01-27 13:29:25,736 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000002350C871AE0> with id None
2026-01-27 13:29:25,737 | DEBUG | openai.agents | Calling LLM
2026-01-27 13:29:25,738 | DEBUG | LiteLLM | 

2026-01-27 13:29:25,738 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-27 13:29:25,739 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "hello"}, {"role": "assistant", "content": "Hello! How can I help you today?"}, {"role": "user", "content": "hello my name is bhumika raheja"}, {"role": "user", "content": "hello my name is bhumika raheja"}], "safety": {"safe": true, "message": null}, "memory_action": {}, "memory_data": [{"memory_id": 7, "user_id": 3, "text": "my name is bhumika raheja", "confidence": 0.95, "created_at": "2026-01-26T22:31:33.870462+05:30", "expires_at": "2026-02-25T22:31:33.867621+05:30", "score": 0.9172467589378357}], "memory_found": true, "tool_context": {}}'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=True, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-27 13:29:25,739 | DEBUG | LiteLLM | 

2026-01-27 13:29:25,739 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-27 13:29:25,740 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-27 13:29:25,740 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-27 13:29:25,741 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:29:25,742 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-27 13:29:25,742 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "hello"}, {"role": "assistant", "content": "Hello! How can I help you today?"}, {"role": "user", "content": "hello my name is bhumika raheja"}, {"role": "user", "content": "hello my name is bhumika raheja"}], "safety": {"safe": true, "message": null}, "memory_action": {}, "memory_data": [{"memory_id": 7, "user_id": 3, "text": "my name is bhumika raheja", "confidence": 0.95, "created_at": "2026-01-26T22:31:33.870462+05:30", "expires_at": "2026-02-25T22:31:33.867621+05:30", "score": 0.9172467589378357}], "memory_found": true, "tool_context": {}}'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-27 13:29:25,743 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': True}
2026-01-27 13:29:25,743 | DEBUG | LiteLLM | Final returned optional params: {'stream': True}
2026-01-27 13:29:25,743 | DEBUG | LiteLLM | self.optional_params: {'stream': True}
2026-01-27 13:29:25,745 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:25,745 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:25,747 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?key=*****=sse \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "hello"}, {"role": "assistant", "content": "Hello! How can I help you today?"}, {"role": "user", "content": "hello my name is bhumika raheja"}, {"role": "user", "content": "hello my name is bhumika raheja"}], "safety": {"safe": true, "message": null}, "memory_action": {}, "memory_data": [{"memory_id": 7, "user_id": 3, "text": "my name is bhumika raheja", "confidence": 0.95, "created_at": "2026-01-26T22:31:33.870462+05:30", "expires_at": "2026-02-25T22:31:33.867621+05:30", "score": 0.9172467589378357}], "memory_found": true, "tool_context": {}}'}]}], 'system_instruction': {'parts': [{'text': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.'}]}, 'generationConfig': {}}'
[0m

2026-01-27 13:29:25,752 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 13:29:25,752 | DEBUG | LiteLLM | messages in token_counter: None, text in token_counter: 
2026-01-27 13:29:25,752 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:25,753 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:25,753 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:29:25,754 | DEBUG | LiteLLM | response_cost: 0.0
2026-01-27 13:29:27,105 | DEBUG | LiteLLM | RAW RESPONSE:
first stream response received


2026-01-27 13:29:27,106 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': "Hello Bhumika! It's nice to meet you. How can I assist you today?"}], 'role': 'model'}, 'finishReason': 'STOP', 'index': 0}], 'usageMetadata': {'promptTokenCount': 460, 'candidatesTokenCount': 20, 'totalTokenCount': 520, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 460}], 'thoughtsTokenCount': 40}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'XnB4aamRIaOBg8UP1bqNuQ0'}
2026-01-27 13:29:27,107 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='XnB4aamRIaOBg8UP1bqNuQ0', created=1769500767, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content="Hello Bhumika! It's nice to meet you. How can I assist you today?", role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=60, prompt_tokens=460, total_tokens=520, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=40, rejected_prediction_tokens=None, text_tokens=20, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=460, image_tokens=None)))
2026-01-27 13:29:27,108 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content="Hello Bhumika! It's nice to meet you. How can I assist you today?", role='assistant', function_call=None, tool_calls=None, audio=None)
2026-01-27 13:29:27,109 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='XnB4aamRIaOBg8UP1bqNuQ0', created=1769500767, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content="Hello Bhumika! It's nice to meet you. How can I assist you today?", role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=60, prompt_tokens=460, total_tokens=520, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=40, rejected_prediction_tokens=None, text_tokens=20, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=460, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-27 13:29:27,110 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=False
2026-01-27 13:29:27,110 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=False
2026-01-27 13:29:27,110 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-27 13:29:27,110 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-27 13:29:27,112 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 13:29:27,112 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-27 13:29:27,112 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:27,113 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:27,113 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:27,113 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:27,113 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:29:27,114 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:29:27,114 | DEBUG | LiteLLM | response_cost: 0.000288
2026-01-27 13:29:27,114 | DEBUG | LiteLLM | response_cost: 0.000288
2026-01-27 13:29:27,114 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.000288
2026-01-27 13:29:27,115 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:29:27,116 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-27 13:29:27,116 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:27,116 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-27 13:29:27,117 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:29:27,117 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-27 13:29:27,118 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-27 13:29:27,119 | DEBUG | openai.agents | Resetting current trace
2026-01-27 13:29:27,125 | INFO | session_summary | üß† SUMMARY_START | session_id=29af8d62-64a8-4520-910a-d9462c3cddbc
2026-01-27 13:29:27,131 | INFO | session_summary | üìä Unsummarized messages = 4
2026-01-27 13:29:27,131 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-27 13:29:27,132 | INFO | chat | ‚úÖ Stream complete | tokens=1 | time=6.1s
2026-01-27 13:29:29,367 | DEBUG | httpcore.connection | close.started
2026-01-27 13:29:29,368 | DEBUG | httpcore.connection | close.complete
2026-01-27 13:29:29,368 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-27 13:29:29,398 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002350BFDAD70>
2026-01-27 13:29:29,398 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000023500D2D540> server_hostname='api.openai.com' timeout=5.0
2026-01-27 13:29:29,408 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002350BFDAE60>
2026-01-27 13:29:29,408 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-27 13:29:29,409 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-27 13:29:29,409 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-27 13:29:29,409 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-27 13:29:29,409 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-27 13:29:29,874 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Tue, 27 Jan 2026 07:59:29 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_7695db21b8a5a28a784ac596f8e1fd08'), (b'openai-processing-ms', b'131'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'134'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c46b6002b558b19-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-27 13:29:29,875 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-27 13:29:29,875 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-27 13:29:29,875 | DEBUG | httpcore.http11 | response_closed.started
2026-01-27 13:29:29,876 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-27 13:29:29,876 | DEBUG | openai.agents | Exported 8 items
2026-01-27 13:33:06,178 | DEBUG | LiteLLM | LoggingWorker cancelled during shutdown
2026-01-27 13:33:06,181 | DEBUG | LiteLLM | [LoggingWorker] atexit: Queue is empty
2026-01-27 13:33:06,184 | DEBUG | asyncio | Using proactor: IocpProactor
2026-01-27 13:33:06,186 | DEBUG | openai.agents | Shutting down trace provider
2026-01-27 13:33:06,186 | DEBUG | openai.agents | Shutting down trace processor <agents.tracing.processors.BatchTraceProcessor object at 0x0000023500D74190>
2026-01-27 13:33:13,596 | INFO | sentence_transformers.SentenceTransformer | Use pytorch device_name: cpu
2026-01-27 13:33:13,597 | INFO | sentence_transformers.SentenceTransformer | Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2026-01-27 13:33:13,600 | DEBUG | urllib3.connectionpool | Starting new HTTPS connection (1): huggingface.co:443
2026-01-27 13:33:13,920 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-27 13:33:13,931 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-27 13:33:14,171 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-27 13:33:14,180 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-27 13:33:14,408 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-27 13:33:14,419 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-27 13:33:14,643 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md HTTP/1.1" 307 0
2026-01-27 13:33:14,653 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/README.md HTTP/1.1" 200 0
2026-01-27 13:33:14,881 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-27 13:33:14,893 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-27 13:33:15,132 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json HTTP/1.1" 307 0
2026-01-27 13:33:15,142 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/sentence_bert_config.json HTTP/1.1" 200 0
2026-01-27 13:33:15,376 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json HTTP/1.1" 404 0
2026-01-27 13:33:15,605 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1" 307 0
2026-01-27 13:33:15,617 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json HTTP/1.1" 200 0
2026-01-27 13:33:15,916 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/1.1" 307 0
2026-01-27 13:33:15,928 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json HTTP/1.1" 200 0
2026-01-27 13:33:16,188 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1" 404 64
2026-01-27 13:33:16,419 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main?recursive=True&expand=False HTTP/1.1" 200 6465
2026-01-27 13:33:16,703 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json HTTP/1.1" 307 0
2026-01-27 13:33:16,716 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json HTTP/1.1" 200 0
2026-01-27 13:33:16,964 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2 HTTP/1.1" 200 6842
2026-01-27 13:33:18,338 | DEBUG | httpcore.connection | connect_tcp.started host='raw.githubusercontent.com' port=443 local_address=None timeout=5 socket_options=None
2026-01-27 13:33:18,369 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000016CBB7C5330>
2026-01-27 13:33:18,370 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000016CBB601C40> server_hostname='raw.githubusercontent.com' timeout=5
2026-01-27 13:33:18,403 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000016CBB7C5300>
2026-01-27 13:33:18,404 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'GET']>
2026-01-27 13:33:18,404 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-27 13:33:18,404 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'GET']>
2026-01-27 13:33:18,405 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-27 13:33:18,405 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'GET']>
2026-01-27 13:33:18,434 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Connection', b'keep-alive'), (b'Content-Length', b'62322'), (b'Cache-Control', b'max-age=300'), (b'Content-Security-Policy', b"default-src 'none'; style-src 'unsafe-inline'; sandbox"), (b'Content-Type', b'text/plain; charset=utf-8'), (b'ETag', b'W/"5f0be0ed2fe5ecf561c4b4488d64c304c35b980a45012ef1d1253b4b5d438af1"'), (b'Strict-Transport-Security', b'max-age=31536000'), (b'X-Content-Type-Options', b'nosniff'), (b'X-Frame-Options', b'deny'), (b'X-XSS-Protection', b'1; mode=block'), (b'X-GitHub-Request-Id', b'415E:3865D1:30D7E6:7A896E:69753835'), (b'Content-Encoding', b'gzip'), (b'Accept-Ranges', b'bytes'), (b'Date', b'Tue, 27 Jan 2026 08:03:18 GMT'), (b'Via', b'1.1 varnish'), (b'X-Served-By', b'cache-bom-vanm7210051-BOM'), (b'X-Cache', b'HIT'), (b'X-Cache-Hits', b'28'), (b'X-Timer', b'S1769500998.332937,VS0,VE0'), (b'Vary', b'Authorization,Accept-Encoding'), (b'Access-Control-Allow-Origin', b'*'), (b'Cross-Origin-Resource-Policy', b'cross-origin'), (b'X-Fastly-Request-ID', b'ec9465836a120cafb746a03e5efcc8db493100c4'), (b'Expires', b'Tue, 27 Jan 2026 08:08:18 GMT'), (b'Source-Age', b'232')])
2026-01-27 13:33:18,435 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'GET']>
2026-01-27 13:33:18,476 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-27 13:33:18,477 | DEBUG | httpcore.http11 | response_closed.started
2026-01-27 13:33:18,477 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-27 13:33:18,478 | DEBUG | httpcore.connection | close.started
2026-01-27 13:33:18,478 | DEBUG | httpcore.connection | close.complete
2026-01-27 13:33:19,159 | DEBUG | LiteLLM | [Non-Blocking] Unable to import GenericAPILogger - LiteLLM Enterprise Feature - No module named 'litellm_enterprise'
2026-01-27 13:33:19,493 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 13:33:19,494 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 13:33:19,494 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 13:33:19,499 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 13:33:19,500 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 13:33:19,500 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 13:33:19,501 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 13:33:19,501 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 13:33:19,501 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 13:33:19,502 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 13:33:19,502 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 13:33:19,502 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 13:33:19,529 | INFO | main | üöÄ FastAPI application starting
2026-01-27 13:33:25,910 | DEBUG | asyncio | Using proactor: IocpProactor
2026-01-27 13:33:25,912 | DEBUG | openai.agents | Shutting down trace provider
2026-01-27 13:33:25,913 | DEBUG | openai.agents | Shutting down trace processor <agents.tracing.processors.BatchTraceProcessor object at 0x0000016CB95B8160>
2026-01-27 13:33:32,904 | INFO | sentence_transformers.SentenceTransformer | Use pytorch device_name: cpu
2026-01-27 13:33:32,904 | INFO | sentence_transformers.SentenceTransformer | Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2026-01-27 13:33:32,906 | DEBUG | urllib3.connectionpool | Starting new HTTPS connection (1): huggingface.co:443
2026-01-27 13:33:33,178 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-27 13:33:33,188 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-27 13:33:33,418 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-27 13:33:33,429 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-27 13:33:33,664 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-27 13:33:33,682 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-27 13:33:33,916 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md HTTP/1.1" 307 0
2026-01-27 13:33:33,934 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/README.md HTTP/1.1" 200 0
2026-01-27 13:33:34,169 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-27 13:33:34,181 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-27 13:33:34,407 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json HTTP/1.1" 307 0
2026-01-27 13:33:34,422 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/sentence_bert_config.json HTTP/1.1" 200 0
2026-01-27 13:33:34,656 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json HTTP/1.1" 404 0
2026-01-27 13:33:34,893 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1" 307 0
2026-01-27 13:33:34,926 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json HTTP/1.1" 200 0
2026-01-27 13:33:35,241 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/1.1" 307 0
2026-01-27 13:33:35,250 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json HTTP/1.1" 200 0
2026-01-27 13:33:35,485 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1" 404 64
2026-01-27 13:33:35,723 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main?recursive=True&expand=False HTTP/1.1" 200 6465
2026-01-27 13:33:35,978 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json HTTP/1.1" 307 0
2026-01-27 13:33:36,006 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json HTTP/1.1" 200 0
2026-01-27 13:33:36,262 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2 HTTP/1.1" 200 6842
2026-01-27 13:33:37,781 | DEBUG | httpcore.connection | connect_tcp.started host='raw.githubusercontent.com' port=443 local_address=None timeout=5 socket_options=None
2026-01-27 13:33:37,838 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000027FA57154B0>
2026-01-27 13:33:37,839 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000027FA554DC40> server_hostname='raw.githubusercontent.com' timeout=5
2026-01-27 13:33:37,887 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000027FA5715480>
2026-01-27 13:33:37,887 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'GET']>
2026-01-27 13:33:37,888 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-27 13:33:37,888 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'GET']>
2026-01-27 13:33:37,888 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-27 13:33:37,888 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'GET']>
2026-01-27 13:33:37,924 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Connection', b'keep-alive'), (b'Content-Length', b'62322'), (b'Cache-Control', b'max-age=300'), (b'Content-Security-Policy', b"default-src 'none'; style-src 'unsafe-inline'; sandbox"), (b'Content-Type', b'text/plain; charset=utf-8'), (b'ETag', b'W/"5f0be0ed2fe5ecf561c4b4488d64c304c35b980a45012ef1d1253b4b5d438af1"'), (b'Strict-Transport-Security', b'max-age=31536000'), (b'X-Content-Type-Options', b'nosniff'), (b'X-Frame-Options', b'deny'), (b'X-XSS-Protection', b'1; mode=block'), (b'X-GitHub-Request-Id', b'415E:3865D1:30D7E6:7A896E:69753835'), (b'Content-Encoding', b'gzip'), (b'Accept-Ranges', b'bytes'), (b'Date', b'Tue, 27 Jan 2026 08:03:37 GMT'), (b'Via', b'1.1 varnish'), (b'X-Served-By', b'cache-bom-vanm7210030-BOM'), (b'X-Cache', b'HIT'), (b'X-Cache-Hits', b'39'), (b'X-Timer', b'S1769501018.824214,VS0,VE0'), (b'Vary', b'Authorization,Accept-Encoding'), (b'Access-Control-Allow-Origin', b'*'), (b'Cross-Origin-Resource-Policy', b'cross-origin'), (b'X-Fastly-Request-ID', b'f38d1ad2b3c05f467ae0cf096f2a65e5f6197c9b'), (b'Expires', b'Tue, 27 Jan 2026 08:08:37 GMT'), (b'Source-Age', b'252')])
2026-01-27 13:33:37,925 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'GET']>
2026-01-27 13:33:37,975 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-27 13:33:37,975 | DEBUG | httpcore.http11 | response_closed.started
2026-01-27 13:33:37,975 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-27 13:33:37,976 | DEBUG | httpcore.connection | close.started
2026-01-27 13:33:37,976 | DEBUG | httpcore.connection | close.complete
2026-01-27 13:33:38,795 | DEBUG | LiteLLM | [Non-Blocking] Unable to import GenericAPILogger - LiteLLM Enterprise Feature - No module named 'litellm_enterprise'
2026-01-27 13:33:39,310 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 13:33:39,311 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 13:33:39,311 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 13:33:39,317 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 13:33:39,319 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 13:33:39,319 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 13:33:39,319 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 13:33:39,319 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 13:33:39,320 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 13:33:39,322 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 13:33:39,322 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 13:33:39,322 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 13:33:39,358 | INFO | main | üöÄ FastAPI application starting
2026-01-27 13:33:48,714 | DEBUG | asyncio | Using proactor: IocpProactor
2026-01-27 13:33:48,715 | DEBUG | openai.agents | Shutting down trace provider
2026-01-27 13:33:48,716 | DEBUG | openai.agents | Shutting down trace processor <agents.tracing.processors.BatchTraceProcessor object at 0x0000027FA34F81F0>
2026-01-27 13:33:55,903 | INFO | sentence_transformers.SentenceTransformer | Use pytorch device_name: cpu
2026-01-27 13:33:55,903 | INFO | sentence_transformers.SentenceTransformer | Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2026-01-27 13:33:55,908 | DEBUG | urllib3.connectionpool | Starting new HTTPS connection (1): huggingface.co:443
2026-01-27 13:33:56,207 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-27 13:33:56,219 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-27 13:33:56,651 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-27 13:33:56,691 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-27 13:33:56,921 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-27 13:33:56,932 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-27 13:33:57,175 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md HTTP/1.1" 307 0
2026-01-27 13:33:57,191 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/README.md HTTP/1.1" 200 0
2026-01-27 13:33:57,438 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-27 13:33:57,449 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-27 13:33:57,677 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json HTTP/1.1" 307 0
2026-01-27 13:33:57,689 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/sentence_bert_config.json HTTP/1.1" 200 0
2026-01-27 13:33:57,922 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json HTTP/1.1" 404 0
2026-01-27 13:33:58,155 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1" 307 0
2026-01-27 13:33:58,165 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json HTTP/1.1" 200 0
2026-01-27 13:33:58,466 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/1.1" 307 0
2026-01-27 13:33:58,478 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json HTTP/1.1" 200 0
2026-01-27 13:33:58,732 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1" 404 64
2026-01-27 13:33:58,991 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main?recursive=True&expand=False HTTP/1.1" 200 6465
2026-01-27 13:33:59,249 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json HTTP/1.1" 307 0
2026-01-27 13:33:59,264 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json HTTP/1.1" 200 0
2026-01-27 13:33:59,505 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2 HTTP/1.1" 200 6842
2026-01-27 13:34:01,109 | DEBUG | httpcore.connection | connect_tcp.started host='raw.githubusercontent.com' port=443 local_address=None timeout=5 socket_options=None
2026-01-27 13:34:01,159 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001E297145360>
2026-01-27 13:34:01,159 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x000001E296F7DC40> server_hostname='raw.githubusercontent.com' timeout=5
2026-01-27 13:34:01,193 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001E297145330>
2026-01-27 13:34:01,193 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'GET']>
2026-01-27 13:34:01,193 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-27 13:34:01,195 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'GET']>
2026-01-27 13:34:01,195 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-27 13:34:01,195 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'GET']>
2026-01-27 13:34:01,225 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Connection', b'keep-alive'), (b'Content-Length', b'62322'), (b'Cache-Control', b'max-age=300'), (b'Content-Security-Policy', b"default-src 'none'; style-src 'unsafe-inline'; sandbox"), (b'Content-Type', b'text/plain; charset=utf-8'), (b'ETag', b'W/"5f0be0ed2fe5ecf561c4b4488d64c304c35b980a45012ef1d1253b4b5d438af1"'), (b'Strict-Transport-Security', b'max-age=31536000'), (b'X-Content-Type-Options', b'nosniff'), (b'X-Frame-Options', b'deny'), (b'X-XSS-Protection', b'1; mode=block'), (b'X-GitHub-Request-Id', b'415E:3865D1:30D7E6:7A896E:69753835'), (b'Content-Encoding', b'gzip'), (b'Accept-Ranges', b'bytes'), (b'Date', b'Tue, 27 Jan 2026 08:04:01 GMT'), (b'Via', b'1.1 varnish'), (b'X-Served-By', b'cache-bom-vanm7210073-BOM'), (b'X-Cache', b'HIT'), (b'X-Cache-Hits', b'43'), (b'X-Timer', b'S1769501041.125317,VS0,VE0'), (b'Vary', b'Authorization,Accept-Encoding'), (b'Access-Control-Allow-Origin', b'*'), (b'Cross-Origin-Resource-Policy', b'cross-origin'), (b'X-Fastly-Request-ID', b'3031a76198e62ea1a4ed7e391c74f166d3328a7e'), (b'Expires', b'Tue, 27 Jan 2026 08:09:01 GMT'), (b'Source-Age', b'275')])
2026-01-27 13:34:01,227 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'GET']>
2026-01-27 13:34:01,269 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-27 13:34:01,269 | DEBUG | httpcore.http11 | response_closed.started
2026-01-27 13:34:01,270 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-27 13:34:01,270 | DEBUG | httpcore.connection | close.started
2026-01-27 13:34:01,271 | DEBUG | httpcore.connection | close.complete
2026-01-27 13:34:02,114 | DEBUG | LiteLLM | [Non-Blocking] Unable to import GenericAPILogger - LiteLLM Enterprise Feature - No module named 'litellm_enterprise'
2026-01-27 13:34:02,583 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 13:34:02,583 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 13:34:02,583 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 13:34:02,589 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 13:34:02,590 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 13:34:02,590 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 13:34:02,591 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 13:34:02,591 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 13:34:02,591 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 13:34:02,592 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-27 13:34:02,592 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-27 13:34:02,592 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-27 13:34:02,624 | INFO | main | üöÄ FastAPI application starting
