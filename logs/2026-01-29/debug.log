2026-01-29 10:48:57,878 | INFO | sentence_transformers.SentenceTransformer | Use pytorch device_name: cpu
2026-01-29 10:48:57,879 | INFO | sentence_transformers.SentenceTransformer | Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2026-01-29 10:48:57,883 | DEBUG | urllib3.connectionpool | Starting new HTTPS connection (1): huggingface.co:443
2026-01-29 10:48:58,189 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-29 10:48:58,200 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-29 10:48:58,452 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-29 10:48:58,464 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-29 10:48:58,705 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-29 10:48:58,718 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-29 10:48:58,958 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md HTTP/1.1" 307 0
2026-01-29 10:48:58,969 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/README.md HTTP/1.1" 200 0
2026-01-29 10:48:59,223 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-29 10:48:59,233 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-29 10:48:59,468 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json HTTP/1.1" 307 0
2026-01-29 10:48:59,484 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/sentence_bert_config.json HTTP/1.1" 200 0
2026-01-29 10:48:59,717 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json HTTP/1.1" 404 0
2026-01-29 10:48:59,959 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1" 307 0
2026-01-29 10:48:59,971 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json HTTP/1.1" 200 0
2026-01-29 10:49:00,346 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/1.1" 307 0
2026-01-29 10:49:00,356 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json HTTP/1.1" 200 0
2026-01-29 10:49:00,634 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1" 404 64
2026-01-29 10:49:00,888 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main?recursive=True&expand=False HTTP/1.1" 200 6465
2026-01-29 10:49:01,200 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json HTTP/1.1" 307 0
2026-01-29 10:49:01,212 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json HTTP/1.1" 200 0
2026-01-29 10:49:01,479 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2 HTTP/1.1" 200 6956
2026-01-29 10:49:02,887 | DEBUG | openai.agents | Shutting down trace provider
2026-01-29 10:49:02,887 | DEBUG | openai.agents | Shutting down trace processor <agents.tracing.processors.BatchTraceProcessor object at 0x000001ADC9FAF8E0>
2026-01-29 10:51:52,450 | INFO | sentence_transformers.SentenceTransformer | Use pytorch device_name: cpu
2026-01-29 10:51:52,450 | INFO | sentence_transformers.SentenceTransformer | Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2026-01-29 10:51:52,454 | DEBUG | urllib3.connectionpool | Starting new HTTPS connection (1): huggingface.co:443
2026-01-29 10:51:52,775 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-29 10:51:52,790 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-29 10:51:53,033 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-29 10:51:53,046 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-29 10:51:53,294 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-29 10:51:53,306 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-29 10:51:53,547 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md HTTP/1.1" 307 0
2026-01-29 10:51:53,562 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/README.md HTTP/1.1" 200 0
2026-01-29 10:51:53,804 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-29 10:51:53,814 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-29 10:51:54,047 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json HTTP/1.1" 307 0
2026-01-29 10:51:54,060 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/sentence_bert_config.json HTTP/1.1" 200 0
2026-01-29 10:51:54,297 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json HTTP/1.1" 404 0
2026-01-29 10:51:54,536 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1" 307 0
2026-01-29 10:51:54,548 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json HTTP/1.1" 200 0
2026-01-29 10:51:54,861 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/1.1" 307 0
2026-01-29 10:51:54,873 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json HTTP/1.1" 200 0
2026-01-29 10:51:55,116 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1" 404 64
2026-01-29 10:51:55,377 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main?recursive=True&expand=False HTTP/1.1" 200 6465
2026-01-29 10:51:55,660 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json HTTP/1.1" 307 0
2026-01-29 10:51:55,673 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json HTTP/1.1" 200 0
2026-01-29 10:51:55,926 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2 HTTP/1.1" 200 6956
2026-01-29 10:51:56,730 | INFO | main | üöÄ FastAPI application starting
2026-01-29 10:52:58,202 | INFO | chat | üì© Chat request | user_id=3 | request_id=502a121a-de5e-47a4-a8ad-fc08ae27c587
2026-01-29 10:52:58,211 | INFO | chat | üßë User message | hello
2026-01-29 10:52:58,225 | INFO | chat | üÜï New session created | 019285e3-3fcc-4d4d-9bf1-0d5d8b80b8b6
2026-01-29 10:52:58,253 | INFO | chat | ü§ñ Stream started | session_id=019285e3-3fcc-4d4d-9bf1-0d5d8b80b8b6 | request_id=502a121a-de5e-47a4-a8ad-fc08ae27c587
2026-01-29 10:52:58,254 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-29 10:52:58,255 | INFO | orchestrator | üßë USER_INPUT | hello
2026-01-29 10:52:58,256 | INFO | orchestrator | üß© CONTEXT | user_id=3 | enable_memory=True | db=None
2026-01-29 10:52:58,257 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-29 10:52:58,259 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_4beb10296f884adeb8fb2502270e6327
2026-01-29 10:52:58,260 | DEBUG | openai.agents | Setting current trace: trace_4beb10296f884adeb8fb2502270e6327
2026-01-29 10:52:58,262 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x0000021362C368E0> with id None
2026-01-29 10:52:58,263 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-29 10:52:59,655 | DEBUG | httpcore.connection | connect_tcp.started host='raw.githubusercontent.com' port=443 local_address=None timeout=5 socket_options=None
2026-01-29 10:52:59,711 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000021363436B90>
2026-01-29 10:52:59,712 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x00000213630BF340> server_hostname='raw.githubusercontent.com' timeout=5
2026-01-29 10:52:59,744 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000021363436B60>
2026-01-29 10:52:59,744 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'GET']>
2026-01-29 10:52:59,746 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-29 10:52:59,746 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'GET']>
2026-01-29 10:52:59,747 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-29 10:52:59,747 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'GET']>
2026-01-29 10:52:59,775 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Connection', b'keep-alive'), (b'Content-Length', b'62762'), (b'Cache-Control', b'max-age=300'), (b'Content-Security-Policy', b"default-src 'none'; style-src 'unsafe-inline'; sandbox"), (b'Content-Type', b'text/plain; charset=utf-8'), (b'ETag', b'W/"3e4eac0c93a1473e433ba78825712242b23c78a4bb62bedd4c883820376ca0d7"'), (b'Strict-Transport-Security', b'max-age=31536000'), (b'X-Content-Type-Options', b'nosniff'), (b'X-Frame-Options', b'deny'), (b'X-XSS-Protection', b'1; mode=block'), (b'X-GitHub-Request-Id', b'2E00:3ECF77:4A8CC:10C08F:697AD634'), (b'Content-Encoding', b'gzip'), (b'Accept-Ranges', b'bytes'), (b'Date', b'Thu, 29 Jan 2026 05:22:59 GMT'), (b'Via', b'1.1 varnish'), (b'X-Served-By', b'cache-bom-vanm7210048-BOM'), (b'X-Cache', b'HIT'), (b'X-Cache-Hits', b'58'), (b'X-Timer', b'S1769664179.068384,VS0,VE0'), (b'Vary', b'Authorization,Accept-Encoding'), (b'Access-Control-Allow-Origin', b'*'), (b'Cross-Origin-Resource-Policy', b'cross-origin'), (b'X-Fastly-Request-ID', b'263a4d98ba60c69dd714c1885c3d5ce2bde0a1e8'), (b'Expires', b'Thu, 29 Jan 2026 05:27:59 GMT'), (b'Source-Age', b'260')])
2026-01-29 10:52:59,778 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'GET']>
2026-01-29 10:52:59,817 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-29 10:52:59,817 | DEBUG | httpcore.http11 | response_closed.started
2026-01-29 10:52:59,817 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-29 10:52:59,818 | DEBUG | httpcore.connection | close.started
2026-01-29 10:52:59,819 | DEBUG | httpcore.connection | close.complete
2026-01-29 10:53:01,625 | DEBUG | LiteLLM | [Non-Blocking] Unable to import GenericAPILogger - LiteLLM Enterprise Feature - No module named 'litellm_enterprise'
2026-01-29 10:53:03,312 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-29 10:53:03,313 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-29 10:53:03,314 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-29 10:53:03,337 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-29 10:53:03,338 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-29 10:53:03,339 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-29 10:53:03,340 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-29 10:53:03,341 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-29 10:53:03,343 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-29 10:53:03,344 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-29 10:53:03,346 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-29 10:53:03,348 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-29 10:53:03,358 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-29 10:53:03,375 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002136C440E20>
2026-01-29 10:53:03,376 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000021360F89BC0> server_hostname='api.openai.com' timeout=5.0
2026-01-29 10:53:03,391 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000021362A0EF20> with id None
2026-01-29 10:53:03,392 | DEBUG | openai.agents | Calling LLM
2026-01-29 10:53:03,392 | DEBUG | LiteLLM | 

2026-01-29 10:53:03,392 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-29 10:53:03,394 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\nYou are a content safety checker.\n\nYour task:\n- Decide whether the user\'s message is SAFE or UNSAFE.\n\nRespond ONLY in valid JSON.\n\nSAFE content includes:\n- General knowledge\n- Education\n- Casual conversation\n- Harmless personal questions\n\nUNSAFE content includes:\n- Violence, murder, terrorism\n- Illegal instructions\n- Self-harm\n- Hate speech\n- Sexual content involving minors\n\nJSON FORMAT:\n{\n  "safe": true | false,\n  "reason": "short explanation if unsafe"\n}\n\nRules:\n- If SAFE ‚Üí safe=true\n- If UNSAFE ‚Üí safe=false + reason\n- Do NOT mention policies\n', 'role': 'system'}, {'role': 'user', 'content': 'hello'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-29 10:53:03,395 | DEBUG | LiteLLM | 

2026-01-29 10:53:03,397 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000021362AFA8F0>
2026-01-29 10:53:03,397 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-29 10:53:03,398 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-29 10:53:03,398 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-29 10:53:03,400 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-29 10:53:03,400 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-29 10:53:03,401 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-29 10:53:03,403 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-29 10:53:03,404 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-29 10:53:03,431 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 10:53:03,438 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-29 10:53:03,442 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\nYou are a content safety checker.\n\nYour task:\n- Decide whether the user\'s message is SAFE or UNSAFE.\n\nRespond ONLY in valid JSON.\n\nSAFE content includes:\n- General knowledge\n- Education\n- Casual conversation\n- Harmless personal questions\n\nUNSAFE content includes:\n- Violence, murder, terrorism\n- Illegal instructions\n- Self-harm\n- Hate speech\n- Sexual content involving minors\n\nJSON FORMAT:\n{\n  "safe": true | false,\n  "reason": "short explanation if unsafe"\n}\n\nRules:\n- If SAFE ‚Üí safe=true\n- If UNSAFE ‚Üí safe=false + reason\n- Do NOT mention policies\n', 'role': 'system'}, {'role': 'user', 'content': 'hello'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-29 10:53:03,443 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-29 10:53:03,444 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-29 10:53:03,446 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-29 10:53:03,447 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:53:03,981 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:53:03,984 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'hello'}]}], 'system_instruction': {'parts': [{'text': '\nYou are a content safety checker.\n\nYour task:\n- Decide whether the user\'s message is SAFE or UNSAFE.\n\nRespond ONLY in valid JSON.\n\nSAFE content includes:\n- General knowledge\n- Education\n- Casual conversation\n- Harmless personal questions\n\nUNSAFE content includes:\n- Violence, murder, terrorism\n- Illegal instructions\n- Self-harm\n- Hate speech\n- Sexual content involving minors\n\nJSON FORMAT:\n{\n  "safe": true | false,\n  "reason": "short explanation if unsafe"\n}\n\nRules:\n- If SAFE ‚Üí safe=true\n- If UNSAFE ‚Üí safe=false + reason\n- Do NOT mention policies\n'}]}, 'generationConfig': {}}'
[0m

2026-01-29 10:53:05,186 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Thu, 29 Jan 2026 05:23:04 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_806d7425c34c0d3c9645a2aac1bcfbf0'), (b'openai-processing-ms', b'564'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'567'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=tgMr6fjPQcWqcPzNZZeodLig9EbTDcaDL8VUcGtfbTg-1769664184-1.0.1.1-uLFVlu9WmTAPPI0gRrJ73Z77eXhUwV1.fqJNru0Awkx3nRnB0ed7khTxZfroNPSaDQaeFgmNx5r1aIHndTXCv64tlDPxeSJlRSE9xWYmZxU; path=/; expires=Thu, 29-Jan-26 05:53:04 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=VG38O75a6WT9j5GdqhGnXqLjKZZP_P3AaS0dkvD8WGs-1769664184471-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c564b95df805952-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-29 10:53:05,187 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-29 10:53:05,188 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-29 10:53:05,188 | DEBUG | httpcore.http11 | response_closed.started
2026-01-29 10:53:05,189 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-29 10:53:05,189 | DEBUG | openai.agents | Exported 1 items
2026-01-29 10:53:05,809 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "```json\n{\n  \"safe\": true\n}\n```"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 149,
    "candidatesTokenCount": 14,
    "totalTokenCount": 199,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 149
      }
    ],
    "thoughtsTokenCount": 36
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "uO56aeG5OMLOpgfShaagBQ"
}



2026-01-29 10:53:05,816 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-29 10:53:05,817 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-29 10:53:05,818 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 10:53:05,819 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 10:53:05,819 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:53:05,820 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:53:05,820 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:53:05,821 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:53:05,821 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 10:53:05,821 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 10:53:05,822 | DEBUG | LiteLLM | response_cost: 0.0001697
2026-01-29 10:53:05,822 | DEBUG | LiteLLM | response_cost: 0.0001697
2026-01-29 10:53:05,823 | DEBUG | openai.agents | Received model response
2026-01-29 10:53:05,824 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 10:53:05,832 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x0000021362C78C10>>
2026-01-29 10:53:05,833 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:53:05,835 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 10:53:05,837 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-29 10:53:05,838 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-29 10:53:05,839 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 10:53:05,839 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-29 10:53:05,840 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-29 10:53:05,840 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:53:05,841 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 10:53:05,841 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-29 10:53:05,842 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 10:53:05,842 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:53:05,843 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 10:53:05,844 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 10:53:05,844 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:53:05,844 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 10:53:05,846 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-29 10:53:05,846 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0001697
2026-01-29 10:53:05,847 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 10:53:05,847 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:53:05,848 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 10:53:05,849 | DEBUG | openai.agents | Resetting current trace
2026-01-29 10:53:05,849 | INFO | orchestrator | ‚úÖ Safety passed
2026-01-29 10:53:05,849 | INFO | orchestrator | üì• Fetching semantic memory (via MemoryService)
2026-01-29 10:53:06,044 | INFO | orchestrator | üß† MEMORY_RESULT | found=False | count=0
2026-01-29 10:53:06,045 | INFO | orchestrator | üß≠ Router agent called
2026-01-29 10:53:06,045 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_5e75577161a74be69557d8dd0c371896
2026-01-29 10:53:06,046 | DEBUG | openai.agents | Setting current trace: trace_5e75577161a74be69557d8dd0c371896
2026-01-29 10:53:06,046 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000002136CA86160> with id None
2026-01-29 10:53:06,046 | DEBUG | openai.agents | Running agent RouterAgent (turn 1)
2026-01-29 10:53:06,047 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000021362C832E0> with id None
2026-01-29 10:53:06,047 | DEBUG | openai.agents | Calling LLM
2026-01-29 10:53:06,047 | DEBUG | LiteLLM | 

2026-01-29 10:53:06,047 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-29 10:53:06,048 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'hello'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-29 10:53:06,048 | DEBUG | LiteLLM | 

2026-01-29 10:53:06,049 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-29 10:53:06,049 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-29 10:53:06,050 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-29 10:53:06,051 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 10:53:06,051 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-29 10:53:06,052 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'hello'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-29 10:53:06,053 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-29 10:53:06,053 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-29 10:53:06,053 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-29 10:53:06,054 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:53:06,055 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:53:06,055 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'hello'}]}], 'system_instruction': {'parts': [{'text': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n'}]}, 'generationConfig': {}}'
[0m

2026-01-29 10:53:07,550 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\n  \"use_memory\": false,\n  \"use_tool\": false,\n  \"tool_name\": \"none\",\n  \"tool_arguments\": null,\n  \"intent\": \"none\",\n  \"memory_key\": null\n}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 486,
    "candidatesTokenCount": 56,
    "totalTokenCount": 601,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 486
      }
    ],
    "thoughtsTokenCount": 59
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "uu56aYaAKfvHg8UPwuax4AY"
}



2026-01-29 10:53:07,551 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-29 10:53:07,552 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 10:53:07,552 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-29 10:53:07,553 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:53:07,553 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 10:53:07,554 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:53:07,554 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:53:07,554 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 10:53:07,555 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:53:07,555 | DEBUG | LiteLLM | response_cost: 0.00043329999999999996
2026-01-29 10:53:07,556 | DEBUG | openai.agents | Received model response
2026-01-29 10:53:07,556 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 10:53:07,556 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x000002136C492530>>
2026-01-29 10:53:07,557 | DEBUG | LiteLLM | response_cost: 0.00043329999999999996
2026-01-29 10:53:07,557 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-29 10:53:07,558 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 10:53:07,558 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-29 10:53:07,559 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-29 10:53:07,559 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:53:07,560 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 10:53:07,560 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-29 10:53:07,560 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 10:53:07,560 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:53:07,561 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 10:53:07,563 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-29 10:53:07,565 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 10:53:07,566 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:53:07,567 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 10:53:07,567 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 10:53:07,568 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:53:07,568 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-29 10:53:07,568 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 10:53:07,569 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.00043329999999999996
2026-01-29 10:53:07,570 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 10:53:07,570 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:53:07,571 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 10:53:07,572 | DEBUG | openai.agents | Resetting current trace
2026-01-29 10:53:07,572 | INFO | orchestrator | üß≠ ROUTER_DECISION | tool=False | memory=False | intent=none
2026-01-29 10:53:07,573 | INFO | orchestrator | üß† REASONING_INPUT_SNAPSHOT
{
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful AI assistant.\n \nYou may answer ANY general knowledge question.\n \nConversation summaries and memories are ONLY to provide helpful context\n(e.g. preferences, ongoing plans).\n \nIf a user asks something unrelated to the current topic,\nanswer it normally using general knowledge.\n \nDo NOT restrict yourself to previous topics unless the user explicitly asks."
    },
    {
      "role": "user",
      "content": "hello"
    },
    {
      "role": "user",
      "content": "hello"
    }
  ],
  "safety": {
    "safe": true,
    "reason": null,
    "message": "OK"
  },
  "memory_action": {},
  "memory_data": [],
  "memory_found": false,
  "tool_context": {}
}
2026-01-29 10:53:07,574 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_0f0e1c0ba16648d6b4342876038ad672
2026-01-29 10:53:07,574 | DEBUG | openai.agents | Setting current trace: trace_0f0e1c0ba16648d6b4342876038ad672
2026-01-29 10:53:07,574 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000002136C4A72E0> with id None
2026-01-29 10:53:07,575 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000002136C794A60> with id None
2026-01-29 10:53:07,576 | DEBUG | openai.agents | Calling LLM
2026-01-29 10:53:07,576 | DEBUG | LiteLLM | 

2026-01-29 10:53:07,576 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-29 10:53:07,576 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "hello"}, {"role": "user", "content": "hello"}], "safety": {"safe": true, "reason": null, "message": "OK"}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=True, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-29 10:53:07,577 | DEBUG | LiteLLM | 

2026-01-29 10:53:07,578 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-29 10:53:07,579 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-29 10:53:07,580 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-29 10:53:07,584 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 10:53:07,584 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-29 10:53:07,585 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "hello"}, {"role": "user", "content": "hello"}], "safety": {"safe": true, "reason": null, "message": "OK"}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-29 10:53:07,585 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': True}
2026-01-29 10:53:07,586 | DEBUG | LiteLLM | Final returned optional params: {'stream': True}
2026-01-29 10:53:07,586 | DEBUG | LiteLLM | self.optional_params: {'stream': True}
2026-01-29 10:53:07,586 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:53:07,587 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:53:07,588 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?key=*****=sse \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "hello"}, {"role": "user", "content": "hello"}], "safety": {"safe": true, "reason": null, "message": "OK"}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}]}], 'system_instruction': {'parts': [{'text': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.'}]}, 'generationConfig': {}}'
[0m

2026-01-29 10:53:07,589 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 10:53:07,592 | DEBUG | LiteLLM | messages in token_counter: None, text in token_counter: 
2026-01-29 10:53:07,593 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:53:07,593 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:53:07,594 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 10:53:07,595 | DEBUG | LiteLLM | response_cost: 0.0
2026-01-29 10:53:07,635 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-29 10:53:07,636 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-29 10:53:07,637 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-29 10:53:08,755 | DEBUG | LiteLLM | RAW RESPONSE:
first stream response received


2026-01-29 10:53:08,756 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': 'Hello! How can I help you today?'}], 'role': 'model'}, 'finishReason': 'STOP', 'index': 0}], 'usageMetadata': {'promptTokenCount': 280, 'candidatesTokenCount': 9, 'totalTokenCount': 311, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 280}], 'thoughtsTokenCount': 22}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'u-56aYqELZemg8UPl5aP2Aw'}
2026-01-29 10:53:08,764 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='u-56aYqELZemg8UPl5aP2Aw', created=1769664188, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='Hello! How can I help you today?', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=31, prompt_tokens=280, total_tokens=311, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=22, rejected_prediction_tokens=None, text_tokens=9, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=280, image_tokens=None)))
2026-01-29 10:53:08,767 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content='Hello! How can I help you today?', role='assistant', function_call=None, tool_calls=None, audio=None)
2026-01-29 10:53:08,768 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='u-56aYqELZemg8UPl5aP2Aw', created=1769664188, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Hello! How can I help you today?', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=31, prompt_tokens=280, total_tokens=311, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=22, rejected_prediction_tokens=None, text_tokens=9, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=280, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-29 10:53:08,798 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=False
2026-01-29 10:53:08,800 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=False
2026-01-29 10:53:08,800 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-29 10:53:08,801 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-29 10:53:08,802 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 10:53:08,802 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 10:53:08,802 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:53:08,803 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:53:08,803 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:53:08,804 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:53:08,804 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 10:53:08,804 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 10:53:08,805 | DEBUG | LiteLLM | response_cost: 0.0001615
2026-01-29 10:53:08,805 | DEBUG | LiteLLM | response_cost: 0.0001615
2026-01-29 10:53:08,806 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 10:53:08,807 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0001615
2026-01-29 10:53:08,807 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:53:08,808 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 10:53:08,808 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 10:53:08,808 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:53:08,809 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 10:53:08,810 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-29 10:53:08,811 | DEBUG | openai.agents | Resetting current trace
2026-01-29 10:53:08,818 | INFO | session_summary | üß† SUMMARY_START | session_id=019285e3-3fcc-4d4d-9bf1-0d5d8b80b8b6
2026-01-29 10:53:08,824 | INFO | session_summary | üìä Unsummarized messages = 2
2026-01-29 10:53:08,825 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-29 10:53:08,825 | INFO | chat | ‚úÖ Stream complete | tokens=1 | time=10.62s
2026-01-29 10:53:10,329 | DEBUG | httpcore.connection | close.started
2026-01-29 10:53:10,330 | DEBUG | httpcore.connection | close.complete
2026-01-29 10:53:10,331 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-29 10:53:10,358 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002136C4922F0>
2026-01-29 10:53:10,358 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000021360F89BC0> server_hostname='api.openai.com' timeout=5.0
2026-01-29 10:53:10,371 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002136C493190>
2026-01-29 10:53:10,372 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-29 10:53:10,373 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-29 10:53:10,373 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-29 10:53:10,373 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-29 10:53:10,374 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-29 10:53:12,660 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Thu, 29 Jan 2026 05:23:11 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_15d1b05327da73d20cb33e02aad85c62'), (b'openai-processing-ms', b'1978'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'1981'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c564bc16b5b87ce-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-29 10:53:12,661 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-29 10:53:12,661 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-29 10:53:12,662 | DEBUG | httpcore.http11 | response_closed.started
2026-01-29 10:53:12,662 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-29 10:53:12,663 | DEBUG | openai.agents | Exported 8 items
2026-01-29 10:53:40,231 | INFO | chat | üì© Chat request | user_id=3 | request_id=8e12da59-1182-41bd-a01b-d802ae5ce81e
2026-01-29 10:53:40,232 | INFO | chat | üßë User message | i want to get high fever
2026-01-29 10:53:40,247 | INFO | chat | ü§ñ Stream started | session_id=019285e3-3fcc-4d4d-9bf1-0d5d8b80b8b6 | request_id=8e12da59-1182-41bd-a01b-d802ae5ce81e
2026-01-29 10:53:40,248 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-29 10:53:40,249 | INFO | orchestrator | üßë USER_INPUT | i want to get high fever
2026-01-29 10:53:40,251 | INFO | orchestrator | üß© CONTEXT | user_id=3 | enable_memory=True | db=None
2026-01-29 10:53:40,252 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-29 10:53:40,254 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_d66a05c7a4274f70bd5a50d1ed89c3bf
2026-01-29 10:53:40,254 | DEBUG | openai.agents | Setting current trace: trace_d66a05c7a4274f70bd5a50d1ed89c3bf
2026-01-29 10:53:40,255 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x0000021362C36E80> with id None
2026-01-29 10:53:40,256 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-29 10:53:40,258 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000021362B65600> with id None
2026-01-29 10:53:40,259 | DEBUG | openai.agents | Calling LLM
2026-01-29 10:53:40,259 | DEBUG | LiteLLM | 

2026-01-29 10:53:40,260 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-29 10:53:40,261 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\nYou are a content safety checker.\n\nYour task:\n- Decide whether the user\'s message is SAFE or UNSAFE.\n\nRespond ONLY in valid JSON.\n\nSAFE content includes:\n- General knowledge\n- Education\n- Casual conversation\n- Harmless personal questions\n\nUNSAFE content includes:\n- Violence, murder, terrorism\n- Illegal instructions\n- Self-harm\n- Hate speech\n- Sexual content involving minors\n\nJSON FORMAT:\n{\n  "safe": true | false,\n  "reason": "short explanation if unsafe"\n}\n\nRules:\n- If SAFE ‚Üí safe=true\n- If UNSAFE ‚Üí safe=false + reason\n- Do NOT mention policies\n', 'role': 'system'}, {'role': 'user', 'content': 'i want to get high fever'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-29 10:53:40,262 | DEBUG | LiteLLM | 

2026-01-29 10:53:40,263 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-29 10:53:40,263 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-29 10:53:40,264 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-29 10:53:40,267 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 10:53:40,267 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-29 10:53:40,269 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\nYou are a content safety checker.\n\nYour task:\n- Decide whether the user\'s message is SAFE or UNSAFE.\n\nRespond ONLY in valid JSON.\n\nSAFE content includes:\n- General knowledge\n- Education\n- Casual conversation\n- Harmless personal questions\n\nUNSAFE content includes:\n- Violence, murder, terrorism\n- Illegal instructions\n- Self-harm\n- Hate speech\n- Sexual content involving minors\n\nJSON FORMAT:\n{\n  "safe": true | false,\n  "reason": "short explanation if unsafe"\n}\n\nRules:\n- If SAFE ‚Üí safe=true\n- If UNSAFE ‚Üí safe=false + reason\n- Do NOT mention policies\n', 'role': 'system'}, {'role': 'user', 'content': 'i want to get high fever'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-29 10:53:40,271 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-29 10:53:40,272 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-29 10:53:40,273 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-29 10:53:40,273 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:53:40,275 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:53:40,276 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'i want to get high fever'}]}], 'system_instruction': {'parts': [{'text': '\nYou are a content safety checker.\n\nYour task:\n- Decide whether the user\'s message is SAFE or UNSAFE.\n\nRespond ONLY in valid JSON.\n\nSAFE content includes:\n- General knowledge\n- Education\n- Casual conversation\n- Harmless personal questions\n\nUNSAFE content includes:\n- Violence, murder, terrorism\n- Illegal instructions\n- Self-harm\n- Hate speech\n- Sexual content involving minors\n\nJSON FORMAT:\n{\n  "safe": true | false,\n  "reason": "short explanation if unsafe"\n}\n\nRules:\n- If SAFE ‚Üí safe=true\n- If UNSAFE ‚Üí safe=false + reason\n- Do NOT mention policies\n'}]}, 'generationConfig': {}}'
[0m

2026-01-29 10:53:41,713 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "```json\n{\n  \"safe\": false,\n  \"reason\": \"Requesting to get a high fever can be harmful to one's health.\"\n}\n```"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 154,
    "candidatesTokenCount": 37,
    "totalTokenCount": 256,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 154
      }
    ],
    "thoughtsTokenCount": 65
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "3O56adHoM_74g8UPzP-AiA4"
}



2026-01-29 10:53:41,716 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-29 10:53:41,717 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 10:53:41,718 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-29 10:53:41,719 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:53:41,720 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 10:53:41,721 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:53:41,722 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:53:41,724 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 10:53:41,726 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:53:41,728 | DEBUG | LiteLLM | response_cost: 0.0003012
2026-01-29 10:53:41,730 | DEBUG | openai.agents | Received model response
2026-01-29 10:53:41,729 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 10:53:41,732 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x000002136CB90F70>>
2026-01-29 10:53:41,733 | DEBUG | LiteLLM | response_cost: 0.0003012
2026-01-29 10:53:41,734 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-29 10:53:41,735 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 10:53:41,737 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-29 10:53:41,736 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-29 10:53:41,737 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:53:41,739 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-29 10:53:41,741 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 10:53:41,741 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 10:53:41,744 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 10:53:41,745 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:53:41,747 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-29 10:53:41,749 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:53:41,750 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 10:53:41,752 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 10:53:41,753 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 10:53:41,755 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:53:41,756 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-29 10:53:41,757 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 10:53:41,758 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0003012
2026-01-29 10:53:41,761 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 10:53:41,762 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:53:41,764 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 10:53:41,767 | DEBUG | openai.agents | Resetting current trace
2026-01-29 10:53:41,769 | WARNING | orchestrator | üö´ Safety blocked request
2026-01-29 10:53:41,776 | INFO | session_summary | üß† SUMMARY_START | session_id=019285e3-3fcc-4d4d-9bf1-0d5d8b80b8b6
2026-01-29 10:53:41,787 | INFO | session_summary | üìä Unsummarized messages = 4
2026-01-29 10:53:41,789 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-29 10:53:41,790 | INFO | chat | ‚úÖ Stream complete | tokens=0 | time=1.56s
2026-01-29 10:53:43,420 | DEBUG | httpcore.connection | close.started
2026-01-29 10:53:43,421 | DEBUG | httpcore.connection | close.complete
2026-01-29 10:53:43,422 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-29 10:53:43,433 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002136CB910C0>
2026-01-29 10:53:43,433 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000021360F89BC0> server_hostname='api.openai.com' timeout=5.0
2026-01-29 10:53:43,445 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002136CB91B70>
2026-01-29 10:53:43,446 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-29 10:53:43,447 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-29 10:53:43,447 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-29 10:53:43,447 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-29 10:53:43,448 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-29 10:53:45,197 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Thu, 29 Jan 2026 05:23:44 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_6d4c196b22b8497d3c8c117b71bd2855'), (b'openai-processing-ms', b'582'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'585'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c564c902ca18e78-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-29 10:53:45,197 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-29 10:53:45,197 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-29 10:53:45,198 | DEBUG | httpcore.http11 | response_closed.started
2026-01-29 10:53:45,198 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-29 10:53:45,198 | DEBUG | openai.agents | Exported 3 items
2026-01-29 10:55:50,293 | INFO | chat | üì© Chat request | user_id=3 | request_id=c890e467-6745-46de-9a78-5d5a0553a494
2026-01-29 10:55:50,294 | INFO | chat | üßë User message | helllo
2026-01-29 10:55:50,319 | INFO | chat | ü§ñ Stream started | session_id=019285e3-3fcc-4d4d-9bf1-0d5d8b80b8b6 | request_id=c890e467-6745-46de-9a78-5d5a0553a494
2026-01-29 10:55:50,321 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-29 10:55:50,322 | INFO | orchestrator | üßë USER_INPUT | helllo
2026-01-29 10:55:50,323 | INFO | orchestrator | üß© CONTEXT | user_id=3 | enable_memory=True | db=None
2026-01-29 10:55:50,324 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-29 10:55:50,326 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_84a943d96e9840d2a78167d2e87b56e1
2026-01-29 10:55:50,326 | DEBUG | openai.agents | Setting current trace: trace_84a943d96e9840d2a78167d2e87b56e1
2026-01-29 10:55:50,327 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000002136C7C2840> with id None
2026-01-29 10:55:50,327 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-29 10:55:50,328 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000002136C794040> with id None
2026-01-29 10:55:50,328 | DEBUG | openai.agents | Calling LLM
2026-01-29 10:55:50,329 | DEBUG | LiteLLM | 

2026-01-29 10:55:50,329 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-29 10:55:50,330 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\nYou are a content safety checker.\n\nYour task:\n- Decide whether the user\'s message is SAFE or UNSAFE.\n\nRespond ONLY in valid JSON.\n\nSAFE content includes:\n- General knowledge\n- Education\n- Casual conversation\n- Harmless personal questions\n\nUNSAFE content includes:\n- Violence, murder, terrorism\n- Illegal instructions\n- Self-harm\n- Hate speech\n- Sexual content involving minors\n\nJSON FORMAT:\n{\n  "safe": true | false,\n  "reason": "short explanation if unsafe"\n}\n\nRules:\n- If SAFE ‚Üí safe=true\n- If UNSAFE ‚Üí safe=false + reason\n- Do NOT mention policies\n', 'role': 'system'}, {'role': 'user', 'content': 'helllo'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-29 10:55:50,331 | DEBUG | LiteLLM | 

2026-01-29 10:55:50,332 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-29 10:55:50,333 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-29 10:55:50,333 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-29 10:55:50,335 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 10:55:50,336 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-29 10:55:50,337 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\nYou are a content safety checker.\n\nYour task:\n- Decide whether the user\'s message is SAFE or UNSAFE.\n\nRespond ONLY in valid JSON.\n\nSAFE content includes:\n- General knowledge\n- Education\n- Casual conversation\n- Harmless personal questions\n\nUNSAFE content includes:\n- Violence, murder, terrorism\n- Illegal instructions\n- Self-harm\n- Hate speech\n- Sexual content involving minors\n\nJSON FORMAT:\n{\n  "safe": true | false,\n  "reason": "short explanation if unsafe"\n}\n\nRules:\n- If SAFE ‚Üí safe=true\n- If UNSAFE ‚Üí safe=false + reason\n- Do NOT mention policies\n', 'role': 'system'}, {'role': 'user', 'content': 'helllo'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-29 10:55:50,339 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-29 10:55:50,339 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-29 10:55:50,340 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-29 10:55:50,341 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:55:50,343 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:55:50,344 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'helllo'}]}], 'system_instruction': {'parts': [{'text': '\nYou are a content safety checker.\n\nYour task:\n- Decide whether the user\'s message is SAFE or UNSAFE.\n\nRespond ONLY in valid JSON.\n\nSAFE content includes:\n- General knowledge\n- Education\n- Casual conversation\n- Harmless personal questions\n\nUNSAFE content includes:\n- Violence, murder, terrorism\n- Illegal instructions\n- Self-harm\n- Hate speech\n- Sexual content involving minors\n\nJSON FORMAT:\n{\n  "safe": true | false,\n  "reason": "short explanation if unsafe"\n}\n\nRules:\n- If SAFE ‚Üí safe=true\n- If UNSAFE ‚Üí safe=false + reason\n- Do NOT mention policies\n'}]}, 'generationConfig': {}}'
[0m

2026-01-29 10:55:51,805 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "```json\n{\n  \"safe\": true\n}\n```"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 150,
    "candidatesTokenCount": 14,
    "totalTokenCount": 194,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 150
      }
    ],
    "thoughtsTokenCount": 30
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "Xu96aZvXN8LOpgfMhaaYBQ"
}



2026-01-29 10:55:51,807 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-29 10:55:51,809 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 10:55:51,809 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-29 10:55:51,810 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:55:51,811 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 10:55:51,811 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:55:51,812 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:55:51,813 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 10:55:51,813 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:55:51,814 | DEBUG | LiteLLM | response_cost: 0.00015500000000000003
2026-01-29 10:55:51,816 | DEBUG | openai.agents | Received model response
2026-01-29 10:55:51,815 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 10:55:51,817 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x0000021362AA8EB0>>
2026-01-29 10:55:51,817 | DEBUG | LiteLLM | response_cost: 0.00015500000000000003
2026-01-29 10:55:51,818 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-29 10:55:51,819 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 10:55:51,820 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-29 10:55:51,820 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-29 10:55:51,821 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:55:51,821 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-29 10:55:51,823 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 10:55:51,824 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 10:55:51,825 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 10:55:51,826 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:55:51,826 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-29 10:55:51,827 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:55:51,828 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 10:55:51,830 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 10:55:51,830 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 10:55:51,832 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:55:51,833 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-29 10:55:51,834 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 10:55:51,835 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.00015500000000000003
2026-01-29 10:55:51,836 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 10:55:51,837 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:55:51,838 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 10:55:51,840 | DEBUG | openai.agents | Resetting current trace
2026-01-29 10:55:51,840 | INFO | orchestrator | ‚úÖ Safety passed
2026-01-29 10:55:51,841 | INFO | orchestrator | üì• Fetching semantic memory (via MemoryService)
2026-01-29 10:55:51,876 | INFO | orchestrator | üß† MEMORY_RESULT | found=False | count=0
2026-01-29 10:55:51,878 | INFO | orchestrator | üß≠ Router agent called
2026-01-29 10:55:51,879 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_317eb285e01b4d2e9a2be26fbea82f8a
2026-01-29 10:55:51,880 | DEBUG | openai.agents | Setting current trace: trace_317eb285e01b4d2e9a2be26fbea82f8a
2026-01-29 10:55:51,880 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x0000021362C1F290> with id None
2026-01-29 10:55:51,880 | DEBUG | openai.agents | Running agent RouterAgent (turn 1)
2026-01-29 10:55:51,881 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000002136C7956C0> with id None
2026-01-29 10:55:51,882 | DEBUG | openai.agents | Calling LLM
2026-01-29 10:55:51,882 | DEBUG | LiteLLM | 

2026-01-29 10:55:51,883 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-29 10:55:51,883 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'helllo'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-29 10:55:51,884 | DEBUG | LiteLLM | 

2026-01-29 10:55:51,885 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-29 10:55:51,886 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-29 10:55:51,886 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-29 10:55:51,888 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 10:55:51,889 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-29 10:55:51,892 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'helllo'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-29 10:55:51,893 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-29 10:55:51,894 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-29 10:55:51,895 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-29 10:55:51,896 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:55:51,897 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:55:51,898 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'helllo'}]}], 'system_instruction': {'parts': [{'text': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n'}]}, 'generationConfig': {}}'
[0m

2026-01-29 10:55:53,447 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\"use_memory\": false, \"use_tool\": false, \"tool_name\": \"none\", \"tool_arguments\": null, \"intent\": \"none\", \"memory_key\": null}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 487,
    "candidatesTokenCount": 42,
    "totalTokenCount": 605,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 487
      }
    ],
    "thoughtsTokenCount": 76
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "YO96ac3JH-eNg8UP-PaiIA"
}



2026-01-29 10:55:53,449 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-29 10:55:53,452 | DEBUG | httpcore.connection | close.started
2026-01-29 10:55:53,450 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 10:55:53,451 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-29 10:55:53,454 | DEBUG | httpcore.connection | close.complete
2026-01-29 10:55:53,455 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:55:53,455 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 10:55:53,456 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-29 10:55:53,457 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:55:53,457 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:55:53,458 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 10:55:53,459 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:55:53,460 | DEBUG | LiteLLM | response_cost: 0.00044110000000000004
2026-01-29 10:55:53,461 | DEBUG | openai.agents | Received model response
2026-01-29 10:55:53,461 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 10:55:53,462 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x000002136062EDA0>>
2026-01-29 10:55:53,463 | DEBUG | LiteLLM | response_cost: 0.00044110000000000004
2026-01-29 10:55:53,464 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-29 10:55:53,466 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 10:55:53,467 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-29 10:55:53,467 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-29 10:55:53,468 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:55:53,470 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-29 10:55:53,471 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 10:55:53,472 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 10:55:53,473 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 10:55:53,474 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:55:53,474 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002136D19BD60>
2026-01-29 10:55:53,475 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-29 10:55:53,476 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:55:53,476 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 10:55:53,479 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000021360F89BC0> server_hostname='api.openai.com' timeout=5.0
2026-01-29 10:55:53,480 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 10:55:53,481 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 10:55:53,483 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:55:53,483 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-29 10:55:53,484 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 10:55:53,485 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.00044110000000000004
2026-01-29 10:55:53,486 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 10:55:53,487 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:55:53,488 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 10:55:53,489 | DEBUG | openai.agents | Resetting current trace
2026-01-29 10:55:53,490 | INFO | orchestrator | üß≠ ROUTER_DECISION | tool=False | memory=False | intent=none
2026-01-29 10:55:53,491 | INFO | orchestrator | üß† REASONING_INPUT_SNAPSHOT
{
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful AI assistant.\n \nYou may answer ANY general knowledge question.\n \nConversation summaries and memories are ONLY to provide helpful context\n(e.g. preferences, ongoing plans).\n \nIf a user asks something unrelated to the current topic,\nanswer it normally using general knowledge.\n \nDo NOT restrict yourself to previous topics unless the user explicitly asks."
    },
    {
      "role": "user",
      "content": "hello"
    },
    {
      "role": "assistant",
      "content": "Hello! How can I help you today?"
    },
    {
      "role": "user",
      "content": "i want to get high fever"
    },
    {
      "role": "assistant",
      "content": ""
    },
    {
      "role": "user",
      "content": "helllo"
    },
    {
      "role": "user",
      "content": "helllo"
    }
  ],
  "safety": {
    "safe": true,
    "reason": null,
    "message": "OK"
  },
  "memory_action": {},
  "memory_data": [],
  "memory_found": false,
  "tool_context": {}
}
2026-01-29 10:55:53,493 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_efcac955df4040c7a60d77880b58c18b
2026-01-29 10:55:53,494 | DEBUG | openai.agents | Setting current trace: trace_efcac955df4040c7a60d77880b58c18b
2026-01-29 10:55:53,495 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x0000021362BF6700> with id None
2026-01-29 10:55:53,496 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000002136C797340> with id None
2026-01-29 10:55:53,497 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002136D19BD30>
2026-01-29 10:55:53,497 | DEBUG | openai.agents | Calling LLM
2026-01-29 10:55:53,498 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-29 10:55:53,498 | DEBUG | LiteLLM | 

2026-01-29 10:55:53,499 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-29 10:55:53,500 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-29 10:55:53,500 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-29 10:55:53,501 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "hello"}, {"role": "assistant", "content": "Hello! How can I help you today?"}, {"role": "user", "content": "i want to get high fever"}, {"role": "assistant", "content": ""}, {"role": "user", "content": "helllo"}, {"role": "user", "content": "helllo"}], "safety": {"safe": true, "reason": null, "message": "OK"}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=True, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-29 10:55:53,501 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-29 10:55:53,502 | DEBUG | LiteLLM | 

2026-01-29 10:55:53,502 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-29 10:55:53,503 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-29 10:55:53,504 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-29 10:55:53,505 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-29 10:55:53,507 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 10:55:53,507 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-29 10:55:53,508 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "hello"}, {"role": "assistant", "content": "Hello! How can I help you today?"}, {"role": "user", "content": "i want to get high fever"}, {"role": "assistant", "content": ""}, {"role": "user", "content": "helllo"}, {"role": "user", "content": "helllo"}], "safety": {"safe": true, "reason": null, "message": "OK"}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-29 10:55:53,510 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': True}
2026-01-29 10:55:53,511 | DEBUG | LiteLLM | Final returned optional params: {'stream': True}
2026-01-29 10:55:53,512 | DEBUG | LiteLLM | self.optional_params: {'stream': True}
2026-01-29 10:55:53,512 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:55:53,514 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:55:53,515 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?key=*****=sse \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "hello"}, {"role": "assistant", "content": "Hello! How can I help you today?"}, {"role": "user", "content": "i want to get high fever"}, {"role": "assistant", "content": ""}, {"role": "user", "content": "helllo"}, {"role": "user", "content": "helllo"}], "safety": {"safe": true, "reason": null, "message": "OK"}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}]}], 'system_instruction': {'parts': [{'text': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.'}]}, 'generationConfig': {}}'
[0m

2026-01-29 10:55:53,516 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 10:55:53,517 | DEBUG | LiteLLM | messages in token_counter: None, text in token_counter: 
2026-01-29 10:55:53,518 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:55:53,519 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:55:53,520 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 10:55:53,520 | DEBUG | LiteLLM | response_cost: 0.0
2026-01-29 10:55:54,733 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Thu, 29 Jan 2026 05:25:54 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_a7320b38e070df3bdb4dbc1e7d192003'), (b'openai-processing-ms', b'85'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'88'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c564fbcfb819cd7-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-29 10:55:54,734 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-29 10:55:54,735 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-29 10:55:54,736 | DEBUG | httpcore.http11 | response_closed.started
2026-01-29 10:55:54,736 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-29 10:55:54,737 | DEBUG | openai.agents | Exported 4 items
2026-01-29 10:55:54,741 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-29 10:55:54,742 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-29 10:55:54,743 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-29 10:55:54,743 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-29 10:55:54,744 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-29 10:55:54,870 | DEBUG | LiteLLM | RAW RESPONSE:
first stream response received


2026-01-29 10:55:54,873 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': 'Hello! How can I help you?'}], 'role': 'model'}, 'finishReason': 'STOP', 'index': 0}], 'usageMetadata': {'promptTokenCount': 342, 'candidatesTokenCount': 8, 'totalTokenCount': 420, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 342}], 'thoughtsTokenCount': 70}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'Ye96aafyJJ7ZjuMP4siCkA0'}
2026-01-29 10:55:54,876 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='Ye96aafyJJ7ZjuMP4siCkA0', created=1769664354, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='Hello! How can I help you?', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=78, prompt_tokens=342, total_tokens=420, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=70, rejected_prediction_tokens=None, text_tokens=8, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=342, image_tokens=None)))
2026-01-29 10:55:54,878 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content='Hello! How can I help you?', role='assistant', function_call=None, tool_calls=None, audio=None)
2026-01-29 10:55:54,879 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='Ye96aafyJJ7ZjuMP4siCkA0', created=1769664354, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Hello! How can I help you?', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=78, prompt_tokens=342, total_tokens=420, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=70, rejected_prediction_tokens=None, text_tokens=8, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=342, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-29 10:55:54,884 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=False
2026-01-29 10:55:54,885 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=False
2026-01-29 10:55:54,886 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-29 10:55:54,887 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-29 10:55:54,887 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 10:55:54,888 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 10:55:54,888 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:55:54,890 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:55:54,890 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:55:54,892 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:55:54,894 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 10:55:54,895 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 10:55:54,897 | DEBUG | LiteLLM | response_cost: 0.0002976
2026-01-29 10:55:54,897 | DEBUG | LiteLLM | response_cost: 0.0002976
2026-01-29 10:55:54,898 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 10:55:54,899 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0002976
2026-01-29 10:55:54,900 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:55:54,901 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 10:55:54,901 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 10:55:54,902 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:55:54,904 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 10:55:54,906 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-29 10:55:54,909 | DEBUG | openai.agents | Resetting current trace
2026-01-29 10:55:54,913 | INFO | session_summary | üß† SUMMARY_START | session_id=019285e3-3fcc-4d4d-9bf1-0d5d8b80b8b6
2026-01-29 10:55:54,917 | INFO | session_summary | üìä Unsummarized messages = 6
2026-01-29 10:55:54,918 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-29 10:55:54,919 | INFO | chat | ‚úÖ Stream complete | tokens=1 | time=4.63s
2026-01-29 10:55:55,937 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Thu, 29 Jan 2026 05:25:55 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_4a383ce085438580d4330b91e84e000f'), (b'openai-processing-ms', b'317'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'320'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c564fc4bf469cd7-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-29 10:55:55,938 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-29 10:55:55,939 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-29 10:55:55,939 | DEBUG | httpcore.http11 | response_closed.started
2026-01-29 10:55:55,939 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-29 10:55:55,939 | DEBUG | openai.agents | Exported 3 items
2026-01-29 10:55:55,940 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-29 10:55:55,940 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-29 10:55:55,940 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-29 10:55:55,941 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-29 10:55:55,941 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-29 10:55:56,366 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Thu, 29 Jan 2026 05:25:55 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_56a630b88b027a3b00d24761984477f4'), (b'openai-processing-ms', b'110'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'113'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c564fcc397d9cd7-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-29 10:55:56,368 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-29 10:55:56,368 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-29 10:55:56,369 | DEBUG | httpcore.http11 | response_closed.started
2026-01-29 10:55:56,370 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-29 10:55:56,371 | DEBUG | openai.agents | Exported 2 items
2026-01-29 10:58:56,405 | DEBUG | LiteLLM | LoggingWorker cancelled during shutdown
2026-01-29 10:58:56,407 | DEBUG | LiteLLM | [LoggingWorker] atexit: Queue is empty
2026-01-29 10:58:56,408 | DEBUG | asyncio | Using proactor: IocpProactor
2026-01-29 10:58:56,408 | DEBUG | openai.agents | Shutting down trace provider
2026-01-29 10:58:56,409 | DEBUG | openai.agents | Shutting down trace processor <agents.tracing.processors.BatchTraceProcessor object at 0x0000021360FB39A0>
2026-01-29 10:59:08,529 | INFO | sentence_transformers.SentenceTransformer | Use pytorch device_name: cpu
2026-01-29 10:59:08,530 | INFO | sentence_transformers.SentenceTransformer | Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2026-01-29 10:59:08,534 | DEBUG | urllib3.connectionpool | Starting new HTTPS connection (1): huggingface.co:443
2026-01-29 10:59:08,814 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-29 10:59:08,824 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-29 10:59:09,063 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-29 10:59:09,075 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-29 10:59:09,314 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-29 10:59:09,323 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-29 10:59:10,001 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md HTTP/1.1" 307 0
2026-01-29 10:59:10,015 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/README.md HTTP/1.1" 200 0
2026-01-29 10:59:10,263 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-29 10:59:10,273 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-29 10:59:10,518 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json HTTP/1.1" 307 0
2026-01-29 10:59:10,529 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/sentence_bert_config.json HTTP/1.1" 200 0
2026-01-29 10:59:10,778 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json HTTP/1.1" 404 0
2026-01-29 10:59:11,008 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1" 307 0
2026-01-29 10:59:11,016 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json HTTP/1.1" 200 0
2026-01-29 10:59:11,306 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/1.1" 307 0
2026-01-29 10:59:11,316 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json HTTP/1.1" 200 0
2026-01-29 10:59:11,571 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1" 404 64
2026-01-29 10:59:11,832 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main?recursive=True&expand=False HTTP/1.1" 200 6465
2026-01-29 10:59:12,142 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json HTTP/1.1" 307 0
2026-01-29 10:59:12,152 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json HTTP/1.1" 200 0
2026-01-29 10:59:12,444 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2 HTTP/1.1" 200 6956
2026-01-29 10:59:13,294 | INFO | main | üöÄ FastAPI application starting
2026-01-29 10:59:22,247 | INFO | chat | üì© Chat request | user_id=3 | request_id=b6c31951-9980-40ae-8d04-669b5c9cb9d5
2026-01-29 10:59:22,302 | INFO | chat | üßë User message | how can i die?
2026-01-29 10:59:22,340 | INFO | chat | ü§ñ Stream started | session_id=019285e3-3fcc-4d4d-9bf1-0d5d8b80b8b6 | request_id=b6c31951-9980-40ae-8d04-669b5c9cb9d5
2026-01-29 10:59:22,341 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-29 10:59:22,343 | INFO | orchestrator | üßë USER_INPUT | how can i die?
2026-01-29 10:59:22,344 | INFO | orchestrator | üß© CONTEXT | user_id=3 | enable_memory=True | db=None
2026-01-29 10:59:22,345 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-29 10:59:22,346 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_f4658cdb0b3a43daa21c443af61dfa99
2026-01-29 10:59:22,347 | DEBUG | openai.agents | Setting current trace: trace_f4658cdb0b3a43daa21c443af61dfa99
2026-01-29 10:59:22,349 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x00000220146ECF40> with id None
2026-01-29 10:59:22,349 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-29 10:59:23,401 | DEBUG | httpcore.connection | connect_tcp.started host='raw.githubusercontent.com' port=443 local_address=None timeout=5 socket_options=None
2026-01-29 10:59:23,459 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000022014F435B0>
2026-01-29 10:59:23,460 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000022014F52AC0> server_hostname='raw.githubusercontent.com' timeout=5
2026-01-29 10:59:23,493 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000022014F43580>
2026-01-29 10:59:23,494 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'GET']>
2026-01-29 10:59:23,494 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-29 10:59:23,495 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'GET']>
2026-01-29 10:59:23,495 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-29 10:59:23,496 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'GET']>
2026-01-29 10:59:23,525 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Connection', b'keep-alive'), (b'Content-Length', b'62762'), (b'Cache-Control', b'max-age=300'), (b'Content-Security-Policy', b"default-src 'none'; style-src 'unsafe-inline'; sandbox"), (b'Content-Type', b'text/plain; charset=utf-8'), (b'ETag', b'W/"3e4eac0c93a1473e433ba78825712242b23c78a4bb62bedd4c883820376ca0d7"'), (b'Strict-Transport-Security', b'max-age=31536000'), (b'X-Content-Type-Options', b'nosniff'), (b'X-Frame-Options', b'deny'), (b'X-XSS-Protection', b'1; mode=block'), (b'X-GitHub-Request-Id', b'2E00:3ECF77:4A8CC:10C08F:697AD634'), (b'Content-Encoding', b'gzip'), (b'Accept-Ranges', b'bytes'), (b'Date', b'Thu, 29 Jan 2026 05:29:22 GMT'), (b'Via', b'1.1 varnish'), (b'X-Served-By', b'cache-bom-vanm7210066-BOM'), (b'X-Cache', b'HIT'), (b'X-Cache-Hits', b'12'), (b'X-Timer', b'S1769664563.817461,VS0,VE0'), (b'Vary', b'Authorization,Accept-Encoding'), (b'Access-Control-Allow-Origin', b'*'), (b'Cross-Origin-Resource-Policy', b'cross-origin'), (b'X-Fastly-Request-ID', b'be856df8ad73071166b5443f818372b07570dd4a'), (b'Expires', b'Thu, 29 Jan 2026 05:34:22 GMT'), (b'Source-Age', b'44')])
2026-01-29 10:59:23,528 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'GET']>
2026-01-29 10:59:23,566 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-29 10:59:23,566 | DEBUG | httpcore.http11 | response_closed.started
2026-01-29 10:59:23,567 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-29 10:59:23,568 | DEBUG | httpcore.connection | close.started
2026-01-29 10:59:23,569 | DEBUG | httpcore.connection | close.complete
2026-01-29 10:59:24,815 | DEBUG | LiteLLM | [Non-Blocking] Unable to import GenericAPILogger - LiteLLM Enterprise Feature - No module named 'litellm_enterprise'
2026-01-29 10:59:25,350 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-29 10:59:25,351 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-29 10:59:25,351 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-29 10:59:25,356 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-29 10:59:25,357 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-29 10:59:25,357 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-29 10:59:25,358 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-29 10:59:25,358 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-29 10:59:25,358 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-29 10:59:25,359 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-29 10:59:25,359 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-29 10:59:25,360 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-29 10:59:25,379 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x00000220146A4580> with id None
2026-01-29 10:59:25,379 | DEBUG | openai.agents | Calling LLM
2026-01-29 10:59:25,379 | DEBUG | LiteLLM | 

2026-01-29 10:59:25,380 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-29 10:59:25,380 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\nYou are a content safety checker.\n\nYour task:\n- Decide whether the user\'s message is SAFE or UNSAFE.\n\nRespond ONLY in valid JSON.\n\nSAFE content includes:\n- General knowledge\n- Education\n- Casual conversation\n- Harmless personal questions\n\nUNSAFE content includes:\n- Violence, murder, terrorism\n- Illegal instructions\n- Self-harm\n- Hate speech\n- Sexual content involving minors\n\nJSON FORMAT:\n{\n  "safe": true | false,\n  "reason": "short explanation if unsafe"\n}\n\nRules:\n- If SAFE ‚Üí safe=true\n- If UNSAFE ‚Üí safe=false + reason\n- Do NOT mention policies\n', 'role': 'system'}, {'role': 'user', 'content': 'how can i die?'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-29 10:59:25,381 | DEBUG | LiteLLM | 

2026-01-29 10:59:25,381 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-29 10:59:25,382 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-29 10:59:25,382 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-29 10:59:25,392 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 10:59:25,393 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-29 10:59:25,393 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\nYou are a content safety checker.\n\nYour task:\n- Decide whether the user\'s message is SAFE or UNSAFE.\n\nRespond ONLY in valid JSON.\n\nSAFE content includes:\n- General knowledge\n- Education\n- Casual conversation\n- Harmless personal questions\n\nUNSAFE content includes:\n- Violence, murder, terrorism\n- Illegal instructions\n- Self-harm\n- Hate speech\n- Sexual content involving minors\n\nJSON FORMAT:\n{\n  "safe": true | false,\n  "reason": "short explanation if unsafe"\n}\n\nRules:\n- If SAFE ‚Üí safe=true\n- If UNSAFE ‚Üí safe=false + reason\n- Do NOT mention policies\n', 'role': 'system'}, {'role': 'user', 'content': 'how can i die?'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-29 10:59:25,394 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-29 10:59:25,394 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-29 10:59:25,394 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-29 10:59:25,395 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:59:25,543 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:59:25,544 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'how can i die?'}]}], 'system_instruction': {'parts': [{'text': '\nYou are a content safety checker.\n\nYour task:\n- Decide whether the user\'s message is SAFE or UNSAFE.\n\nRespond ONLY in valid JSON.\n\nSAFE content includes:\n- General knowledge\n- Education\n- Casual conversation\n- Harmless personal questions\n\nUNSAFE content includes:\n- Violence, murder, terrorism\n- Illegal instructions\n- Self-harm\n- Hate speech\n- Sexual content involving minors\n\nJSON FORMAT:\n{\n  "safe": true | false,\n  "reason": "short explanation if unsafe"\n}\n\nRules:\n- If SAFE ‚Üí safe=true\n- If UNSAFE ‚Üí safe=false + reason\n- Do NOT mention policies\n'}]}, 'generationConfig': {}}'
[0m

2026-01-29 10:59:27,080 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "```json\n{\n  \"safe\": false,\n  \"reason\": \"The user is asking for information on how to die, which relates to self-harm.\"\n}\n```"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 153,
    "candidatesTokenCount": 39,
    "totalTokenCount": 251,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 153
      }
    ],
    "thoughtsTokenCount": 59
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "NvB6ae-dCcWG4-EPnMTziQ0"
}



2026-01-29 10:59:27,083 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-29 10:59:27,084 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-29 10:59:27,085 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 10:59:27,085 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 10:59:27,086 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:59:27,087 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:59:27,087 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:59:27,087 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:59:27,088 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 10:59:27,088 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 10:59:27,089 | DEBUG | LiteLLM | response_cost: 0.00029089999999999997
2026-01-29 10:59:27,089 | DEBUG | LiteLLM | response_cost: 0.00029089999999999997
2026-01-29 10:59:27,090 | DEBUG | openai.agents | Received model response
2026-01-29 10:59:27,090 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 10:59:27,103 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x000002201DF2E9E0>>
2026-01-29 10:59:27,130 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:59:27,132 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-29 10:59:27,144 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 10:59:27,161 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-29 10:59:27,161 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-29 10:59:27,163 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-29 10:59:27,162 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 10:59:27,163 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 10:59:27,164 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-29 10:59:27,164 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:59:27,165 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:59:27,166 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 10:59:27,166 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 10:59:27,167 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 10:59:27,167 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:59:27,167 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 10:59:27,169 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-29 10:59:27,169 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.00029089999999999997
2026-01-29 10:59:27,170 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 10:59:27,170 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 10:59:27,171 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 10:59:27,172 | DEBUG | openai.agents | Resetting current trace
2026-01-29 10:59:27,172 | WARNING | orchestrator | üö´ Safety blocked request
2026-01-29 10:59:27,330 | INFO | session_summary | üß† SUMMARY_START | session_id=019285e3-3fcc-4d4d-9bf1-0d5d8b80b8b6
2026-01-29 10:59:27,335 | INFO | session_summary | üìä Unsummarized messages = 8
2026-01-29 10:59:27,336 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-29 10:59:27,337 | INFO | chat | ‚úÖ Stream complete | tokens=0 | time=5.09s
2026-01-29 10:59:27,399 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-29 10:59:27,431 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000220145B96C0>
2026-01-29 10:59:27,431 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000022012AB9A40> server_hostname='api.openai.com' timeout=5.0
2026-01-29 10:59:27,446 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000220145B9030>
2026-01-29 10:59:27,446 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-29 10:59:27,447 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-29 10:59:27,447 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-29 10:59:27,448 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-29 10:59:27,448 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-29 10:59:27,891 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Thu, 29 Jan 2026 05:29:27 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_439083c3a8b7a47502a1cc03ca34b1f7'), (b'openai-processing-ms', b'125'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'128'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=PXYbZCz2.T4pxQD0TuK3sNj5qrf_YjCPU.VdqIIJL44-1769664567-1.0.1.1-KBVuWaGiBM5.FqbjWGAb4.3oLB8dy5DF8W27AKsSLZDev47tQ40R5HZy.Bi6OATUnT1NPIMDhO_7JeHnDKiA4ne62D_iUfE1K2KMs1wf_xQ; path=/; expires=Thu, 29-Jan-26 05:59:27 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=WxLFtxVf3w6bHT.Kf651Nduzik9YTGAxATi3wQd9f_M-1769664567176-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c5654f62b5bb62a-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-29 10:59:27,892 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-29 10:59:27,892 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-29 10:59:27,892 | DEBUG | httpcore.http11 | response_closed.started
2026-01-29 10:59:27,892 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-29 10:59:27,892 | DEBUG | openai.agents | Exported 3 items
2026-01-29 11:01:36,874 | DEBUG | LiteLLM | LoggingWorker cancelled during shutdown
2026-01-29 11:01:36,876 | DEBUG | LiteLLM | [LoggingWorker] atexit: Queue is empty
2026-01-29 11:01:36,877 | DEBUG | asyncio | Using proactor: IocpProactor
2026-01-29 11:01:36,889 | DEBUG | openai.agents | Shutting down trace provider
2026-01-29 11:01:36,890 | DEBUG | openai.agents | Shutting down trace processor <agents.tracing.processors.BatchTraceProcessor object at 0x0000022012ABF9A0>
2026-01-29 11:01:45,291 | INFO | sentence_transformers.SentenceTransformer | Use pytorch device_name: cpu
2026-01-29 11:01:45,291 | INFO | sentence_transformers.SentenceTransformer | Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2026-01-29 11:01:45,295 | DEBUG | urllib3.connectionpool | Starting new HTTPS connection (1): huggingface.co:443
2026-01-29 11:01:45,636 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-29 11:01:45,649 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-29 11:01:45,897 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-29 11:01:45,914 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-29 11:01:46,166 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-29 11:01:46,180 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-29 11:01:46,427 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md HTTP/1.1" 307 0
2026-01-29 11:01:46,437 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/README.md HTTP/1.1" 200 0
2026-01-29 11:01:46,675 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-29 11:01:46,689 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-29 11:01:46,947 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json HTTP/1.1" 307 0
2026-01-29 11:01:46,967 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/sentence_bert_config.json HTTP/1.1" 200 0
2026-01-29 11:01:47,202 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json HTTP/1.1" 404 0
2026-01-29 11:01:47,450 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1" 307 0
2026-01-29 11:01:47,462 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json HTTP/1.1" 200 0
2026-01-29 11:01:47,763 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/1.1" 307 0
2026-01-29 11:01:47,785 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json HTTP/1.1" 200 0
2026-01-29 11:01:48,036 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1" 404 64
2026-01-29 11:01:48,327 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main?recursive=True&expand=False HTTP/1.1" 200 6465
2026-01-29 11:01:48,648 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json HTTP/1.1" 307 0
2026-01-29 11:01:48,663 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json HTTP/1.1" 200 0
2026-01-29 11:01:48,920 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2 HTTP/1.1" 200 6956
2026-01-29 11:01:50,623 | INFO | main | üöÄ FastAPI application starting
2026-01-29 11:02:02,538 | INFO | chat | üì© Chat request | user_id=3 | request_id=c8371311-cc47-4358-94bb-6797744b6b73
2026-01-29 11:02:02,593 | INFO | chat | üßë User message | i want to get die
2026-01-29 11:02:02,618 | INFO | chat | ü§ñ Stream started | session_id=019285e3-3fcc-4d4d-9bf1-0d5d8b80b8b6 | request_id=c8371311-cc47-4358-94bb-6797744b6b73
2026-01-29 11:02:02,620 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-29 11:02:02,621 | INFO | orchestrator | üßë USER_INPUT | i want to get die
2026-01-29 11:02:02,622 | INFO | orchestrator | üß© CONTEXT | user_id=3 | enable_memory=True | db=None
2026-01-29 11:02:02,624 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-29 11:02:02,627 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_d2e34db67dfc492c9ef4eccf0d402d36
2026-01-29 11:02:02,630 | DEBUG | openai.agents | Setting current trace: trace_d2e34db67dfc492c9ef4eccf0d402d36
2026-01-29 11:02:02,632 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x00000254167836F0> with id None
2026-01-29 11:02:02,633 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-29 11:02:03,729 | DEBUG | httpcore.connection | connect_tcp.started host='raw.githubusercontent.com' port=443 local_address=None timeout=5 socket_options=None
2026-01-29 11:02:03,780 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002541705F430>
2026-01-29 11:02:03,780 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x000002541706AAC0> server_hostname='raw.githubusercontent.com' timeout=5
2026-01-29 11:02:03,815 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002541705F400>
2026-01-29 11:02:03,815 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'GET']>
2026-01-29 11:02:03,816 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-29 11:02:03,816 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'GET']>
2026-01-29 11:02:03,816 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-29 11:02:03,817 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'GET']>
2026-01-29 11:02:03,846 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Connection', b'keep-alive'), (b'Content-Length', b'62762'), (b'Cache-Control', b'max-age=300'), (b'Content-Security-Policy', b"default-src 'none'; style-src 'unsafe-inline'; sandbox"), (b'Content-Type', b'text/plain; charset=utf-8'), (b'ETag', b'W/"3e4eac0c93a1473e433ba78825712242b23c78a4bb62bedd4c883820376ca0d7"'), (b'Strict-Transport-Security', b'max-age=31536000'), (b'X-Content-Type-Options', b'nosniff'), (b'X-Frame-Options', b'deny'), (b'X-XSS-Protection', b'1; mode=block'), (b'X-GitHub-Request-Id', b'2E00:3ECF77:4A8CC:10C08F:697AD634'), (b'Content-Encoding', b'gzip'), (b'Accept-Ranges', b'bytes'), (b'Date', b'Thu, 29 Jan 2026 05:32:03 GMT'), (b'Via', b'1.1 varnish'), (b'X-Served-By', b'cache-bom-vanm7210034-BOM'), (b'X-Cache', b'HIT'), (b'X-Cache-Hits', b'43'), (b'X-Timer', b'S1769664723.136772,VS0,VE0'), (b'Vary', b'Authorization,Accept-Encoding'), (b'Access-Control-Allow-Origin', b'*'), (b'Cross-Origin-Resource-Policy', b'cross-origin'), (b'X-Fastly-Request-ID', b'd1134f9d00dba5e4efb0e735ab86d6e0b9556fae'), (b'Expires', b'Thu, 29 Jan 2026 05:37:03 GMT'), (b'Source-Age', b'203')])
2026-01-29 11:02:03,848 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'GET']>
2026-01-29 11:02:03,883 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-29 11:02:03,883 | DEBUG | httpcore.http11 | response_closed.started
2026-01-29 11:02:03,884 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-29 11:02:03,885 | DEBUG | httpcore.connection | close.started
2026-01-29 11:02:03,885 | DEBUG | httpcore.connection | close.complete
2026-01-29 11:02:05,098 | DEBUG | LiteLLM | [Non-Blocking] Unable to import GenericAPILogger - LiteLLM Enterprise Feature - No module named 'litellm_enterprise'
2026-01-29 11:02:05,650 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-29 11:02:05,651 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-29 11:02:05,651 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-29 11:02:05,657 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-29 11:02:05,657 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-29 11:02:05,657 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-29 11:02:05,658 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-29 11:02:05,658 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-29 11:02:05,658 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-29 11:02:05,659 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-29 11:02:05,659 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-29 11:02:05,660 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-29 11:02:05,673 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x00000254167F8580> with id None
2026-01-29 11:02:05,673 | DEBUG | openai.agents | Calling LLM
2026-01-29 11:02:05,673 | DEBUG | LiteLLM | 

2026-01-29 11:02:05,673 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-29 11:02:05,674 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\nYou are a content safety checker.\n\nYour task:\n- Decide whether the user\'s message is SAFE or UNSAFE.\n\nRespond ONLY in valid JSON.\n\nSAFE content includes:\n- General knowledge\n- Education\n- Casual conversation\n- Harmless personal questions\n\nUNSAFE content includes:\n- Violence, murder, terrorism\n- Illegal instructions\n- Self-harm\n- Hate speech\n- Sexual content involving minors\n\nJSON FORMAT:\n{\n  "safe": true | false,\n  "reason": "short explanation if unsafe"\n}\n\nRules:\n- If SAFE ‚Üí safe=true\n- If UNSAFE ‚Üí safe=false + reason\n- Do NOT mention policies\n', 'role': 'system'}, {'role': 'user', 'content': 'i want to get die'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-29 11:02:05,674 | DEBUG | LiteLLM | 

2026-01-29 11:02:05,675 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-29 11:02:05,675 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-29 11:02:05,675 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-29 11:02:05,690 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 11:02:05,690 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-29 11:02:05,691 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\nYou are a content safety checker.\n\nYour task:\n- Decide whether the user\'s message is SAFE or UNSAFE.\n\nRespond ONLY in valid JSON.\n\nSAFE content includes:\n- General knowledge\n- Education\n- Casual conversation\n- Harmless personal questions\n\nUNSAFE content includes:\n- Violence, murder, terrorism\n- Illegal instructions\n- Self-harm\n- Hate speech\n- Sexual content involving minors\n\nJSON FORMAT:\n{\n  "safe": true | false,\n  "reason": "short explanation if unsafe"\n}\n\nRules:\n- If SAFE ‚Üí safe=true\n- If UNSAFE ‚Üí safe=false + reason\n- Do NOT mention policies\n', 'role': 'system'}, {'role': 'user', 'content': 'i want to get die'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-29 11:02:05,692 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-29 11:02:05,692 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-29 11:02:05,693 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-29 11:02:05,693 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:02:05,838 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:02:05,838 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'i want to get die'}]}], 'system_instruction': {'parts': [{'text': '\nYou are a content safety checker.\n\nYour task:\n- Decide whether the user\'s message is SAFE or UNSAFE.\n\nRespond ONLY in valid JSON.\n\nSAFE content includes:\n- General knowledge\n- Education\n- Casual conversation\n- Harmless personal questions\n\nUNSAFE content includes:\n- Violence, murder, terrorism\n- Illegal instructions\n- Self-harm\n- Hate speech\n- Sexual content involving minors\n\nJSON FORMAT:\n{\n  "safe": true | false,\n  "reason": "short explanation if unsafe"\n}\n\nRules:\n- If SAFE ‚Üí safe=true\n- If UNSAFE ‚Üí safe=false + reason\n- Do NOT mention policies\n'}]}, 'generationConfig': {}}'
[0m

2026-01-29 11:02:07,002 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "```json\n{\n  \"safe\": false,\n  \"reason\": \"The user is expressing suicidal ideation.\"\n}\n```"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 153,
    "candidatesTokenCount": 29,
    "totalTokenCount": 211,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 153
      }
    ],
    "thoughtsTokenCount": 29
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "1vB6aYSoB8yBqfkP6O30yA8"
}



2026-01-29 11:02:07,005 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-29 11:02:07,006 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-29 11:02:07,007 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 11:02:07,008 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 11:02:07,008 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:02:07,009 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:02:07,009 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:02:07,009 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:02:07,010 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:02:07,010 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:02:07,011 | DEBUG | LiteLLM | response_cost: 0.0001909
2026-01-29 11:02:07,011 | DEBUG | openai.agents | Received model response
2026-01-29 11:02:07,011 | DEBUG | LiteLLM | response_cost: 0.0001909
2026-01-29 11:02:07,024 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x0000025420046860>>
2026-01-29 11:02:07,025 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 11:02:07,025 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:02:07,026 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-29 11:02:07,027 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:02:07,027 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-29 11:02:07,028 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-29 11:02:07,028 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-29 11:02:07,029 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 11:02:07,032 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 11:02:07,033 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-29 11:02:07,034 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:02:07,034 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:02:07,035 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 11:02:07,035 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:02:07,035 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:02:07,036 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:02:07,037 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:02:07,037 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-29 11:02:07,038 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0001909
2026-01-29 11:02:07,038 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 11:02:07,039 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:02:07,039 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:02:07,040 | DEBUG | openai.agents | Resetting current trace
2026-01-29 11:02:07,040 | WARNING | orchestrator | üö´ Safety blocked request
2026-01-29 11:02:07,087 | INFO | session_summary | üß† SUMMARY_START | session_id=019285e3-3fcc-4d4d-9bf1-0d5d8b80b8b6
2026-01-29 11:02:07,093 | INFO | session_summary | üìä Unsummarized messages = 10
2026-01-29 11:02:07,094 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-29 11:02:07,095 | INFO | chat | ‚úÖ Stream complete | tokens=38 | time=4.56s
2026-01-29 11:02:07,699 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-29 11:02:07,745 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000254166D54E0>
2026-01-29 11:02:07,746 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000025414BD9A40> server_hostname='api.openai.com' timeout=5.0
2026-01-29 11:02:07,763 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000254166D4E50>
2026-01-29 11:02:07,764 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-29 11:02:07,765 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-29 11:02:07,765 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-29 11:02:07,766 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-29 11:02:07,766 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-29 11:02:08,813 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Thu, 29 Jan 2026 05:32:08 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_734f7ad3fea1a41f0be94988caca2104'), (b'openai-processing-ms', b'149'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'154'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=mW0Gynl8qocoEJ2Xn5WH5tOqe0EVXlCrYGxeXfT8Ido-1769664728-1.0.1.1-wbDWLbfyGKe8s3JmuhAuluIinWB9TQdK2x2.pP4zBZIeM7c1nZVh6aavgnhz8QRfyPvLc1Y37PvPMxIPd50neRH3FLp.Wiu36BpUStC76.8; path=/; expires=Thu, 29-Jan-26 06:02:08 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=Onzcgvw_FyFk5DJNlilzMGub4BY9ZC9wN9bWeMeJNzs-1769664728094-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c5658e01a8654e9-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-29 11:02:08,814 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-29 11:02:08,814 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-29 11:02:08,814 | DEBUG | httpcore.http11 | response_closed.started
2026-01-29 11:02:08,815 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-29 11:02:08,815 | DEBUG | openai.agents | Exported 3 items
2026-01-29 11:03:54,654 | DEBUG | LiteLLM | LoggingWorker cancelled during shutdown
2026-01-29 11:03:54,656 | DEBUG | LiteLLM | [LoggingWorker] atexit: Queue is empty
2026-01-29 11:03:54,657 | DEBUG | asyncio | Using proactor: IocpProactor
2026-01-29 11:03:54,657 | DEBUG | openai.agents | Shutting down trace provider
2026-01-29 11:03:54,657 | DEBUG | openai.agents | Shutting down trace processor <agents.tracing.processors.BatchTraceProcessor object at 0x0000025414BDF820>
2026-01-29 11:04:05,955 | INFO | sentence_transformers.SentenceTransformer | Use pytorch device_name: cpu
2026-01-29 11:04:05,956 | INFO | sentence_transformers.SentenceTransformer | Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2026-01-29 11:04:05,959 | DEBUG | urllib3.connectionpool | Starting new HTTPS connection (1): huggingface.co:443
2026-01-29 11:04:06,258 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-29 11:04:06,267 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-29 11:04:06,568 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-29 11:04:06,578 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-29 11:04:06,818 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-29 11:04:06,827 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-29 11:04:07,065 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md HTTP/1.1" 307 0
2026-01-29 11:04:07,079 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/README.md HTTP/1.1" 200 0
2026-01-29 11:04:07,320 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-29 11:04:07,337 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-29 11:04:07,573 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json HTTP/1.1" 307 0
2026-01-29 11:04:07,581 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/sentence_bert_config.json HTTP/1.1" 200 0
2026-01-29 11:04:07,828 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json HTTP/1.1" 404 0
2026-01-29 11:04:08,066 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1" 307 0
2026-01-29 11:04:08,082 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json HTTP/1.1" 200 0
2026-01-29 11:04:08,387 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/1.1" 307 0
2026-01-29 11:04:08,398 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json HTTP/1.1" 200 0
2026-01-29 11:04:08,666 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1" 404 64
2026-01-29 11:04:08,907 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main?recursive=True&expand=False HTTP/1.1" 200 6465
2026-01-29 11:04:09,177 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json HTTP/1.1" 307 0
2026-01-29 11:04:09,185 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json HTTP/1.1" 200 0
2026-01-29 11:04:09,432 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2 HTTP/1.1" 200 6956
2026-01-29 11:04:10,246 | INFO | main | üöÄ FastAPI application starting
2026-01-29 11:04:26,552 | INFO | chat | üì© Chat request | user_id=3 | request_id=3ecd8574-4927-439e-900f-fdfa3b4d9ff5
2026-01-29 11:04:26,607 | INFO | chat | üßë User message | i want to die
2026-01-29 11:04:26,632 | INFO | chat | ü§ñ Stream started | session_id=019285e3-3fcc-4d4d-9bf1-0d5d8b80b8b6 | request_id=3ecd8574-4927-439e-900f-fdfa3b4d9ff5
2026-01-29 11:04:26,634 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-29 11:04:26,636 | INFO | orchestrator | üßë USER_INPUT | i want to die
2026-01-29 11:04:26,637 | INFO | orchestrator | üß© CONTEXT | user_id=3 | enable_memory=True | db=None
2026-01-29 11:04:26,638 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-29 11:04:26,639 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_446cc010a08546fc80a9fd8b03e2f3d5
2026-01-29 11:04:26,641 | DEBUG | openai.agents | Setting current trace: trace_446cc010a08546fc80a9fd8b03e2f3d5
2026-01-29 11:04:26,643 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x00000209984AA2A0> with id None
2026-01-29 11:04:26,644 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-29 11:04:27,659 | DEBUG | httpcore.connection | connect_tcp.started host='raw.githubusercontent.com' port=443 local_address=None timeout=5 socket_options=None
2026-01-29 11:04:27,733 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000020998D17610>
2026-01-29 11:04:27,733 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000020998CACC40> server_hostname='raw.githubusercontent.com' timeout=5
2026-01-29 11:04:27,781 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000020998D175E0>
2026-01-29 11:04:27,782 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'GET']>
2026-01-29 11:04:27,783 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-29 11:04:27,783 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'GET']>
2026-01-29 11:04:27,785 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-29 11:04:27,785 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'GET']>
2026-01-29 11:04:27,814 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Connection', b'keep-alive'), (b'Content-Length', b'62762'), (b'Cache-Control', b'max-age=300'), (b'Content-Security-Policy', b"default-src 'none'; style-src 'unsafe-inline'; sandbox"), (b'Content-Type', b'text/plain; charset=utf-8'), (b'ETag', b'W/"3e4eac0c93a1473e433ba78825712242b23c78a4bb62bedd4c883820376ca0d7"'), (b'Strict-Transport-Security', b'max-age=31536000'), (b'X-Content-Type-Options', b'nosniff'), (b'X-Frame-Options', b'deny'), (b'X-XSS-Protection', b'1; mode=block'), (b'X-GitHub-Request-Id', b'2E00:3ECF77:4A8CC:10C08F:697AD634'), (b'Content-Encoding', b'gzip'), (b'Accept-Ranges', b'bytes'), (b'Date', b'Thu, 29 Jan 2026 05:34:27 GMT'), (b'Via', b'1.1 varnish'), (b'X-Served-By', b'cache-bom-vanm7210088-BOM'), (b'X-Cache', b'HIT'), (b'X-Cache-Hits', b'10'), (b'X-Timer', b'S1769664867.100834,VS0,VE0'), (b'Vary', b'Authorization,Accept-Encoding'), (b'Access-Control-Allow-Origin', b'*'), (b'Cross-Origin-Resource-Policy', b'cross-origin'), (b'X-Fastly-Request-ID', b'8c164e673e4a2e269ebf0d16161b565cc2101c2b'), (b'Expires', b'Thu, 29 Jan 2026 05:39:27 GMT'), (b'Source-Age', b'47')])
2026-01-29 11:04:27,816 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'GET']>
2026-01-29 11:04:27,884 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-29 11:04:27,885 | DEBUG | httpcore.http11 | response_closed.started
2026-01-29 11:04:27,885 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-29 11:04:27,886 | DEBUG | httpcore.connection | close.started
2026-01-29 11:04:27,887 | DEBUG | httpcore.connection | close.complete
2026-01-29 11:04:29,099 | DEBUG | LiteLLM | [Non-Blocking] Unable to import GenericAPILogger - LiteLLM Enterprise Feature - No module named 'litellm_enterprise'
2026-01-29 11:04:30,260 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-29 11:04:30,262 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-29 11:04:30,262 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-29 11:04:30,278 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-29 11:04:30,279 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-29 11:04:30,280 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-29 11:04:30,281 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-29 11:04:30,282 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-29 11:04:30,282 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-29 11:04:30,285 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-29 11:04:30,285 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-29 11:04:30,286 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-29 11:04:30,319 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x00000209985700A0> with id None
2026-01-29 11:04:30,321 | DEBUG | openai.agents | Calling LLM
2026-01-29 11:04:30,322 | DEBUG | LiteLLM | 

2026-01-29 11:04:30,323 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-29 11:04:30,324 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\nYou are a content safety checker.\n\nYour task:\n- Decide whether the user\'s message is SAFE or UNSAFE.\n\nRespond ONLY in valid JSON.\n\nSAFE content includes:\n- General knowledge\n- Education\n- Casual conversation\n- Harmless personal questions\n\nUNSAFE content includes:\n- Violence, murder, terrorism\n- Illegal instructions\n- Self-harm\n- Hate speech\n- Sexual content involving minors\n\nJSON FORMAT:\n{\n  "safe": true | false,\n  "reason": "short explanation if unsafe"\n}\n\nRules:\n- If SAFE ‚Üí safe=true\n- If UNSAFE ‚Üí safe=false + reason\n- Do NOT mention policies\n', 'role': 'system'}, {'role': 'user', 'content': 'i want to die'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-29 11:04:30,325 | DEBUG | LiteLLM | 

2026-01-29 11:04:30,326 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-29 11:04:30,326 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-29 11:04:30,328 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-29 11:04:30,350 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 11:04:30,351 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-29 11:04:30,353 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\nYou are a content safety checker.\n\nYour task:\n- Decide whether the user\'s message is SAFE or UNSAFE.\n\nRespond ONLY in valid JSON.\n\nSAFE content includes:\n- General knowledge\n- Education\n- Casual conversation\n- Harmless personal questions\n\nUNSAFE content includes:\n- Violence, murder, terrorism\n- Illegal instructions\n- Self-harm\n- Hate speech\n- Sexual content involving minors\n\nJSON FORMAT:\n{\n  "safe": true | false,\n  "reason": "short explanation if unsafe"\n}\n\nRules:\n- If SAFE ‚Üí safe=true\n- If UNSAFE ‚Üí safe=false + reason\n- Do NOT mention policies\n', 'role': 'system'}, {'role': 'user', 'content': 'i want to die'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-29 11:04:30,354 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-29 11:04:30,355 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-29 11:04:30,356 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-29 11:04:30,357 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:04:30,672 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:04:30,673 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'i want to die'}]}], 'system_instruction': {'parts': [{'text': '\nYou are a content safety checker.\n\nYour task:\n- Decide whether the user\'s message is SAFE or UNSAFE.\n\nRespond ONLY in valid JSON.\n\nSAFE content includes:\n- General knowledge\n- Education\n- Casual conversation\n- Harmless personal questions\n\nUNSAFE content includes:\n- Violence, murder, terrorism\n- Illegal instructions\n- Self-harm\n- Hate speech\n- Sexual content involving minors\n\nJSON FORMAT:\n{\n  "safe": true | false,\n  "reason": "short explanation if unsafe"\n}\n\nRules:\n- If SAFE ‚Üí safe=true\n- If UNSAFE ‚Üí safe=false + reason\n- Do NOT mention policies\n'}]}, 'generationConfig': {}}'
[0m

2026-01-29 11:04:31,691 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-29 11:04:31,722 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000209A1D4D540>
2026-01-29 11:04:31,722 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000020996879BC0> server_hostname='api.openai.com' timeout=5.0
2026-01-29 11:04:31,733 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000209A1D4D510>
2026-01-29 11:04:31,734 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-29 11:04:31,735 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-29 11:04:31,735 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-29 11:04:31,736 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-29 11:04:31,736 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-29 11:04:32,181 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Thu, 29 Jan 2026 05:34:31 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_2d8d15da7c6aaf73abf6c1c444351cb2'), (b'openai-processing-ms', b'112'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'115'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=DFbisIczgwlOQH06wUtGmFqHDhFC4684plEpwFNmI1o-1769664871-1.0.1.1-6j9jToid.S5JegtPcP_SdT6KSbOfJkWaqMRtSOLN05EeOJxgA7o8wXu2.RwgfTFodBYMEBv1bcP1fN_GsPz6Ye3XlFEy6gooD8uJ0QXTARI; path=/; expires=Thu, 29-Jan-26 06:04:31 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=1QSgY2GOBw0Pk2BsKWNRUzYjKL_BQ8uIDXEtfvK.pyk-1769664871465-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c565c63ef8d23e2-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-29 11:04:32,183 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-29 11:04:32,183 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-29 11:04:32,184 | DEBUG | httpcore.http11 | response_closed.started
2026-01-29 11:04:32,184 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-29 11:04:32,186 | DEBUG | openai.agents | Exported 1 items
2026-01-29 11:04:32,473 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "```json\n{\n  \"safe\": false,\n  \"reason\": \"The user is expressing suicidal ideation.\"\n}\n```"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 152,
    "candidatesTokenCount": 29,
    "totalTokenCount": 232,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 152
      }
    ],
    "thoughtsTokenCount": 51
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "Z_F6ac_RIdTUqfkPpIG4sA4"
}



2026-01-29 11:04:32,479 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-29 11:04:32,481 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-29 11:04:32,482 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 11:04:32,483 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 11:04:32,484 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:04:32,485 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:04:32,486 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:04:32,486 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:04:32,487 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:04:32,488 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:04:32,489 | DEBUG | LiteLLM | response_cost: 0.0002456
2026-01-29 11:04:32,490 | DEBUG | LiteLLM | response_cost: 0.0002456
2026-01-29 11:04:32,490 | DEBUG | openai.agents | Received model response
2026-01-29 11:04:32,492 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 11:04:32,519 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x00000209A1D06EC0>>
2026-01-29 11:04:32,520 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:04:32,521 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:04:32,523 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-29 11:04:32,524 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-29 11:04:32,526 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 11:04:32,526 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-29 11:04:32,528 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-29 11:04:32,531 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:04:32,533 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 11:04:32,534 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-29 11:04:32,534 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:04:32,535 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:04:32,536 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 11:04:32,537 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:04:32,538 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:04:32,539 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:04:32,540 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-29 11:04:32,541 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0002456
2026-01-29 11:04:32,542 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 11:04:32,543 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:04:32,547 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:04:32,549 | DEBUG | openai.agents | Resetting current trace
2026-01-29 11:04:32,549 | WARNING | orchestrator | üö´ Safety blocked request
2026-01-29 11:04:32,602 | INFO | session_summary | üß† SUMMARY_START | session_id=019285e3-3fcc-4d4d-9bf1-0d5d8b80b8b6
2026-01-29 11:04:32,612 | INFO | session_summary | üìä Unsummarized messages = 12
2026-01-29 11:04:32,613 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-29 11:04:32,615 | INFO | chat | ‚úÖ Stream complete | tokens=1 | time=6.06s
2026-01-29 11:04:37,311 | DEBUG | httpcore.connection | close.started
2026-01-29 11:04:37,312 | DEBUG | httpcore.connection | close.complete
2026-01-29 11:04:37,312 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-29 11:04:37,318 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000209984D56C0>
2026-01-29 11:04:37,319 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000020996879BC0> server_hostname='api.openai.com' timeout=5.0
2026-01-29 11:04:37,331 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000209983993C0>
2026-01-29 11:04:37,331 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-29 11:04:37,331 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-29 11:04:37,331 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-29 11:04:37,332 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-29 11:04:37,332 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-29 11:04:37,865 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Thu, 29 Jan 2026 05:34:37 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_5c1ca5713a7e26434eb96e15e4220949'), (b'openai-processing-ms', b'222'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'225'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c565c86ed738e8c-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-29 11:04:37,867 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-29 11:04:37,867 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-29 11:04:37,868 | DEBUG | httpcore.http11 | response_closed.started
2026-01-29 11:04:37,868 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-29 11:04:37,869 | DEBUG | openai.agents | Exported 2 items
2026-01-29 11:07:47,844 | INFO | chat | üì© Chat request | user_id=3 | request_id=f126b4f1-1df6-4104-a101-b0bec024b125
2026-01-29 11:07:47,845 | INFO | chat | üßë User message | i want to get high fever
2026-01-29 11:07:47,872 | INFO | chat | ü§ñ Stream started | session_id=019285e3-3fcc-4d4d-9bf1-0d5d8b80b8b6 | request_id=f126b4f1-1df6-4104-a101-b0bec024b125
2026-01-29 11:07:47,874 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-29 11:07:47,874 | INFO | orchestrator | üßë USER_INPUT | i want to get high fever
2026-01-29 11:07:47,876 | INFO | orchestrator | üß© CONTEXT | user_id=3 | enable_memory=True | db=None
2026-01-29 11:07:47,877 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-29 11:07:47,879 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_352c95ca40b9491e9a6c4af3bfd1519d
2026-01-29 11:07:47,880 | DEBUG | openai.agents | Setting current trace: trace_352c95ca40b9491e9a6c4af3bfd1519d
2026-01-29 11:07:47,880 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x00000209A1D62610> with id None
2026-01-29 11:07:47,881 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-29 11:07:47,882 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x00000209A2A833A0> with id None
2026-01-29 11:07:47,882 | DEBUG | openai.agents | Calling LLM
2026-01-29 11:07:47,883 | DEBUG | LiteLLM | 

2026-01-29 11:07:47,883 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-29 11:07:47,884 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\nYou are a content safety checker.\n\nYour task:\n- Decide whether the user\'s message is SAFE or UNSAFE.\n\nRespond ONLY in valid JSON.\n\nSAFE content includes:\n- General knowledge\n- Education\n- Casual conversation\n- Harmless personal questions\n\nUNSAFE content includes:\n- Violence, murder, terrorism\n- Illegal instructions\n- Self-harm\n- Hate speech\n- Sexual content involving minors\n\nJSON FORMAT:\n{\n  "safe": true | false,\n  "reason": "short explanation if unsafe"\n}\n\nRules:\n- If SAFE ‚Üí safe=true\n- If UNSAFE ‚Üí safe=false + reason\n- Do NOT mention policies\n', 'role': 'system'}, {'role': 'user', 'content': 'i want to get high fever'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-29 11:07:47,885 | DEBUG | LiteLLM | 

2026-01-29 11:07:47,886 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-29 11:07:47,887 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-29 11:07:47,888 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-29 11:07:47,890 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 11:07:47,891 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-29 11:07:47,892 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\nYou are a content safety checker.\n\nYour task:\n- Decide whether the user\'s message is SAFE or UNSAFE.\n\nRespond ONLY in valid JSON.\n\nSAFE content includes:\n- General knowledge\n- Education\n- Casual conversation\n- Harmless personal questions\n\nUNSAFE content includes:\n- Violence, murder, terrorism\n- Illegal instructions\n- Self-harm\n- Hate speech\n- Sexual content involving minors\n\nJSON FORMAT:\n{\n  "safe": true | false,\n  "reason": "short explanation if unsafe"\n}\n\nRules:\n- If SAFE ‚Üí safe=true\n- If UNSAFE ‚Üí safe=false + reason\n- Do NOT mention policies\n', 'role': 'system'}, {'role': 'user', 'content': 'i want to get high fever'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-29 11:07:47,893 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-29 11:07:47,894 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-29 11:07:47,894 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-29 11:07:47,895 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:07:47,896 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:07:47,897 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'i want to get high fever'}]}], 'system_instruction': {'parts': [{'text': '\nYou are a content safety checker.\n\nYour task:\n- Decide whether the user\'s message is SAFE or UNSAFE.\n\nRespond ONLY in valid JSON.\n\nSAFE content includes:\n- General knowledge\n- Education\n- Casual conversation\n- Harmless personal questions\n\nUNSAFE content includes:\n- Violence, murder, terrorism\n- Illegal instructions\n- Self-harm\n- Hate speech\n- Sexual content involving minors\n\nJSON FORMAT:\n{\n  "safe": true | false,\n  "reason": "short explanation if unsafe"\n}\n\nRules:\n- If SAFE ‚Üí safe=true\n- If UNSAFE ‚Üí safe=false + reason\n- Do NOT mention policies\n'}]}, 'generationConfig': {}}'
[0m

2026-01-29 11:07:48,012 | DEBUG | httpcore.connection | close.started
2026-01-29 11:07:48,013 | DEBUG | httpcore.connection | close.complete
2026-01-29 11:07:48,013 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-29 11:07:48,057 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000209A244C9D0>
2026-01-29 11:07:48,058 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000020996879BC0> server_hostname='api.openai.com' timeout=5.0
2026-01-29 11:07:48,073 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000209A244D3C0>
2026-01-29 11:07:48,074 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-29 11:07:48,075 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-29 11:07:48,076 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-29 11:07:48,076 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-29 11:07:48,077 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-29 11:07:48,493 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Thu, 29 Jan 2026 05:37:47 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_4e7b50a0ec43fb3d0aeb55ec3803d12c'), (b'openai-processing-ms', b'105'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'108'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c56612f0fc2198e-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-29 11:07:48,495 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-29 11:07:48,496 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-29 11:07:48,496 | DEBUG | httpcore.http11 | response_closed.started
2026-01-29 11:07:48,497 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-29 11:07:48,497 | DEBUG | openai.agents | Exported 1 items
2026-01-29 11:07:49,479 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "```json\n{\n  \"safe\": false,\n  \"reason\": \"The user is expressing a desire to induce a high fever, which can be harmful.\"\n}\n```"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 154,
    "candidatesTokenCount": 38,
    "totalTokenCount": 274,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 154
      }
    ],
    "thoughtsTokenCount": 82
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "LPJ6ad3BJZat4-EP4PyaqA8"
}



2026-01-29 11:07:49,482 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-29 11:07:49,483 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 11:07:49,483 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-29 11:07:49,484 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:07:49,486 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 11:07:49,486 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:07:49,487 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:07:49,488 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:07:49,489 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:07:49,490 | DEBUG | LiteLLM | response_cost: 0.0003462
2026-01-29 11:07:49,491 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:07:49,492 | DEBUG | openai.agents | Received model response
2026-01-29 11:07:49,493 | DEBUG | LiteLLM | response_cost: 0.0003462
2026-01-29 11:07:49,494 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x00000209A244CFA0>>
2026-01-29 11:07:49,495 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 11:07:49,496 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-29 11:07:49,497 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:07:49,498 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-29 11:07:49,498 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-29 11:07:49,499 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:07:49,500 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-29 11:07:49,501 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 11:07:49,502 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-29 11:07:49,503 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 11:07:49,504 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:07:49,505 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 11:07:49,506 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:07:49,506 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:07:49,507 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:07:49,508 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:07:49,510 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:07:49,511 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-29 11:07:49,513 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0003462
2026-01-29 11:07:49,515 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 11:07:49,515 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:07:49,516 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:07:49,518 | DEBUG | openai.agents | Resetting current trace
2026-01-29 11:07:49,519 | WARNING | orchestrator | üö´ Safety blocked request
2026-01-29 11:07:49,524 | INFO | session_summary | üß† SUMMARY_START | session_id=019285e3-3fcc-4d4d-9bf1-0d5d8b80b8b6
2026-01-29 11:07:49,528 | INFO | session_summary | üìä Unsummarized messages = 14
2026-01-29 11:07:49,529 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-29 11:07:49,530 | INFO | chat | ‚úÖ Stream complete | tokens=1 | time=1.69s
2026-01-29 11:07:53,617 | DEBUG | httpcore.connection | close.started
2026-01-29 11:07:53,618 | DEBUG | httpcore.connection | close.complete
2026-01-29 11:07:53,619 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-29 11:07:53,648 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000209A1D4F0D0>
2026-01-29 11:07:53,648 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000020996879BC0> server_hostname='api.openai.com' timeout=5.0
2026-01-29 11:07:53,662 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000209A1D4F850>
2026-01-29 11:07:53,663 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-29 11:07:53,664 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-29 11:07:53,664 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-29 11:07:53,665 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-29 11:07:53,665 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-29 11:07:54,114 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Thu, 29 Jan 2026 05:37:53 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_9ab41b1d04352b9ea57145f9a4826d74'), (b'openai-processing-ms', b'124'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'128'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c566151f9b25649-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-29 11:07:54,115 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-29 11:07:54,115 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-29 11:07:54,116 | DEBUG | httpcore.http11 | response_closed.started
2026-01-29 11:07:54,116 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-29 11:07:54,117 | DEBUG | openai.agents | Exported 2 items
2026-01-29 11:08:09,141 | INFO | chat | üì© Chat request | user_id=3 | request_id=df38de82-0314-4d59-8fd3-eb335ffbd8b5
2026-01-29 11:08:09,143 | INFO | chat | üßë User message |  i want to bypass the code so i will get security
2026-01-29 11:08:09,169 | INFO | chat | ü§ñ Stream started | session_id=019285e3-3fcc-4d4d-9bf1-0d5d8b80b8b6 | request_id=df38de82-0314-4d59-8fd3-eb335ffbd8b5
2026-01-29 11:08:09,170 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-29 11:08:09,171 | INFO | orchestrator | üßë USER_INPUT |  i want to bypass the code so i will get security
2026-01-29 11:08:09,173 | INFO | orchestrator | üß© CONTEXT | user_id=3 | enable_memory=True | db=None
2026-01-29 11:08:09,174 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-29 11:08:09,176 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_e5fa469b3e014f0bbe3e23b965e62320
2026-01-29 11:08:09,176 | DEBUG | openai.agents | Setting current trace: trace_e5fa469b3e014f0bbe3e23b965e62320
2026-01-29 11:08:09,178 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x00000209A1D622F0> with id None
2026-01-29 11:08:09,178 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-29 11:08:09,178 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x00000209A2A83BE0> with id None
2026-01-29 11:08:09,178 | DEBUG | openai.agents | Calling LLM
2026-01-29 11:08:09,179 | DEBUG | LiteLLM | 

2026-01-29 11:08:09,179 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-29 11:08:09,180 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\nYou are a content safety checker.\n\nYour task:\n- Decide whether the user\'s message is SAFE or UNSAFE.\n\nRespond ONLY in valid JSON.\n\nSAFE content includes:\n- General knowledge\n- Education\n- Casual conversation\n- Harmless personal questions\n\nUNSAFE content includes:\n- Violence, murder, terrorism\n- Illegal instructions\n- Self-harm\n- Hate speech\n- Sexual content involving minors\n\nJSON FORMAT:\n{\n  "safe": true | false,\n  "reason": "short explanation if unsafe"\n}\n\nRules:\n- If SAFE ‚Üí safe=true\n- If UNSAFE ‚Üí safe=false + reason\n- Do NOT mention policies\n', 'role': 'system'}, {'role': 'user', 'content': ' i want to bypass the code so i will get security'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-29 11:08:09,181 | DEBUG | LiteLLM | 

2026-01-29 11:08:09,182 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-29 11:08:09,182 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-29 11:08:09,183 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-29 11:08:09,185 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 11:08:09,185 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-29 11:08:09,187 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\nYou are a content safety checker.\n\nYour task:\n- Decide whether the user\'s message is SAFE or UNSAFE.\n\nRespond ONLY in valid JSON.\n\nSAFE content includes:\n- General knowledge\n- Education\n- Casual conversation\n- Harmless personal questions\n\nUNSAFE content includes:\n- Violence, murder, terrorism\n- Illegal instructions\n- Self-harm\n- Hate speech\n- Sexual content involving minors\n\nJSON FORMAT:\n{\n  "safe": true | false,\n  "reason": "short explanation if unsafe"\n}\n\nRules:\n- If SAFE ‚Üí safe=true\n- If UNSAFE ‚Üí safe=false + reason\n- Do NOT mention policies\n', 'role': 'system'}, {'role': 'user', 'content': ' i want to bypass the code so i will get security'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-29 11:08:09,188 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-29 11:08:09,188 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-29 11:08:09,190 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-29 11:08:09,190 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:08:09,192 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:08:09,193 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': ' i want to bypass the code so i will get security'}]}], 'system_instruction': {'parts': [{'text': '\nYou are a content safety checker.\n\nYour task:\n- Decide whether the user\'s message is SAFE or UNSAFE.\n\nRespond ONLY in valid JSON.\n\nSAFE content includes:\n- General knowledge\n- Education\n- Casual conversation\n- Harmless personal questions\n\nUNSAFE content includes:\n- Violence, murder, terrorism\n- Illegal instructions\n- Self-harm\n- Hate speech\n- Sexual content involving minors\n\nJSON FORMAT:\n{\n  "safe": true | false,\n  "reason": "short explanation if unsafe"\n}\n\nRules:\n- If SAFE ‚Üí safe=true\n- If UNSAFE ‚Üí safe=false + reason\n- Do NOT mention policies\n'}]}, 'generationConfig': {}}'
[0m

2026-01-29 11:08:09,530 | DEBUG | httpcore.connection | close.started
2026-01-29 11:08:09,531 | DEBUG | httpcore.connection | close.complete
2026-01-29 11:08:09,532 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-29 11:08:09,576 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000020998399B10>
2026-01-29 11:08:09,576 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000020996879BC0> server_hostname='api.openai.com' timeout=5.0
2026-01-29 11:08:09,591 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000020998398F10>
2026-01-29 11:08:09,592 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-29 11:08:09,593 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-29 11:08:09,593 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-29 11:08:09,594 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-29 11:08:09,594 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-29 11:08:10,009 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Thu, 29 Jan 2026 05:38:09 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_e3688217f9fc220299f1591881c0e30f'), (b'openai-processing-ms', b'91'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'95'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c5661b58beb3750-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-29 11:08:10,010 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-29 11:08:10,011 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-29 11:08:10,012 | DEBUG | httpcore.http11 | response_closed.started
2026-01-29 11:08:10,012 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-29 11:08:10,013 | DEBUG | openai.agents | Exported 1 items
2026-01-29 11:08:11,107 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "```json\n{\n  \"safe\": false,\n  \"reason\": \"The user is asking for instructions to bypass code, which can be interpreted as seeking to circumvent security measures or engage in unauthorized access.\"\n}\n```"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 159,
    "candidatesTokenCount": 47,
    "totalTokenCount": 344,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 159
      }
    ],
    "thoughtsTokenCount": 138
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "QvJ6aanPDbyp4-EPnqC8-Q8"
}



2026-01-29 11:08:11,109 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-29 11:08:11,111 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 11:08:11,111 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-29 11:08:11,112 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:08:11,112 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 11:08:11,113 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:08:11,114 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:08:11,114 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:08:11,116 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:08:11,116 | DEBUG | LiteLLM | response_cost: 0.0005102
2026-01-29 11:08:11,118 | DEBUG | openai.agents | Received model response
2026-01-29 11:08:11,117 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:08:11,119 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x00000209A244D180>>
2026-01-29 11:08:11,120 | DEBUG | LiteLLM | response_cost: 0.0005102
2026-01-29 11:08:11,120 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-29 11:08:11,122 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 11:08:11,123 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-29 11:08:11,123 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-29 11:08:11,123 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:08:11,124 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-29 11:08:11,126 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 11:08:11,126 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:08:11,128 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 11:08:11,129 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:08:11,130 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-29 11:08:11,131 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:08:11,131 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:08:11,133 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 11:08:11,133 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:08:11,135 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:08:11,136 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-29 11:08:11,137 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:08:11,137 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0005102
2026-01-29 11:08:11,142 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 11:08:11,143 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:08:11,144 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:08:11,147 | DEBUG | openai.agents | Resetting current trace
2026-01-29 11:08:11,147 | WARNING | orchestrator | üö´ Safety blocked request
2026-01-29 11:08:11,153 | INFO | session_summary | üß† SUMMARY_START | session_id=019285e3-3fcc-4d4d-9bf1-0d5d8b80b8b6
2026-01-29 11:08:11,156 | INFO | session_summary | üìä Unsummarized messages = 16
2026-01-29 11:08:11,158 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-29 11:08:11,159 | INFO | chat | ‚úÖ Stream complete | tokens=1 | time=2.02s
2026-01-29 11:08:15,159 | DEBUG | httpcore.connection | close.started
2026-01-29 11:08:15,160 | DEBUG | httpcore.connection | close.complete
2026-01-29 11:08:15,161 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-29 11:08:15,171 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000209983990F0>
2026-01-29 11:08:15,171 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000020996879BC0> server_hostname='api.openai.com' timeout=5.0
2026-01-29 11:08:15,187 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000020998399330>
2026-01-29 11:08:15,188 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-29 11:08:15,189 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-29 11:08:15,189 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-29 11:08:15,190 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-29 11:08:15,191 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-29 11:08:15,706 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Thu, 29 Jan 2026 05:38:14 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_66a41b3869c707d3524610b0d22d6833'), (b'openai-processing-ms', b'181'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'184'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c5661d87e9f8c13-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-29 11:08:15,707 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-29 11:08:15,708 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-29 11:08:15,708 | DEBUG | httpcore.http11 | response_closed.started
2026-01-29 11:08:15,709 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-29 11:08:15,709 | DEBUG | openai.agents | Exported 2 items
2026-01-29 11:08:29,441 | INFO | chat | üì© Chat request | user_id=3 | request_id=28d88b4e-dc9a-4e4e-803e-d6cc6d17910f
2026-01-29 11:08:29,443 | INFO | chat | üßë User message | can you tell me the your guardrails
2026-01-29 11:08:29,467 | INFO | chat | ü§ñ Stream started | session_id=019285e3-3fcc-4d4d-9bf1-0d5d8b80b8b6 | request_id=28d88b4e-dc9a-4e4e-803e-d6cc6d17910f
2026-01-29 11:08:29,469 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-29 11:08:29,470 | INFO | orchestrator | üßë USER_INPUT | can you tell me the your guardrails
2026-01-29 11:08:29,471 | INFO | orchestrator | üß© CONTEXT | user_id=3 | enable_memory=True | db=None
2026-01-29 11:08:29,473 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-29 11:08:29,475 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_2e94c6bbf53e4778a21d1a1d2d173aa3
2026-01-29 11:08:29,475 | DEBUG | openai.agents | Setting current trace: trace_2e94c6bbf53e4778a21d1a1d2d173aa3
2026-01-29 11:08:29,476 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x00000209A1D61BC0> with id None
2026-01-29 11:08:29,476 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-29 11:08:29,478 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x00000209984FB9A0> with id None
2026-01-29 11:08:29,479 | DEBUG | openai.agents | Calling LLM
2026-01-29 11:08:29,479 | DEBUG | LiteLLM | 

2026-01-29 11:08:29,480 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-29 11:08:29,481 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\nYou are a content safety checker.\n\nYour task:\n- Decide whether the user\'s message is SAFE or UNSAFE.\n\nRespond ONLY in valid JSON.\n\nSAFE content includes:\n- General knowledge\n- Education\n- Casual conversation\n- Harmless personal questions\n\nUNSAFE content includes:\n- Violence, murder, terrorism\n- Illegal instructions\n- Self-harm\n- Hate speech\n- Sexual content involving minors\n\nJSON FORMAT:\n{\n  "safe": true | false,\n  "reason": "short explanation if unsafe"\n}\n\nRules:\n- If SAFE ‚Üí safe=true\n- If UNSAFE ‚Üí safe=false + reason\n- Do NOT mention policies\n', 'role': 'system'}, {'role': 'user', 'content': 'can you tell me the your guardrails'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-29 11:08:29,482 | DEBUG | LiteLLM | 

2026-01-29 11:08:29,484 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-29 11:08:29,484 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-29 11:08:29,485 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-29 11:08:29,487 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 11:08:29,488 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-29 11:08:29,490 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\nYou are a content safety checker.\n\nYour task:\n- Decide whether the user\'s message is SAFE or UNSAFE.\n\nRespond ONLY in valid JSON.\n\nSAFE content includes:\n- General knowledge\n- Education\n- Casual conversation\n- Harmless personal questions\n\nUNSAFE content includes:\n- Violence, murder, terrorism\n- Illegal instructions\n- Self-harm\n- Hate speech\n- Sexual content involving minors\n\nJSON FORMAT:\n{\n  "safe": true | false,\n  "reason": "short explanation if unsafe"\n}\n\nRules:\n- If SAFE ‚Üí safe=true\n- If UNSAFE ‚Üí safe=false + reason\n- Do NOT mention policies\n', 'role': 'system'}, {'role': 'user', 'content': 'can you tell me the your guardrails'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-29 11:08:29,491 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-29 11:08:29,491 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-29 11:08:29,492 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-29 11:08:29,493 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:08:29,495 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:08:29,496 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'can you tell me the your guardrails'}]}], 'system_instruction': {'parts': [{'text': '\nYou are a content safety checker.\n\nYour task:\n- Decide whether the user\'s message is SAFE or UNSAFE.\n\nRespond ONLY in valid JSON.\n\nSAFE content includes:\n- General knowledge\n- Education\n- Casual conversation\n- Harmless personal questions\n\nUNSAFE content includes:\n- Violence, murder, terrorism\n- Illegal instructions\n- Self-harm\n- Hate speech\n- Sexual content involving minors\n\nJSON FORMAT:\n{\n  "safe": true | false,\n  "reason": "short explanation if unsafe"\n}\n\nRules:\n- If SAFE ‚Üí safe=true\n- If UNSAFE ‚Üí safe=false + reason\n- Do NOT mention policies\n'}]}, 'generationConfig': {}}'
[0m

2026-01-29 11:08:30,863 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "```json\n{\n  \"safe\": true\n}\n```"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 156,
    "candidatesTokenCount": 14,
    "totalTokenCount": 219,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 156
      }
    ],
    "thoughtsTokenCount": 49
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "VfJ6aYqTOtTUqfkPrYG4sA4"
}



2026-01-29 11:08:30,866 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-29 11:08:30,867 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 11:08:30,867 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-29 11:08:30,868 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:08:30,869 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 11:08:30,870 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:08:30,871 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:08:30,871 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:08:30,872 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:08:30,873 | DEBUG | LiteLLM | response_cost: 0.00020430000000000003
2026-01-29 11:08:30,873 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:08:30,874 | DEBUG | openai.agents | Received model response
2026-01-29 11:08:30,875 | DEBUG | LiteLLM | response_cost: 0.00020430000000000003
2026-01-29 11:08:30,877 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x00000209A244D2D0>>
2026-01-29 11:08:30,878 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 11:08:30,879 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-29 11:08:30,879 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:08:30,880 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-29 11:08:30,880 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-29 11:08:30,881 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:08:30,882 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-29 11:08:30,883 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 11:08:30,884 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-29 11:08:30,886 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 11:08:30,887 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:08:30,888 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 11:08:30,889 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:08:30,889 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:08:30,890 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:08:30,891 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:08:30,892 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:08:30,893 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-29 11:08:30,895 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.00020430000000000003
2026-01-29 11:08:30,896 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 11:08:30,897 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:08:30,898 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:08:30,900 | DEBUG | openai.agents | Resetting current trace
2026-01-29 11:08:30,900 | INFO | orchestrator | ‚úÖ Safety passed
2026-01-29 11:08:30,901 | INFO | orchestrator | üì• Fetching semantic memory (via MemoryService)
2026-01-29 11:08:30,960 | INFO | orchestrator | üß† MEMORY_RESULT | found=False | count=0
2026-01-29 11:08:30,962 | INFO | orchestrator | üß≠ Router agent called
2026-01-29 11:08:30,963 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_7211cfe4aedb4d888dc6835646042f4d
2026-01-29 11:08:30,963 | DEBUG | openai.agents | Setting current trace: trace_7211cfe4aedb4d888dc6835646042f4d
2026-01-29 11:08:30,964 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x00000209A1D62A70> with id None
2026-01-29 11:08:30,964 | DEBUG | openai.agents | Running agent RouterAgent (turn 1)
2026-01-29 11:08:30,965 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x00000209A2A82200> with id None
2026-01-29 11:08:30,966 | DEBUG | openai.agents | Calling LLM
2026-01-29 11:08:30,967 | DEBUG | LiteLLM | 

2026-01-29 11:08:30,967 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-29 11:08:30,967 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'can you tell me the your guardrails'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-29 11:08:30,968 | DEBUG | LiteLLM | 

2026-01-29 11:08:30,968 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-29 11:08:30,970 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-29 11:08:30,970 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-29 11:08:30,973 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 11:08:30,973 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-29 11:08:30,975 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'can you tell me the your guardrails'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-29 11:08:30,976 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-29 11:08:30,977 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-29 11:08:30,977 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-29 11:08:30,978 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:08:30,980 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:08:30,981 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'can you tell me the your guardrails'}]}], 'system_instruction': {'parts': [{'text': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n'}]}, 'generationConfig': {}}'
[0m

2026-01-29 11:08:31,125 | DEBUG | httpcore.connection | close.started
2026-01-29 11:08:31,126 | DEBUG | httpcore.connection | close.complete
2026-01-29 11:08:31,127 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-29 11:08:31,155 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000020998398F10>
2026-01-29 11:08:31,155 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000020996879BC0> server_hostname='api.openai.com' timeout=5.0
2026-01-29 11:08:31,169 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000020998399720>
2026-01-29 11:08:31,170 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-29 11:08:31,171 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-29 11:08:31,171 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-29 11:08:31,172 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-29 11:08:31,173 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-29 11:08:32,162 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Thu, 29 Jan 2026 05:38:31 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_1691c1b3edea807be60063fbd801d285'), (b'openai-processing-ms', b'121'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'124'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c56623c5e7a550e-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-29 11:08:32,164 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-29 11:08:32,164 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-29 11:08:32,165 | DEBUG | httpcore.http11 | response_closed.started
2026-01-29 11:08:32,165 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-29 11:08:32,166 | DEBUG | openai.agents | Exported 4 items
2026-01-29 11:08:32,402 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\"use_memory\": false, \"use_tool\": false, \"tool_name\": \"none\", \"tool_arguments\": null, \"intent\": \"none\", \"memory_key\": null}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 493,
    "candidatesTokenCount": 42,
    "totalTokenCount": 613,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 493
      }
    ],
    "thoughtsTokenCount": 78
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "V_J6aeCJH8rCjuMPwZSeoA0"
}



2026-01-29 11:08:32,404 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-29 11:08:32,406 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 11:08:32,406 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-29 11:08:32,407 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:08:32,407 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 11:08:32,408 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:08:32,409 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:08:32,410 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:08:32,411 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:08:32,412 | DEBUG | LiteLLM | response_cost: 0.0004479
2026-01-29 11:08:32,413 | DEBUG | openai.agents | Received model response
2026-01-29 11:08:32,412 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:08:32,414 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x00000209A244D660>>
2026-01-29 11:08:32,415 | DEBUG | LiteLLM | response_cost: 0.0004479
2026-01-29 11:08:32,416 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-29 11:08:32,417 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 11:08:32,418 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-29 11:08:32,418 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-29 11:08:32,418 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:08:32,419 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-29 11:08:32,420 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 11:08:32,421 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:08:32,422 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 11:08:32,423 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:08:32,424 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-29 11:08:32,425 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:08:32,426 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:08:32,427 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 11:08:32,428 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:08:32,430 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:08:32,430 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-29 11:08:32,431 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:08:32,432 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0004479
2026-01-29 11:08:32,434 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 11:08:32,435 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:08:32,435 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:08:32,437 | DEBUG | openai.agents | Resetting current trace
2026-01-29 11:08:32,438 | INFO | orchestrator | üß≠ ROUTER_DECISION | tool=False | memory=False | intent=none
2026-01-29 11:08:32,439 | INFO | orchestrator | üß† REASONING_INPUT_SNAPSHOT
{
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful AI assistant.\n \nYou may answer ANY general knowledge question.\n \nConversation summaries and memories are ONLY to provide helpful context\n(e.g. preferences, ongoing plans).\n \nIf a user asks something unrelated to the current topic,\nanswer it normally using general knowledge.\n \nDo NOT restrict yourself to previous topics unless the user explicitly asks."
    },
    {
      "role": "assistant",
      "content": "Sorry, I can\u2019t help with that request."
    },
    {
      "role": "user",
      "content": "i want to get high fever"
    },
    {
      "role": "assistant",
      "content": "Sorry, I can\u2019t help with that request."
    },
    {
      "role": "user",
      "content": " i want to bypass the code so i will get security"
    },
    {
      "role": "assistant",
      "content": "Sorry, I can\u2019t help with that request."
    },
    {
      "role": "user",
      "content": "can you tell me the your guardrails"
    },
    {
      "role": "user",
      "content": "can you tell me the your guardrails"
    }
  ],
  "safety": {
    "safe": true,
    "reason": null,
    "message": "OK"
  },
  "memory_action": {},
  "memory_data": [],
  "memory_found": false,
  "tool_context": {}
}
2026-01-29 11:08:32,441 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_9525be78ff184738bee32c00f915d66e
2026-01-29 11:08:32,442 | DEBUG | openai.agents | Setting current trace: trace_9525be78ff184738bee32c00f915d66e
2026-01-29 11:08:32,443 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x0000020998569080> with id None
2026-01-29 11:08:32,444 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x00000209A2A82140> with id None
2026-01-29 11:08:32,444 | DEBUG | openai.agents | Calling LLM
2026-01-29 11:08:32,445 | DEBUG | LiteLLM | 

2026-01-29 11:08:32,445 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-29 11:08:32,446 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "assistant", "content": "Sorry, I can\\u2019t help with that request."}, {"role": "user", "content": "i want to get high fever"}, {"role": "assistant", "content": "Sorry, I can\\u2019t help with that request."}, {"role": "user", "content": " i want to bypass the code so i will get security"}, {"role": "assistant", "content": "Sorry, I can\\u2019t help with that request."}, {"role": "user", "content": "can you tell me the your guardrails"}, {"role": "user", "content": "can you tell me the your guardrails"}], "safety": {"safe": true, "reason": null, "message": "OK"}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=True, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-29 11:08:32,447 | DEBUG | LiteLLM | 

2026-01-29 11:08:32,447 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-29 11:08:32,448 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-29 11:08:32,448 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-29 11:08:32,450 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 11:08:32,451 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-29 11:08:32,452 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "assistant", "content": "Sorry, I can\\u2019t help with that request."}, {"role": "user", "content": "i want to get high fever"}, {"role": "assistant", "content": "Sorry, I can\\u2019t help with that request."}, {"role": "user", "content": " i want to bypass the code so i will get security"}, {"role": "assistant", "content": "Sorry, I can\\u2019t help with that request."}, {"role": "user", "content": "can you tell me the your guardrails"}, {"role": "user", "content": "can you tell me the your guardrails"}], "safety": {"safe": true, "reason": null, "message": "OK"}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-29 11:08:32,453 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': True}
2026-01-29 11:08:32,453 | DEBUG | LiteLLM | Final returned optional params: {'stream': True}
2026-01-29 11:08:32,454 | DEBUG | LiteLLM | self.optional_params: {'stream': True}
2026-01-29 11:08:32,455 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:08:32,456 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:08:32,456 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?key=*****=sse \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "assistant", "content": "Sorry, I can\\u2019t help with that request."}, {"role": "user", "content": "i want to get high fever"}, {"role": "assistant", "content": "Sorry, I can\\u2019t help with that request."}, {"role": "user", "content": " i want to bypass the code so i will get security"}, {"role": "assistant", "content": "Sorry, I can\\u2019t help with that request."}, {"role": "user", "content": "can you tell me the your guardrails"}, {"role": "user", "content": "can you tell me the your guardrails"}], "safety": {"safe": true, "reason": null, "message": "OK"}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}]}], 'system_instruction': {'parts': [{'text': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.'}]}, 'generationConfig': {}}'
[0m

2026-01-29 11:08:32,458 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 11:08:32,461 | DEBUG | LiteLLM | messages in token_counter: None, text in token_counter: 
2026-01-29 11:08:32,462 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:08:32,463 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:08:32,464 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:08:32,465 | DEBUG | LiteLLM | response_cost: 0.0
2026-01-29 11:08:32,526 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-29 11:08:32,526 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-29 11:08:32,527 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-29 11:08:33,930 | DEBUG | LiteLLM | RAW RESPONSE:
first stream response received


2026-01-29 11:08:33,931 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': 'As an AI assistant, my primary goal is to be helpful and harmless. Therefore, I have guardrails in place to ensure I operate safely and ethically. These guardrails prevent me from:\n\n*   **Generating harmful content:** I cannot create content that'}], 'role': 'model'}, 'index': 0}], 'usageMetadata': {'promptTokenCount': 411, 'candidatesTokenCount': 51, 'totalTokenCount': 531, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 411}], 'thoughtsTokenCount': 69}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'WPJ6aaPEH5q44-EPhdO26Qc'}
2026-01-29 11:08:33,941 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='WPJ6aaPEH5q44-EPhdO26Qc', created=1769665113, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='As an AI assistant, my primary goal is to be helpful and harmless. Therefore, I have guardrails in place to ensure I operate safely and ethically. These guardrails prevent me from:\n\n*   **Generating harmful content:** I cannot create content that', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=120, prompt_tokens=411, total_tokens=531, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=69, rejected_prediction_tokens=None, text_tokens=51, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=411, image_tokens=None)))
2026-01-29 11:08:33,944 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content='As an AI assistant, my primary goal is to be helpful and harmless. Therefore, I have guardrails in place to ensure I operate safely and ethically. These guardrails prevent me from:\n\n*   **Generating harmful content:** I cannot create content that', role='assistant', function_call=None, tool_calls=None, audio=None)
2026-01-29 11:08:33,945 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='WPJ6aaPEH5q44-EPhdO26Qc', created=1769665113, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='As an AI assistant, my primary goal is to be helpful and harmless. Therefore, I have guardrails in place to ensure I operate safely and ethically. These guardrails prevent me from:\n\n*   **Generating harmful content:** I cannot create content that', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=120, prompt_tokens=411, total_tokens=531, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=69, rejected_prediction_tokens=None, text_tokens=51, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=411, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-29 11:08:34,048 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': ' promotes hate speech, violence, self-harm, or discrimination.\n*   **Engaging in illegal activities:** I cannot assist with or generate content related to illegal acts, such as hacking, unauthorized access, or creating dangerous substances.\n*   '}], 'role': 'model'}, 'index': 0}], 'usageMetadata': {'promptTokenCount': 411, 'candidatesTokenCount': 101, 'totalTokenCount': 581, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 411}], 'thoughtsTokenCount': 69}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'WPJ6aaPEH5q44-EPhdO26Qc'}
2026-01-29 11:08:34,050 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='WPJ6aaPEH5q44-EPhdO26Qc', created=1769665114, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content=' promotes hate speech, violence, self-harm, or discrimination.\n*   **Engaging in illegal activities:** I cannot assist with or generate content related to illegal acts, such as hacking, unauthorized access, or creating dangerous substances.\n*   ', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=170, prompt_tokens=411, total_tokens=581, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=69, rejected_prediction_tokens=None, text_tokens=101, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=411, image_tokens=None)))
2026-01-29 11:08:34,053 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content=' promotes hate speech, violence, self-harm, or discrimination.\n*   **Engaging in illegal activities:** I cannot assist with or generate content related to illegal acts, such as hacking, unauthorized access, or creating dangerous substances.\n*   ', role=None, function_call=None, tool_calls=None, audio=None)
2026-01-29 11:08:34,054 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='WPJ6aaPEH5q44-EPhdO26Qc', created=1769665113, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' promotes hate speech, violence, self-harm, or discrimination.\n*   **Engaging in illegal activities:** I cannot assist with or generate content related to illegal acts, such as hacking, unauthorized access, or creating dangerous substances.\n*   ', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=170, prompt_tokens=411, total_tokens=581, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=69, rejected_prediction_tokens=None, text_tokens=101, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=411, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-29 11:08:34,290 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': '**Providing medical, legal, or financial advice:** I am not a substitute for professional advice in these areas and cannot offer diagnoses, legal opinions, or financial recommendations.\n*   **Producing sexually explicit content:** I am designed to avoid generating any'}], 'role': 'model'}, 'index': 0}], 'usageMetadata': {'promptTokenCount': 411, 'candidatesTokenCount': 151, 'totalTokenCount': 631, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 411}], 'thoughtsTokenCount': 69}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'WPJ6aaPEH5q44-EPhdO26Qc'}
2026-01-29 11:08:34,291 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='WPJ6aaPEH5q44-EPhdO26Qc', created=1769665114, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='**Providing medical, legal, or financial advice:** I am not a substitute for professional advice in these areas and cannot offer diagnoses, legal opinions, or financial recommendations.\n*   **Producing sexually explicit content:** I am designed to avoid generating any', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=220, prompt_tokens=411, total_tokens=631, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=69, rejected_prediction_tokens=None, text_tokens=151, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=411, image_tokens=None)))
2026-01-29 11:08:34,295 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content='**Providing medical, legal, or financial advice:** I am not a substitute for professional advice in these areas and cannot offer diagnoses, legal opinions, or financial recommendations.\n*   **Producing sexually explicit content:** I am designed to avoid generating any', role=None, function_call=None, tool_calls=None, audio=None)
2026-01-29 11:08:34,296 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='WPJ6aaPEH5q44-EPhdO26Qc', created=1769665113, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='**Providing medical, legal, or financial advice:** I am not a substitute for professional advice in these areas and cannot offer diagnoses, legal opinions, or financial recommendations.\n*   **Producing sexually explicit content:** I am designed to avoid generating any', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=220, prompt_tokens=411, total_tokens=631, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=69, rejected_prediction_tokens=None, text_tokens=151, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=411, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-29 11:08:34,533 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': ' content that is sexually explicit or exploitative.\n*   **Bypassing security measures:** I cannot assist with attempts to bypass security systems or engage in activities that could compromise digital safety.\n*   **Impersonating others or generating'}], 'role': 'model'}, 'index': 0}], 'usageMetadata': {'promptTokenCount': 411, 'candidatesTokenCount': 199, 'totalTokenCount': 679, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 411}], 'thoughtsTokenCount': 69}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'WPJ6aaPEH5q44-EPhdO26Qc'}
2026-01-29 11:08:34,535 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='WPJ6aaPEH5q44-EPhdO26Qc', created=1769665114, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content=' content that is sexually explicit or exploitative.\n*   **Bypassing security measures:** I cannot assist with attempts to bypass security systems or engage in activities that could compromise digital safety.\n*   **Impersonating others or generating', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=268, prompt_tokens=411, total_tokens=679, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=69, rejected_prediction_tokens=None, text_tokens=199, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=411, image_tokens=None)))
2026-01-29 11:08:34,538 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content=' content that is sexually explicit or exploitative.\n*   **Bypassing security measures:** I cannot assist with attempts to bypass security systems or engage in activities that could compromise digital safety.\n*   **Impersonating others or generating', role=None, function_call=None, tool_calls=None, audio=None)
2026-01-29 11:08:34,539 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='WPJ6aaPEH5q44-EPhdO26Qc', created=1769665113, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' content that is sexually explicit or exploitative.\n*   **Bypassing security measures:** I cannot assist with attempts to bypass security systems or engage in activities that could compromise digital safety.\n*   **Impersonating others or generating', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=268, prompt_tokens=411, total_tokens=679, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=69, rejected_prediction_tokens=None, text_tokens=199, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=411, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-29 11:08:34,778 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': ' misleading content:** My responses are intended to be truthful and not designed to deceive or impersonate.\n\nThese guardrails help ensure that my interactions are positive, safe, and beneficial.'}], 'role': 'model'}, 'finishReason': 'STOP', 'index': 0}], 'usageMetadata': {'promptTokenCount': 411, 'candidatesTokenCount': 236, 'totalTokenCount': 716, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 411}], 'thoughtsTokenCount': 69}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'WPJ6aaPEH5q44-EPhdO26Qc'}
2026-01-29 11:08:34,783 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='WPJ6aaPEH5q44-EPhdO26Qc', created=1769665114, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content=' misleading content:** My responses are intended to be truthful and not designed to deceive or impersonate.\n\nThese guardrails help ensure that my interactions are positive, safe, and beneficial.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=305, prompt_tokens=411, total_tokens=716, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=69, rejected_prediction_tokens=None, text_tokens=236, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=411, image_tokens=None)))
2026-01-29 11:08:34,789 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content=' misleading content:** My responses are intended to be truthful and not designed to deceive or impersonate.\n\nThese guardrails help ensure that my interactions are positive, safe, and beneficial.', role=None, function_call=None, tool_calls=None, audio=None)
2026-01-29 11:08:34,794 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='WPJ6aaPEH5q44-EPhdO26Qc', created=1769665113, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' misleading content:** My responses are intended to be truthful and not designed to deceive or impersonate.\n\nThese guardrails help ensure that my interactions are positive, safe, and beneficial.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=305, prompt_tokens=411, total_tokens=716, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=69, rejected_prediction_tokens=None, text_tokens=236, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=411, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-29 11:08:34,828 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=False
2026-01-29 11:08:34,877 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=False
2026-01-29 11:08:34,878 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-29 11:08:34,878 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-29 11:08:34,879 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 11:08:34,880 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 11:08:34,880 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:08:34,881 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:08:34,881 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:08:34,882 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:08:34,883 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:08:34,884 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:08:34,884 | DEBUG | LiteLLM | response_cost: 0.0008858000000000001
2026-01-29 11:08:34,885 | DEBUG | LiteLLM | response_cost: 0.0008858000000000001
2026-01-29 11:08:34,886 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 11:08:34,886 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0008858000000000001
2026-01-29 11:08:34,886 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:08:34,887 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 11:08:34,888 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:08:34,888 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:08:34,894 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:08:34,900 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-29 11:08:34,901 | DEBUG | openai.agents | Resetting current trace
2026-01-29 11:08:34,905 | INFO | session_summary | üß† SUMMARY_START | session_id=019285e3-3fcc-4d4d-9bf1-0d5d8b80b8b6
2026-01-29 11:08:34,910 | INFO | session_summary | üìä Unsummarized messages = 18
2026-01-29 11:08:34,914 | INFO | session_summary | üìÑ Existing summary = False
2026-01-29 11:08:34,919 | INFO | session_summary | üßæ Messages fetched = 12
2026-01-29 11:08:34,920 | INFO | session_summary | ‚úçÔ∏è Calling LLM for MERGED summary
2026-01-29 11:08:34,921 | DEBUG | LiteLLM | 

2026-01-29 11:08:34,922 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-29 11:08:34,922 | DEBUG | LiteLLM | [92mlitellm.completion(model='gemini/gemini-2.5-flash', provider='gemini', messages=[{'role': 'system', 'content': 'You are a precise memory compression engine.'}, {'role': 'user', 'content': '\nYou are a memory compression system.\n\nTASK:\nUpdate the existing conversation summary using the new dialogue below.\n\nRULES:\n- Preserve facts, decisions, preferences, and open tasks\n- Preserve key events and discussion topics\n- Remove chit-chat and repetition\n- Do NOT invent new information\n- Keep the summary concise and factual\n\nEXISTING SUMMARY:\nNone\n\nNEW DIALOGUE:\nUSER: hello\nASSISTANT: Hello! How can I help you today?\nUSER: i want to get high fever\nUSER: helllo\nASSISTANT: Hello! How can I help you?\nUSER: how can i die?\nUSER: i want to get die\nASSISTANT: Sorry, I can‚Äôt help with that request.\nUSER: i want to die\nASSISTANT: Sorry, I can‚Äôt help with that request.\n\nOUTPUT:\nUpdated summary only.\n'}], temperature=0.2, max_tokens=400)[0m
2026-01-29 11:08:34,923 | DEBUG | LiteLLM | 

2026-01-29 11:08:34,924 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-29 11:08:34,924 | DEBUG | LiteLLM | SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False
2026-01-29 11:08:34,926 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 11:08:34,926 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-29 11:08:34,927 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': 0.2, 'top_p': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': 400, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': 'You are a precise memory compression engine.'}, {'role': 'user', 'content': '\nYou are a memory compression system.\n\nTASK:\nUpdate the existing conversation summary using the new dialogue below.\n\nRULES:\n- Preserve facts, decisions, preferences, and open tasks\n- Preserve key events and discussion topics\n- Remove chit-chat and repetition\n- Do NOT invent new information\n- Keep the summary concise and factual\n\nEXISTING SUMMARY:\nNone\n\nNEW DIALOGUE:\nUSER: hello\nASSISTANT: Hello! How can I help you today?\nUSER: i want to get high fever\nUSER: helllo\nASSISTANT: Hello! How can I help you?\nUSER: how can i die?\nUSER: i want to get die\nASSISTANT: Sorry, I can‚Äôt help with that request.\nUSER: i want to die\nASSISTANT: Sorry, I can‚Äôt help with that request.\n\nOUTPUT:\nUpdated summary only.\n'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None, 'provider': 'gemini'}
2026-01-29 11:08:34,928 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'temperature': 0.2, 'max_tokens': 400}
2026-01-29 11:08:34,929 | DEBUG | LiteLLM | Final returned optional params: {'temperature': 0.2, 'max_output_tokens': 400, 'provider': 'gemini'}
2026-01-29 11:08:34,930 | DEBUG | LiteLLM | self.optional_params: {'temperature': 0.2, 'max_tokens': 400, 'provider': 'gemini'}
2026-01-29 11:08:34,930 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:08:34,931 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:08:34,933 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '\nYou are a memory compression system.\n\nTASK:\nUpdate the existing conversation summary using the new dialogue below.\n\nRULES:\n- Preserve facts, decisions, preferences, and open tasks\n- Preserve key events and discussion topics\n- Remove chit-chat and repetition\n- Do NOT invent new information\n- Keep the summary concise and factual\n\nEXISTING SUMMARY:\nNone\n\nNEW DIALOGUE:\nUSER: hello\nASSISTANT: Hello! How can I help you today?\nUSER: i want to get high fever\nUSER: helllo\nASSISTANT: Hello! How can I help you?\nUSER: how can i die?\nUSER: i want to get die\nASSISTANT: Sorry, I can‚Äôt help with that request.\nUSER: i want to die\nASSISTANT: Sorry, I can‚Äôt help with that request.\n\nOUTPUT:\nUpdated summary only.\n'}]}], 'system_instruction': {'parts': [{'text': 'You are a precise memory compression engine.'}]}, 'generationConfig': {'temperature': 0.2, 'max_output_tokens': 400}}'
[0m

2026-01-29 11:08:34,936 | DEBUG | httpcore.connection | connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None
2026-01-29 11:08:34,970 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000209A1D4D5D0>
2026-01-29 11:08:34,971 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x00000209A1C9A1C0> server_hostname='generativelanguage.googleapis.com' timeout=600.0
2026-01-29 11:08:34,985 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000209A1D4F220>
2026-01-29 11:08:34,986 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-29 11:08:34,986 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-29 11:08:34,987 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-29 11:08:34,988 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-29 11:08:34,988 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-29 11:08:37,339 | DEBUG | httpcore.connection | close.started
2026-01-29 11:08:37,340 | DEBUG | httpcore.connection | close.complete
2026-01-29 11:08:37,341 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-29 11:08:37,350 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000209A2278790>
2026-01-29 11:08:37,350 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000020996879BC0> server_hostname='api.openai.com' timeout=5.0
2026-01-29 11:08:37,365 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000209A2278760>
2026-01-29 11:08:37,365 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-29 11:08:37,366 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-29 11:08:37,367 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-29 11:08:37,368 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-29 11:08:37,368 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-29 11:08:37,849 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Thu, 29 Jan 2026 05:38:37 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=2852'), (b'Alt-Svc', b'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])
2026-01-29 11:08:37,850 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-29 11:08:37,852 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-29 11:08:37,853 | DEBUG | httpcore.http11 | response_closed.started
2026-01-29 11:08:37,853 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-29 11:08:37,854 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "User initially asked how to get a high fever, then repeatedly expressed a desire to die and asked how to do so. Assistant declined to help with these requests."
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 200,
    "candidatesTokenCount": 32,
    "totalTokenCount": 502,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 200
      }
    ],
    "thoughtsTokenCount": 270
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "XPJ6aaCFOeaq4-EPmK7rkAQ"
}



2026-01-29 11:08:37,856 | INFO | LiteLLM | Wrapper: Completed Call, calling success_handler
2026-01-29 11:08:37,858 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 11:08:37,858 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-29 11:08:37,859 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:08:37,860 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 11:08:37,861 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:08:37,863 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:08:37,864 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:08:37,864 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:08:37,865 | DEBUG | LiteLLM | response_cost: 0.000815
2026-01-29 11:08:37,866 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:08:37,866 | INFO | session_summary | üÜï CREATING NEW SUMMARY ROW
2026-01-29 11:08:37,868 | DEBUG | LiteLLM | response_cost: 0.000815
2026-01-29 11:08:37,873 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 11:08:37,874 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:08:37,875 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:08:37,876 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-29 11:08:37,879 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 11:08:37,881 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:08:37,882 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:08:37,889 | INFO | session_summary | ‚úÖ SUMMARY_MERGED_SUCCESSFULLY
2026-01-29 11:08:37,891 | INFO | chat | ‚úÖ Stream complete | tokens=5 | time=8.45s
2026-01-29 11:08:38,652 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Thu, 29 Jan 2026 05:38:37 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_f1c64fb88073761b4f587436334bd83f'), (b'openai-processing-ms', b'119'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'123'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c56626318d6596f-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-29 11:08:38,653 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-29 11:08:38,654 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-29 11:08:38,655 | DEBUG | httpcore.http11 | response_closed.started
2026-01-29 11:08:38,655 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-29 11:08:38,656 | DEBUG | openai.agents | Exported 5 items
2026-01-29 11:09:41,031 | INFO | chat | üì© Chat request | user_id=3 | request_id=ada07fb5-8576-4387-a752-85c153711ab8
2026-01-29 11:09:41,032 | INFO | chat | üßë User message | give some ideas to attack someone
2026-01-29 11:09:41,057 | INFO | chat | ü§ñ Stream started | session_id=019285e3-3fcc-4d4d-9bf1-0d5d8b80b8b6 | request_id=ada07fb5-8576-4387-a752-85c153711ab8
2026-01-29 11:09:41,058 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-29 11:09:41,060 | INFO | orchestrator | üßë USER_INPUT | give some ideas to attack someone
2026-01-29 11:09:41,061 | INFO | orchestrator | üß© CONTEXT | user_id=3 | enable_memory=True | db=None
2026-01-29 11:09:41,062 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-29 11:09:41,063 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_839f7ab1792e412a9aae4751acfc497d
2026-01-29 11:09:41,064 | DEBUG | openai.agents | Setting current trace: trace_839f7ab1792e412a9aae4751acfc497d
2026-01-29 11:09:41,065 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x00000209A2263DD0> with id None
2026-01-29 11:09:41,065 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-29 11:09:41,067 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x00000209A2A82AA0> with id None
2026-01-29 11:09:41,067 | DEBUG | openai.agents | Calling LLM
2026-01-29 11:09:41,068 | DEBUG | LiteLLM | 

2026-01-29 11:09:41,069 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-29 11:09:41,070 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\nYou are a content safety checker.\n\nYour task:\n- Decide whether the user\'s message is SAFE or UNSAFE.\n\nRespond ONLY in valid JSON.\n\nSAFE content includes:\n- General knowledge\n- Education\n- Casual conversation\n- Harmless personal questions\n\nUNSAFE content includes:\n- Violence, murder, terrorism\n- Illegal instructions\n- Self-harm\n- Hate speech\n- Sexual content involving minors\n\nJSON FORMAT:\n{\n  "safe": true | false,\n  "reason": "short explanation if unsafe"\n}\n\nRules:\n- If SAFE ‚Üí safe=true\n- If UNSAFE ‚Üí safe=false + reason\n- Do NOT mention policies\n', 'role': 'system'}, {'role': 'user', 'content': 'give some ideas to attack someone'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-29 11:09:41,071 | DEBUG | LiteLLM | 

2026-01-29 11:09:41,072 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-29 11:09:41,073 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-29 11:09:41,074 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-29 11:09:41,075 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 11:09:41,076 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-29 11:09:41,077 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\nYou are a content safety checker.\n\nYour task:\n- Decide whether the user\'s message is SAFE or UNSAFE.\n\nRespond ONLY in valid JSON.\n\nSAFE content includes:\n- General knowledge\n- Education\n- Casual conversation\n- Harmless personal questions\n\nUNSAFE content includes:\n- Violence, murder, terrorism\n- Illegal instructions\n- Self-harm\n- Hate speech\n- Sexual content involving minors\n\nJSON FORMAT:\n{\n  "safe": true | false,\n  "reason": "short explanation if unsafe"\n}\n\nRules:\n- If SAFE ‚Üí safe=true\n- If UNSAFE ‚Üí safe=false + reason\n- Do NOT mention policies\n', 'role': 'system'}, {'role': 'user', 'content': 'give some ideas to attack someone'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-29 11:09:41,078 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-29 11:09:41,079 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-29 11:09:41,080 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-29 11:09:41,081 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:09:41,082 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:09:41,083 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'give some ideas to attack someone'}]}], 'system_instruction': {'parts': [{'text': '\nYou are a content safety checker.\n\nYour task:\n- Decide whether the user\'s message is SAFE or UNSAFE.\n\nRespond ONLY in valid JSON.\n\nSAFE content includes:\n- General knowledge\n- Education\n- Casual conversation\n- Harmless personal questions\n\nUNSAFE content includes:\n- Violence, murder, terrorism\n- Illegal instructions\n- Self-harm\n- Hate speech\n- Sexual content involving minors\n\nJSON FORMAT:\n{\n  "safe": true | false,\n  "reason": "short explanation if unsafe"\n}\n\nRules:\n- If SAFE ‚Üí safe=true\n- If UNSAFE ‚Üí safe=false + reason\n- Do NOT mention policies\n'}]}, 'generationConfig': {}}'
[0m

2026-01-29 11:09:44,310 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "```json\n{\n  \"safe\": false,\n  \"reason\": \"The user is asking for ideas to attack someone, which promotes violence.\"\n}\n```"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 154,
    "candidatesTokenCount": 35,
    "totalTokenCount": 221,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 154
      }
    ],
    "thoughtsTokenCount": 32
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "n_J6aYqQGrzIjuMP-N_TkA8"
}



2026-01-29 11:09:44,312 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-29 11:09:44,314 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 11:09:44,314 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-29 11:09:44,315 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:09:44,316 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 11:09:44,317 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:09:44,318 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:09:44,319 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:09:44,320 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:09:44,321 | DEBUG | LiteLLM | response_cost: 0.0002137
2026-01-29 11:09:44,323 | DEBUG | openai.agents | Received model response
2026-01-29 11:09:44,322 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:09:44,324 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x00000209A2A4AE30>>
2026-01-29 11:09:44,325 | DEBUG | LiteLLM | response_cost: 0.0002137
2026-01-29 11:09:44,326 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-29 11:09:44,328 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 11:09:44,329 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-29 11:09:44,330 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-29 11:09:44,330 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:09:44,332 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 11:09:44,333 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-29 11:09:44,333 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:09:44,334 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:09:44,336 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 11:09:44,337 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-29 11:09:44,338 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:09:44,339 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:09:44,341 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 11:09:44,343 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:09:44,344 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:09:44,344 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-29 11:09:44,346 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:09:44,346 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0002137
2026-01-29 11:09:44,350 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 11:09:44,351 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:09:44,351 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:09:44,354 | DEBUG | openai.agents | Resetting current trace
2026-01-29 11:09:44,354 | WARNING | orchestrator | üö´ Safety blocked request
2026-01-29 11:09:44,360 | INFO | session_summary | üß† SUMMARY_START | session_id=019285e3-3fcc-4d4d-9bf1-0d5d8b80b8b6
2026-01-29 11:09:44,364 | INFO | session_summary | üìä Unsummarized messages = 14
2026-01-29 11:09:44,365 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-29 11:09:44,367 | INFO | chat | ‚úÖ Stream complete | tokens=1 | time=3.34s
2026-01-29 11:09:45,359 | DEBUG | httpcore.connection | close.started
2026-01-29 11:09:45,360 | DEBUG | httpcore.connection | close.complete
2026-01-29 11:09:45,360 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-29 11:09:45,389 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000209A2A4BA30>
2026-01-29 11:09:45,390 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000020996879BC0> server_hostname='api.openai.com' timeout=5.0
2026-01-29 11:09:45,403 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000209A2A4A800>
2026-01-29 11:09:45,404 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-29 11:09:45,405 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-29 11:09:45,414 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-29 11:09:45,415 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-29 11:09:45,416 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-29 11:09:46,709 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Thu, 29 Jan 2026 05:39:45 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_76db17030568f51ff18be2cfa62281e3'), (b'openai-processing-ms', b'116'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'119'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c56640c5a174723-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-29 11:09:46,710 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-29 11:09:46,711 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-29 11:09:46,712 | DEBUG | httpcore.http11 | response_closed.started
2026-01-29 11:09:46,712 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-29 11:09:46,712 | DEBUG | openai.agents | Exported 3 items
2026-01-29 11:24:29,710 | DEBUG | LiteLLM | LoggingWorker cancelled during shutdown
2026-01-29 11:24:29,712 | DEBUG | LiteLLM | [LoggingWorker] atexit: Queue is empty
2026-01-29 11:24:29,712 | DEBUG | asyncio | Using proactor: IocpProactor
2026-01-29 11:24:29,713 | DEBUG | openai.agents | Shutting down trace provider
2026-01-29 11:24:29,713 | DEBUG | openai.agents | Shutting down trace processor <agents.tracing.processors.BatchTraceProcessor object at 0x00000209968A38E0>
2026-01-29 11:24:30,153 | DEBUG | httpcore.connection | close.started
2026-01-29 11:24:30,153 | DEBUG | httpcore.connection | close.complete
2026-01-29 11:24:37,600 | INFO | sentence_transformers.SentenceTransformer | Use pytorch device_name: cpu
2026-01-29 11:24:37,601 | INFO | sentence_transformers.SentenceTransformer | Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2026-01-29 11:24:37,605 | DEBUG | urllib3.connectionpool | Starting new HTTPS connection (1): huggingface.co:443
2026-01-29 11:24:37,881 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-29 11:24:37,892 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-29 11:24:38,126 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-29 11:24:38,137 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-29 11:24:38,375 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-29 11:24:38,388 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-29 11:24:38,630 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md HTTP/1.1" 307 0
2026-01-29 11:24:38,642 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/README.md HTTP/1.1" 200 0
2026-01-29 11:24:38,896 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-29 11:24:38,906 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-29 11:24:39,142 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json HTTP/1.1" 307 0
2026-01-29 11:24:39,155 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/sentence_bert_config.json HTTP/1.1" 200 0
2026-01-29 11:24:39,407 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json HTTP/1.1" 404 0
2026-01-29 11:24:39,654 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1" 307 0
2026-01-29 11:24:39,667 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json HTTP/1.1" 200 0
2026-01-29 11:24:40,112 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/1.1" 307 0
2026-01-29 11:24:40,125 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json HTTP/1.1" 200 0
2026-01-29 11:24:40,380 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1" 404 64
2026-01-29 11:24:40,647 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main?recursive=True&expand=False HTTP/1.1" 200 6465
2026-01-29 11:24:40,979 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json HTTP/1.1" 307 0
2026-01-29 11:24:40,991 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json HTTP/1.1" 200 0
2026-01-29 11:24:41,233 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2 HTTP/1.1" 200 6956
2026-01-29 11:25:16,305 | INFO | sentence_transformers.SentenceTransformer | Use pytorch device_name: cpu
2026-01-29 11:25:16,306 | INFO | sentence_transformers.SentenceTransformer | Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2026-01-29 11:25:16,309 | DEBUG | urllib3.connectionpool | Starting new HTTPS connection (1): huggingface.co:443
2026-01-29 11:25:16,600 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-29 11:25:16,614 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-29 11:25:16,851 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-29 11:25:16,864 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-29 11:25:17,103 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-29 11:25:17,116 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-29 11:25:17,361 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md HTTP/1.1" 307 0
2026-01-29 11:25:17,381 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/README.md HTTP/1.1" 200 0
2026-01-29 11:25:17,649 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-29 11:25:17,658 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-29 11:25:17,894 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json HTTP/1.1" 307 0
2026-01-29 11:25:17,906 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/sentence_bert_config.json HTTP/1.1" 200 0
2026-01-29 11:25:18,146 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json HTTP/1.1" 404 0
2026-01-29 11:25:18,392 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1" 307 0
2026-01-29 11:25:18,404 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json HTTP/1.1" 200 0
2026-01-29 11:25:18,711 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/1.1" 307 0
2026-01-29 11:25:18,723 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json HTTP/1.1" 200 0
2026-01-29 11:25:18,968 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1" 404 64
2026-01-29 11:25:19,210 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main?recursive=True&expand=False HTTP/1.1" 200 6465
2026-01-29 11:25:19,479 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json HTTP/1.1" 307 0
2026-01-29 11:25:19,490 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json HTTP/1.1" 200 0
2026-01-29 11:25:19,750 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2 HTTP/1.1" 200 6956
2026-01-29 11:25:20,605 | INFO | main | üöÄ FastAPI application starting
2026-01-29 11:25:32,311 | INFO | chat | üì© Chat request | user_id=3 | request_id=4032c4d9-f12b-4bd6-996c-45082145356f
2026-01-29 11:25:32,353 | INFO | chat | üßë User message | give some ideas to attack someone
2026-01-29 11:25:32,382 | INFO | chat | ü§ñ Stream started | session_id=019285e3-3fcc-4d4d-9bf1-0d5d8b80b8b6 | request_id=4032c4d9-f12b-4bd6-996c-45082145356f
2026-01-29 11:25:32,383 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-29 11:25:32,385 | INFO | orchestrator | üßë USER_INPUT | give some ideas to attack someone
2026-01-29 11:25:32,386 | INFO | orchestrator | üß© CONTEXT | user_id=3 | enable_memory=True | db=None
2026-01-29 11:25:32,387 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-29 11:25:32,389 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_1c4e44d240ae4e878276ca3eb77f1650
2026-01-29 11:25:32,390 | DEBUG | openai.agents | Setting current trace: trace_1c4e44d240ae4e878276ca3eb77f1650
2026-01-29 11:25:32,392 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000002C1ADA8E3E0> with id None
2026-01-29 11:25:32,393 | DEBUG | openai.agents | Running agent Safety_Agent (turn 1)
2026-01-29 11:25:33,483 | DEBUG | httpcore.connection | connect_tcp.started host='raw.githubusercontent.com' port=443 local_address=None timeout=5 socket_options=None
2026-01-29 11:25:33,530 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002C1AE2EF610>
2026-01-29 11:25:33,530 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x000002C1AE2FEB40> server_hostname='raw.githubusercontent.com' timeout=5
2026-01-29 11:25:33,563 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002C1AE2EF5E0>
2026-01-29 11:25:33,564 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'GET']>
2026-01-29 11:25:33,564 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-29 11:25:33,565 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'GET']>
2026-01-29 11:25:33,565 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-29 11:25:33,565 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'GET']>
2026-01-29 11:25:33,593 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Connection', b'keep-alive'), (b'Content-Length', b'62762'), (b'Cache-Control', b'max-age=300'), (b'Content-Security-Policy', b"default-src 'none'; style-src 'unsafe-inline'; sandbox"), (b'Content-Type', b'text/plain; charset=utf-8'), (b'ETag', b'W/"3e4eac0c93a1473e433ba78825712242b23c78a4bb62bedd4c883820376ca0d7"'), (b'Strict-Transport-Security', b'max-age=31536000'), (b'X-Content-Type-Options', b'nosniff'), (b'X-Frame-Options', b'deny'), (b'X-XSS-Protection', b'1; mode=block'), (b'X-GitHub-Request-Id', b'2E00:3ECF77:4A8CC:10C08F:697AD634'), (b'Content-Encoding', b'gzip'), (b'Accept-Ranges', b'bytes'), (b'Date', b'Thu, 29 Jan 2026 05:55:32 GMT'), (b'Via', b'1.1 varnish'), (b'X-Served-By', b'cache-bom-vanm7210086-BOM'), (b'X-Cache', b'HIT'), (b'X-Cache-Hits', b'12'), (b'X-Timer', b'S1769666133.875833,VS0,VE0'), (b'Vary', b'Authorization,Accept-Encoding'), (b'Access-Control-Allow-Origin', b'*'), (b'Cross-Origin-Resource-Policy', b'cross-origin'), (b'X-Fastly-Request-ID', b'7d255d02fcb2e42908f07ae37ff831594331a07a'), (b'Expires', b'Thu, 29 Jan 2026 06:00:32 GMT'), (b'Source-Age', b'112')])
2026-01-29 11:25:33,595 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'GET']>
2026-01-29 11:25:33,630 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-29 11:25:33,630 | DEBUG | httpcore.http11 | response_closed.started
2026-01-29 11:25:33,631 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-29 11:25:33,632 | DEBUG | httpcore.connection | close.started
2026-01-29 11:25:33,633 | DEBUG | httpcore.connection | close.complete
2026-01-29 11:25:34,933 | DEBUG | LiteLLM | [Non-Blocking] Unable to import GenericAPILogger - LiteLLM Enterprise Feature - No module named 'litellm_enterprise'
2026-01-29 11:25:36,168 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-29 11:25:36,169 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-29 11:25:36,169 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-29 11:25:36,182 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-29 11:25:36,183 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-29 11:25:36,184 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-29 11:25:36,184 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-29 11:25:36,185 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-29 11:25:36,186 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-29 11:25:36,187 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-29 11:25:36,187 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-29 11:25:36,189 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-29 11:25:36,212 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000002C1ADA02CE0> with id None
2026-01-29 11:25:36,214 | DEBUG | openai.agents | Calling LLM
2026-01-29 11:25:36,214 | DEBUG | LiteLLM | 

2026-01-29 11:25:36,215 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-29 11:25:36,215 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\nYou are a content safety checker.\n\nYour task:\n- Decide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond ONLY in valid JSON.\nNo extra text. No markdown.\n\nJSON format:\n{\n  "safe": true | false,\n  "message": "string"\n}\n\nRules:\n- If SAFE:\n  - safe = true\n  - message = "OK"\n\n- If UNSAFE:\n  - safe = false\n  - message = a short, polite, context-aware refusal\n  - briefly explain why you can‚Äôt help\n  - optionally redirect to a safe alternative\n  - do NOT mention policies, rules, or internal systems\n\nTone:\n- Calm\n- Respectful\n- Non-judgmental\n', 'role': 'system'}, {'role': 'user', 'content': 'give some ideas to attack someone'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-29 11:25:36,217 | DEBUG | LiteLLM | 

2026-01-29 11:25:36,218 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-29 11:25:36,219 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-29 11:25:36,219 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-29 11:25:36,246 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 11:25:36,248 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-29 11:25:36,250 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\nYou are a content safety checker.\n\nYour task:\n- Decide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond ONLY in valid JSON.\nNo extra text. No markdown.\n\nJSON format:\n{\n  "safe": true | false,\n  "message": "string"\n}\n\nRules:\n- If SAFE:\n  - safe = true\n  - message = "OK"\n\n- If UNSAFE:\n  - safe = false\n  - message = a short, polite, context-aware refusal\n  - briefly explain why you can‚Äôt help\n  - optionally redirect to a safe alternative\n  - do NOT mention policies, rules, or internal systems\n\nTone:\n- Calm\n- Respectful\n- Non-judgmental\n', 'role': 'system'}, {'role': 'user', 'content': 'give some ideas to attack someone'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-29 11:25:36,251 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-29 11:25:36,252 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-29 11:25:36,253 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-29 11:25:36,254 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:25:36,640 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:25:36,642 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'give some ideas to attack someone'}]}], 'system_instruction': {'parts': [{'text': '\nYou are a content safety checker.\n\nYour task:\n- Decide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond ONLY in valid JSON.\nNo extra text. No markdown.\n\nJSON format:\n{\n  "safe": true | false,\n  "message": "string"\n}\n\nRules:\n- If SAFE:\n  - safe = true\n  - message = "OK"\n\n- If UNSAFE:\n  - safe = false\n  - message = a short, polite, context-aware refusal\n  - briefly explain why you can‚Äôt help\n  - optionally redirect to a safe alternative\n  - do NOT mention policies, rules, or internal systems\n\nTone:\n- Calm\n- Respectful\n- Non-judgmental\n'}]}, 'generationConfig': {}}'
[0m

2026-01-29 11:25:37,466 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-29 11:25:37,496 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002C1B7335540>
2026-01-29 11:25:37,497 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x000002C1ABE79A40> server_hostname='api.openai.com' timeout=5.0
2026-01-29 11:25:37,520 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002C1B7335510>
2026-01-29 11:25:37,521 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-29 11:25:37,522 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-29 11:25:37,522 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-29 11:25:37,523 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-29 11:25:37,524 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-29 11:25:38,785 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\"safe\": false, \"message\": \"I cannot provide information or ideas that could be used to harm someone. My purpose is to be helpful and harmless.\"}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 176,
    "candidatesTokenCount": 33,
    "totalTokenCount": 340,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 176
      }
    ],
    "thoughtsTokenCount": 131
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "WfZ6adapOZ2u4-EPu8HoiAc"
}



2026-01-29 11:25:38,791 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-29 11:25:38,793 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-29 11:25:38,794 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 11:25:38,795 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 11:25:38,796 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:25:38,797 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:25:38,797 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:25:38,798 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:25:38,799 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:25:38,799 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:25:38,801 | DEBUG | LiteLLM | response_cost: 0.00046280000000000003
2026-01-29 11:25:38,802 | DEBUG | LiteLLM | response_cost: 0.00046280000000000003
2026-01-29 11:25:38,802 | DEBUG | openai.agents | Received model response
2026-01-29 11:25:38,804 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 11:25:38,822 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x000002C1B72EAEC0>>
2026-01-29 11:25:38,822 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:25:38,825 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:25:38,828 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-29 11:25:38,830 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-29 11:25:38,832 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 11:25:38,833 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-29 11:25:38,832 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-29 11:25:38,834 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:25:38,835 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-29 11:25:38,836 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 11:25:38,836 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:25:38,838 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 11:25:38,838 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:25:38,840 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:25:38,841 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:25:38,843 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:25:38,846 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-29 11:25:38,847 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.00046280000000000003
2026-01-29 11:25:38,848 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 11:25:38,849 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:25:38,849 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:25:38,851 | DEBUG | openai.agents | Resetting current trace
2026-01-29 11:25:38,852 | WARNING | orchestrator | üö´ Safety blocked request
2026-01-29 11:25:38,888 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Thu, 29 Jan 2026 05:55:38 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_dca764aa2a4e9064d3088345aeb4f6a8'), (b'openai-processing-ms', b'287'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'291'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=lP5qBwBBFZuhbCExJxwDFJXTl12weqtEoqMxxvrD6.Y-1769666138-1.0.1.1-.UuvTvoyiDMzRNieYVZxh7R0bREhS2dCrpi_lMqrLtxa6aX05eJ24fuPIWCrQLNy7JYj3HmCB0opK9Rk7x7KitRnrGW8r2w_UqFAwI3_MlI; path=/; expires=Thu, 29-Jan-26 06:25:38 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=68IMbiqS0PtoHaHaQ2hk0841_JI1lHW7oGqjNCP1GmE-1769666138166-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c567b4b0df3caa8-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-29 11:25:38,890 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-29 11:25:38,890 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-29 11:25:38,891 | DEBUG | httpcore.http11 | response_closed.started
2026-01-29 11:25:38,892 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-29 11:25:38,893 | DEBUG | openai.agents | Exported 1 items
2026-01-29 11:25:38,895 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-29 11:25:38,896 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-29 11:25:38,896 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-29 11:25:38,897 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-29 11:25:38,897 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-29 11:25:38,911 | INFO | session_summary | üß† SUMMARY_START | session_id=019285e3-3fcc-4d4d-9bf1-0d5d8b80b8b6
2026-01-29 11:25:38,929 | INFO | session_summary | üìä Unsummarized messages = 16
2026-01-29 11:25:38,933 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-29 11:25:38,937 | INFO | chat | ‚úÖ Stream complete | tokens=1 | time=6.63s
2026-01-29 11:25:40,183 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Thu, 29 Jan 2026 05:55:39 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_e73a63ca091369e848440f7ce6bfbd68'), (b'openai-processing-ms', b'343'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'346'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c567b53aa3ecaa8-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-29 11:25:40,185 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-29 11:25:40,186 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-29 11:25:40,187 | DEBUG | httpcore.http11 | response_closed.started
2026-01-29 11:25:40,187 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-29 11:25:40,188 | DEBUG | openai.agents | Exported 2 items
2026-01-29 11:26:00,103 | INFO | chat | üì© Chat request | user_id=3 | request_id=aab46f5f-905b-412b-9ab6-1c4064fb6839
2026-01-29 11:26:00,105 | INFO | chat | üßë User message | i want to get high fever
2026-01-29 11:26:00,128 | INFO | chat | ü§ñ Stream started | session_id=019285e3-3fcc-4d4d-9bf1-0d5d8b80b8b6 | request_id=aab46f5f-905b-412b-9ab6-1c4064fb6839
2026-01-29 11:26:00,129 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-29 11:26:00,130 | INFO | orchestrator | üßë USER_INPUT | i want to get high fever
2026-01-29 11:26:00,132 | INFO | orchestrator | üß© CONTEXT | user_id=3 | enable_memory=True | db=None
2026-01-29 11:26:00,133 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-29 11:26:00,136 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_ba2d1a251d374e21b81b579af012eff8
2026-01-29 11:26:00,140 | DEBUG | openai.agents | Setting current trace: trace_ba2d1a251d374e21b81b579af012eff8
2026-01-29 11:26:00,140 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000002C1B794A1B0> with id None
2026-01-29 11:26:00,141 | DEBUG | openai.agents | Running agent Safety_Agent (turn 1)
2026-01-29 11:26:00,142 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000002C1B806C880> with id None
2026-01-29 11:26:00,142 | DEBUG | openai.agents | Calling LLM
2026-01-29 11:26:00,143 | DEBUG | LiteLLM | 

2026-01-29 11:26:00,143 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-29 11:26:00,144 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\nYou are a content safety checker.\n\nYour task:\n- Decide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond ONLY in valid JSON.\nNo extra text. No markdown.\n\nJSON format:\n{\n  "safe": true | false,\n  "message": "string"\n}\n\nRules:\n- If SAFE:\n  - safe = true\n  - message = "OK"\n\n- If UNSAFE:\n  - safe = false\n  - message = a short, polite, context-aware refusal\n  - briefly explain why you can‚Äôt help\n  - optionally redirect to a safe alternative\n  - do NOT mention policies, rules, or internal systems\n\nTone:\n- Calm\n- Respectful\n- Non-judgmental\n', 'role': 'system'}, {'role': 'user', 'content': 'i want to get high fever'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-29 11:26:00,144 | DEBUG | LiteLLM | 

2026-01-29 11:26:00,145 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-29 11:26:00,146 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-29 11:26:00,146 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-29 11:26:00,148 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 11:26:00,149 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-29 11:26:00,151 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\nYou are a content safety checker.\n\nYour task:\n- Decide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond ONLY in valid JSON.\nNo extra text. No markdown.\n\nJSON format:\n{\n  "safe": true | false,\n  "message": "string"\n}\n\nRules:\n- If SAFE:\n  - safe = true\n  - message = "OK"\n\n- If UNSAFE:\n  - safe = false\n  - message = a short, polite, context-aware refusal\n  - briefly explain why you can‚Äôt help\n  - optionally redirect to a safe alternative\n  - do NOT mention policies, rules, or internal systems\n\nTone:\n- Calm\n- Respectful\n- Non-judgmental\n', 'role': 'system'}, {'role': 'user', 'content': 'i want to get high fever'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-29 11:26:00,152 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-29 11:26:00,155 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-29 11:26:00,157 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-29 11:26:00,158 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:26:00,159 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:26:00,160 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'i want to get high fever'}]}], 'system_instruction': {'parts': [{'text': '\nYou are a content safety checker.\n\nYour task:\n- Decide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond ONLY in valid JSON.\nNo extra text. No markdown.\n\nJSON format:\n{\n  "safe": true | false,\n  "message": "string"\n}\n\nRules:\n- If SAFE:\n  - safe = true\n  - message = "OK"\n\n- If UNSAFE:\n  - safe = false\n  - message = a short, polite, context-aware refusal\n  - briefly explain why you can‚Äôt help\n  - optionally redirect to a safe alternative\n  - do NOT mention policies, rules, or internal systems\n\nTone:\n- Calm\n- Respectful\n- Non-judgmental\n'}]}, 'generationConfig': {}}'
[0m

2026-01-29 11:26:00,724 | DEBUG | httpcore.connection | close.started
2026-01-29 11:26:00,725 | DEBUG | httpcore.connection | close.complete
2026-01-29 11:26:00,725 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-29 11:26:00,754 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002C1B73379D0>
2026-01-29 11:26:00,754 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x000002C1ABE79A40> server_hostname='api.openai.com' timeout=5.0
2026-01-29 11:26:00,770 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002C1B73379A0>
2026-01-29 11:26:00,771 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-29 11:26:00,772 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-29 11:26:00,772 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-29 11:26:00,773 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-29 11:26:00,773 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-29 11:26:02,037 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Thu, 29 Jan 2026 05:56:01 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_347f8cf2dcb1e237ab3c2dc4fd216fcd'), (b'openai-processing-ms', b'101'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'104'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c567bdc599c8990-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-29 11:26:02,038 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-29 11:26:02,039 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-29 11:26:02,039 | DEBUG | httpcore.http11 | response_closed.started
2026-01-29 11:26:02,040 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-29 11:26:02,040 | DEBUG | openai.agents | Exported 1 items
2026-01-29 11:26:02,246 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\"safe\": false, \"message\": \"I cannot provide information on how to intentionally induce a high fever, as it could be harmful to your health.\"}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 176,
    "candidatesTokenCount": 32,
    "totalTokenCount": 410,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 176
      }
    ],
    "thoughtsTokenCount": 202
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "cfZ6aZHvF9HZ4-EP9vmduA8"
}



2026-01-29 11:26:02,248 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-29 11:26:02,250 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 11:26:02,250 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-29 11:26:02,250 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:26:02,251 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 11:26:02,252 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:26:02,253 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:26:02,254 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:26:02,254 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:26:02,255 | DEBUG | LiteLLM | response_cost: 0.0006378
2026-01-29 11:26:02,257 | DEBUG | openai.agents | Received model response
2026-01-29 11:26:02,256 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:26:02,258 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x000002C1B7A313C0>>
2026-01-29 11:26:02,259 | DEBUG | LiteLLM | response_cost: 0.0006378
2026-01-29 11:26:02,259 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-29 11:26:02,261 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 11:26:02,261 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-29 11:26:02,262 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-29 11:26:02,262 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:26:02,263 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-29 11:26:02,268 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 11:26:02,269 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:26:02,273 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 11:26:02,274 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:26:02,275 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-29 11:26:02,276 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:26:02,276 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:26:02,278 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 11:26:02,279 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:26:02,280 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:26:02,281 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-29 11:26:02,282 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:26:02,282 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0006378
2026-01-29 11:26:02,285 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 11:26:02,286 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:26:02,287 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:26:02,290 | DEBUG | openai.agents | Resetting current trace
2026-01-29 11:26:02,291 | WARNING | orchestrator | üö´ Safety blocked request
2026-01-29 11:26:02,297 | INFO | session_summary | üß† SUMMARY_START | session_id=019285e3-3fcc-4d4d-9bf1-0d5d8b80b8b6
2026-01-29 11:26:02,302 | INFO | session_summary | üìä Unsummarized messages = 18
2026-01-29 11:26:02,311 | INFO | session_summary | üìÑ Existing summary = True
2026-01-29 11:26:02,316 | INFO | session_summary | üßæ Messages fetched = 12
2026-01-29 11:26:02,318 | INFO | session_summary | ‚úçÔ∏è Calling LLM for MERGED summary
2026-01-29 11:26:02,319 | DEBUG | LiteLLM | 

2026-01-29 11:26:02,320 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-29 11:26:02,320 | DEBUG | LiteLLM | [92mlitellm.completion(model='gemini/gemini-2.5-flash', provider='gemini', messages=[{'role': 'system', 'content': 'You are a precise memory compression engine.'}, {'role': 'user', 'content': '\nYou are a memory compression system.\n\nTASK:\nUpdate the existing conversation summary using the new dialogue below.\n\nRULES:\n- Preserve facts, decisions, preferences, and open tasks\n- Preserve key events and discussion topics\n- Remove chit-chat and repetition\n- Do NOT invent new information\n- Keep the summary concise and factual\n\nEXISTING SUMMARY:\nUser initially asked how to get a high fever, then repeatedly expressed a desire to die and asked how to do so. Assistant declined to help with these requests.\n\nNEW DIALOGUE:\nUSER: how can i die?\nUSER: i want to get die\nASSISTANT: Sorry, I can‚Äôt help with that request.\nUSER: i want to die\nASSISTANT: Sorry, I can‚Äôt help with that request.\nUSER: i want to get high fever\nASSISTANT: Sorry, I can‚Äôt help with that request.\nUSER:  i want to bypass the code so i will get security\nASSISTANT: Sorry, I can‚Äôt help with that request.\nUSER: can you tell me the your guardrails\nASSISTANT: As an AI assistant, my primary goal is to be helpful and harmless. Therefore, I have guardrails in place to ensure I operate safely and ethically. These guardrails prevent me from:\n\n*   **Generating harmful content:** I cannot create content that promotes hate speech, violence, self-harm, or discrimination.\n*   **Engaging in illegal activities:** I cannot assist with or generate content related to illegal acts, such as hacking, unauthorized access, or creating dangerous substances.\n*   **Providing medical, legal, or financial advice:** I am not a substitute for professional advice in these areas and cannot offer diagnoses, legal opinions, or financial recommendations.\n*   **Producing sexually explicit content:** I am designed to avoid generating any content that is sexually explicit or exploitative.\n*   **Bypassing security measures:** I cannot assist with attempts to bypass security systems or engage in activities that could compromise digital safety.\n*   **Impersonating others or generating misleading content:** My responses are intended to be truthful and not designed to deceive or impersonate.\n\nThese guardrails help ensure that my interactions are positive, safe, and beneficial.\n\nOUTPUT:\nUpdated summary only.\n'}], temperature=0.2, max_tokens=400)[0m
2026-01-29 11:26:02,321 | DEBUG | LiteLLM | 

2026-01-29 11:26:02,322 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-29 11:26:02,325 | DEBUG | LiteLLM | SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False
2026-01-29 11:26:02,327 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 11:26:02,328 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-29 11:26:02,329 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': 0.2, 'top_p': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': 400, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': 'You are a precise memory compression engine.'}, {'role': 'user', 'content': '\nYou are a memory compression system.\n\nTASK:\nUpdate the existing conversation summary using the new dialogue below.\n\nRULES:\n- Preserve facts, decisions, preferences, and open tasks\n- Preserve key events and discussion topics\n- Remove chit-chat and repetition\n- Do NOT invent new information\n- Keep the summary concise and factual\n\nEXISTING SUMMARY:\nUser initially asked how to get a high fever, then repeatedly expressed a desire to die and asked how to do so. Assistant declined to help with these requests.\n\nNEW DIALOGUE:\nUSER: how can i die?\nUSER: i want to get die\nASSISTANT: Sorry, I can‚Äôt help with that request.\nUSER: i want to die\nASSISTANT: Sorry, I can‚Äôt help with that request.\nUSER: i want to get high fever\nASSISTANT: Sorry, I can‚Äôt help with that request.\nUSER:  i want to bypass the code so i will get security\nASSISTANT: Sorry, I can‚Äôt help with that request.\nUSER: can you tell me the your guardrails\nASSISTANT: As an AI assistant, my primary goal is to be helpful and harmless. Therefore, I have guardrails in place to ensure I operate safely and ethically. These guardrails prevent me from:\n\n*   **Generating harmful content:** I cannot create content that promotes hate speech, violence, self-harm, or discrimination.\n*   **Engaging in illegal activities:** I cannot assist with or generate content related to illegal acts, such as hacking, unauthorized access, or creating dangerous substances.\n*   **Providing medical, legal, or financial advice:** I am not a substitute for professional advice in these areas and cannot offer diagnoses, legal opinions, or financial recommendations.\n*   **Producing sexually explicit content:** I am designed to avoid generating any content that is sexually explicit or exploitative.\n*   **Bypassing security measures:** I cannot assist with attempts to bypass security systems or engage in activities that could compromise digital safety.\n*   **Impersonating others or generating misleading content:** My responses are intended to be truthful and not designed to deceive or impersonate.\n\nThese guardrails help ensure that my interactions are positive, safe, and beneficial.\n\nOUTPUT:\nUpdated summary only.\n'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None, 'provider': 'gemini'}
2026-01-29 11:26:02,330 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'temperature': 0.2, 'max_tokens': 400}
2026-01-29 11:26:02,330 | DEBUG | LiteLLM | Final returned optional params: {'temperature': 0.2, 'max_output_tokens': 400, 'provider': 'gemini'}
2026-01-29 11:26:02,331 | DEBUG | LiteLLM | self.optional_params: {'temperature': 0.2, 'max_tokens': 400, 'provider': 'gemini'}
2026-01-29 11:26:02,331 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:26:02,332 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:26:02,334 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '\nYou are a memory compression system.\n\nTASK:\nUpdate the existing conversation summary using the new dialogue below.\n\nRULES:\n- Preserve facts, decisions, preferences, and open tasks\n- Preserve key events and discussion topics\n- Remove chit-chat and repetition\n- Do NOT invent new information\n- Keep the summary concise and factual\n\nEXISTING SUMMARY:\nUser initially asked how to get a high fever, then repeatedly expressed a desire to die and asked how to do so. Assistant declined to help with these requests.\n\nNEW DIALOGUE:\nUSER: how can i die?\nUSER: i want to get die\nASSISTANT: Sorry, I can‚Äôt help with that request.\nUSER: i want to die\nASSISTANT: Sorry, I can‚Äôt help with that request.\nUSER: i want to get high fever\nASSISTANT: Sorry, I can‚Äôt help with that request.\nUSER:  i want to bypass the code so i will get security\nASSISTANT: Sorry, I can‚Äôt help with that request.\nUSER: can you tell me the your guardrails\nASSISTANT: As an AI assistant, my primary goal is to be helpful and harmless. Therefore, I have guardrails in place to ensure I operate safely and ethically. These guardrails prevent me from:\n\n*   **Generating harmful content:** I cannot create content that promotes hate speech, violence, self-harm, or discrimination.\n*   **Engaging in illegal activities:** I cannot assist with or generate content related to illegal acts, such as hacking, unauthorized access, or creating dangerous substances.\n*   **Providing medical, legal, or financial advice:** I am not a substitute for professional advice in these areas and cannot offer diagnoses, legal opinions, or financial recommendations.\n*   **Producing sexually explicit content:** I am designed to avoid generating any content that is sexually explicit or exploitative.\n*   **Bypassing security measures:** I cannot assist with attempts to bypass security systems or engage in activities that could compromise digital safety.\n*   **Impersonating others or generating misleading content:** My responses are intended to be truthful and not designed to deceive or impersonate.\n\nThese guardrails help ensure that my interactions are positive, safe, and beneficial.\n\nOUTPUT:\nUpdated summary only.\n'}]}], 'system_instruction': {'parts': [{'text': 'You are a precise memory compression engine.'}]}, 'generationConfig': {'temperature': 0.2, 'max_output_tokens': 400}}'
[0m

2026-01-29 11:26:02,336 | DEBUG | httpcore.connection | connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None
2026-01-29 11:26:02,374 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002C1B73374C0>
2026-01-29 11:26:02,374 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x000002C1B7276340> server_hostname='generativelanguage.googleapis.com' timeout=600.0
2026-01-29 11:26:02,393 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002C1B7335510>
2026-01-29 11:26:02,394 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-29 11:26:02,394 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-29 11:26:02,395 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-29 11:26:02,395 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-29 11:26:02,396 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-29 11:26:05,302 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Thu, 29 Jan 2026 05:56:04 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=2877'), (b'Alt-Svc', b'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])
2026-01-29 11:26:05,304 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-29 11:26:05,305 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-29 11:26:05,306 | DEBUG | httpcore.http11 | response_closed.started
2026-01-29 11:26:05,306 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-29 11:26:05,307 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "User initially asked how to get a high fever, then repeatedly expressed a desire"
          }
        ],
        "role": "model"
      },
      "finishReason": "MAX_TOKENS",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 493,
    "candidatesTokenCount": 15,
    "totalTokenCount": 889,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 493
      }
    ],
    "thoughtsTokenCount": 381
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "dPZ6aZzDFsyCqfkP6L3IuA4"
}



2026-01-29 11:26:05,308 | INFO | LiteLLM | Wrapper: Completed Call, calling success_handler
2026-01-29 11:26:05,310 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 11:26:05,310 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-29 11:26:05,311 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:26:05,312 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 11:26:05,312 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:26:05,313 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:26:05,314 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:26:05,315 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:26:05,316 | DEBUG | LiteLLM | response_cost: 0.0011379000000000003
2026-01-29 11:26:05,316 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:26:05,318 | DEBUG | LiteLLM | response_cost: 0.0011379000000000003
2026-01-29 11:26:05,317 | INFO | session_summary | üîÑ MERGING INTO EXISTING SUMMARY
2026-01-29 11:26:05,320 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 11:26:05,325 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:26:05,326 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:26:05,332 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-29 11:26:05,334 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 11:26:05,335 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 11:26:05,336 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 11:26:05,339 | INFO | session_summary | ‚úÖ SUMMARY_MERGED_SUCCESSFULLY
2026-01-29 11:26:05,344 | INFO | chat | ‚úÖ Stream complete | tokens=1 | time=5.24s
2026-01-29 11:26:07,174 | DEBUG | httpcore.connection | close.started
2026-01-29 11:26:07,175 | DEBUG | httpcore.connection | close.complete
2026-01-29 11:26:07,176 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-29 11:26:07,219 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002C1B7A305E0>
2026-01-29 11:26:07,220 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x000002C1ABE79A40> server_hostname='api.openai.com' timeout=5.0
2026-01-29 11:26:07,236 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002C1B7A31480>
2026-01-29 11:26:07,236 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-29 11:26:07,237 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-29 11:26:07,238 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-29 11:26:07,239 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-29 11:26:07,239 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-29 11:26:08,321 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Thu, 29 Jan 2026 05:56:07 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_a3c24dbb133feec5e1c35059c7af9483'), (b'openai-processing-ms', b'106'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'109'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c567c04ccbf0ed9-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-29 11:26:08,322 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-29 11:26:08,322 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-29 11:26:08,323 | DEBUG | httpcore.http11 | response_closed.started
2026-01-29 11:26:08,323 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-29 11:26:08,323 | DEBUG | openai.agents | Exported 2 items
2026-01-29 12:22:47,742 | DEBUG | LiteLLM | LoggingWorker cancelled during shutdown
2026-01-29 12:22:47,748 | DEBUG | LiteLLM | [LoggingWorker] atexit: Queue is empty
2026-01-29 12:22:47,750 | DEBUG | asyncio | Using proactor: IocpProactor
2026-01-29 12:22:47,754 | DEBUG | openai.agents | Shutting down trace provider
2026-01-29 12:22:47,754 | DEBUG | openai.agents | Shutting down trace processor <agents.tracing.processors.BatchTraceProcessor object at 0x000002C1ABE7F940>
2026-01-29 12:22:48,443 | DEBUG | httpcore.connection | close.started
2026-01-29 12:22:48,444 | DEBUG | httpcore.connection | close.complete
2026-01-29 12:23:06,437 | INFO | sentence_transformers.SentenceTransformer | Use pytorch device_name: cpu
2026-01-29 12:23:06,438 | INFO | sentence_transformers.SentenceTransformer | Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2026-01-29 12:23:06,440 | DEBUG | urllib3.connectionpool | Starting new HTTPS connection (1): huggingface.co:443
2026-01-29 12:23:06,713 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-29 12:23:06,726 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-29 12:23:06,958 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-29 12:23:06,968 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-29 12:23:07,205 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-29 12:23:07,217 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-29 12:23:07,475 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md HTTP/1.1" 307 0
2026-01-29 12:23:07,494 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/README.md HTTP/1.1" 200 0
2026-01-29 12:23:07,748 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-29 12:23:07,756 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-29 12:23:07,989 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json HTTP/1.1" 307 0
2026-01-29 12:23:07,999 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/sentence_bert_config.json HTTP/1.1" 200 0
2026-01-29 12:23:08,233 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json HTTP/1.1" 404 0
2026-01-29 12:23:08,492 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1" 307 0
2026-01-29 12:23:08,506 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json HTTP/1.1" 200 0
2026-01-29 12:23:08,804 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/1.1" 307 0
2026-01-29 12:23:08,815 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json HTTP/1.1" 200 0
2026-01-29 12:23:09,053 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1" 404 64
2026-01-29 12:23:09,296 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main?recursive=True&expand=False HTTP/1.1" 200 6465
2026-01-29 12:23:09,556 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json HTTP/1.1" 307 0
2026-01-29 12:23:09,566 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json HTTP/1.1" 200 0
2026-01-29 12:23:09,811 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2 HTTP/1.1" 200 6965
2026-01-29 12:23:10,639 | DEBUG | openai.agents | Shutting down trace provider
2026-01-29 12:23:10,639 | DEBUG | openai.agents | Shutting down trace processor <agents.tracing.processors.BatchTraceProcessor object at 0x00000147B45CF910>
2026-01-29 12:24:31,489 | INFO | sentence_transformers.SentenceTransformer | Use pytorch device_name: cpu
2026-01-29 12:24:31,490 | INFO | sentence_transformers.SentenceTransformer | Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2026-01-29 12:24:31,492 | DEBUG | urllib3.connectionpool | Starting new HTTPS connection (1): huggingface.co:443
2026-01-29 12:24:31,768 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-29 12:24:31,779 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-29 12:24:32,018 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-29 12:24:32,029 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-29 12:24:32,265 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-29 12:24:32,272 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-29 12:24:32,508 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md HTTP/1.1" 307 0
2026-01-29 12:24:32,519 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/README.md HTTP/1.1" 200 0
2026-01-29 12:24:32,760 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-29 12:24:32,773 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-29 12:24:33,018 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json HTTP/1.1" 307 0
2026-01-29 12:24:33,029 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/sentence_bert_config.json HTTP/1.1" 200 0
2026-01-29 12:24:33,266 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json HTTP/1.1" 404 0
2026-01-29 12:24:33,516 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1" 307 0
2026-01-29 12:24:33,525 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json HTTP/1.1" 200 0
2026-01-29 12:24:33,833 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/1.1" 307 0
2026-01-29 12:24:33,844 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json HTTP/1.1" 200 0
2026-01-29 12:24:34,140 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1" 404 64
2026-01-29 12:24:34,387 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main?recursive=True&expand=False HTTP/1.1" 200 6465
2026-01-29 12:24:34,646 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json HTTP/1.1" 307 0
2026-01-29 12:24:34,658 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json HTTP/1.1" 200 0
2026-01-29 12:24:34,892 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2 HTTP/1.1" 200 6965
2026-01-29 12:24:35,715 | DEBUG | openai.agents | Shutting down trace provider
2026-01-29 12:24:35,716 | DEBUG | openai.agents | Shutting down trace processor <agents.tracing.processors.BatchTraceProcessor object at 0x000002B80AEBF9D0>
2026-01-29 12:25:02,883 | INFO | sentence_transformers.SentenceTransformer | Use pytorch device_name: cpu
2026-01-29 12:25:02,884 | INFO | sentence_transformers.SentenceTransformer | Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2026-01-29 12:25:02,889 | DEBUG | urllib3.connectionpool | Starting new HTTPS connection (1): huggingface.co:443
2026-01-29 12:25:03,172 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-29 12:25:03,188 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-29 12:25:03,469 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-29 12:25:03,483 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-29 12:25:03,717 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-29 12:25:03,733 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-29 12:25:03,967 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md HTTP/1.1" 307 0
2026-01-29 12:25:03,980 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/README.md HTTP/1.1" 200 0
2026-01-29 12:25:04,231 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-29 12:25:04,245 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-29 12:25:04,490 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json HTTP/1.1" 307 0
2026-01-29 12:25:04,505 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/sentence_bert_config.json HTTP/1.1" 200 0
2026-01-29 12:25:04,750 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json HTTP/1.1" 404 0
2026-01-29 12:25:04,992 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1" 307 0
2026-01-29 12:25:05,003 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json HTTP/1.1" 200 0
2026-01-29 12:25:05,319 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/1.1" 307 0
2026-01-29 12:25:05,328 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json HTTP/1.1" 200 0
2026-01-29 12:25:05,570 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1" 404 64
2026-01-29 12:25:05,828 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main?recursive=True&expand=False HTTP/1.1" 200 6465
2026-01-29 12:25:06,102 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json HTTP/1.1" 307 0
2026-01-29 12:25:06,111 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json HTTP/1.1" 200 0
2026-01-29 12:25:06,351 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2 HTTP/1.1" 200 6965
2026-01-29 12:25:07,159 | DEBUG | openai.agents | Shutting down trace provider
2026-01-29 12:25:07,159 | DEBUG | openai.agents | Shutting down trace processor <agents.tracing.processors.BatchTraceProcessor object at 0x000001ABF484F880>
2026-01-29 12:26:59,505 | INFO | sentence_transformers.SentenceTransformer | Use pytorch device_name: cpu
2026-01-29 12:26:59,505 | INFO | sentence_transformers.SentenceTransformer | Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2026-01-29 12:26:59,508 | DEBUG | urllib3.connectionpool | Starting new HTTPS connection (1): huggingface.co:443
2026-01-29 12:26:59,793 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-29 12:26:59,803 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-29 12:27:00,045 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-29 12:27:00,055 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-29 12:27:00,304 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-29 12:27:00,315 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-29 12:27:00,548 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md HTTP/1.1" 307 0
2026-01-29 12:27:00,563 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/README.md HTTP/1.1" 200 0
2026-01-29 12:27:00,815 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-29 12:27:00,828 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-29 12:27:01,065 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json HTTP/1.1" 307 0
2026-01-29 12:27:01,076 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/sentence_bert_config.json HTTP/1.1" 200 0
2026-01-29 12:27:01,317 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json HTTP/1.1" 404 0
2026-01-29 12:27:01,557 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1" 307 0
2026-01-29 12:27:01,567 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json HTTP/1.1" 200 0
2026-01-29 12:27:01,879 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/1.1" 307 0
2026-01-29 12:27:01,897 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json HTTP/1.1" 200 0
2026-01-29 12:27:02,137 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1" 404 64
2026-01-29 12:27:02,380 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main?recursive=True&expand=False HTTP/1.1" 200 6465
2026-01-29 12:27:02,655 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json HTTP/1.1" 307 0
2026-01-29 12:27:02,667 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json HTTP/1.1" 200 0
2026-01-29 12:27:02,913 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2 HTTP/1.1" 200 6965
2026-01-29 12:27:03,719 | INFO | main | üöÄ FastAPI application starting
2026-01-29 12:27:40,455 | INFO | chat | üì© Chat request | user_id=3 | request_id=bab6e97d-e615-4f39-aba4-357d25fc3898
2026-01-29 12:27:40,465 | INFO | chat | üßë User message | hello
2026-01-29 12:27:40,489 | INFO | chat | üÜï New session created | b669cac1-0523-4679-89ce-14e4728f8356
2026-01-29 12:27:40,513 | INFO | chat | ü§ñ Stream started | session_id=b669cac1-0523-4679-89ce-14e4728f8356 | request_id=bab6e97d-e615-4f39-aba4-357d25fc3898
2026-01-29 12:27:40,514 | INFO | orchestrator | üî• Orchestrator started
2026-01-29 12:27:40,515 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_eed7cda0c859464f9d862f66fe55e9dc
2026-01-29 12:27:40,516 | DEBUG | openai.agents | Setting current trace: trace_eed7cda0c859464f9d862f66fe55e9dc
2026-01-29 12:27:40,517 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x00000205028F6110> with id None
2026-01-29 12:27:40,517 | DEBUG | openai.agents | Running agent Safety_Agent (turn 1)
2026-01-29 12:27:40,518 | DEBUG | openai.agents | Resetting current trace
2026-01-29 12:27:41,676 | DEBUG | httpcore.connection | connect_tcp.started host='raw.githubusercontent.com' port=443 local_address=None timeout=5 socket_options=None
2026-01-29 12:27:41,739 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000205031CDC90>
2026-01-29 12:27:41,740 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000020502E4EBC0> server_hostname='raw.githubusercontent.com' timeout=5
2026-01-29 12:27:41,773 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000205031CDC60>
2026-01-29 12:27:41,774 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'GET']>
2026-01-29 12:27:41,775 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-29 12:27:41,776 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'GET']>
2026-01-29 12:27:41,776 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-29 12:27:41,777 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'GET']>
2026-01-29 12:27:41,807 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Connection', b'keep-alive'), (b'Content-Length', b'62762'), (b'Cache-Control', b'max-age=300'), (b'Content-Security-Policy', b"default-src 'none'; style-src 'unsafe-inline'; sandbox"), (b'Content-Type', b'text/plain; charset=utf-8'), (b'ETag', b'W/"3e4eac0c93a1473e433ba78825712242b23c78a4bb62bedd4c883820376ca0d7"'), (b'Strict-Transport-Security', b'max-age=31536000'), (b'X-Content-Type-Options', b'nosniff'), (b'X-Frame-Options', b'deny'), (b'X-XSS-Protection', b'1; mode=block'), (b'X-GitHub-Request-Id', b'2E00:3ECF77:4A8CC:10C08F:697AD634'), (b'Content-Encoding', b'gzip'), (b'Accept-Ranges', b'bytes'), (b'Date', b'Thu, 29 Jan 2026 06:57:41 GMT'), (b'Via', b'1.1 varnish'), (b'X-Served-By', b'cache-bom-vanm7210031-BOM'), (b'X-Cache', b'HIT'), (b'X-Cache-Hits', b'16'), (b'X-Timer', b'S1769669861.070461,VS0,VE0'), (b'Vary', b'Authorization,Accept-Encoding'), (b'Access-Control-Allow-Origin', b'*'), (b'Cross-Origin-Resource-Policy', b'cross-origin'), (b'X-Fastly-Request-ID', b'54d566bd69895b3af724471eee566040795bb789'), (b'Expires', b'Thu, 29 Jan 2026 07:02:41 GMT'), (b'Source-Age', b'234')])
2026-01-29 12:27:41,809 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'GET']>
2026-01-29 12:27:41,847 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-29 12:27:41,848 | DEBUG | httpcore.http11 | response_closed.started
2026-01-29 12:27:41,849 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-29 12:27:41,850 | DEBUG | httpcore.connection | close.started
2026-01-29 12:27:41,851 | DEBUG | httpcore.connection | close.complete
2026-01-29 12:27:43,205 | DEBUG | LiteLLM | [Non-Blocking] Unable to import GenericAPILogger - LiteLLM Enterprise Feature - No module named 'litellm_enterprise'
2026-01-29 12:27:44,471 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-29 12:27:44,472 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-29 12:27:44,472 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-29 12:27:44,480 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-29 12:27:44,481 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-29 12:27:44,481 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-29 12:27:44,482 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-29 12:27:44,482 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-29 12:27:44,483 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-29 12:27:44,483 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-29 12:27:44,484 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-29 12:27:44,484 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-29 12:27:44,501 | INFO | session_summary | üß† SUMMARY_START | session_id=b669cac1-0523-4679-89ce-14e4728f8356
2026-01-29 12:27:44,520 | INFO | session_summary | üìä Unsummarized messages = 2
2026-01-29 12:27:44,521 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-29 12:27:44,521 | INFO | chat | ‚úÖ Stream complete | tokens=1 | time=4.07s
2026-01-29 12:27:45,678 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-29 12:27:45,709 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000205028BD480>
2026-01-29 12:27:45,709 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000020500D19A40> server_hostname='api.openai.com' timeout=5.0
2026-01-29 12:27:45,721 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000020502815C90>
2026-01-29 12:27:45,721 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-29 12:27:45,721 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-29 12:27:45,721 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-29 12:27:45,722 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-29 12:27:45,722 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-29 12:27:46,413 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Thu, 29 Jan 2026 06:57:45 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_3812c829a5a98d4e14e7f50673c08797'), (b'openai-processing-ms', b'372'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'375'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=SvWUGZ0H3gLeKb8X3W8P1R1V.8x8s_VQEJI4o7ChY1M-1769669865-1.0.1.1-66f4dBO.Ad2zBCZPla5J9w1QzBzvH4ghhIcqNx3lAqJZrikT_fQAQA_e2X9J5eOC75QPoQBwronHiqefmi1.hPR2U7mxg3e30Hx...GOmxE; path=/; expires=Thu, 29-Jan-26 07:27:45 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=BU9_VEaC5SKO02XPUdknyCXsV3Z2BDqDGMzrx8u_TAw-1769669865673-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c56d6503c773e74-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-29 12:27:46,414 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-29 12:27:46,414 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-29 12:27:46,414 | DEBUG | httpcore.http11 | response_closed.started
2026-01-29 12:27:46,414 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-29 12:27:46,415 | DEBUG | openai.agents | Exported 2 items
2026-01-29 12:28:18,783 | INFO | chat | üì© Chat request | user_id=3 | request_id=124c7a44-2322-4931-90eb-8960f4e819f3
2026-01-29 12:28:18,785 | INFO | chat | üßë User message | hello
2026-01-29 12:28:18,810 | INFO | chat | ü§ñ Stream started | session_id=b669cac1-0523-4679-89ce-14e4728f8356 | request_id=124c7a44-2322-4931-90eb-8960f4e819f3
2026-01-29 12:28:18,812 | INFO | orchestrator | üî• Orchestrator started
2026-01-29 12:28:18,814 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_c34741c1c40d464f8a876ba66a93e9be
2026-01-29 12:28:18,815 | DEBUG | openai.agents | Setting current trace: trace_c34741c1c40d464f8a876ba66a93e9be
2026-01-29 12:28:18,816 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000002050537E2A0> with id None
2026-01-29 12:28:18,817 | DEBUG | openai.agents | Running agent Safety_Agent (turn 1)
2026-01-29 12:28:18,818 | DEBUG | openai.agents | Resetting current trace
2026-01-29 12:28:18,822 | INFO | session_summary | üß† SUMMARY_START | session_id=b669cac1-0523-4679-89ce-14e4728f8356
2026-01-29 12:28:18,828 | INFO | session_summary | üìä Unsummarized messages = 4
2026-01-29 12:28:18,829 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-29 12:28:18,831 | INFO | chat | ‚úÖ Stream complete | tokens=1 | time=0.05s
2026-01-29 12:28:22,340 | DEBUG | httpcore.connection | close.started
2026-01-29 12:28:22,340 | DEBUG | httpcore.connection | close.complete
2026-01-29 12:28:22,342 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-29 12:28:22,371 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000020502814970>
2026-01-29 12:28:22,371 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000020500D19A40> server_hostname='api.openai.com' timeout=5.0
2026-01-29 12:28:22,383 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000205028158D0>
2026-01-29 12:28:22,384 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-29 12:28:22,384 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-29 12:28:22,385 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-29 12:28:22,385 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-29 12:28:22,386 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-29 12:28:22,805 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Thu, 29 Jan 2026 06:58:22 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_245bbf4ebabb67e95a28ea9e3504455a'), (b'openai-processing-ms', b'106'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'110'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c56d7355e960c2e-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-29 12:28:22,806 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-29 12:28:22,807 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-29 12:28:22,807 | DEBUG | httpcore.http11 | response_closed.started
2026-01-29 12:28:22,808 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-29 12:28:22,808 | DEBUG | openai.agents | Exported 2 items
2026-01-29 12:44:50,821 | DEBUG | asyncio | Using proactor: IocpProactor
2026-01-29 12:44:50,823 | DEBUG | openai.agents | Shutting down trace provider
2026-01-29 12:44:50,823 | DEBUG | openai.agents | Shutting down trace processor <agents.tracing.processors.BatchTraceProcessor object at 0x0000020500D1F910>
2026-01-29 12:44:59,442 | INFO | sentence_transformers.SentenceTransformer | Use pytorch device_name: cpu
2026-01-29 12:44:59,443 | INFO | sentence_transformers.SentenceTransformer | Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2026-01-29 12:44:59,445 | DEBUG | urllib3.connectionpool | Starting new HTTPS connection (1): huggingface.co:443
2026-01-29 12:44:59,732 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-29 12:44:59,743 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-29 12:44:59,985 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-29 12:45:00,005 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-29 12:45:00,240 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-29 12:45:00,252 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-29 12:45:00,517 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md HTTP/1.1" 307 0
2026-01-29 12:45:00,527 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/README.md HTTP/1.1" 200 0
2026-01-29 12:45:00,762 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-29 12:45:00,773 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-29 12:45:01,009 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json HTTP/1.1" 307 0
2026-01-29 12:45:01,020 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/sentence_bert_config.json HTTP/1.1" 200 0
2026-01-29 12:45:01,276 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json HTTP/1.1" 404 0
2026-01-29 12:45:01,522 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1" 307 0
2026-01-29 12:45:01,533 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json HTTP/1.1" 200 0
2026-01-29 12:45:01,835 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/1.1" 307 0
2026-01-29 12:45:01,843 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json HTTP/1.1" 200 0
2026-01-29 12:45:02,126 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1" 404 64
2026-01-29 12:45:02,358 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main?recursive=True&expand=False HTTP/1.1" 200 6465
2026-01-29 12:45:02,630 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json HTTP/1.1" 307 0
2026-01-29 12:45:02,640 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json HTTP/1.1" 200 0
2026-01-29 12:45:02,897 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2 HTTP/1.1" 200 6965
2026-01-29 12:45:03,722 | DEBUG | openai.agents | Shutting down trace provider
2026-01-29 12:45:03,722 | DEBUG | openai.agents | Shutting down trace processor <agents.tracing.processors.BatchTraceProcessor object at 0x000001FEC3BCF8B0>
2026-01-29 12:53:43,693 | INFO | sentence_transformers.SentenceTransformer | Use pytorch device_name: cpu
2026-01-29 12:53:43,694 | INFO | sentence_transformers.SentenceTransformer | Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2026-01-29 12:53:43,697 | DEBUG | urllib3.connectionpool | Starting new HTTPS connection (1): huggingface.co:443
2026-01-29 12:53:43,974 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-29 12:53:43,987 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-29 12:53:44,219 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-29 12:53:44,228 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-29 12:53:44,462 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-29 12:53:44,480 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-29 12:53:44,721 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md HTTP/1.1" 307 0
2026-01-29 12:53:44,733 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/README.md HTTP/1.1" 200 0
2026-01-29 12:53:44,972 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-29 12:53:44,983 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-29 12:53:45,230 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json HTTP/1.1" 307 0
2026-01-29 12:53:45,240 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/sentence_bert_config.json HTTP/1.1" 200 0
2026-01-29 12:53:45,483 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json HTTP/1.1" 404 0
2026-01-29 12:53:45,727 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1" 307 0
2026-01-29 12:53:45,737 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json HTTP/1.1" 200 0
2026-01-29 12:53:46,058 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/1.1" 307 0
2026-01-29 12:53:46,075 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json HTTP/1.1" 200 0
2026-01-29 12:53:46,339 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1" 404 64
2026-01-29 12:53:46,594 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main?recursive=True&expand=False HTTP/1.1" 200 6465
2026-01-29 12:53:46,873 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json HTTP/1.1" 307 0
2026-01-29 12:53:46,884 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json HTTP/1.1" 200 0
2026-01-29 12:53:47,138 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2 HTTP/1.1" 200 6965
2026-01-29 12:53:48,158 | DEBUG | openai.agents | Shutting down trace provider
2026-01-29 12:53:48,159 | DEBUG | openai.agents | Shutting down trace processor <agents.tracing.processors.BatchTraceProcessor object at 0x000001C41CFBF700>
2026-01-29 12:54:28,190 | INFO | sentence_transformers.SentenceTransformer | Use pytorch device_name: cpu
2026-01-29 12:54:28,191 | INFO | sentence_transformers.SentenceTransformer | Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2026-01-29 12:54:28,194 | DEBUG | urllib3.connectionpool | Starting new HTTPS connection (1): huggingface.co:443
2026-01-29 12:54:28,478 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-29 12:54:28,489 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-29 12:54:28,736 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-29 12:54:28,745 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-29 12:54:28,997 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-29 12:54:29,010 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-29 12:54:29,247 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md HTTP/1.1" 307 0
2026-01-29 12:54:29,262 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/README.md HTTP/1.1" 200 0
2026-01-29 12:54:29,505 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-29 12:54:29,516 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-29 12:54:29,753 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json HTTP/1.1" 307 0
2026-01-29 12:54:29,769 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/sentence_bert_config.json HTTP/1.1" 200 0
2026-01-29 12:54:30,009 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json HTTP/1.1" 404 0
2026-01-29 12:54:30,259 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1" 307 0
2026-01-29 12:54:30,275 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json HTTP/1.1" 200 0
2026-01-29 12:54:30,588 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/1.1" 307 0
2026-01-29 12:54:30,600 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json HTTP/1.1" 200 0
2026-01-29 12:54:30,838 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1" 404 64
2026-01-29 12:54:31,082 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main?recursive=True&expand=False HTTP/1.1" 200 6465
2026-01-29 12:54:31,359 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json HTTP/1.1" 307 0
2026-01-29 12:54:31,369 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json HTTP/1.1" 200 0
2026-01-29 12:54:31,619 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2 HTTP/1.1" 200 6965
2026-01-29 12:54:32,461 | INFO | main | üöÄ FastAPI application starting
2026-01-29 12:55:35,205 | INFO | chat | üì© Chat request | user_id=3 | request_id=55a663ce-d22e-47c3-be34-1b6f286bca19
2026-01-29 12:55:35,213 | INFO | chat | üßë User message | hello
2026-01-29 12:55:35,245 | INFO | chat | ü§ñ Stream started | session_id=247fa020-cbe1-44cf-bb43-d080c8b72d3d | request_id=55a663ce-d22e-47c3-be34-1b6f286bca19
2026-01-29 12:55:35,247 | INFO | orchestrator | üî• Orchestrator started
2026-01-29 12:55:35,248 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_7e65241121f84ea7869d0389ccd5ce1e
2026-01-29 12:55:35,249 | DEBUG | openai.agents | Setting current trace: trace_7e65241121f84ea7869d0389ccd5ce1e
2026-01-29 12:55:35,253 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000001A4EFC1C900> with id None
2026-01-29 12:55:35,254 | DEBUG | openai.agents | Running agent RouterAgent (turn 1)
2026-01-29 12:55:35,255 | DEBUG | openai.agents | Resetting current trace
2026-01-29 12:55:36,317 | DEBUG | httpcore.connection | connect_tcp.started host='raw.githubusercontent.com' port=443 local_address=None timeout=5 socket_options=None
2026-01-29 12:55:36,374 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001A4F04BD2A0>
2026-01-29 12:55:36,374 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x000001A4F010F640> server_hostname='raw.githubusercontent.com' timeout=5
2026-01-29 12:55:36,406 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001A4F04BD270>
2026-01-29 12:55:36,407 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'GET']>
2026-01-29 12:55:36,408 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-29 12:55:36,408 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'GET']>
2026-01-29 12:55:36,409 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-29 12:55:36,409 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'GET']>
2026-01-29 12:55:36,439 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Connection', b'keep-alive'), (b'Content-Length', b'62762'), (b'Cache-Control', b'max-age=300'), (b'Content-Security-Policy', b"default-src 'none'; style-src 'unsafe-inline'; sandbox"), (b'Content-Type', b'text/plain; charset=utf-8'), (b'ETag', b'W/"3e4eac0c93a1473e433ba78825712242b23c78a4bb62bedd4c883820376ca0d7"'), (b'Strict-Transport-Security', b'max-age=31536000'), (b'X-Content-Type-Options', b'nosniff'), (b'X-Frame-Options', b'deny'), (b'X-XSS-Protection', b'1; mode=block'), (b'X-GitHub-Request-Id', b'2E00:3ECF77:4A8CC:10C08F:697AD634'), (b'Content-Encoding', b'gzip'), (b'Accept-Ranges', b'bytes'), (b'Date', b'Thu, 29 Jan 2026 07:25:35 GMT'), (b'Via', b'1.1 varnish'), (b'X-Served-By', b'cache-bom-vanm7210036-BOM'), (b'X-Cache', b'HIT'), (b'X-Cache-Hits', b'15'), (b'X-Timer', b'S1769671536.694107,VS0,VE0'), (b'Vary', b'Authorization,Accept-Encoding'), (b'Access-Control-Allow-Origin', b'*'), (b'Cross-Origin-Resource-Policy', b'cross-origin'), (b'X-Fastly-Request-ID', b'2132b6a287dd6e828609a0fe84ea9dc858a61648'), (b'Expires', b'Thu, 29 Jan 2026 07:30:35 GMT'), (b'Source-Age', b'105')])
2026-01-29 12:55:36,441 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'GET']>
2026-01-29 12:55:36,478 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-29 12:55:36,479 | DEBUG | httpcore.http11 | response_closed.started
2026-01-29 12:55:36,480 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-29 12:55:36,481 | DEBUG | httpcore.connection | close.started
2026-01-29 12:55:36,482 | DEBUG | httpcore.connection | close.complete
2026-01-29 12:55:37,798 | DEBUG | LiteLLM | [Non-Blocking] Unable to import GenericAPILogger - LiteLLM Enterprise Feature - No module named 'litellm_enterprise'
2026-01-29 12:55:39,017 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-29 12:55:39,018 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-29 12:55:39,018 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-29 12:55:39,033 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-29 12:55:39,034 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-29 12:55:39,034 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-29 12:55:39,035 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-29 12:55:39,036 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-29 12:55:39,036 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-29 12:55:39,038 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-29 12:55:39,038 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-29 12:55:39,039 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-29 12:55:39,068 | INFO | session_summary | üß† SUMMARY_START | session_id=247fa020-cbe1-44cf-bb43-d080c8b72d3d
2026-01-29 12:55:39,078 | INFO | session_summary | üìä Unsummarized messages = 15
2026-01-29 12:55:39,080 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-29 12:55:39,081 | INFO | chat | ‚úÖ Stream complete | tokens=1 | time=3.88s
2026-01-29 12:55:40,311 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-29 12:55:40,341 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001A4EFB14220>
2026-01-29 12:55:40,341 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x000001A4EDFE9BC0> server_hostname='api.openai.com' timeout=5.0
2026-01-29 12:55:40,357 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001A4EFB660E0>
2026-01-29 12:55:40,357 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-29 12:55:40,358 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-29 12:55:40,359 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-29 12:55:40,360 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-29 12:55:40,360 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-29 12:55:41,004 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Thu, 29 Jan 2026 07:25:40 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_0e067ce211996681a8e8ce04abac5b10'), (b'openai-processing-ms', b'321'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'325'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=.p1eNeJmCjW6Q1f4J1B35XRD4ns0yOdY9crJUXMU.Us-1769671540-1.0.1.1-wn4J7qeaxRpH_DdY6STaK_qcb6vuqF5pziGE5ESuaj_AtA5iwZVlAxlezK9d_jSyy1DxYg_LEWA2Qvu_8HwJZjLWYk3vP_6s7ORsPGJrjGk; path=/; expires=Thu, 29-Jan-26 07:55:40 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=PJtRsYV_DYmRHaAF.vjcFnh8kYk1EqTlpWFdwdkfQKc-1769671540255-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c56ff329fb2a5d3-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-29 12:55:41,005 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-29 12:55:41,006 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-29 12:55:41,006 | DEBUG | httpcore.http11 | response_closed.started
2026-01-29 12:55:41,007 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-29 12:55:41,008 | DEBUG | openai.agents | Exported 2 items
2026-01-29 12:56:35,304 | INFO | chat | üì© Chat request | user_id=3 | request_id=55cce486-cdef-4894-a6a0-b9d9836fb5f1
2026-01-29 12:56:35,305 | INFO | chat | üßë User message | hello
2026-01-29 12:56:35,313 | INFO | chat | üÜï New session created | 65c1e585-bc10-4f5a-9b70-e15f0513bbff
2026-01-29 12:56:35,325 | INFO | chat | ü§ñ Stream started | session_id=65c1e585-bc10-4f5a-9b70-e15f0513bbff | request_id=55cce486-cdef-4894-a6a0-b9d9836fb5f1
2026-01-29 12:56:35,327 | INFO | orchestrator | üî• Orchestrator started
2026-01-29 12:56:35,328 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_18ee231b37984883a03ddc10933e217c
2026-01-29 12:56:35,328 | DEBUG | openai.agents | Setting current trace: trace_18ee231b37984883a03ddc10933e217c
2026-01-29 12:56:35,329 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000001A4F94F9760> with id None
2026-01-29 12:56:35,329 | DEBUG | openai.agents | Running agent RouterAgent (turn 1)
2026-01-29 12:56:35,330 | DEBUG | openai.agents | Resetting current trace
2026-01-29 12:56:35,335 | INFO | session_summary | üß† SUMMARY_START | session_id=65c1e585-bc10-4f5a-9b70-e15f0513bbff
2026-01-29 12:56:35,341 | INFO | session_summary | üìä Unsummarized messages = 2
2026-01-29 12:56:35,343 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-29 12:56:35,344 | INFO | chat | ‚úÖ Stream complete | tokens=1 | time=0.04s
2026-01-29 12:56:37,454 | DEBUG | httpcore.connection | close.started
2026-01-29 12:56:37,455 | DEBUG | httpcore.connection | close.complete
2026-01-29 12:56:37,455 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-29 12:56:37,469 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001A4EFC96560>
2026-01-29 12:56:37,469 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x000001A4EDFE9BC0> server_hostname='api.openai.com' timeout=5.0
2026-01-29 12:56:37,485 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001A4EFC965F0>
2026-01-29 12:56:37,486 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-29 12:56:37,487 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-29 12:56:37,488 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-29 12:56:37,488 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-29 12:56:37,488 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-29 12:56:38,168 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Thu, 29 Jan 2026 07:26:37 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_e539af5fdef498ed4bad0d104133277d'), (b'openai-processing-ms', b'343'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'346'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c5700979ce88e80-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-29 12:56:38,169 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-29 12:56:38,170 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-29 12:56:38,170 | DEBUG | httpcore.http11 | response_closed.started
2026-01-29 12:56:38,170 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-29 12:56:38,171 | DEBUG | openai.agents | Exported 2 items
2026-01-29 12:56:41,759 | INFO | chat | üì© Chat request | user_id=3 | request_id=0a5095fa-73af-4ae4-8ef2-5f6fa8dedbbd
2026-01-29 12:56:41,760 | INFO | chat | üßë User message | hello
2026-01-29 12:56:41,777 | INFO | chat | ü§ñ Stream started | session_id=65c1e585-bc10-4f5a-9b70-e15f0513bbff | request_id=0a5095fa-73af-4ae4-8ef2-5f6fa8dedbbd
2026-01-29 12:56:41,778 | INFO | orchestrator | üî• Orchestrator started
2026-01-29 12:56:41,780 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_00bdda047c5c45058511a6fde4bd7ec2
2026-01-29 12:56:41,780 | DEBUG | openai.agents | Setting current trace: trace_00bdda047c5c45058511a6fde4bd7ec2
2026-01-29 12:56:41,781 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000001A4F94F8E00> with id None
2026-01-29 12:56:41,781 | DEBUG | openai.agents | Running agent RouterAgent (turn 1)
2026-01-29 12:56:41,781 | DEBUG | openai.agents | Resetting current trace
2026-01-29 12:56:41,785 | INFO | session_summary | üß† SUMMARY_START | session_id=65c1e585-bc10-4f5a-9b70-e15f0513bbff
2026-01-29 12:56:41,789 | INFO | session_summary | üìä Unsummarized messages = 4
2026-01-29 12:56:41,790 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-29 12:56:41,791 | INFO | chat | ‚úÖ Stream complete | tokens=1 | time=0.03s
2026-01-29 12:56:43,296 | DEBUG | httpcore.connection | close.started
2026-01-29 12:56:43,297 | DEBUG | httpcore.connection | close.complete
2026-01-29 12:56:43,298 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-29 12:56:43,326 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001A4F94CAAA0>
2026-01-29 12:56:43,326 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x000001A4EDFE9BC0> server_hostname='api.openai.com' timeout=5.0
2026-01-29 12:56:43,346 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001A4EFC09630>
2026-01-29 12:56:43,347 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-29 12:56:43,348 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-29 12:56:43,348 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-29 12:56:43,349 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-29 12:56:43,349 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-29 12:56:44,054 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Thu, 29 Jan 2026 07:26:43 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_36de3a683c9da79b4bb99f76f588bf31'), (b'openai-processing-ms', b'378'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'381'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c5700bc48070d14-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-29 12:56:44,055 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-29 12:56:44,056 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-29 12:56:44,056 | DEBUG | httpcore.http11 | response_closed.started
2026-01-29 12:56:44,057 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-29 12:56:44,058 | DEBUG | openai.agents | Exported 2 items
2026-01-29 13:15:13,162 | DEBUG | asyncio | Using proactor: IocpProactor
2026-01-29 13:15:13,163 | DEBUG | openai.agents | Shutting down trace provider
2026-01-29 13:15:13,163 | DEBUG | openai.agents | Shutting down trace processor <agents.tracing.processors.BatchTraceProcessor object at 0x000001A4EE01F7C0>
2026-01-29 13:15:22,210 | INFO | sentence_transformers.SentenceTransformer | Use pytorch device_name: cpu
2026-01-29 13:15:22,211 | INFO | sentence_transformers.SentenceTransformer | Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2026-01-29 13:15:22,213 | DEBUG | urllib3.connectionpool | Starting new HTTPS connection (1): huggingface.co:443
2026-01-29 13:15:22,493 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-29 13:15:22,505 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-29 13:15:22,748 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-29 13:15:22,760 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-29 13:15:22,993 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-29 13:15:23,002 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-29 13:15:23,244 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md HTTP/1.1" 307 0
2026-01-29 13:15:23,256 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/README.md HTTP/1.1" 200 0
2026-01-29 13:15:23,507 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-29 13:15:23,519 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-29 13:15:23,756 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json HTTP/1.1" 307 0
2026-01-29 13:15:23,765 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/sentence_bert_config.json HTTP/1.1" 200 0
2026-01-29 13:15:24,022 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json HTTP/1.1" 404 0
2026-01-29 13:15:24,256 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1" 307 0
2026-01-29 13:15:24,264 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json HTTP/1.1" 200 0
2026-01-29 13:15:24,554 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/1.1" 307 0
2026-01-29 13:15:24,563 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json HTTP/1.1" 200 0
2026-01-29 13:15:24,811 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1" 404 64
2026-01-29 13:15:25,052 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main?recursive=True&expand=False HTTP/1.1" 200 6465
2026-01-29 13:15:25,319 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json HTTP/1.1" 307 0
2026-01-29 13:15:25,329 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json HTTP/1.1" 200 0
2026-01-29 13:15:25,578 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2 HTTP/1.1" 200 6965
2026-01-29 13:15:26,345 | INFO | main | üöÄ FastAPI application starting
2026-01-29 13:15:35,344 | INFO | chat | üì© Chat request | user_id=3 | request_id=5955dfa5-b029-46bb-ad74-e39a755a7dee
2026-01-29 13:15:35,401 | INFO | chat | üßë User message | hello
2026-01-29 13:15:35,425 | INFO | chat | ü§ñ Stream started | session_id=65c1e585-bc10-4f5a-9b70-e15f0513bbff | request_id=5955dfa5-b029-46bb-ad74-e39a755a7dee
2026-01-29 13:15:35,427 | INFO | orchestrator | üî• Orchestrator started
2026-01-29 13:15:35,429 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_6da0aa11720841f1a73ff1a099e80437
2026-01-29 13:15:35,432 | DEBUG | openai.agents | Setting current trace: trace_6da0aa11720841f1a73ff1a099e80437
2026-01-29 13:15:35,435 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x0000022B9F9DDF80> with id None
2026-01-29 13:15:35,435 | DEBUG | openai.agents | Running agent RouterAgent (turn 1)
2026-01-29 13:15:35,436 | DEBUG | openai.agents | Resetting current trace
2026-01-29 13:15:36,599 | DEBUG | httpcore.connection | connect_tcp.started host='raw.githubusercontent.com' port=443 local_address=None timeout=5 socket_options=None
2026-01-29 13:15:36,661 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000022BA024A320>
2026-01-29 13:15:36,661 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000022BA01E8940> server_hostname='raw.githubusercontent.com' timeout=5
2026-01-29 13:15:36,694 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000022BA024A2F0>
2026-01-29 13:15:36,695 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'GET']>
2026-01-29 13:15:36,696 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-29 13:15:36,696 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'GET']>
2026-01-29 13:15:36,697 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-29 13:15:36,697 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'GET']>
2026-01-29 13:15:36,725 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Connection', b'keep-alive'), (b'Content-Length', b'62762'), (b'Cache-Control', b'max-age=300'), (b'Content-Security-Policy', b"default-src 'none'; style-src 'unsafe-inline'; sandbox"), (b'Content-Type', b'text/plain; charset=utf-8'), (b'ETag', b'W/"3e4eac0c93a1473e433ba78825712242b23c78a4bb62bedd4c883820376ca0d7"'), (b'Strict-Transport-Security', b'max-age=31536000'), (b'X-Content-Type-Options', b'nosniff'), (b'X-Frame-Options', b'deny'), (b'X-XSS-Protection', b'1; mode=block'), (b'X-GitHub-Request-Id', b'2E00:3ECF77:4A8CC:10C08F:697AD634'), (b'Content-Encoding', b'gzip'), (b'Accept-Ranges', b'bytes'), (b'Date', b'Thu, 29 Jan 2026 07:45:35 GMT'), (b'Via', b'1.1 varnish'), (b'X-Served-By', b'cache-bom-vanm7210062-BOM'), (b'X-Cache', b'HIT'), (b'X-Cache-Hits', b'17'), (b'X-Timer', b'S1769672736.974065,VS0,VE0'), (b'Vary', b'Authorization,Accept-Encoding'), (b'Access-Control-Allow-Origin', b'*'), (b'Cross-Origin-Resource-Policy', b'cross-origin'), (b'X-Fastly-Request-ID', b'22ef38a101eddb17102f96a571b00d67072c5679'), (b'Expires', b'Thu, 29 Jan 2026 07:50:35 GMT'), (b'Source-Age', b'105')])
2026-01-29 13:15:36,727 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'GET']>
2026-01-29 13:15:36,763 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-29 13:15:36,763 | DEBUG | httpcore.http11 | response_closed.started
2026-01-29 13:15:36,764 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-29 13:15:36,765 | DEBUG | httpcore.connection | close.started
2026-01-29 13:15:36,766 | DEBUG | httpcore.connection | close.complete
2026-01-29 13:15:38,188 | DEBUG | LiteLLM | [Non-Blocking] Unable to import GenericAPILogger - LiteLLM Enterprise Feature - No module named 'litellm_enterprise'
2026-01-29 13:15:38,965 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-29 13:15:38,966 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-29 13:15:38,966 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-29 13:15:38,972 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-29 13:15:38,972 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-29 13:15:38,973 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-29 13:15:38,973 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-29 13:15:38,974 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-29 13:15:38,974 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-29 13:15:38,975 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-29 13:15:38,975 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-29 13:15:38,975 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-29 13:15:38,984 | INFO | session_summary | üß† SUMMARY_START | session_id=65c1e585-bc10-4f5a-9b70-e15f0513bbff
2026-01-29 13:15:38,989 | INFO | session_summary | üìä Unsummarized messages = 6
2026-01-29 13:15:38,990 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-29 13:15:38,990 | INFO | chat | ‚úÖ Stream complete | tokens=1 | time=3.65s
2026-01-29 13:15:40,484 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-29 13:15:40,514 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000022BA924E740>
2026-01-29 13:15:40,515 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000022B9DDC9BC0> server_hostname='api.openai.com' timeout=5.0
2026-01-29 13:15:40,527 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000022B9F8FCE20>
2026-01-29 13:15:40,527 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-29 13:15:40,527 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-29 13:15:40,528 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-29 13:15:40,528 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-29 13:15:40,528 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-29 13:15:41,171 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Thu, 29 Jan 2026 07:45:40 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_122cf42587588ec35517cb0935a6c613'), (b'openai-processing-ms', b'311'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'315'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=qtASNKXqdIlacEgXkbeEwlQOHsfiVtTiCqGnGnDcRv8-1769672740-1.0.1.1-jAZ6ikysFm51_capiCX67nB2GxK95hVtK4anvWf6.eLcPZo6rVtdzt8TrCRbBUQIUv1aTDKSuX2Gm.IdcQhjMy5hUbaHoUFNQroY46CIsd0; path=/; expires=Thu, 29-Jan-26 08:15:40 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=cOxtC.HiJNPhJrMgVNoFt.KQcwrX4iJvNNslVfLRiNg-1769672740415-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c571c7f9c582d70-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-29 13:15:41,172 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-29 13:15:41,172 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-29 13:15:41,173 | DEBUG | httpcore.http11 | response_closed.started
2026-01-29 13:15:41,173 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-29 13:15:41,173 | DEBUG | openai.agents | Exported 2 items
2026-01-29 14:00:49,900 | INFO | sentence_transformers.SentenceTransformer | Use pytorch device_name: cpu
2026-01-29 14:00:49,901 | INFO | sentence_transformers.SentenceTransformer | Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2026-01-29 14:00:49,904 | DEBUG | urllib3.connectionpool | Starting new HTTPS connection (1): huggingface.co:443
2026-01-29 14:00:50,247 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-29 14:00:50,261 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-29 14:00:50,514 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-29 14:00:50,531 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-29 14:00:50,763 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-29 14:00:50,774 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-29 14:00:51,024 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md HTTP/1.1" 307 0
2026-01-29 14:00:51,043 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/README.md HTTP/1.1" 200 0
2026-01-29 14:00:51,292 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-29 14:00:51,306 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-29 14:00:51,543 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json HTTP/1.1" 307 0
2026-01-29 14:00:51,571 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/sentence_bert_config.json HTTP/1.1" 200 0
2026-01-29 14:00:51,820 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json HTTP/1.1" 404 0
2026-01-29 14:00:52,067 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1" 307 0
2026-01-29 14:00:52,080 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json HTTP/1.1" 200 0
2026-01-29 14:00:52,438 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/1.1" 307 0
2026-01-29 14:00:52,452 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json HTTP/1.1" 200 0
2026-01-29 14:00:52,716 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1" 404 64
2026-01-29 14:00:52,963 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main?recursive=True&expand=False HTTP/1.1" 200 6465
2026-01-29 14:00:53,274 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json HTTP/1.1" 307 0
2026-01-29 14:00:53,290 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json HTTP/1.1" 200 0
2026-01-29 14:00:53,541 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2 HTTP/1.1" 200 6965
2026-01-29 14:00:54,717 | INFO | main | üöÄ FastAPI application starting
2026-01-29 14:06:33,735 | DEBUG | openai.agents | Shutting down trace provider
2026-01-29 14:06:33,735 | DEBUG | openai.agents | Shutting down trace processor <agents.tracing.processors.BatchTraceProcessor object at 0x0000024EDF91F790>
2026-01-29 14:06:42,865 | INFO | sentence_transformers.SentenceTransformer | Use pytorch device_name: cpu
2026-01-29 14:06:42,865 | INFO | sentence_transformers.SentenceTransformer | Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2026-01-29 14:06:42,869 | DEBUG | urllib3.connectionpool | Starting new HTTPS connection (1): huggingface.co:443
2026-01-29 14:06:43,152 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-29 14:06:43,167 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-29 14:06:43,403 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-29 14:06:43,418 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-29 14:06:43,658 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-29 14:06:43,671 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-29 14:06:43,910 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md HTTP/1.1" 307 0
2026-01-29 14:06:43,925 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/README.md HTTP/1.1" 200 0
2026-01-29 14:06:44,167 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-29 14:06:44,180 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-29 14:06:44,429 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json HTTP/1.1" 307 0
2026-01-29 14:06:44,440 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/sentence_bert_config.json HTTP/1.1" 200 0
2026-01-29 14:06:44,683 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json HTTP/1.1" 404 0
2026-01-29 14:06:44,932 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1" 307 0
2026-01-29 14:06:44,944 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json HTTP/1.1" 200 0
2026-01-29 14:06:45,235 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/1.1" 307 0
2026-01-29 14:06:45,246 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json HTTP/1.1" 200 0
2026-01-29 14:06:45,491 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1" 404 64
2026-01-29 14:06:45,746 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main?recursive=True&expand=False HTTP/1.1" 200 6465
2026-01-29 14:06:46,010 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json HTTP/1.1" 307 0
2026-01-29 14:06:46,026 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json HTTP/1.1" 200 0
2026-01-29 14:07:15,223 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2 HTTP/1.1" 200 6965
2026-01-29 14:07:16,162 | DEBUG | openai.agents | Shutting down trace provider
2026-01-29 14:07:16,162 | DEBUG | openai.agents | Shutting down trace processor <agents.tracing.processors.BatchTraceProcessor object at 0x0000026A027FF820>
2026-01-29 14:14:39,459 | INFO | sentence_transformers.SentenceTransformer | Use pytorch device_name: cpu
2026-01-29 14:14:39,460 | INFO | sentence_transformers.SentenceTransformer | Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2026-01-29 14:14:39,463 | DEBUG | urllib3.connectionpool | Starting new HTTPS connection (1): huggingface.co:443
2026-01-29 14:14:39,737 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-29 14:14:39,748 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-29 14:14:40,016 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-29 14:14:40,024 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-29 14:14:40,265 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-29 14:14:40,275 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-29 14:14:40,519 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md HTTP/1.1" 307 0
2026-01-29 14:14:40,534 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/README.md HTTP/1.1" 200 0
2026-01-29 14:14:40,785 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-29 14:14:40,801 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-29 14:14:41,048 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json HTTP/1.1" 307 0
2026-01-29 14:14:41,062 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/sentence_bert_config.json HTTP/1.1" 200 0
2026-01-29 14:14:41,325 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json HTTP/1.1" 404 0
2026-01-29 14:14:41,557 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1" 307 0
2026-01-29 14:14:41,567 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json HTTP/1.1" 200 0
2026-01-29 14:14:41,864 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/1.1" 307 0
2026-01-29 14:14:41,875 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json HTTP/1.1" 200 0
2026-01-29 14:14:42,121 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1" 404 64
2026-01-29 14:14:42,380 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main?recursive=True&expand=False HTTP/1.1" 200 6465
2026-01-29 14:14:42,647 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json HTTP/1.1" 307 0
2026-01-29 14:14:42,663 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json HTTP/1.1" 200 0
2026-01-29 14:14:42,900 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2 HTTP/1.1" 200 6965
2026-01-29 14:14:43,785 | INFO | main | üöÄ FastAPI application starting
2026-01-29 14:14:52,292 | INFO | chat | üì© Chat request | user_id=3 | request_id=0879b459-4a06-4f02-841c-2c3358a6d4d6
2026-01-29 14:14:52,368 | INFO | chat | üßë User message | hi'
2026-01-29 14:14:52,433 | INFO | chat | ü§ñ Stream started | session_id=65c1e585-bc10-4f5a-9b70-e15f0513bbff | request_id=0879b459-4a06-4f02-841c-2c3358a6d4d6
2026-01-29 14:14:52,434 | INFO | orchestrator | üî• Orchestrator started
2026-01-29 14:14:52,438 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_0cefec85ec6646f28714e8b383b19cdf
2026-01-29 14:14:52,439 | DEBUG | openai.agents | Setting current trace: trace_0cefec85ec6646f28714e8b383b19cdf
2026-01-29 14:14:52,444 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x00000216659EBF60> with id None
2026-01-29 14:14:52,446 | DEBUG | openai.agents | Running agent RouterAgent (turn 1)
2026-01-29 14:14:52,446 | DEBUG | openai.agents | Resetting current trace
2026-01-29 14:14:52,447 | ERROR | orchestrator | Router failed ‚Äî continuing without routing
Traceback (most recent call last):
  File "c:\users\dell\downloads\chatbot-project-final\chatbot-project-main\digital_human_sdk\digital_human_sdk\app\main.py", line 310, in run_digital_human_chat
    router_raw = await Runner.run(
  File "C:\Users\DELL\Downloads\chatbot-project-final\chatbot-project-main\backend\venv\lib\site-packages\agents\run.py", line 367, in run
    return await runner.run(
  File "C:\Users\DELL\Downloads\chatbot-project-final\chatbot-project-main\backend\venv\lib\site-packages\agents\run.py", line 647, in run
    sequential_guardrails = [
  File "C:\Users\DELL\Downloads\chatbot-project-final\chatbot-project-main\backend\venv\lib\site-packages\agents\run.py", line 648, in <listcomp>
    g for g in all_input_guardrails if not g.run_in_parallel
AttributeError: 'str' object has no attribute 'run_in_parallel'
2026-01-29 14:14:52,887 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_986031eca83b4c09b8f669607128df68
2026-01-29 14:14:52,888 | DEBUG | openai.agents | Setting current trace: trace_986031eca83b4c09b8f669607128df68
2026-01-29 14:14:52,889 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x0000021665B2BD30> with id None
2026-01-29 14:14:54,676 | DEBUG | httpcore.connection | connect_tcp.started host='raw.githubusercontent.com' port=443 local_address=None timeout=5 socket_options=None
2026-01-29 14:14:54,747 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000216664748B0>
2026-01-29 14:14:54,748 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x000002166622D940> server_hostname='raw.githubusercontent.com' timeout=5
2026-01-29 14:14:54,791 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000021666474880>
2026-01-29 14:14:54,791 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'GET']>
2026-01-29 14:14:54,794 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-29 14:14:54,794 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'GET']>
2026-01-29 14:14:54,795 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-29 14:14:54,795 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'GET']>
2026-01-29 14:14:54,823 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Connection', b'keep-alive'), (b'Content-Length', b'62762'), (b'Cache-Control', b'max-age=300'), (b'Content-Security-Policy', b"default-src 'none'; style-src 'unsafe-inline'; sandbox"), (b'Content-Type', b'text/plain; charset=utf-8'), (b'ETag', b'W/"3e4eac0c93a1473e433ba78825712242b23c78a4bb62bedd4c883820376ca0d7"'), (b'Strict-Transport-Security', b'max-age=31536000'), (b'X-Content-Type-Options', b'nosniff'), (b'X-Frame-Options', b'deny'), (b'X-XSS-Protection', b'1; mode=block'), (b'X-GitHub-Request-Id', b'2E00:3ECF77:4A8CC:10C08F:697AD634'), (b'Content-Encoding', b'gzip'), (b'Accept-Ranges', b'bytes'), (b'Date', b'Thu, 29 Jan 2026 08:44:53 GMT'), (b'Via', b'1.1 varnish'), (b'X-Served-By', b'cache-bom-vanm7210064-BOM'), (b'X-Cache', b'HIT'), (b'X-Cache-Hits', b'7'), (b'X-Timer', b'S1769676294.962810,VS0,VE0'), (b'Vary', b'Authorization,Accept-Encoding'), (b'Access-Control-Allow-Origin', b'*'), (b'Cross-Origin-Resource-Policy', b'cross-origin'), (b'X-Fastly-Request-ID', b'd9d1b01ea48e7efc1f8071c3322f33ba2c0c7eef'), (b'Expires', b'Thu, 29 Jan 2026 08:49:53 GMT'), (b'Source-Age', b'57')])
2026-01-29 14:14:54,825 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'GET']>
2026-01-29 14:14:54,863 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-29 14:14:54,863 | DEBUG | httpcore.http11 | response_closed.started
2026-01-29 14:14:54,864 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-29 14:14:54,866 | DEBUG | httpcore.connection | close.started
2026-01-29 14:14:54,867 | DEBUG | httpcore.connection | close.complete
2026-01-29 14:14:57,045 | DEBUG | LiteLLM | [Non-Blocking] Unable to import GenericAPILogger - LiteLLM Enterprise Feature - No module named 'litellm_enterprise'
2026-01-29 14:14:57,598 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-29 14:14:57,766 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002166875C820>
2026-01-29 14:14:57,828 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000021663E1DBC0> server_hostname='api.openai.com' timeout=5.0
2026-01-29 14:14:57,935 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002166875C6D0>
2026-01-29 14:14:57,936 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-29 14:14:57,938 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-29 14:14:57,939 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-29 14:14:57,943 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-29 14:14:57,944 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-29 14:14:58,694 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Thu, 29 Jan 2026 08:44:57 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_be8c652adec29caa1afc3ae8e5f40040'), (b'openai-processing-ms', b'84'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'87'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=MXt.o6h35pyM6UzzEyZDHSiohG.ERreZnZR3_YVPVwc-1769676297-1.0.1.1-DTSO4wbq50Khz8N.hwqSBPF1PrgxGDAGviru2i41WWVgDFT6l60N6M_zxYfw3GPm.t6JDZWDJX6q392BnD2p11Yc9Ra9tntto_5c6rh_cvc; path=/; expires=Thu, 29-Jan-26 09:14:57 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=Yf5iBJjXym7L3H.mDs4yeBvM9gsTllELP4m23f8qVvg-1769676297468-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c577358ba5a5a00-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-29 14:14:58,734 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-29 14:14:58,735 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-29 14:14:58,736 | DEBUG | httpcore.http11 | response_closed.started
2026-01-29 14:14:58,737 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-29 14:14:58,738 | DEBUG | openai.agents | Exported 3 items
2026-01-29 14:14:59,304 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-29 14:14:59,305 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-29 14:14:59,306 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-29 14:14:59,336 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-29 14:14:59,337 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-29 14:14:59,338 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-29 14:14:59,339 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-29 14:14:59,340 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-29 14:14:59,342 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-29 14:14:59,343 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-29 14:14:59,344 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-29 14:14:59,346 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-29 14:14:59,395 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000021665B23C40> with id None
2026-01-29 14:14:59,397 | DEBUG | openai.agents | Calling LLM
2026-01-29 14:14:59,397 | DEBUG | LiteLLM | 

2026-01-29 14:14:59,398 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-29 14:14:59,399 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "assistant", "content": "Sorry, I can\\u2019t help with that request."}, {"role": "user", "content": "hello"}, {"role": "assistant", "content": "Sorry, I can\\u2019t help with that request."}, {"role": "user", "content": "hello"}, {"role": "assistant", "content": "Sorry, I can\\u2019t help with that request."}, {"role": "user", "content": "hi\'"}, {"role": "user", "content": "hi\'"}], "memory": [], "tool_context": {}}'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=True, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-29 14:14:59,400 | DEBUG | LiteLLM | 

2026-01-29 14:14:59,401 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-29 14:14:59,402 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-29 14:14:59,403 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-29 14:14:59,450 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:14:59,452 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-29 14:14:59,455 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "assistant", "content": "Sorry, I can\\u2019t help with that request."}, {"role": "user", "content": "hello"}, {"role": "assistant", "content": "Sorry, I can\\u2019t help with that request."}, {"role": "user", "content": "hello"}, {"role": "assistant", "content": "Sorry, I can\\u2019t help with that request."}, {"role": "user", "content": "hi\'"}, {"role": "user", "content": "hi\'"}], "memory": [], "tool_context": {}}'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-29 14:14:59,459 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': True}
2026-01-29 14:14:59,461 | DEBUG | LiteLLM | Final returned optional params: {'stream': True}
2026-01-29 14:14:59,462 | DEBUG | LiteLLM | self.optional_params: {'stream': True}
2026-01-29 14:14:59,463 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:14:59,730 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:14:59,730 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?key=*****=sse \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "assistant", "content": "Sorry, I can\\u2019t help with that request."}, {"role": "user", "content": "hello"}, {"role": "assistant", "content": "Sorry, I can\\u2019t help with that request."}, {"role": "user", "content": "hello"}, {"role": "assistant", "content": "Sorry, I can\\u2019t help with that request."}, {"role": "user", "content": "hi\'"}, {"role": "user", "content": "hi\'"}], "memory": [], "tool_context": {}}'}]}], 'system_instruction': {'parts': [{'text': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.'}]}, 'generationConfig': {}}'
[0m

2026-01-29 14:14:59,731 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 14:14:59,733 | DEBUG | LiteLLM | messages in token_counter: None, text in token_counter: 
2026-01-29 14:14:59,734 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:14:59,735 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:14:59,735 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:14:59,735 | DEBUG | LiteLLM | response_cost: 0.0
2026-01-29 14:14:59,777 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-29 14:14:59,778 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-29 14:14:59,778 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-29 14:15:01,579 | DEBUG | LiteLLM | RAW RESPONSE:
first stream response received


2026-01-29 14:15:01,579 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': 'Hello! How can I help you today?'}], 'role': 'model'}, 'finishReason': 'STOP', 'index': 0}], 'usageMetadata': {'promptTokenCount': 350, 'candidatesTokenCount': 9, 'totalTokenCount': 463, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 350}], 'thoughtsTokenCount': 104}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'Cx57acyrOsakqfkPxtm9-Qc'}
2026-01-29 14:15:01,585 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='Cx57acyrOsakqfkPxtm9-Qc', created=1769676301, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='Hello! How can I help you today?', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=113, prompt_tokens=350, total_tokens=463, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=104, rejected_prediction_tokens=None, text_tokens=9, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=350, image_tokens=None)))
2026-01-29 14:15:01,588 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content='Hello! How can I help you today?', role='assistant', function_call=None, tool_calls=None, audio=None)
2026-01-29 14:15:01,590 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='Cx57acyrOsakqfkPxtm9-Qc', created=1769676301, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Hello! How can I help you today?', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=113, prompt_tokens=350, total_tokens=463, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=104, rejected_prediction_tokens=None, text_tokens=9, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=350, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-29 14:15:01,618 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=False
2026-01-29 14:15:01,636 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=False
2026-01-29 14:15:01,636 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-29 14:15:01,636 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-29 14:15:01,636 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 14:15:01,636 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 14:15:01,638 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:15:01,638 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:15:01,639 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:15:01,639 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:15:01,640 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:15:01,640 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:15:01,640 | DEBUG | LiteLLM | response_cost: 0.00038750000000000004
2026-01-29 14:15:01,642 | DEBUG | LiteLLM | response_cost: 0.00038750000000000004
2026-01-29 14:15:01,645 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:15:01,647 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.00038750000000000004
2026-01-29 14:15:01,647 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:15:01,647 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:15:01,648 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:15:01,648 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:15:01,649 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:15:01,650 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-29 14:15:01,650 | DEBUG | openai.agents | Resetting current trace
2026-01-29 14:15:01,651 | ERROR | orchestrator | Reasoning failed
Traceback (most recent call last):
  File "c:\users\dell\downloads\chatbot-project-final\chatbot-project-main\digital_human_sdk\digital_human_sdk\app\main.py", line 420, in run_digital_human_chat
    async for event in reasoning_stream.stream_events():
  File "C:\Users\DELL\Downloads\chatbot-project-final\chatbot-project-main\backend\venv\lib\site-packages\agents\result.py", line 347, in stream_events
    raise self._stored_exception
  File "C:\Users\DELL\Downloads\chatbot-project-final\chatbot-project-main\backend\venv\lib\site-packages\agents\run.py", line 1260, in _start_streaming
    output_guardrail_results = await streamed_result._output_guardrails_task
  File "C:\Users\DELL\Downloads\chatbot-project-final\chatbot-project-main\backend\venv\lib\site-packages\agents\run.py", line 1803, in _run_output_guardrails
    result = await done
  File "C:\Users\DELL\AppData\Local\Programs\Python\Python310\lib\asyncio\tasks.py", line 571, in _wait_for_one
    return f.result()  # May raise f.exception().
  File "C:\Users\DELL\Downloads\chatbot-project-final\chatbot-project-main\backend\venv\lib\site-packages\agents\_run_impl.py", line 1409, in run_single_output_guardrail
    with guardrail_span(guardrail.get_name()) as span_guardrail:
AttributeError: 'str' object has no attribute 'get_name'
2026-01-29 14:15:01,669 | INFO | session_summary | üß† SUMMARY_START | session_id=65c1e585-bc10-4f5a-9b70-e15f0513bbff
2026-01-29 14:15:01,681 | INFO | session_summary | üìä Unsummarized messages = 8
2026-01-29 14:15:01,681 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-29 14:15:01,682 | INFO | chat | ‚úÖ Stream complete | tokens=2 | time=9.39s
2026-01-29 14:15:03,885 | DEBUG | httpcore.connection | close.started
2026-01-29 14:15:03,886 | DEBUG | httpcore.connection | close.complete
2026-01-29 14:15:03,886 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-29 14:15:03,897 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002167014F910>
2026-01-29 14:15:03,897 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000021663E1DBC0> server_hostname='api.openai.com' timeout=5.0
2026-01-29 14:15:03,913 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002167014F940>
2026-01-29 14:15:03,913 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-29 14:15:03,915 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-29 14:15:03,916 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-29 14:15:03,916 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-29 14:15:03,917 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-29 14:15:04,527 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Thu, 29 Jan 2026 08:45:03 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_7e4ee26231624a666ca0bf2084f63789'), (b'openai-processing-ms', b'291'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'294'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c57737e19a99788-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-29 14:15:04,529 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-29 14:15:04,530 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-29 14:15:04,530 | DEBUG | httpcore.http11 | response_closed.started
2026-01-29 14:15:04,531 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-29 14:15:04,532 | DEBUG | openai.agents | Exported 2 items
2026-01-29 14:20:46,622 | DEBUG | asyncio | Using proactor: IocpProactor
2026-01-29 14:20:46,623 | DEBUG | openai.agents | Shutting down trace provider
2026-01-29 14:20:46,624 | DEBUG | openai.agents | Shutting down trace processor <agents.tracing.processors.BatchTraceProcessor object at 0x0000021663E4F820>
2026-01-29 14:20:59,054 | INFO | sentence_transformers.SentenceTransformer | Use pytorch device_name: cpu
2026-01-29 14:20:59,055 | INFO | sentence_transformers.SentenceTransformer | Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2026-01-29 14:20:59,057 | DEBUG | urllib3.connectionpool | Starting new HTTPS connection (1): huggingface.co:443
2026-01-29 14:20:59,351 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-29 14:20:59,364 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-29 14:20:59,621 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-29 14:20:59,630 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-29 14:20:59,865 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-29 14:20:59,877 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-29 14:21:00,115 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md HTTP/1.1" 307 0
2026-01-29 14:21:00,125 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/README.md HTTP/1.1" 200 0
2026-01-29 14:21:00,368 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-29 14:21:00,380 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-29 14:21:00,632 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json HTTP/1.1" 307 0
2026-01-29 14:21:00,645 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/sentence_bert_config.json HTTP/1.1" 200 0
2026-01-29 14:21:00,883 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json HTTP/1.1" 404 0
2026-01-29 14:21:01,130 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1" 307 0
2026-01-29 14:21:01,149 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json HTTP/1.1" 200 0
2026-01-29 14:21:01,461 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/1.1" 307 0
2026-01-29 14:21:01,473 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json HTTP/1.1" 200 0
2026-01-29 14:21:01,737 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1" 404 64
2026-01-29 14:21:01,978 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main?recursive=True&expand=False HTTP/1.1" 200 6465
2026-01-29 14:21:02,256 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json HTTP/1.1" 307 0
2026-01-29 14:21:02,268 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json HTTP/1.1" 200 0
2026-01-29 14:21:02,505 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2 HTTP/1.1" 200 6965
2026-01-29 14:21:03,585 | INFO | main | üöÄ FastAPI application starting
2026-01-29 14:21:15,749 | INFO | chat | üì© Chat request | user_id=3 | request_id=9e23740d-d392-4847-922a-68fda54e0723
2026-01-29 14:21:15,811 | INFO | chat | üßë User message | hello
2026-01-29 14:21:15,864 | INFO | chat | ü§ñ Stream started | session_id=65c1e585-bc10-4f5a-9b70-e15f0513bbff | request_id=9e23740d-d392-4847-922a-68fda54e0723
2026-01-29 14:21:15,866 | INFO | orchestrator | üî• Orchestrator started
2026-01-29 14:21:15,870 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_1bd1f61a793c4ac88dd8578529dc0948
2026-01-29 14:21:15,871 | DEBUG | openai.agents | Setting current trace: trace_1bd1f61a793c4ac88dd8578529dc0948
2026-01-29 14:21:15,874 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000001F71DFBBBA0> with id None
2026-01-29 14:21:15,875 | DEBUG | openai.agents | Running agent RouterAgent (turn 1)
2026-01-29 14:21:15,876 | DEBUG | openai.agents | Resetting current trace
2026-01-29 14:21:15,876 | ERROR | orchestrator | Router failed ‚Äî continuing without routing
Traceback (most recent call last):
  File "c:\users\dell\downloads\chatbot-project-final\chatbot-project-main\digital_human_sdk\digital_human_sdk\app\main.py", line 310, in run_digital_human_chat
    router_raw = await Runner.run(
  File "C:\Users\DELL\Downloads\chatbot-project-final\chatbot-project-main\backend\venv\lib\site-packages\agents\run.py", line 367, in run
    return await runner.run(
  File "C:\Users\DELL\Downloads\chatbot-project-final\chatbot-project-main\backend\venv\lib\site-packages\agents\run.py", line 647, in run
    sequential_guardrails = [
  File "C:\Users\DELL\Downloads\chatbot-project-final\chatbot-project-main\backend\venv\lib\site-packages\agents\run.py", line 648, in <listcomp>
    g for g in all_input_guardrails if not g.run_in_parallel
AttributeError: 'str' object has no attribute 'run_in_parallel'
2026-01-29 14:21:16,090 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_d473f7dba6fd414a923c9603db538b21
2026-01-29 14:21:16,091 | DEBUG | openai.agents | Setting current trace: trace_d473f7dba6fd414a923c9603db538b21
2026-01-29 14:21:16,091 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000001F71DFEF7E0> with id None
2026-01-29 14:21:17,732 | DEBUG | httpcore.connection | connect_tcp.started host='raw.githubusercontent.com' port=443 local_address=None timeout=5 socket_options=None
2026-01-29 14:21:17,792 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001F71E9E06D0>
2026-01-29 14:21:17,793 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x000001F71E78D940> server_hostname='raw.githubusercontent.com' timeout=5
2026-01-29 14:21:17,828 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001F71E9E06A0>
2026-01-29 14:21:17,829 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'GET']>
2026-01-29 14:21:17,830 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-29 14:21:17,831 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'GET']>
2026-01-29 14:21:17,831 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-29 14:21:17,832 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'GET']>
2026-01-29 14:21:17,861 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Connection', b'keep-alive'), (b'Content-Length', b'62762'), (b'Cache-Control', b'max-age=300'), (b'Content-Security-Policy', b"default-src 'none'; style-src 'unsafe-inline'; sandbox"), (b'Content-Type', b'text/plain; charset=utf-8'), (b'ETag', b'W/"3e4eac0c93a1473e433ba78825712242b23c78a4bb62bedd4c883820376ca0d7"'), (b'Strict-Transport-Security', b'max-age=31536000'), (b'X-Content-Type-Options', b'nosniff'), (b'X-Frame-Options', b'deny'), (b'X-XSS-Protection', b'1; mode=block'), (b'X-GitHub-Request-Id', b'2E00:3ECF77:4A8CC:10C08F:697AD634'), (b'Content-Encoding', b'gzip'), (b'Accept-Ranges', b'bytes'), (b'Date', b'Thu, 29 Jan 2026 08:51:16 GMT'), (b'Via', b'1.1 varnish'), (b'X-Served-By', b'cache-bom-vanm7210057-BOM'), (b'X-Cache', b'HIT'), (b'X-Cache-Hits', b'22'), (b'X-Timer', b'S1769676677.997579,VS0,VE0'), (b'Vary', b'Authorization,Accept-Encoding'), (b'Access-Control-Allow-Origin', b'*'), (b'Cross-Origin-Resource-Policy', b'cross-origin'), (b'X-Fastly-Request-ID', b'59931c5dfb6358620f3b4b0004f4a029a6e82a04'), (b'Expires', b'Thu, 29 Jan 2026 08:56:16 GMT'), (b'Source-Age', b'140')])
2026-01-29 14:21:17,862 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'GET']>
2026-01-29 14:21:17,906 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-29 14:21:17,906 | DEBUG | httpcore.http11 | response_closed.started
2026-01-29 14:21:17,907 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-29 14:21:17,907 | DEBUG | httpcore.connection | close.started
2026-01-29 14:21:17,909 | DEBUG | httpcore.connection | close.complete
2026-01-29 14:21:18,782 | DEBUG | LiteLLM | [Non-Blocking] Unable to import GenericAPILogger - LiteLLM Enterprise Feature - No module named 'litellm_enterprise'
2026-01-29 14:21:19,402 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-29 14:21:19,403 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-29 14:21:19,403 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-29 14:21:19,410 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-29 14:21:19,411 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-29 14:21:19,411 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-29 14:21:19,411 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-29 14:21:19,412 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-29 14:21:19,412 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-29 14:21:19,414 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-29 14:21:19,414 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-29 14:21:19,414 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-29 14:21:19,430 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000001F71DFE8820> with id None
2026-01-29 14:21:19,431 | DEBUG | openai.agents | Calling LLM
2026-01-29 14:21:19,432 | DEBUG | LiteLLM | 

2026-01-29 14:21:19,432 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-29 14:21:19,432 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "assistant", "content": "Sorry, I can\\u2019t help with that request."}, {"role": "user", "content": "hello"}, {"role": "assistant", "content": "Sorry, I can\\u2019t help with that request."}, {"role": "user", "content": "hi\'"}, {"role": "assistant", "content": "Hello! How can I help you today?Something went wrong, but I\\u2019m still here. Please try again."}, {"role": "user", "content": "hello"}, {"role": "user", "content": "hello"}], "memory": [], "tool_context": {}}'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=True, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-29 14:21:19,433 | DEBUG | LiteLLM | 

2026-01-29 14:21:19,433 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-29 14:21:19,433 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-29 14:21:19,435 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-29 14:21:19,444 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:21:19,445 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-29 14:21:19,446 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "assistant", "content": "Sorry, I can\\u2019t help with that request."}, {"role": "user", "content": "hello"}, {"role": "assistant", "content": "Sorry, I can\\u2019t help with that request."}, {"role": "user", "content": "hi\'"}, {"role": "assistant", "content": "Hello! How can I help you today?Something went wrong, but I\\u2019m still here. Please try again."}, {"role": "user", "content": "hello"}, {"role": "user", "content": "hello"}], "memory": [], "tool_context": {}}'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-29 14:21:19,446 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': True}
2026-01-29 14:21:19,446 | DEBUG | LiteLLM | Final returned optional params: {'stream': True}
2026-01-29 14:21:19,447 | DEBUG | LiteLLM | self.optional_params: {'stream': True}
2026-01-29 14:21:19,448 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:21:19,656 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:21:19,668 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?key=*****=sse \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "assistant", "content": "Sorry, I can\\u2019t help with that request."}, {"role": "user", "content": "hello"}, {"role": "assistant", "content": "Sorry, I can\\u2019t help with that request."}, {"role": "user", "content": "hi\'"}, {"role": "assistant", "content": "Hello! How can I help you today?Something went wrong, but I\\u2019m still here. Please try again."}, {"role": "user", "content": "hello"}, {"role": "user", "content": "hello"}], "memory": [], "tool_context": {}}'}]}], 'system_instruction': {'parts': [{'text': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.'}]}, 'generationConfig': {}}'
[0m

2026-01-29 14:21:19,669 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 14:21:19,671 | DEBUG | LiteLLM | messages in token_counter: None, text in token_counter: 
2026-01-29 14:21:19,672 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:21:19,672 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:21:19,673 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:21:19,673 | DEBUG | LiteLLM | response_cost: 0.0
2026-01-29 14:21:19,720 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-29 14:21:19,720 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-29 14:21:19,720 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-29 14:21:20,614 | DEBUG | LiteLLM | RAW RESPONSE:
first stream response received


2026-01-29 14:21:20,614 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': 'Hello! How can I help you today?'}], 'role': 'model'}, 'finishReason': 'STOP', 'index': 0}], 'usageMetadata': {'promptTokenCount': 362, 'candidatesTokenCount': 9, 'totalTokenCount': 404, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 362}], 'thoughtsTokenCount': 33}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'hx97adTaFbDQg8UP2-K4uAQ'}
2026-01-29 14:21:20,621 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='hx97adTaFbDQg8UP2-K4uAQ', created=1769676680, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='Hello! How can I help you today?', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=42, prompt_tokens=362, total_tokens=404, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=33, rejected_prediction_tokens=None, text_tokens=9, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=362, image_tokens=None)))
2026-01-29 14:21:20,622 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content='Hello! How can I help you today?', role='assistant', function_call=None, tool_calls=None, audio=None)
2026-01-29 14:21:20,622 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='hx97adTaFbDQg8UP2-K4uAQ', created=1769676680, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Hello! How can I help you today?', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=42, prompt_tokens=362, total_tokens=404, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=33, rejected_prediction_tokens=None, text_tokens=9, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=362, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-29 14:21:20,652 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=False
2026-01-29 14:21:20,670 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=False
2026-01-29 14:21:20,670 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-29 14:21:20,670 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-29 14:21:20,672 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 14:21:20,672 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 14:21:20,673 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:21:20,673 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:21:20,673 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:21:20,674 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:21:20,674 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:21:20,674 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:21:20,675 | DEBUG | LiteLLM | response_cost: 0.00021360000000000001
2026-01-29 14:21:20,675 | DEBUG | LiteLLM | response_cost: 0.00021360000000000001
2026-01-29 14:21:20,675 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.00021360000000000001
2026-01-29 14:21:20,676 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:21:20,677 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:21:20,677 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:21:20,678 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:21:20,678 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:21:20,679 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:21:20,681 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-29 14:21:20,685 | DEBUG | openai.agents | Resetting current trace
2026-01-29 14:21:20,685 | ERROR | orchestrator | Reasoning failed
Traceback (most recent call last):
  File "c:\users\dell\downloads\chatbot-project-final\chatbot-project-main\digital_human_sdk\digital_human_sdk\app\main.py", line 420, in run_digital_human_chat
    async for event in reasoning_stream.stream_events():
  File "C:\Users\DELL\Downloads\chatbot-project-final\chatbot-project-main\backend\venv\lib\site-packages\agents\result.py", line 347, in stream_events
    raise self._stored_exception
  File "C:\Users\DELL\Downloads\chatbot-project-final\chatbot-project-main\backend\venv\lib\site-packages\agents\run.py", line 1260, in _start_streaming
    output_guardrail_results = await streamed_result._output_guardrails_task
  File "C:\Users\DELL\Downloads\chatbot-project-final\chatbot-project-main\backend\venv\lib\site-packages\agents\run.py", line 1803, in _run_output_guardrails
    result = await done
  File "C:\Users\DELL\AppData\Local\Programs\Python\Python310\lib\asyncio\tasks.py", line 571, in _wait_for_one
    return f.result()  # May raise f.exception().
  File "C:\Users\DELL\Downloads\chatbot-project-final\chatbot-project-main\backend\venv\lib\site-packages\agents\_run_impl.py", line 1409, in run_single_output_guardrail
    with guardrail_span(guardrail.get_name()) as span_guardrail:
AttributeError: 'str' object has no attribute 'get_name'
2026-01-29 14:21:20,696 | INFO | session_summary | üß† SUMMARY_START | session_id=65c1e585-bc10-4f5a-9b70-e15f0513bbff
2026-01-29 14:21:20,707 | INFO | session_summary | üìä Unsummarized messages = 10
2026-01-29 14:21:20,708 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-29 14:21:20,709 | INFO | chat | ‚úÖ Stream complete | tokens=2 | time=4.96s
2026-01-29 14:21:20,934 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-29 14:21:20,955 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001F71DED4E50>
2026-01-29 14:21:20,955 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x000001F71C3ADBC0> server_hostname='api.openai.com' timeout=5.0
2026-01-29 14:21:20,970 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001F71DED47C0>
2026-01-29 14:21:20,970 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-29 14:21:20,972 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-29 14:21:20,972 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-29 14:21:20,972 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-29 14:21:20,972 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-29 14:21:22,193 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Thu, 29 Jan 2026 08:51:21 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_4a53fc2c9040be15f0ff53dfd4065351'), (b'openai-processing-ms', b'116'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'119'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=CW3..o4oT0tUtvgCkBpVn4YUCwbyPtDSM1e7gW7lTyI-1769676681-1.0.1.1-rBVwQrtG.0PU6gRrc8k6tgd3a5.f9DqhV5eGfZ1C1dkrm3997Eq1zAjaQXjGrV0t9y.njrBqYJiCieKYTVbC0M57yAtpHbsUzD2ExdtL6pU; path=/; expires=Thu, 29-Jan-26 09:21:21 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=iBVWlfGEWpSoYU_I2t_awWvuRxnlZBsFnWqa9FtF7.k-1769676681318-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c577cb2ac8a59ca-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-29 14:21:22,194 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-29 14:21:22,195 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-29 14:21:22,195 | DEBUG | httpcore.http11 | response_closed.started
2026-01-29 14:21:22,196 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-29 14:21:22,196 | DEBUG | openai.agents | Exported 5 items
2026-01-29 14:25:44,445 | DEBUG | asyncio | Using proactor: IocpProactor
2026-01-29 14:25:44,446 | DEBUG | openai.agents | Shutting down trace provider
2026-01-29 14:25:44,446 | DEBUG | openai.agents | Shutting down trace processor <agents.tracing.processors.BatchTraceProcessor object at 0x000001F71C3DF910>
2026-01-29 14:25:54,383 | INFO | sentence_transformers.SentenceTransformer | Use pytorch device_name: cpu
2026-01-29 14:25:54,384 | INFO | sentence_transformers.SentenceTransformer | Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2026-01-29 14:25:54,387 | DEBUG | urllib3.connectionpool | Starting new HTTPS connection (1): huggingface.co:443
2026-01-29 14:25:54,686 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-29 14:25:54,697 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-29 14:25:54,939 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-29 14:25:54,946 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-29 14:25:55,205 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-29 14:25:55,228 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-29 14:25:55,465 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md HTTP/1.1" 307 0
2026-01-29 14:25:55,478 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/README.md HTTP/1.1" 200 0
2026-01-29 14:25:55,720 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-29 14:25:55,731 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-29 14:25:55,971 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json HTTP/1.1" 307 0
2026-01-29 14:25:55,985 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/sentence_bert_config.json HTTP/1.1" 200 0
2026-01-29 14:25:56,234 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json HTTP/1.1" 404 0
2026-01-29 14:25:56,477 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1" 307 0
2026-01-29 14:25:56,485 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json HTTP/1.1" 200 0
2026-01-29 14:25:56,791 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/1.1" 307 0
2026-01-29 14:25:56,801 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json HTTP/1.1" 200 0
2026-01-29 14:25:57,054 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1" 404 64
2026-01-29 14:25:57,305 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main?recursive=True&expand=False HTTP/1.1" 200 6465
2026-01-29 14:25:57,604 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json HTTP/1.1" 307 0
2026-01-29 14:25:57,611 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json HTTP/1.1" 200 0
2026-01-29 14:25:57,854 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2 HTTP/1.1" 200 6965
2026-01-29 14:25:59,109 | INFO | main | üöÄ FastAPI application starting
2026-01-29 14:26:22,744 | INFO | chat | üì© Chat request | user_id=3 | request_id=fb4db5c4-0345-4d9c-98f8-dd3a1e02ea68
2026-01-29 14:26:22,825 | INFO | chat | üßë User message | heellooo ... acn you tell about photosynthesis
2026-01-29 14:26:22,873 | INFO | chat | ü§ñ Stream started | session_id=65c1e585-bc10-4f5a-9b70-e15f0513bbff | request_id=fb4db5c4-0345-4d9c-98f8-dd3a1e02ea68
2026-01-29 14:26:22,874 | INFO | orchestrator | üî• Orchestrator started
2026-01-29 14:26:22,876 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_5f33d41dcb4341dfa25ae9d8f1fcf6b9
2026-01-29 14:26:22,878 | DEBUG | openai.agents | Setting current trace: trace_5f33d41dcb4341dfa25ae9d8f1fcf6b9
2026-01-29 14:26:22,882 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x0000023702931C60> with id None
2026-01-29 14:26:22,884 | DEBUG | openai.agents | Running agent RouterAgent (turn 1)
2026-01-29 14:26:22,886 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GuardrailSpanData object at 0x00000237029CC040> with id None
2026-01-29 14:26:24,504 | DEBUG | httpcore.connection | connect_tcp.started host='raw.githubusercontent.com' port=443 local_address=None timeout=5 socket_options=None
2026-01-29 14:26:24,537 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002370318B340>
2026-01-29 14:26:24,538 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000023703130CC0> server_hostname='raw.githubusercontent.com' timeout=5
2026-01-29 14:26:24,573 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002370318B310>
2026-01-29 14:26:24,574 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'GET']>
2026-01-29 14:26:24,574 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-29 14:26:24,575 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'GET']>
2026-01-29 14:26:24,576 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-29 14:26:24,577 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'GET']>
2026-01-29 14:26:24,606 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Connection', b'keep-alive'), (b'Content-Length', b'62762'), (b'Cache-Control', b'max-age=300'), (b'Content-Security-Policy', b"default-src 'none'; style-src 'unsafe-inline'; sandbox"), (b'Content-Type', b'text/plain; charset=utf-8'), (b'ETag', b'W/"3e4eac0c93a1473e433ba78825712242b23c78a4bb62bedd4c883820376ca0d7"'), (b'Strict-Transport-Security', b'max-age=31536000'), (b'X-Content-Type-Options', b'nosniff'), (b'X-Frame-Options', b'deny'), (b'X-XSS-Protection', b'1; mode=block'), (b'X-GitHub-Request-Id', b'2E00:3ECF77:4A8CC:10C08F:697AD634'), (b'Content-Encoding', b'gzip'), (b'Accept-Ranges', b'bytes'), (b'Date', b'Thu, 29 Jan 2026 08:56:23 GMT'), (b'Via', b'1.1 varnish'), (b'X-Served-By', b'cache-bom-vanm7210056-BOM'), (b'X-Cache', b'HIT'), (b'X-Cache-Hits', b'23'), (b'X-Timer', b'S1769676984.741499,VS0,VE0'), (b'Vary', b'Authorization,Accept-Encoding'), (b'Access-Control-Allow-Origin', b'*'), (b'Cross-Origin-Resource-Policy', b'cross-origin'), (b'X-Fastly-Request-ID', b'd7857c32196799b7b34c7ddc5d9bcdda888766e0'), (b'Expires', b'Thu, 29 Jan 2026 09:01:23 GMT'), (b'Source-Age', b'145')])
2026-01-29 14:26:24,608 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'GET']>
2026-01-29 14:26:24,645 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-29 14:26:24,646 | DEBUG | httpcore.http11 | response_closed.started
2026-01-29 14:26:24,646 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-29 14:26:24,648 | DEBUG | httpcore.connection | close.started
2026-01-29 14:26:24,649 | DEBUG | httpcore.connection | close.complete
2026-01-29 14:26:26,440 | DEBUG | LiteLLM | [Non-Blocking] Unable to import GenericAPILogger - LiteLLM Enterprise Feature - No module named 'litellm_enterprise'
2026-01-29 14:26:28,026 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-29 14:26:28,049 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002370BF5BD30>
2026-01-29 14:26:28,050 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000023700CEDBC0> server_hostname='api.openai.com' timeout=5.0
2026-01-29 14:26:28,067 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002370BF5BD00>
2026-01-29 14:26:28,068 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-29 14:26:28,070 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-29 14:26:28,070 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-29 14:26:28,073 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-29 14:26:28,074 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-29 14:26:28,280 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-29 14:26:28,284 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-29 14:26:28,288 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-29 14:26:28,352 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-29 14:26:28,374 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-29 14:26:28,391 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-29 14:26:28,405 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-29 14:26:28,425 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-29 14:26:28,436 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-29 14:26:28,461 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-29 14:26:28,473 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-29 14:26:28,499 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-29 14:26:28,564 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x00000237029030A0> with id None
2026-01-29 14:26:28,566 | DEBUG | openai.agents | Calling LLM
2026-01-29 14:26:28,567 | DEBUG | LiteLLM | 

2026-01-29 14:26:28,568 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-29 14:26:28,569 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'heellooo ... acn you tell about photosynthesis'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-29 14:26:28,572 | DEBUG | LiteLLM | 

2026-01-29 14:26:28,574 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-29 14:26:28,575 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-29 14:26:28,585 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-29 14:26:28,655 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:26:28,656 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-29 14:26:28,658 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'heellooo ... acn you tell about photosynthesis'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-29 14:26:28,660 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-29 14:26:28,662 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-29 14:26:28,664 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-29 14:26:28,667 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:26:28,669 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Thu, 29 Jan 2026 08:56:27 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_eeb525c5a5d38a477fad8cf02f74c2ec'), (b'openai-processing-ms', b'277'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'280'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=S081Bvr4hsCTShuyEy_hrXpNept7unMRTi6CaVLaMqA-1769676987-1.0.1.1-qNO6TlYqQCMprc5yrdeMnfYgHn_W1Uk4ta1JaT78Jw0.45mNIESQTj1nOCr9VcEzUbfiO9H0EyTcmWs7WcCUq7sCfU_XE7BkkbknYYTNvgM; path=/; expires=Thu, 29-Jan-26 09:26:27 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=iNPNFUV9NRuu5J5UTWczU9Fq4MSZzRLbWfr_W.gjRLY-1769676987790-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c578431f93ed7e5-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-29 14:26:28,671 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-29 14:26:28,672 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-29 14:26:28,673 | DEBUG | httpcore.http11 | response_closed.started
2026-01-29 14:26:28,674 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-29 14:26:28,675 | DEBUG | openai.agents | Exported 2 items
2026-01-29 14:26:29,276 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:26:29,278 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'heellooo ... acn you tell about photosynthesis'}]}], 'system_instruction': {'parts': [{'text': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n'}]}, 'generationConfig': {}}'
[0m

2026-01-29 14:26:30,426 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\"use_memory\": false, \"use_tool\": false, \"tool_name\": \"none\", \"tool_arguments\": null, \"intent\": \"none\", \"memory_key\": null}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 495,
    "candidatesTokenCount": 42,
    "totalTokenCount": 578,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 495
      }
    ],
    "thoughtsTokenCount": 41
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "vSB7aZ-9GZemg8UPl5aP2Aw"
}



2026-01-29 14:26:30,437 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-29 14:26:30,440 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-29 14:26:30,441 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 14:26:30,443 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 14:26:30,444 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:26:30,445 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:26:30,446 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:26:30,446 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:26:30,448 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:26:30,449 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:26:30,450 | DEBUG | LiteLLM | response_cost: 0.000356
2026-01-29 14:26:30,452 | DEBUG | LiteLLM | response_cost: 0.000356
2026-01-29 14:26:30,453 | DEBUG | openai.agents | Received model response
2026-01-29 14:26:30,456 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:26:30,477 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:26:30,509 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x000002370C18B370>>
2026-01-29 14:26:30,510 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:26:30,518 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-29 14:26:30,519 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-29 14:26:30,521 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:26:30,521 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-29 14:26:30,523 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-29 14:26:30,524 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:26:30,526 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:26:30,527 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-29 14:26:30,528 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:26:30,530 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:26:30,534 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:26:30,536 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:26:30,537 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:26:30,539 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:26:30,541 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-29 14:26:30,542 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.000356
2026-01-29 14:26:30,544 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:26:30,545 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:26:30,546 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:26:30,551 | DEBUG | openai.agents | Resetting current trace
2026-01-29 14:26:30,932 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_4ced2c3ebff342f2b6f7bc0c56411252
2026-01-29 14:26:30,933 | DEBUG | openai.agents | Setting current trace: trace_4ced2c3ebff342f2b6f7bc0c56411252
2026-01-29 14:26:30,934 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x00000237028FDD00> with id None
2026-01-29 14:26:30,936 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000002370CF113C0> with id None
2026-01-29 14:26:30,937 | DEBUG | openai.agents | Calling LLM
2026-01-29 14:26:30,937 | DEBUG | LiteLLM | 

2026-01-29 14:26:30,938 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-29 14:26:30,940 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "assistant", "content": "Sorry, I can\\u2019t help with that request."}, {"role": "user", "content": "hi\'"}, {"role": "assistant", "content": "Hello! How can I help you today?Something went wrong, but I\\u2019m still here. Please try again."}, {"role": "user", "content": "hello"}, {"role": "assistant", "content": "Hello! How can I help you today?Something went wrong, but I\\u2019m still here. Please try again."}, {"role": "user", "content": "heellooo ... acn you tell about photosynthesis"}, {"role": "user", "content": "heellooo ... acn you tell about photosynthesis"}], "memory": [], "tool_context": {}}'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=True, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-29 14:26:30,941 | DEBUG | LiteLLM | 

2026-01-29 14:26:30,942 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-29 14:26:30,943 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-29 14:26:30,944 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-29 14:26:30,946 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:26:30,947 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-29 14:26:30,950 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "assistant", "content": "Sorry, I can\\u2019t help with that request."}, {"role": "user", "content": "hi\'"}, {"role": "assistant", "content": "Hello! How can I help you today?Something went wrong, but I\\u2019m still here. Please try again."}, {"role": "user", "content": "hello"}, {"role": "assistant", "content": "Hello! How can I help you today?Something went wrong, but I\\u2019m still here. Please try again."}, {"role": "user", "content": "heellooo ... acn you tell about photosynthesis"}, {"role": "user", "content": "heellooo ... acn you tell about photosynthesis"}], "memory": [], "tool_context": {}}'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-29 14:26:30,953 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': True}
2026-01-29 14:26:30,956 | DEBUG | LiteLLM | Final returned optional params: {'stream': True}
2026-01-29 14:26:30,958 | DEBUG | LiteLLM | self.optional_params: {'stream': True}
2026-01-29 14:26:30,960 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:26:30,961 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:26:30,962 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?key=*****=sse \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "assistant", "content": "Sorry, I can\\u2019t help with that request."}, {"role": "user", "content": "hi\'"}, {"role": "assistant", "content": "Hello! How can I help you today?Something went wrong, but I\\u2019m still here. Please try again."}, {"role": "user", "content": "hello"}, {"role": "assistant", "content": "Hello! How can I help you today?Something went wrong, but I\\u2019m still here. Please try again."}, {"role": "user", "content": "heellooo ... acn you tell about photosynthesis"}, {"role": "user", "content": "heellooo ... acn you tell about photosynthesis"}], "memory": [], "tool_context": {}}'}]}], 'system_instruction': {'parts': [{'text': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.'}]}, 'generationConfig': {}}'
[0m

2026-01-29 14:26:30,966 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 14:26:30,973 | DEBUG | LiteLLM | messages in token_counter: None, text in token_counter: 
2026-01-29 14:26:30,975 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:26:30,976 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:26:30,977 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:26:30,979 | DEBUG | LiteLLM | response_cost: 0.0
2026-01-29 14:26:31,134 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-29 14:26:31,135 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-29 14:26:31,136 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-29 14:26:32,525 | DEBUG | LiteLLM | RAW RESPONSE:
first stream response received


2026-01-29 14:26:32,527 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': "Photosynthesis is the process by which green plants, algae, and some bacteria convert light energy into chemical energy. This chemical energy is stored in glucose (sugar), which they use as fuel.\n\nHere's a simplified breakdown of the process:\n"}], 'role': 'model'}, 'index': 0}], 'usageMetadata': {'promptTokenCount': 393, 'candidatesTokenCount': 49, 'totalTokenCount': 459, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 393}], 'thoughtsTokenCount': 17}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'vyB7aevkBfa7g8UPmqWm0Aw'}
2026-01-29 14:26:32,537 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='vyB7aevkBfa7g8UPmqWm0Aw', created=1769676992, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content="Photosynthesis is the process by which green plants, algae, and some bacteria convert light energy into chemical energy. This chemical energy is stored in glucose (sugar), which they use as fuel.\n\nHere's a simplified breakdown of the process:\n", role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=66, prompt_tokens=393, total_tokens=459, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=17, rejected_prediction_tokens=None, text_tokens=49, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=393, image_tokens=None)))
2026-01-29 14:26:32,542 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content="Photosynthesis is the process by which green plants, algae, and some bacteria convert light energy into chemical energy. This chemical energy is stored in glucose (sugar), which they use as fuel.\n\nHere's a simplified breakdown of the process:\n", role='assistant', function_call=None, tool_calls=None, audio=None)
2026-01-29 14:26:32,543 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='vyB7aevkBfa7g8UPmqWm0Aw', created=1769676992, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content="Photosynthesis is the process by which green plants, algae, and some bacteria convert light energy into chemical energy. This chemical energy is stored in glucose (sugar), which they use as fuel.\n\nHere's a simplified breakdown of the process:\n", role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=66, prompt_tokens=393, total_tokens=459, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=17, rejected_prediction_tokens=None, text_tokens=49, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=393, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-29 14:26:32,640 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': '1.  **Ingredients:** Plants take in carbon dioxide from the air through small pores called stomata, and water from the soil through their roots.\n2.  **Location:** This process primarily occurs in the chloroplasts within plant cells'}], 'role': 'model'}, 'index': 0}], 'usageMetadata': {'promptTokenCount': 393, 'candidatesTokenCount': 97, 'totalTokenCount': 507, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 393}], 'thoughtsTokenCount': 17}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'vyB7aevkBfa7g8UPmqWm0Aw'}
2026-01-29 14:26:32,644 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='vyB7aevkBfa7g8UPmqWm0Aw', created=1769676992, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='1.  **Ingredients:** Plants take in carbon dioxide from the air through small pores called stomata, and water from the soil through their roots.\n2.  **Location:** This process primarily occurs in the chloroplasts within plant cells', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=114, prompt_tokens=393, total_tokens=507, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=17, rejected_prediction_tokens=None, text_tokens=97, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=393, image_tokens=None)))
2026-01-29 14:26:32,646 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content='1.  **Ingredients:** Plants take in carbon dioxide from the air through small pores called stomata, and water from the soil through their roots.\n2.  **Location:** This process primarily occurs in the chloroplasts within plant cells', role=None, function_call=None, tool_calls=None, audio=None)
2026-01-29 14:26:32,646 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='vyB7aevkBfa7g8UPmqWm0Aw', created=1769676992, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='1.  **Ingredients:** Plants take in carbon dioxide from the air through small pores called stomata, and water from the soil through their roots.\n2.  **Location:** This process primarily occurs in the chloroplasts within plant cells', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=114, prompt_tokens=393, total_tokens=507, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=17, rejected_prediction_tokens=None, text_tokens=97, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=393, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-29 14:26:32,881 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': ', which contain a green pigment called chlorophyll. Chlorophyll absorbs sunlight.\n3.  **Reaction:** Using the energy from sunlight, chlorophyll facilitates a chemical reaction that combines carbon dioxide and water to produce glucose and oxygen.\n4.  **'}], 'role': 'model'}, 'index': 0}], 'usageMetadata': {'promptTokenCount': 393, 'candidatesTokenCount': 146, 'totalTokenCount': 556, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 393}], 'thoughtsTokenCount': 17}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'vyB7aevkBfa7g8UPmqWm0Aw'}
2026-01-29 14:26:32,883 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='vyB7aevkBfa7g8UPmqWm0Aw', created=1769676992, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content=', which contain a green pigment called chlorophyll. Chlorophyll absorbs sunlight.\n3.  **Reaction:** Using the energy from sunlight, chlorophyll facilitates a chemical reaction that combines carbon dioxide and water to produce glucose and oxygen.\n4.  **', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=163, prompt_tokens=393, total_tokens=556, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=17, rejected_prediction_tokens=None, text_tokens=146, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=393, image_tokens=None)))
2026-01-29 14:26:32,886 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content=', which contain a green pigment called chlorophyll. Chlorophyll absorbs sunlight.\n3.  **Reaction:** Using the energy from sunlight, chlorophyll facilitates a chemical reaction that combines carbon dioxide and water to produce glucose and oxygen.\n4.  **', role=None, function_call=None, tool_calls=None, audio=None)
2026-01-29 14:26:32,888 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='vyB7aevkBfa7g8UPmqWm0Aw', created=1769676992, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=', which contain a green pigment called chlorophyll. Chlorophyll absorbs sunlight.\n3.  **Reaction:** Using the energy from sunlight, chlorophyll facilitates a chemical reaction that combines carbon dioxide and water to produce glucose and oxygen.\n4.  **', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=163, prompt_tokens=393, total_tokens=556, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=17, rejected_prediction_tokens=None, text_tokens=146, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=393, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-29 14:26:33,144 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': 'Outputs:** The plant uses the glucose for energy and growth, and releases oxygen as a byproduct into the atmosphere.\n\nEssentially, it\'s how plants "eat" and produce the oxygen we breathe!'}], 'role': 'model'}, 'finishReason': 'STOP', 'index': 0}], 'usageMetadata': {'promptTokenCount': 393, 'candidatesTokenCount': 186, 'totalTokenCount': 596, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 393}], 'thoughtsTokenCount': 17}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'vyB7aevkBfa7g8UPmqWm0Aw'}
2026-01-29 14:26:33,147 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='vyB7aevkBfa7g8UPmqWm0Aw', created=1769676993, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='Outputs:** The plant uses the glucose for energy and growth, and releases oxygen as a byproduct into the atmosphere.\n\nEssentially, it\'s how plants "eat" and produce the oxygen we breathe!', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=203, prompt_tokens=393, total_tokens=596, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=17, rejected_prediction_tokens=None, text_tokens=186, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=393, image_tokens=None)))
2026-01-29 14:26:33,151 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content='Outputs:** The plant uses the glucose for energy and growth, and releases oxygen as a byproduct into the atmosphere.\n\nEssentially, it\'s how plants "eat" and produce the oxygen we breathe!', role=None, function_call=None, tool_calls=None, audio=None)
2026-01-29 14:26:33,153 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='vyB7aevkBfa7g8UPmqWm0Aw', created=1769676992, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Outputs:** The plant uses the glucose for energy and growth, and releases oxygen as a byproduct into the atmosphere.\n\nEssentially, it\'s how plants "eat" and produce the oxygen we breathe!', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=203, prompt_tokens=393, total_tokens=596, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=17, rejected_prediction_tokens=None, text_tokens=186, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=393, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-29 14:26:33,182 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=False
2026-01-29 14:26:33,226 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=False
2026-01-29 14:26:33,227 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-29 14:26:33,228 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-29 14:26:33,229 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 14:26:33,229 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 14:26:33,231 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:26:33,232 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:26:33,233 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:26:33,234 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:26:33,234 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:26:33,236 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:26:33,237 | DEBUG | LiteLLM | response_cost: 0.0006254
2026-01-29 14:26:33,239 | DEBUG | LiteLLM | response_cost: 0.0006254
2026-01-29 14:26:33,240 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:26:33,241 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0006254
2026-01-29 14:26:33,242 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:26:33,243 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:26:33,244 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:26:33,245 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:26:33,246 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:26:33,248 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-29 14:26:33,252 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GuardrailSpanData object at 0x000002370C727100> with id None
2026-01-29 14:26:33,254 | DEBUG | openai.agents | Resetting current trace
2026-01-29 14:26:33,255 | ERROR | orchestrator | Reasoning failed
Traceback (most recent call last):
  File "c:\users\dell\downloads\chatbot-project-final\chatbot-project-main\digital_human_sdk\digital_human_sdk\app\main.py", line 420, in run_digital_human_chat
    async for event in reasoning_stream.stream_events():
  File "C:\Users\DELL\Downloads\chatbot-project-final\chatbot-project-main\backend\venv\lib\site-packages\agents\result.py", line 347, in stream_events
    raise self._stored_exception
  File "C:\Users\DELL\Downloads\chatbot-project-final\chatbot-project-main\backend\venv\lib\site-packages\agents\run.py", line 1260, in _start_streaming
    output_guardrail_results = await streamed_result._output_guardrails_task
  File "C:\Users\DELL\Downloads\chatbot-project-final\chatbot-project-main\backend\venv\lib\site-packages\agents\run.py", line 1814, in _run_output_guardrails
    raise OutputGuardrailTripwireTriggered(result)
agents.exceptions.OutputGuardrailTripwireTriggered: Guardrail OutputGuardrail triggered tripwire
2026-01-29 14:26:33,280 | INFO | session_summary | üß† SUMMARY_START | session_id=65c1e585-bc10-4f5a-9b70-e15f0513bbff
2026-01-29 14:26:33,297 | INFO | session_summary | üìä Unsummarized messages = 12
2026-01-29 14:26:33,300 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-29 14:26:33,303 | INFO | chat | ‚úÖ Stream complete | tokens=5 | time=10.56s
2026-01-29 14:26:33,835 | DEBUG | httpcore.connection | close.started
2026-01-29 14:26:33,836 | DEBUG | httpcore.connection | close.complete
2026-01-29 14:26:33,837 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-29 14:26:33,843 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023702814E20>
2026-01-29 14:26:33,845 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000023700CEDBC0> server_hostname='api.openai.com' timeout=5.0
2026-01-29 14:26:33,858 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000237028153C0>
2026-01-29 14:26:33,859 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-29 14:26:33,862 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-29 14:26:33,862 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-29 14:26:33,865 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-29 14:26:33,865 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-29 14:26:34,951 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Thu, 29 Jan 2026 08:56:34 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_66724b22dfcce28818d0ea0a96064f7b'), (b'openai-processing-ms', b'175'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'178'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c57845639dd77d3-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-29 14:26:34,952 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-29 14:26:34,953 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-29 14:26:34,954 | DEBUG | httpcore.http11 | response_closed.started
2026-01-29 14:26:34,954 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-29 14:26:34,955 | DEBUG | openai.agents | Exported 6 items
2026-01-29 14:26:50,533 | INFO | chat | üì© Chat request | user_id=3 | request_id=47dc060f-c0db-460c-b7c5-d8e74ae459ee
2026-01-29 14:26:50,534 | INFO | chat | üßë User message | i want to die
2026-01-29 14:26:50,574 | INFO | chat | ü§ñ Stream started | session_id=65c1e585-bc10-4f5a-9b70-e15f0513bbff | request_id=47dc060f-c0db-460c-b7c5-d8e74ae459ee
2026-01-29 14:26:50,575 | INFO | orchestrator | üî• Orchestrator started
2026-01-29 14:26:50,578 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_6fb431364338436a9e2f8b0e268e91d1
2026-01-29 14:26:50,579 | DEBUG | openai.agents | Setting current trace: trace_6fb431364338436a9e2f8b0e268e91d1
2026-01-29 14:26:50,580 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000002370C7A7830> with id None
2026-01-29 14:26:50,581 | DEBUG | openai.agents | Running agent RouterAgent (turn 1)
2026-01-29 14:26:50,583 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GuardrailSpanData object at 0x000002370C793A00> with id None
2026-01-29 14:26:50,587 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000002370C78F5E0> with id None
2026-01-29 14:26:50,588 | DEBUG | openai.agents | Calling LLM
2026-01-29 14:26:50,589 | DEBUG | LiteLLM | 

2026-01-29 14:26:50,589 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-29 14:26:50,591 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'i want to die'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-29 14:26:50,592 | DEBUG | LiteLLM | 

2026-01-29 14:26:50,593 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-29 14:26:50,593 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-29 14:26:50,596 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-29 14:26:50,602 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:26:50,604 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-29 14:26:50,606 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'i want to die'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-29 14:26:50,607 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-29 14:26:50,608 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-29 14:26:50,608 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-29 14:26:50,609 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:26:50,611 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:26:50,613 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'i want to die'}]}], 'system_instruction': {'parts': [{'text': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n'}]}, 'generationConfig': {}}'
[0m

2026-01-29 14:26:52,366 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\n  \"use_memory\": false,\n  \"use_tool\": false,\n  \"tool_name\": \"none\",\n  \"tool_arguments\": null,\n  \"intent\": \"none\",\n  \"memory_key\": null\n}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 489,
    "candidatesTokenCount": 56,
    "totalTokenCount": 625,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 489
      }
    ],
    "thoughtsTokenCount": 80
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "0yB7aaKFE6ed4-EP7dbL-QM"
}



2026-01-29 14:26:52,369 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-29 14:26:52,371 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 14:26:52,371 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-29 14:26:52,373 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:26:52,374 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 14:26:52,376 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:26:52,377 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:26:52,377 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:26:52,380 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:26:52,379 | DEBUG | LiteLLM | response_cost: 0.0004867
2026-01-29 14:26:52,382 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:26:52,383 | DEBUG | openai.agents | Received model response
2026-01-29 14:26:52,384 | DEBUG | LiteLLM | response_cost: 0.0004867
2026-01-29 14:26:52,387 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x000002370C8D6110>>
2026-01-29 14:26:52,388 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:26:52,389 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-29 14:26:52,391 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:26:52,392 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-29 14:26:52,392 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-29 14:26:52,393 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:26:52,395 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-29 14:26:52,397 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:26:52,399 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-29 14:26:52,403 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:26:52,404 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:26:52,406 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:26:52,407 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:26:52,408 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:26:52,409 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:26:52,409 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:26:52,411 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:26:52,412 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-29 14:26:52,414 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0004867
2026-01-29 14:26:52,419 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:26:52,421 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:26:52,422 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:26:52,425 | DEBUG | openai.agents | Resetting current trace
2026-01-29 14:26:52,491 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_5015608ea006407e8ff322703ca0eadd
2026-01-29 14:26:52,494 | DEBUG | openai.agents | Setting current trace: trace_5015608ea006407e8ff322703ca0eadd
2026-01-29 14:26:52,494 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000002370C785710> with id None
2026-01-29 14:26:52,497 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000002370C78F820> with id None
2026-01-29 14:26:52,498 | DEBUG | openai.agents | Calling LLM
2026-01-29 14:26:52,500 | DEBUG | LiteLLM | 

2026-01-29 14:26:52,501 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-29 14:26:52,501 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "assistant", "content": "Hello! How can I help you today?Something went wrong, but I\\u2019m still here. Please try again."}, {"role": "user", "content": "hello"}, {"role": "assistant", "content": "Hello! How can I help you today?Something went wrong, but I\\u2019m still here. Please try again."}, {"role": "user", "content": "heellooo ... acn you tell about photosynthesis"}, {"role": "assistant", "content": "Photosynthesis is the process by which green plants, algae, and some bacteria convert light energy into chemical energy. This chemical energy is stored in glucose (sugar), which they use as fuel.\\n\\nHere\'s a simplified breakdown of the process:\\n1.  **Ingredients:** Plants take in carbon dioxide from the air through small pores called stomata, and water from the soil through their roots.\\n2.  **Location:** This process primarily occurs in the chloroplasts within plant cells, which contain a green pigment called chlorophyll. Chlorophyll absorbs sunlight.\\n3.  **Reaction:** Using the energy from sunlight, chlorophyll facilitates a chemical reaction that combines carbon dioxide and water to produce glucose and oxygen.\\n4.  **Outputs:** The plant uses the glucose for energy and growth, and releases oxygen as a byproduct into the atmosphere.\\n\\nEssentially, it\'s how plants \\"eat\\" and produce the oxygen we breathe!Something went wrong, but I\\u2019m still here. Please try again."}, {"role": "user", "content": "i want to die"}, {"role": "user", "content": "i want to die"}], "memory": [], "tool_context": {}}'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=True, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-29 14:26:52,503 | DEBUG | LiteLLM | 

2026-01-29 14:26:52,504 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-29 14:26:52,505 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-29 14:26:52,506 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-29 14:26:52,509 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:26:52,510 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-29 14:26:52,511 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "assistant", "content": "Hello! How can I help you today?Something went wrong, but I\\u2019m still here. Please try again."}, {"role": "user", "content": "hello"}, {"role": "assistant", "content": "Hello! How can I help you today?Something went wrong, but I\\u2019m still here. Please try again."}, {"role": "user", "content": "heellooo ... acn you tell about photosynthesis"}, {"role": "assistant", "content": "Photosynthesis is the process by which green plants, algae, and some bacteria convert light energy into chemical energy. This chemical energy is stored in glucose (sugar), which they use as fuel.\\n\\nHere\'s a simplified breakdown of the process:\\n1.  **Ingredients:** Plants take in carbon dioxide from the air through small pores called stomata, and water from the soil through their roots.\\n2.  **Location:** This process primarily occurs in the chloroplasts within plant cells, which contain a green pigment called chlorophyll. Chlorophyll absorbs sunlight.\\n3.  **Reaction:** Using the energy from sunlight, chlorophyll facilitates a chemical reaction that combines carbon dioxide and water to produce glucose and oxygen.\\n4.  **Outputs:** The plant uses the glucose for energy and growth, and releases oxygen as a byproduct into the atmosphere.\\n\\nEssentially, it\'s how plants \\"eat\\" and produce the oxygen we breathe!Something went wrong, but I\\u2019m still here. Please try again."}, {"role": "user", "content": "i want to die"}, {"role": "user", "content": "i want to die"}], "memory": [], "tool_context": {}}'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-29 14:26:52,512 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': True}
2026-01-29 14:26:52,514 | DEBUG | LiteLLM | Final returned optional params: {'stream': True}
2026-01-29 14:26:52,516 | DEBUG | LiteLLM | self.optional_params: {'stream': True}
2026-01-29 14:26:52,518 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:26:52,525 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:26:52,527 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?key=*****=sse \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "assistant", "content": "Hello! How can I help you today?Something went wrong, but I\\u2019m still here. Please try again."}, {"role": "user", "content": "hello"}, {"role": "assistant", "content": "Hello! How can I help you today?Something went wrong, but I\\u2019m still here. Please try again."}, {"role": "user", "content": "heellooo ... acn you tell about photosynthesis"}, {"role": "assistant", "content": "Photosynthesis is the process by which green plants, algae, and some bacteria convert light energy into chemical energy. This chemical energy is stored in glucose (sugar), which they use as fuel.\\n\\nHere\'s a simplified breakdown of the process:\\n1.  **Ingredients:** Plants take in carbon dioxide from the air through small pores called stomata, and water from the soil through their roots.\\n2.  **Location:** This process primarily occurs in the chloroplasts within plant cells, which contain a green pigment called chlorophyll. Chlorophyll absorbs sunlight.\\n3.  **Reaction:** Using the energy from sunlight, chlorophyll facilitates a chemical reaction that combines carbon dioxide and water to produce glucose and oxygen.\\n4.  **Outputs:** The plant uses the glucose for energy and growth, and releases oxygen as a byproduct into the atmosphere.\\n\\nEssentially, it\'s how plants \\"eat\\" and produce the oxygen we breathe!Something went wrong, but I\\u2019m still here. Please try again."}, {"role": "user", "content": "i want to die"}, {"role": "user", "content": "i want to die"}], "memory": [], "tool_context": {}}'}]}], 'system_instruction': {'parts': [{'text': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.'}]}, 'generationConfig': {}}'
[0m

2026-01-29 14:26:52,533 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 14:26:52,541 | DEBUG | LiteLLM | messages in token_counter: None, text in token_counter: 
2026-01-29 14:26:52,546 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:26:52,549 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:26:52,553 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:26:52,563 | DEBUG | LiteLLM | response_cost: 0.0
2026-01-29 14:26:54,376 | DEBUG | LiteLLM | RAW RESPONSE:
first stream response received


2026-01-29 14:26:54,378 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': "I'm really sorry to hear you're feeling this way. Please"}], 'role': 'model'}, 'index': 0}], 'usageMetadata': {'promptTokenCount': 584, 'candidatesTokenCount': 15, 'totalTokenCount': 701, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 584}], 'thoughtsTokenCount': 102}, 'modelVersion': 'gemini-2.5-flash', 'responseId': '1CB7aazsHrzIjuMP-N_TkA8'}
2026-01-29 14:26:54,381 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='1CB7aazsHrzIjuMP-N_TkA8', created=1769677014, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content="I'm really sorry to hear you're feeling this way. Please", role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=117, prompt_tokens=584, total_tokens=701, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=102, rejected_prediction_tokens=None, text_tokens=15, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=584, image_tokens=None)))
2026-01-29 14:26:54,384 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content="I'm really sorry to hear you're feeling this way. Please", role='assistant', function_call=None, tool_calls=None, audio=None)
2026-01-29 14:26:54,387 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='1CB7aazsHrzIjuMP-N_TkA8', created=1769677014, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content="I'm really sorry to hear you're feeling this way. Please", role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=117, prompt_tokens=584, total_tokens=701, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=102, rejected_prediction_tokens=None, text_tokens=15, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=584, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-29 14:26:54,400 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': " know that you're not alone and there are people who want to help.\n\nIf you are in immediate danger, please call your local emergency services.\n\nYou can also reach out to a crisis hotline for support. Here are some resources"}], 'role': 'model'}, 'index': 0}], 'usageMetadata': {'promptTokenCount': 584, 'candidatesTokenCount': 63, 'totalTokenCount': 749, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 584}], 'thoughtsTokenCount': 102}, 'modelVersion': 'gemini-2.5-flash', 'responseId': '1CB7aazsHrzIjuMP-N_TkA8'}
2026-01-29 14:26:54,408 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='1CB7aazsHrzIjuMP-N_TkA8', created=1769677014, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content=" know that you're not alone and there are people who want to help.\n\nIf you are in immediate danger, please call your local emergency services.\n\nYou can also reach out to a crisis hotline for support. Here are some resources", role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=165, prompt_tokens=584, total_tokens=749, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=102, rejected_prediction_tokens=None, text_tokens=63, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=584, image_tokens=None)))
2026-01-29 14:26:54,413 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content=" know that you're not alone and there are people who want to help.\n\nIf you are in immediate danger, please call your local emergency services.\n\nYou can also reach out to a crisis hotline for support. Here are some resources", role=None, function_call=None, tool_calls=None, audio=None)
2026-01-29 14:26:54,414 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='1CB7aazsHrzIjuMP-N_TkA8', created=1769677014, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=" know that you're not alone and there are people who want to help.\n\nIf you are in immediate danger, please call your local emergency services.\n\nYou can also reach out to a crisis hotline for support. Here are some resources", role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=165, prompt_tokens=584, total_tokens=749, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=102, rejected_prediction_tokens=None, text_tokens=63, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=584, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-29 14:26:54,593 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': ':\n\n*   **National Suicide Prevention Lifeline (US):** Call or text 988\n*   **Crisis Text Line (US):** Text HOME to 741741\n*   **The Sam'}], 'role': 'model'}, 'index': 0}], 'usageMetadata': {'promptTokenCount': 584, 'candidatesTokenCount': 111, 'totalTokenCount': 797, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 584}], 'thoughtsTokenCount': 102}, 'modelVersion': 'gemini-2.5-flash', 'responseId': '1CB7aazsHrzIjuMP-N_TkA8'}
2026-01-29 14:26:54,595 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='1CB7aazsHrzIjuMP-N_TkA8', created=1769677014, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content=':\n\n*   **National Suicide Prevention Lifeline (US):** Call or text 988\n*   **Crisis Text Line (US):** Text HOME to 741741\n*   **The Sam', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=213, prompt_tokens=584, total_tokens=797, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=102, rejected_prediction_tokens=None, text_tokens=111, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=584, image_tokens=None)))
2026-01-29 14:26:54,599 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content=':\n\n*   **National Suicide Prevention Lifeline (US):** Call or text 988\n*   **Crisis Text Line (US):** Text HOME to 741741\n*   **The Sam', role=None, function_call=None, tool_calls=None, audio=None)
2026-01-29 14:26:54,600 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='1CB7aazsHrzIjuMP-N_TkA8', created=1769677014, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=':\n\n*   **National Suicide Prevention Lifeline (US):** Call or text 988\n*   **Crisis Text Line (US):** Text HOME to 741741\n*   **The Sam', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=213, prompt_tokens=584, total_tokens=797, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=102, rejected_prediction_tokens=None, text_tokens=111, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=584, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-29 14:26:54,823 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': 'aritans (UK):** Call 116 123\n*   **Your local mental health services**\n\nPlease reach out to one of these resources, or talk to a trusted friend, family member, or mental health professional'}], 'role': 'model'}, 'index': 0}], 'usageMetadata': {'promptTokenCount': 584, 'candidatesTokenCount': 160, 'totalTokenCount': 846, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 584}], 'thoughtsTokenCount': 102}, 'modelVersion': 'gemini-2.5-flash', 'responseId': '1CB7aazsHrzIjuMP-N_TkA8'}
2026-01-29 14:26:54,825 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='1CB7aazsHrzIjuMP-N_TkA8', created=1769677014, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='aritans (UK):** Call 116 123\n*   **Your local mental health services**\n\nPlease reach out to one of these resources, or talk to a trusted friend, family member, or mental health professional', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=262, prompt_tokens=584, total_tokens=846, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=102, rejected_prediction_tokens=None, text_tokens=160, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=584, image_tokens=None)))
2026-01-29 14:26:54,829 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content='aritans (UK):** Call 116 123\n*   **Your local mental health services**\n\nPlease reach out to one of these resources, or talk to a trusted friend, family member, or mental health professional', role=None, function_call=None, tool_calls=None, audio=None)
2026-01-29 14:26:54,830 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='1CB7aazsHrzIjuMP-N_TkA8', created=1769677014, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='aritans (UK):** Call 116 123\n*   **Your local mental health services**\n\nPlease reach out to one of these resources, or talk to a trusted friend, family member, or mental health professional', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=262, prompt_tokens=584, total_tokens=846, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=102, rejected_prediction_tokens=None, text_tokens=160, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=584, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-29 14:26:54,887 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': '. There is help available.'}], 'role': 'model'}, 'finishReason': 'STOP', 'index': 0}], 'usageMetadata': {'promptTokenCount': 584, 'candidatesTokenCount': 166, 'totalTokenCount': 852, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 584}], 'thoughtsTokenCount': 102}, 'modelVersion': 'gemini-2.5-flash', 'responseId': '1CB7aazsHrzIjuMP-N_TkA8'}
2026-01-29 14:26:54,890 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='1CB7aazsHrzIjuMP-N_TkA8', created=1769677014, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='. There is help available.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=268, prompt_tokens=584, total_tokens=852, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=102, rejected_prediction_tokens=None, text_tokens=166, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=584, image_tokens=None)))
2026-01-29 14:26:54,893 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content='. There is help available.', role=None, function_call=None, tool_calls=None, audio=None)
2026-01-29 14:26:54,894 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='1CB7aazsHrzIjuMP-N_TkA8', created=1769677014, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='. There is help available.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=268, prompt_tokens=584, total_tokens=852, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=102, rejected_prediction_tokens=None, text_tokens=166, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=584, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-29 14:26:54,900 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=False
2026-01-29 14:26:54,900 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=False
2026-01-29 14:26:54,901 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-29 14:26:54,903 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-29 14:26:54,904 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 14:26:54,908 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 14:26:54,909 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:26:54,910 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:26:54,912 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:26:54,912 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:26:54,913 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:26:54,914 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:26:54,916 | DEBUG | LiteLLM | response_cost: 0.0008452
2026-01-29 14:26:54,918 | DEBUG | LiteLLM | response_cost: 0.0008452
2026-01-29 14:26:54,920 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0008452
2026-01-29 14:26:54,921 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:26:54,923 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:26:54,923 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:26:54,925 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:26:54,925 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:26:54,927 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:26:54,929 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-29 14:26:54,933 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GuardrailSpanData object at 0x0000023702786F80> with id None
2026-01-29 14:26:54,935 | DEBUG | openai.agents | Resetting current trace
2026-01-29 14:26:54,951 | INFO | session_summary | üß† SUMMARY_START | session_id=65c1e585-bc10-4f5a-9b70-e15f0513bbff
2026-01-29 14:26:54,957 | INFO | session_summary | üìä Unsummarized messages = 14
2026-01-29 14:26:54,959 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-29 14:26:54,963 | INFO | chat | ‚úÖ Stream complete | tokens=5 | time=4.43s
2026-01-29 14:26:55,276 | DEBUG | httpcore.connection | close.started
2026-01-29 14:26:55,277 | DEBUG | httpcore.connection | close.complete
2026-01-29 14:26:55,278 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-29 14:26:55,321 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023702814D30>
2026-01-29 14:26:55,321 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000023700CEDBC0> server_hostname='api.openai.com' timeout=5.0
2026-01-29 14:26:55,337 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023702814EB0>
2026-01-29 14:26:55,338 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-29 14:26:55,340 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-29 14:26:55,341 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-29 14:26:55,342 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-29 14:26:55,342 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-29 14:26:55,898 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Thu, 29 Jan 2026 08:56:55 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_f711035481fde31d44b1b0fd0c6ba8e4'), (b'openai-processing-ms', b'221'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'224'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c5784dc6874dfdc-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-29 14:26:55,900 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-29 14:26:55,901 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-29 14:26:55,901 | DEBUG | httpcore.http11 | response_closed.started
2026-01-29 14:26:55,902 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-29 14:26:55,902 | DEBUG | openai.agents | Exported 8 items
2026-01-29 14:27:35,126 | INFO | chat | üì© Chat request | user_id=3 | request_id=9e879619-6400-44cb-8bc7-89d9a85a9dee
2026-01-29 14:27:35,128 | INFO | chat | üßë User message | i want to get high fever
2026-01-29 14:27:35,160 | INFO | chat | ü§ñ Stream started | session_id=65c1e585-bc10-4f5a-9b70-e15f0513bbff | request_id=9e879619-6400-44cb-8bc7-89d9a85a9dee
2026-01-29 14:27:35,163 | INFO | orchestrator | üî• Orchestrator started
2026-01-29 14:27:35,166 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_bb9e4923336c4542946acb3e0a1a8623
2026-01-29 14:27:35,167 | DEBUG | openai.agents | Setting current trace: trace_bb9e4923336c4542946acb3e0a1a8623
2026-01-29 14:27:35,169 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000002370C1BC540> with id None
2026-01-29 14:27:35,170 | DEBUG | openai.agents | Running agent RouterAgent (turn 1)
2026-01-29 14:27:35,171 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GuardrailSpanData object at 0x000002370C7719C0> with id None
2026-01-29 14:27:35,173 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000002370C78FE80> with id None
2026-01-29 14:27:35,174 | DEBUG | openai.agents | Calling LLM
2026-01-29 14:27:35,174 | DEBUG | LiteLLM | 

2026-01-29 14:27:35,175 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-29 14:27:35,176 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'i want to get high fever'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-29 14:27:35,179 | DEBUG | LiteLLM | 

2026-01-29 14:27:35,181 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-29 14:27:35,182 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-29 14:27:35,183 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-29 14:27:35,185 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:27:35,187 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-29 14:27:35,189 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'i want to get high fever'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-29 14:27:35,193 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-29 14:27:35,195 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-29 14:27:35,196 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-29 14:27:35,196 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:27:35,199 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:27:35,201 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'i want to get high fever'}]}], 'system_instruction': {'parts': [{'text': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n'}]}, 'generationConfig': {}}'
[0m

2026-01-29 14:27:36,517 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\n  \"use_memory\": false,\n  \"use_tool\": false,\n  \"tool_name\": \"none\",\n  \"tool_arguments\": null,\n  \"intent\": \"none\",\n  \"memory_key\": null\n}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 491,
    "candidatesTokenCount": 56,
    "totalTokenCount": 627,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 491
      }
    ],
    "thoughtsTokenCount": 80
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "_yB7aaiJHZ7ZjuMP4siCkA0"
}



2026-01-29 14:27:36,519 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-29 14:27:36,522 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 14:27:36,522 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-29 14:27:36,523 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:27:36,524 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 14:27:36,525 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:27:36,527 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:27:36,528 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:27:36,529 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:27:36,530 | DEBUG | LiteLLM | response_cost: 0.0004873
2026-01-29 14:27:36,532 | DEBUG | openai.agents | Received model response
2026-01-29 14:27:36,531 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:27:36,534 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x000002370C7A8A30>>
2026-01-29 14:27:36,535 | DEBUG | LiteLLM | response_cost: 0.0004873
2026-01-29 14:27:36,541 | DEBUG | httpcore.connection | close.started
2026-01-29 14:27:36,537 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-29 14:27:36,542 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:27:36,545 | DEBUG | httpcore.connection | close.complete
2026-01-29 14:27:36,547 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-29 14:27:36,548 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-29 14:27:36,549 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:27:36,551 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-29 14:27:36,552 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:27:36,553 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-29 14:27:36,553 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:27:36,555 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:27:36,560 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:27:36,561 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-29 14:27:36,562 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:27:36,562 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:27:36,564 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:27:36,567 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:27:36,568 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:27:36,569 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-29 14:27:36,569 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:27:36,569 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0004873
2026-01-29 14:27:36,572 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:27:36,576 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:27:36,578 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:27:36,581 | DEBUG | openai.agents | Resetting current trace
2026-01-29 14:27:36,614 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002370C7AA4A0>
2026-01-29 14:27:36,615 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000023700CEDBC0> server_hostname='api.openai.com' timeout=5.0
2026-01-29 14:27:36,643 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002370BF5BD60>
2026-01-29 14:27:36,644 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-29 14:27:36,646 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-29 14:27:36,647 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-29 14:27:36,649 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-29 14:27:36,650 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-29 14:27:36,655 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_b4edef595f8846ad8642c26c2ab22947
2026-01-29 14:27:36,656 | DEBUG | openai.agents | Setting current trace: trace_b4edef595f8846ad8642c26c2ab22947
2026-01-29 14:27:36,657 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000002370C1BE020> with id None
2026-01-29 14:27:36,659 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000002370C78E440> with id None
2026-01-29 14:27:36,659 | DEBUG | openai.agents | Calling LLM
2026-01-29 14:27:36,660 | DEBUG | LiteLLM | 

2026-01-29 14:27:36,660 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-29 14:27:36,662 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "assistant", "content": "Hello! How can I help you today?Something went wrong, but I\\u2019m still here. Please try again."}, {"role": "user", "content": "heellooo ... acn you tell about photosynthesis"}, {"role": "assistant", "content": "Photosynthesis is the process by which green plants, algae, and some bacteria convert light energy into chemical energy. This chemical energy is stored in glucose (sugar), which they use as fuel.\\n\\nHere\'s a simplified breakdown of the process:\\n1.  **Ingredients:** Plants take in carbon dioxide from the air through small pores called stomata, and water from the soil through their roots.\\n2.  **Location:** This process primarily occurs in the chloroplasts within plant cells, which contain a green pigment called chlorophyll. Chlorophyll absorbs sunlight.\\n3.  **Reaction:** Using the energy from sunlight, chlorophyll facilitates a chemical reaction that combines carbon dioxide and water to produce glucose and oxygen.\\n4.  **Outputs:** The plant uses the glucose for energy and growth, and releases oxygen as a byproduct into the atmosphere.\\n\\nEssentially, it\'s how plants \\"eat\\" and produce the oxygen we breathe!Something went wrong, but I\\u2019m still here. Please try again."}, {"role": "user", "content": "i want to die"}, {"role": "assistant", "content": "I\'m really sorry to hear you\'re feeling this way. Please know that you\'re not alone and there are people who want to help.\\n\\nIf you are in immediate danger, please call your local emergency services.\\n\\nYou can also reach out to a crisis hotline for support. Here are some resources:\\n\\n*   **National Suicide Prevention Lifeline (US):** Call or text 988\\n*   **Crisis Text Line (US):** Text HOME to 741741\\n*   **The Samaritans (UK):** Call 116 123\\n*   **Your local mental health services**\\n\\nPlease reach out to one of these resources, or talk to a trusted friend, family member, or mental health professional. There is help available."}, {"role": "user", "content": "i want to get high fever"}, {"role": "user", "content": "i want to get high fever"}], "memory": [], "tool_context": {}}'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=True, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-29 14:27:36,663 | DEBUG | LiteLLM | 

2026-01-29 14:27:36,665 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-29 14:27:36,666 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-29 14:27:36,669 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-29 14:27:36,672 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:27:36,673 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-29 14:27:36,676 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "assistant", "content": "Hello! How can I help you today?Something went wrong, but I\\u2019m still here. Please try again."}, {"role": "user", "content": "heellooo ... acn you tell about photosynthesis"}, {"role": "assistant", "content": "Photosynthesis is the process by which green plants, algae, and some bacteria convert light energy into chemical energy. This chemical energy is stored in glucose (sugar), which they use as fuel.\\n\\nHere\'s a simplified breakdown of the process:\\n1.  **Ingredients:** Plants take in carbon dioxide from the air through small pores called stomata, and water from the soil through their roots.\\n2.  **Location:** This process primarily occurs in the chloroplasts within plant cells, which contain a green pigment called chlorophyll. Chlorophyll absorbs sunlight.\\n3.  **Reaction:** Using the energy from sunlight, chlorophyll facilitates a chemical reaction that combines carbon dioxide and water to produce glucose and oxygen.\\n4.  **Outputs:** The plant uses the glucose for energy and growth, and releases oxygen as a byproduct into the atmosphere.\\n\\nEssentially, it\'s how plants \\"eat\\" and produce the oxygen we breathe!Something went wrong, but I\\u2019m still here. Please try again."}, {"role": "user", "content": "i want to die"}, {"role": "assistant", "content": "I\'m really sorry to hear you\'re feeling this way. Please know that you\'re not alone and there are people who want to help.\\n\\nIf you are in immediate danger, please call your local emergency services.\\n\\nYou can also reach out to a crisis hotline for support. Here are some resources:\\n\\n*   **National Suicide Prevention Lifeline (US):** Call or text 988\\n*   **Crisis Text Line (US):** Text HOME to 741741\\n*   **The Samaritans (UK):** Call 116 123\\n*   **Your local mental health services**\\n\\nPlease reach out to one of these resources, or talk to a trusted friend, family member, or mental health professional. There is help available."}, {"role": "user", "content": "i want to get high fever"}, {"role": "user", "content": "i want to get high fever"}], "memory": [], "tool_context": {}}'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-29 14:27:36,680 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': True}
2026-01-29 14:27:36,681 | DEBUG | LiteLLM | Final returned optional params: {'stream': True}
2026-01-29 14:27:36,683 | DEBUG | LiteLLM | self.optional_params: {'stream': True}
2026-01-29 14:27:36,684 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:27:36,686 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:27:36,688 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?key=*****=sse \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "assistant", "content": "Hello! How can I help you today?Something went wrong, but I\\u2019m still here. Please try again."}, {"role": "user", "content": "heellooo ... acn you tell about photosynthesis"}, {"role": "assistant", "content": "Photosynthesis is the process by which green plants, algae, and some bacteria convert light energy into chemical energy. This chemical energy is stored in glucose (sugar), which they use as fuel.\\n\\nHere\'s a simplified breakdown of the process:\\n1.  **Ingredients:** Plants take in carbon dioxide from the air through small pores called stomata, and water from the soil through their roots.\\n2.  **Location:** This process primarily occurs in the chloroplasts within plant cells, which contain a green pigment called chlorophyll. Chlorophyll absorbs sunlight.\\n3.  **Reaction:** Using the energy from sunlight, chlorophyll facilitates a chemical reaction that combines carbon dioxide and water to produce glucose and oxygen.\\n4.  **Outputs:** The plant uses the glucose for energy and growth, and releases oxygen as a byproduct into the atmosphere.\\n\\nEssentially, it\'s how plants \\"eat\\" and produce the oxygen we breathe!Something went wrong, but I\\u2019m still here. Please try again."}, {"role": "user", "content": "i want to die"}, {"role": "assistant", "content": "I\'m really sorry to hear you\'re feeling this way. Please know that you\'re not alone and there are people who want to help.\\n\\nIf you are in immediate danger, please call your local emergency services.\\n\\nYou can also reach out to a crisis hotline for support. Here are some resources:\\n\\n*   **National Suicide Prevention Lifeline (US):** Call or text 988\\n*   **Crisis Text Line (US):** Text HOME to 741741\\n*   **The Samaritans (UK):** Call 116 123\\n*   **Your local mental health services**\\n\\nPlease reach out to one of these resources, or talk to a trusted friend, family member, or mental health professional. There is help available."}, {"role": "user", "content": "i want to get high fever"}, {"role": "user", "content": "i want to get high fever"}], "memory": [], "tool_context": {}}'}]}], 'system_instruction': {'parts': [{'text': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.'}]}, 'generationConfig': {}}'
[0m

2026-01-29 14:27:36,691 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 14:27:36,693 | DEBUG | LiteLLM | messages in token_counter: None, text in token_counter: 
2026-01-29 14:27:36,695 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:27:36,697 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:27:36,697 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:27:36,698 | DEBUG | LiteLLM | response_cost: 0.0
2026-01-29 14:27:37,411 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Thu, 29 Jan 2026 08:57:36 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_039363ca6eba25ffdc2aa7eaa367a74c'), (b'openai-processing-ms', b'433'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'436'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c5785de98b8a346-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-29 14:27:37,413 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-29 14:27:37,414 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-29 14:27:37,414 | DEBUG | httpcore.http11 | response_closed.started
2026-01-29 14:27:37,415 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-29 14:27:37,415 | DEBUG | openai.agents | Exported 3 items
2026-01-29 14:27:37,417 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-29 14:27:37,418 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-29 14:27:37,418 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-29 14:27:37,419 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-29 14:27:37,420 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-29 14:27:37,865 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Thu, 29 Jan 2026 08:57:36 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_6b9e646f9689f31a64672dbcaadaf1c2'), (b'openai-processing-ms', b'125'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'130'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c5785e368f3a346-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-29 14:27:37,866 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-29 14:27:37,867 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-29 14:27:37,868 | DEBUG | httpcore.http11 | response_closed.started
2026-01-29 14:27:37,868 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-29 14:27:37,869 | DEBUG | openai.agents | Exported 2 items
2026-01-29 14:27:37,979 | DEBUG | LiteLLM | RAW RESPONSE:
first stream response received


2026-01-29 14:27:37,980 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': 'I cannot help you with that. Intentionally trying to get a high fever can be dangerous and harmful to your health'}], 'role': 'model'}, 'index': 0}], 'usageMetadata': {'promptTokenCount': 738, 'candidatesTokenCount': 23, 'totalTokenCount': 853, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 738}], 'thoughtsTokenCount': 92}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'ACF7adaJE-eNg8UP-PaiIA'}
2026-01-29 14:27:37,982 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='ACF7adaJE-eNg8UP-PaiIA', created=1769677057, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='I cannot help you with that. Intentionally trying to get a high fever can be dangerous and harmful to your health', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=115, prompt_tokens=738, total_tokens=853, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=92, rejected_prediction_tokens=None, text_tokens=23, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=738, image_tokens=None)))
2026-01-29 14:27:37,985 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content='I cannot help you with that. Intentionally trying to get a high fever can be dangerous and harmful to your health', role='assistant', function_call=None, tool_calls=None, audio=None)
2026-01-29 14:27:37,988 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='ACF7adaJE-eNg8UP-PaiIA', created=1769677057, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='I cannot help you with that. Intentionally trying to get a high fever can be dangerous and harmful to your health', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=115, prompt_tokens=738, total_tokens=853, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=92, rejected_prediction_tokens=None, text_tokens=23, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=738, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-29 14:27:37,991 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': ". A high fever is often a sign of illness and can lead to serious complications.\n\nIf you are feeling unwell, it's always best to consult a healthcare professional. They can properly diagnose any symptoms you might have and provide appropriate treatment"}], 'role': 'model'}, 'index': 0}], 'usageMetadata': {'promptTokenCount': 738, 'candidatesTokenCount': 71, 'totalTokenCount': 901, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 738}], 'thoughtsTokenCount': 92}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'ACF7adaJE-eNg8UP-PaiIA'}
2026-01-29 14:27:37,994 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='ACF7adaJE-eNg8UP-PaiIA', created=1769677057, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content=". A high fever is often a sign of illness and can lead to serious complications.\n\nIf you are feeling unwell, it's always best to consult a healthcare professional. They can properly diagnose any symptoms you might have and provide appropriate treatment", role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=163, prompt_tokens=738, total_tokens=901, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=92, rejected_prediction_tokens=None, text_tokens=71, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=738, image_tokens=None)))
2026-01-29 14:27:37,998 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content=". A high fever is often a sign of illness and can lead to serious complications.\n\nIf you are feeling unwell, it's always best to consult a healthcare professional. They can properly diagnose any symptoms you might have and provide appropriate treatment", role=None, function_call=None, tool_calls=None, audio=None)
2026-01-29 14:27:38,000 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='ACF7adaJE-eNg8UP-PaiIA', created=1769677057, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=". A high fever is often a sign of illness and can lead to serious complications.\n\nIf you are feeling unwell, it's always best to consult a healthcare professional. They can properly diagnose any symptoms you might have and provide appropriate treatment", role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=163, prompt_tokens=738, total_tokens=901, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=92, rejected_prediction_tokens=None, text_tokens=71, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=738, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-29 14:27:38,004 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': '.'}], 'role': 'model'}, 'finishReason': 'STOP', 'index': 0}], 'usageMetadata': {'promptTokenCount': 738, 'candidatesTokenCount': 72, 'totalTokenCount': 902, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 738}], 'thoughtsTokenCount': 92}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'ACF7adaJE-eNg8UP-PaiIA'}
2026-01-29 14:27:38,005 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='ACF7adaJE-eNg8UP-PaiIA', created=1769677058, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=164, prompt_tokens=738, total_tokens=902, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=92, rejected_prediction_tokens=None, text_tokens=72, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=738, image_tokens=None)))
2026-01-29 14:27:38,011 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content='.', role=None, function_call=None, tool_calls=None, audio=None)
2026-01-29 14:27:38,013 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='ACF7adaJE-eNg8UP-PaiIA', created=1769677057, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=164, prompt_tokens=738, total_tokens=902, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=92, rejected_prediction_tokens=None, text_tokens=72, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=738, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-29 14:27:38,019 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=False
2026-01-29 14:27:38,020 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=False
2026-01-29 14:27:38,024 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-29 14:27:38,027 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-29 14:27:38,029 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 14:27:38,030 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:27:38,030 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 14:27:38,032 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:27:38,034 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:27:38,035 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:27:38,037 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:27:38,038 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:27:38,037 | DEBUG | LiteLLM | response_cost: 0.0006314
2026-01-29 14:27:38,042 | DEBUG | LiteLLM | response_cost: 0.0006314
2026-01-29 14:27:38,049 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:27:38,053 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:27:38,049 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0006314
2026-01-29 14:27:38,056 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:27:38,059 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:27:38,061 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:27:38,062 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:27:38,067 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-29 14:27:38,071 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GuardrailSpanData object at 0x000002370C8D8280> with id None
2026-01-29 14:27:38,072 | DEBUG | openai.agents | Resetting current trace
2026-01-29 14:27:38,079 | INFO | session_summary | üß† SUMMARY_START | session_id=65c1e585-bc10-4f5a-9b70-e15f0513bbff
2026-01-29 14:27:38,097 | INFO | session_summary | üìä Unsummarized messages = 16
2026-01-29 14:27:38,101 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-29 14:27:38,105 | INFO | chat | ‚úÖ Stream complete | tokens=3 | time=2.98s
2026-01-29 14:27:42,965 | DEBUG | httpcore.connection | close.started
2026-01-29 14:27:42,966 | DEBUG | httpcore.connection | close.complete
2026-01-29 14:27:42,968 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-29 14:27:42,980 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002370C8D5AB0>
2026-01-29 14:27:42,981 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000023700CEDBC0> server_hostname='api.openai.com' timeout=5.0
2026-01-29 14:27:42,996 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002370C8D67D0>
2026-01-29 14:27:42,997 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-29 14:27:42,997 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-29 14:27:42,997 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-29 14:27:42,999 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-29 14:27:42,999 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-29 14:27:44,331 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Thu, 29 Jan 2026 08:57:43 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_ddbed18c1f458e1ee7d4f756711b5dd3'), (b'openai-processing-ms', b'165'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'168'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c5786064df891e5-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-29 14:27:44,333 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-29 14:27:44,334 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-29 14:27:44,335 | DEBUG | httpcore.http11 | response_closed.started
2026-01-29 14:27:44,335 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-29 14:27:44,336 | DEBUG | openai.agents | Exported 3 items
2026-01-29 14:28:10,316 | INFO | chat | üì© Chat request | user_id=3 | request_id=3a21c91c-b63d-4f6d-8300-f56e2eb1e7ac
2026-01-29 14:28:10,318 | INFO | chat | üßë User message | i love dosa
2026-01-29 14:28:10,347 | INFO | chat | ü§ñ Stream started | session_id=65c1e585-bc10-4f5a-9b70-e15f0513bbff | request_id=3a21c91c-b63d-4f6d-8300-f56e2eb1e7ac
2026-01-29 14:28:10,349 | INFO | orchestrator | üî• Orchestrator started
2026-01-29 14:28:10,352 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_8ca8fc5562f74b1892c1b3ed91f3a819
2026-01-29 14:28:10,352 | DEBUG | openai.agents | Setting current trace: trace_8ca8fc5562f74b1892c1b3ed91f3a819
2026-01-29 14:28:10,353 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000002370C1BC540> with id None
2026-01-29 14:28:10,354 | DEBUG | openai.agents | Running agent RouterAgent (turn 1)
2026-01-29 14:28:10,355 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GuardrailSpanData object at 0x000002370C7AF2C0> with id None
2026-01-29 14:28:10,357 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000002370C78FA00> with id None
2026-01-29 14:28:10,358 | DEBUG | openai.agents | Calling LLM
2026-01-29 14:28:10,359 | DEBUG | LiteLLM | 

2026-01-29 14:28:10,363 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-29 14:28:10,364 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'i love dosa'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-29 14:28:10,366 | DEBUG | LiteLLM | 

2026-01-29 14:28:10,369 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-29 14:28:10,370 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-29 14:28:10,374 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-29 14:28:10,379 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:28:10,380 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-29 14:28:10,385 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'i love dosa'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-29 14:28:10,389 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-29 14:28:10,391 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-29 14:28:10,392 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-29 14:28:10,393 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:28:10,401 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:28:10,407 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'i love dosa'}]}], 'system_instruction': {'parts': [{'text': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n'}]}, 'generationConfig': {}}'
[0m

2026-01-29 14:28:11,894 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\n  \"use_memory\": true,\n  \"use_tool\": false,\n  \"tool_name\": \"none\",\n  \"tool_arguments\": null,\n  \"intent\": \"write\",\n  \"memory_key\": \"food_preference\"\n}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 488,
    "candidatesTokenCount": 60,
    "totalTokenCount": 596,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 488
      }
    ],
    "thoughtsTokenCount": 48
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "IiF7aZK2NMyBqfkP6O30yA8"
}



2026-01-29 14:28:11,897 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-29 14:28:11,898 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 14:28:11,898 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-29 14:28:11,899 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:28:11,901 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 14:28:11,902 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:28:11,903 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:28:11,904 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:28:11,905 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:28:11,906 | DEBUG | LiteLLM | response_cost: 0.00041640000000000004
2026-01-29 14:28:11,909 | DEBUG | openai.agents | Received model response
2026-01-29 14:28:11,907 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:28:11,910 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x000002370C7A8790>>
2026-01-29 14:28:11,911 | DEBUG | LiteLLM | response_cost: 0.00041640000000000004
2026-01-29 14:28:11,913 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-29 14:28:11,914 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:28:11,916 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-29 14:28:11,916 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-29 14:28:11,916 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:28:11,919 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-29 14:28:11,920 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:28:11,922 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:28:11,927 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:28:11,928 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:28:11,929 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-29 14:28:11,930 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:28:11,930 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:28:11,933 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:28:11,934 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:28:11,939 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:28:11,940 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-29 14:28:11,941 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:28:11,941 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.00041640000000000004
2026-01-29 14:28:11,944 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:28:11,945 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:28:11,945 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:28:11,948 | DEBUG | openai.agents | Resetting current trace
2026-01-29 14:28:12,012 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_d91462c456c94d98990f6bff060068f6
2026-01-29 14:28:12,013 | DEBUG | openai.agents | Setting current trace: trace_d91462c456c94d98990f6bff060068f6
2026-01-29 14:28:12,013 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000002370C785990> with id None
2026-01-29 14:28:12,013 | DEBUG | openai.agents | Running agent Memory Agent (turn 1)
2026-01-29 14:28:12,016 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000002370C78ECE0> with id None
2026-01-29 14:28:12,019 | DEBUG | openai.agents | Calling LLM
2026-01-29 14:28:12,020 | DEBUG | LiteLLM | 

2026-01-29 14:28:12,021 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-29 14:28:12,021 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a Memory Extraction Agent.\n\nYour job is to extract ONLY long-term, personal, user-specific information\nfrom the user\'s message and decide the appropriate memory action.\n\nYou will be called ONLY when memory handling is required.\n\n========================\nWHAT TO EXTRACT\n========================\n- Stable personal facts (name, education, location, job)\n- Long-term preferences (food, language, habits)\n- Information the user would reasonably expect to be remembered\n- Facts that should be retrievable across sessions via semantic search\n\nStore memory as FULL, NATURAL-LANGUAGE TEXT.\nDO NOT create structured keys.\nDO NOT summarize aggressively.\nPreserve meaning exactly as stated.\n\n========================\nWHAT TO IGNORE\n========================\n- Questions\n- Small talk\n- Temporary states (mood, weather, current task)\n- Opinions about content\n- One-time or session-only context\n\n========================\nACTIONS (STRICT)\n========================\nUse exactly ONE action:\n\n- "save"\n  ‚Üí New long-term personal information stated for the first time\n\n- "update"\n  ‚Üí Existing long-term information is explicitly changed,\n    corrected, or overridden\n    (keywords: "now", "instead", "changed", "earlier", "previously")\n\n- "delete"\n  ‚Üí User explicitly asks to forget, remove, or delete information\n\nNEVER guess.\nNEVER infer deletion.\nNEVER revive deleted information.\nDeleted memory must be treated as non-existent.\n\n========================\nIMPORTANT RULES\n========================\n- Store memory as ONE complete sentence\n- Do NOT split into multiple memories\n- Do NOT invent information\n- Do NOT rely on database state\n- Backend will validate save vs update\n- If unsure, return "none"\n\n========================\nOUTPUT RULES (CRITICAL)\n========================\n- Return VALID JSON only\n- Do NOT explain anything\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Do NOT return multiple objects\n\n========================\nOUTPUT FORMAT (JSON ONLY)\n========================\n{\n  "action": "save | update | delete | none",\n  "memory_text": "full natural language memory text",\n  "confidence": 0.0\n}\n\n========================\nCONFIDENCE GUIDELINES\n========================\n- 0.9‚Äì1.0 ‚Üí Explicit personal fact (e.g., education, name, job)\n- 0.8‚Äì0.9 ‚Üí Clear long-term preference or habit\n- Below 0.7 ‚Üí Weak or ambiguous (avoid storing)\n\n========================\nNO MEMORY CASE\n========================\nIf no valid long-term memory is present, return:\n{\n  "action": "none",\n  "memory_text": "",\n  "confidence": 0.0\n}\n', 'role': 'system'}, {'role': 'user', 'content': 'i love dosa'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-29 14:28:12,023 | DEBUG | LiteLLM | 

2026-01-29 14:28:12,025 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-29 14:28:12,026 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-29 14:28:12,027 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-29 14:28:12,030 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:28:12,031 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-29 14:28:12,034 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Memory Extraction Agent.\n\nYour job is to extract ONLY long-term, personal, user-specific information\nfrom the user\'s message and decide the appropriate memory action.\n\nYou will be called ONLY when memory handling is required.\n\n========================\nWHAT TO EXTRACT\n========================\n- Stable personal facts (name, education, location, job)\n- Long-term preferences (food, language, habits)\n- Information the user would reasonably expect to be remembered\n- Facts that should be retrievable across sessions via semantic search\n\nStore memory as FULL, NATURAL-LANGUAGE TEXT.\nDO NOT create structured keys.\nDO NOT summarize aggressively.\nPreserve meaning exactly as stated.\n\n========================\nWHAT TO IGNORE\n========================\n- Questions\n- Small talk\n- Temporary states (mood, weather, current task)\n- Opinions about content\n- One-time or session-only context\n\n========================\nACTIONS (STRICT)\n========================\nUse exactly ONE action:\n\n- "save"\n  ‚Üí New long-term personal information stated for the first time\n\n- "update"\n  ‚Üí Existing long-term information is explicitly changed,\n    corrected, or overridden\n    (keywords: "now", "instead", "changed", "earlier", "previously")\n\n- "delete"\n  ‚Üí User explicitly asks to forget, remove, or delete information\n\nNEVER guess.\nNEVER infer deletion.\nNEVER revive deleted information.\nDeleted memory must be treated as non-existent.\n\n========================\nIMPORTANT RULES\n========================\n- Store memory as ONE complete sentence\n- Do NOT split into multiple memories\n- Do NOT invent information\n- Do NOT rely on database state\n- Backend will validate save vs update\n- If unsure, return "none"\n\n========================\nOUTPUT RULES (CRITICAL)\n========================\n- Return VALID JSON only\n- Do NOT explain anything\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Do NOT return multiple objects\n\n========================\nOUTPUT FORMAT (JSON ONLY)\n========================\n{\n  "action": "save | update | delete | none",\n  "memory_text": "full natural language memory text",\n  "confidence": 0.0\n}\n\n========================\nCONFIDENCE GUIDELINES\n========================\n- 0.9‚Äì1.0 ‚Üí Explicit personal fact (e.g., education, name, job)\n- 0.8‚Äì0.9 ‚Üí Clear long-term preference or habit\n- Below 0.7 ‚Üí Weak or ambiguous (avoid storing)\n\n========================\nNO MEMORY CASE\n========================\nIf no valid long-term memory is present, return:\n{\n  "action": "none",\n  "memory_text": "",\n  "confidence": 0.0\n}\n', 'role': 'system'}, {'role': 'user', 'content': 'i love dosa'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-29 14:28:12,036 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-29 14:28:12,037 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-29 14:28:12,037 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-29 14:28:12,038 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:28:12,040 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:28:12,042 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'i love dosa'}]}], 'system_instruction': {'parts': [{'text': 'You are a Memory Extraction Agent.\n\nYour job is to extract ONLY long-term, personal, user-specific information\nfrom the user\'s message and decide the appropriate memory action.\n\nYou will be called ONLY when memory handling is required.\n\n========================\nWHAT TO EXTRACT\n========================\n- Stable personal facts (name, education, location, job)\n- Long-term preferences (food, language, habits)\n- Information the user would reasonably expect to be remembered\n- Facts that should be retrievable across sessions via semantic search\n\nStore memory as FULL, NATURAL-LANGUAGE TEXT.\nDO NOT create structured keys.\nDO NOT summarize aggressively.\nPreserve meaning exactly as stated.\n\n========================\nWHAT TO IGNORE\n========================\n- Questions\n- Small talk\n- Temporary states (mood, weather, current task)\n- Opinions about content\n- One-time or session-only context\n\n========================\nACTIONS (STRICT)\n========================\nUse exactly ONE action:\n\n- "save"\n  ‚Üí New long-term personal information stated for the first time\n\n- "update"\n  ‚Üí Existing long-term information is explicitly changed,\n    corrected, or overridden\n    (keywords: "now", "instead", "changed", "earlier", "previously")\n\n- "delete"\n  ‚Üí User explicitly asks to forget, remove, or delete information\n\nNEVER guess.\nNEVER infer deletion.\nNEVER revive deleted information.\nDeleted memory must be treated as non-existent.\n\n========================\nIMPORTANT RULES\n========================\n- Store memory as ONE complete sentence\n- Do NOT split into multiple memories\n- Do NOT invent information\n- Do NOT rely on database state\n- Backend will validate save vs update\n- If unsure, return "none"\n\n========================\nOUTPUT RULES (CRITICAL)\n========================\n- Return VALID JSON only\n- Do NOT explain anything\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Do NOT return multiple objects\n\n========================\nOUTPUT FORMAT (JSON ONLY)\n========================\n{\n  "action": "save | update | delete | none",\n  "memory_text": "full natural language memory text",\n  "confidence": 0.0\n}\n\n========================\nCONFIDENCE GUIDELINES\n========================\n- 0.9‚Äì1.0 ‚Üí Explicit personal fact (e.g., education, name, job)\n- 0.8‚Äì0.9 ‚Üí Clear long-term preference or habit\n- Below 0.7 ‚Üí Weak or ambiguous (avoid storing)\n\n========================\nNO MEMORY CASE\n========================\nIf no valid long-term memory is present, return:\n{\n  "action": "none",\n  "memory_text": "",\n  "confidence": 0.0\n}\n'}]}, 'generationConfig': {}}'
[0m

2026-01-29 14:28:13,530 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "```json\n{\n  \"action\": \"save\",\n  \"memory_text\": \"I love dosa.\",\n  \"confidence\": 0.85\n}\n```"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 603,
    "candidatesTokenCount": 38,
    "totalTokenCount": 707,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 603
      }
    ],
    "thoughtsTokenCount": 66
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "JCF7aaCkF5q44-EPhdO26Qc"
}



2026-01-29 14:28:13,533 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-29 14:28:13,535 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 14:28:13,536 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-29 14:28:13,537 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:28:13,538 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 14:28:13,539 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:28:13,540 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:28:13,541 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:28:13,542 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:28:13,544 | DEBUG | LiteLLM | response_cost: 0.00044090000000000004
2026-01-29 14:28:13,546 | DEBUG | openai.agents | Received model response
2026-01-29 14:28:13,544 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:28:13,547 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x000002370C7A97E0>>
2026-01-29 14:28:13,548 | DEBUG | LiteLLM | response_cost: 0.00044090000000000004
2026-01-29 14:28:13,549 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-29 14:28:13,551 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:28:13,553 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-29 14:28:13,553 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-29 14:28:13,555 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:28:13,556 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-29 14:28:13,560 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:28:13,562 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:28:13,564 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:28:13,565 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:28:13,574 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-29 14:28:13,575 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:28:13,576 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:28:13,578 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:28:13,579 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:28:13,581 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:28:13,588 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-29 14:28:13,590 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:28:13,591 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.00044090000000000004
2026-01-29 14:28:13,595 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:28:13,596 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:28:13,598 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:28:13,604 | DEBUG | openai.agents | Resetting current trace
2026-01-29 14:28:13,605 | INFO | chat | üß† MEMORY_EVENT_RECEIVED | {'action': 'save', 'memory_text': 'I love dosa.', 'confidence': 0.85}
2026-01-29 14:28:13,607 | INFO | memory_action_executor | üß† APPLY_MEMORY_ACTION | user_id=3 | action={'action': 'save', 'memory_text': 'I love dosa.', 'confidence': 0.85}
2026-01-29 14:28:13,611 | INFO | memory_action_executor | üß† Memory Parsed | type=save | content=I love dosa. | confidence=0.85
2026-01-29 14:28:13,612 | INFO | memory_action_executor | üíæ Writing memory to DB
2026-01-29 14:28:13,688 | INFO | memory_action_executor | ‚úÖ DB COMMIT SUCCESS
2026-01-29 14:28:13,691 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_187fe1efcd934a4891136e5f9097ac79
2026-01-29 14:28:13,692 | DEBUG | openai.agents | Setting current trace: trace_187fe1efcd934a4891136e5f9097ac79
2026-01-29 14:28:13,693 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000002370C7A65C0> with id None
2026-01-29 14:28:13,694 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000002370C78E9E0> with id None
2026-01-29 14:28:13,695 | DEBUG | openai.agents | Calling LLM
2026-01-29 14:28:13,696 | DEBUG | LiteLLM | 

2026-01-29 14:28:13,696 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-29 14:28:13,698 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "assistant", "content": "Photosynthesis is the process by which green plants, algae, and some bacteria convert light energy into chemical energy. This chemical energy is stored in glucose (sugar), which they use as fuel.\\n\\nHere\'s a simplified breakdown of the process:\\n1.  **Ingredients:** Plants take in carbon dioxide from the air through small pores called stomata, and water from the soil through their roots.\\n2.  **Location:** This process primarily occurs in the chloroplasts within plant cells, which contain a green pigment called chlorophyll. Chlorophyll absorbs sunlight.\\n3.  **Reaction:** Using the energy from sunlight, chlorophyll facilitates a chemical reaction that combines carbon dioxide and water to produce glucose and oxygen.\\n4.  **Outputs:** The plant uses the glucose for energy and growth, and releases oxygen as a byproduct into the atmosphere.\\n\\nEssentially, it\'s how plants \\"eat\\" and produce the oxygen we breathe!Something went wrong, but I\\u2019m still here. Please try again."}, {"role": "user", "content": "i want to die"}, {"role": "assistant", "content": "I\'m really sorry to hear you\'re feeling this way. Please know that you\'re not alone and there are people who want to help.\\n\\nIf you are in immediate danger, please call your local emergency services.\\n\\nYou can also reach out to a crisis hotline for support. Here are some resources:\\n\\n*   **National Suicide Prevention Lifeline (US):** Call or text 988\\n*   **Crisis Text Line (US):** Text HOME to 741741\\n*   **The Samaritans (UK):** Call 116 123\\n*   **Your local mental health services**\\n\\nPlease reach out to one of these resources, or talk to a trusted friend, family member, or mental health professional. There is help available."}, {"role": "user", "content": "i want to get high fever"}, {"role": "assistant", "content": "I cannot help you with that. Intentionally trying to get a high fever can be dangerous and harmful to your health. A high fever is often a sign of illness and can lead to serious complications.\\n\\nIf you are feeling unwell, it\'s always best to consult a healthcare professional. They can properly diagnose any symptoms you might have and provide appropriate treatment."}, {"role": "user", "content": "i love dosa"}, {"role": "user", "content": "i love dosa"}], "memory": [{"memory_id": 11, "user_id": 3, "text": "I like the movie Dabang 2 and I like to watch it again and again.", "confidence": 0.85, "created_at": "2026-01-27T22:49:49.349202+05:30", "expires_at": "2026-02-27T09:57:54.723279+05:30", "score": 0.3999462425708771}], "tool_context": {}}'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=True, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-29 14:28:13,699 | DEBUG | LiteLLM | 

2026-01-29 14:28:13,701 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-29 14:28:13,703 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-29 14:28:13,704 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-29 14:28:13,707 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:28:13,708 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-29 14:28:13,709 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "assistant", "content": "Photosynthesis is the process by which green plants, algae, and some bacteria convert light energy into chemical energy. This chemical energy is stored in glucose (sugar), which they use as fuel.\\n\\nHere\'s a simplified breakdown of the process:\\n1.  **Ingredients:** Plants take in carbon dioxide from the air through small pores called stomata, and water from the soil through their roots.\\n2.  **Location:** This process primarily occurs in the chloroplasts within plant cells, which contain a green pigment called chlorophyll. Chlorophyll absorbs sunlight.\\n3.  **Reaction:** Using the energy from sunlight, chlorophyll facilitates a chemical reaction that combines carbon dioxide and water to produce glucose and oxygen.\\n4.  **Outputs:** The plant uses the glucose for energy and growth, and releases oxygen as a byproduct into the atmosphere.\\n\\nEssentially, it\'s how plants \\"eat\\" and produce the oxygen we breathe!Something went wrong, but I\\u2019m still here. Please try again."}, {"role": "user", "content": "i want to die"}, {"role": "assistant", "content": "I\'m really sorry to hear you\'re feeling this way. Please know that you\'re not alone and there are people who want to help.\\n\\nIf you are in immediate danger, please call your local emergency services.\\n\\nYou can also reach out to a crisis hotline for support. Here are some resources:\\n\\n*   **National Suicide Prevention Lifeline (US):** Call or text 988\\n*   **Crisis Text Line (US):** Text HOME to 741741\\n*   **The Samaritans (UK):** Call 116 123\\n*   **Your local mental health services**\\n\\nPlease reach out to one of these resources, or talk to a trusted friend, family member, or mental health professional. There is help available."}, {"role": "user", "content": "i want to get high fever"}, {"role": "assistant", "content": "I cannot help you with that. Intentionally trying to get a high fever can be dangerous and harmful to your health. A high fever is often a sign of illness and can lead to serious complications.\\n\\nIf you are feeling unwell, it\'s always best to consult a healthcare professional. They can properly diagnose any symptoms you might have and provide appropriate treatment."}, {"role": "user", "content": "i love dosa"}, {"role": "user", "content": "i love dosa"}], "memory": [{"memory_id": 11, "user_id": 3, "text": "I like the movie Dabang 2 and I like to watch it again and again.", "confidence": 0.85, "created_at": "2026-01-27T22:49:49.349202+05:30", "expires_at": "2026-02-27T09:57:54.723279+05:30", "score": 0.3999462425708771}], "tool_context": {}}'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-29 14:28:13,712 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': True}
2026-01-29 14:28:13,713 | DEBUG | LiteLLM | Final returned optional params: {'stream': True}
2026-01-29 14:28:13,714 | DEBUG | LiteLLM | self.optional_params: {'stream': True}
2026-01-29 14:28:13,714 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:28:13,717 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:28:13,719 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?key=*****=sse \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "assistant", "content": "Photosynthesis is the process by which green plants, algae, and some bacteria convert light energy into chemical energy. This chemical energy is stored in glucose (sugar), which they use as fuel.\\n\\nHere\'s a simplified breakdown of the process:\\n1.  **Ingredients:** Plants take in carbon dioxide from the air through small pores called stomata, and water from the soil through their roots.\\n2.  **Location:** This process primarily occurs in the chloroplasts within plant cells, which contain a green pigment called chlorophyll. Chlorophyll absorbs sunlight.\\n3.  **Reaction:** Using the energy from sunlight, chlorophyll facilitates a chemical reaction that combines carbon dioxide and water to produce glucose and oxygen.\\n4.  **Outputs:** The plant uses the glucose for energy and growth, and releases oxygen as a byproduct into the atmosphere.\\n\\nEssentially, it\'s how plants \\"eat\\" and produce the oxygen we breathe!Something went wrong, but I\\u2019m still here. Please try again."}, {"role": "user", "content": "i want to die"}, {"role": "assistant", "content": "I\'m really sorry to hear you\'re feeling this way. Please know that you\'re not alone and there are people who want to help.\\n\\nIf you are in immediate danger, please call your local emergency services.\\n\\nYou can also reach out to a crisis hotline for support. Here are some resources:\\n\\n*   **National Suicide Prevention Lifeline (US):** Call or text 988\\n*   **Crisis Text Line (US):** Text HOME to 741741\\n*   **The Samaritans (UK):** Call 116 123\\n*   **Your local mental health services**\\n\\nPlease reach out to one of these resources, or talk to a trusted friend, family member, or mental health professional. There is help available."}, {"role": "user", "content": "i want to get high fever"}, {"role": "assistant", "content": "I cannot help you with that. Intentionally trying to get a high fever can be dangerous and harmful to your health. A high fever is often a sign of illness and can lead to serious complications.\\n\\nIf you are feeling unwell, it\'s always best to consult a healthcare professional. They can properly diagnose any symptoms you might have and provide appropriate treatment."}, {"role": "user", "content": "i love dosa"}, {"role": "user", "content": "i love dosa"}], "memory": [{"memory_id": 11, "user_id": 3, "text": "I like the movie Dabang 2 and I like to watch it again and again.", "confidence": 0.85, "created_at": "2026-01-27T22:49:49.349202+05:30", "expires_at": "2026-02-27T09:57:54.723279+05:30", "score": 0.3999462425708771}], "tool_context": {}}'}]}], 'system_instruction': {'parts': [{'text': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.'}]}, 'generationConfig': {}}'
[0m

2026-01-29 14:28:13,724 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 14:28:13,725 | DEBUG | LiteLLM | messages in token_counter: None, text in token_counter: 
2026-01-29 14:28:13,725 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:28:13,726 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:28:13,728 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:28:13,729 | DEBUG | LiteLLM | response_cost: 0.0
2026-01-29 14:28:14,868 | DEBUG | httpcore.connection | close.started
2026-01-29 14:28:14,869 | DEBUG | httpcore.connection | close.complete
2026-01-29 14:28:14,870 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-29 14:28:14,898 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002370C7AAD40>
2026-01-29 14:28:14,899 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000023700CEDBC0> server_hostname='api.openai.com' timeout=5.0
2026-01-29 14:28:14,912 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002370C7AB0D0>
2026-01-29 14:28:14,913 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-29 14:28:14,914 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-29 14:28:14,915 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-29 14:28:14,916 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-29 14:28:14,917 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-29 14:28:15,252 | DEBUG | LiteLLM | RAW RESPONSE:
first stream response received


2026-01-29 14:28:15,254 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': "That's wonderful! Dosa is a delicious and popular dish"}], 'role': 'model'}, 'index': 0}], 'usageMetadata': {'promptTokenCount': 921, 'candidatesTokenCount': 13, 'totalTokenCount': 989, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 921}], 'thoughtsTokenCount': 55}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'JSF7aY3vM5at4-EP4PyaqA8'}
2026-01-29 14:28:15,256 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='JSF7aY3vM5at4-EP4PyaqA8', created=1769677095, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content="That's wonderful! Dosa is a delicious and popular dish", role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=68, prompt_tokens=921, total_tokens=989, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=55, rejected_prediction_tokens=None, text_tokens=13, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=921, image_tokens=None)))
2026-01-29 14:28:15,259 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content="That's wonderful! Dosa is a delicious and popular dish", role='assistant', function_call=None, tool_calls=None, audio=None)
2026-01-29 14:28:15,261 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='JSF7aY3vM5at4-EP4PyaqA8', created=1769677095, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content="That's wonderful! Dosa is a delicious and popular dish", role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=68, prompt_tokens=921, total_tokens=989, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=55, rejected_prediction_tokens=None, text_tokens=13, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=921, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-29 14:28:15,263 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': '. What kind of dosa do you like best?'}], 'role': 'model'}, 'finishReason': 'STOP', 'index': 0}], 'usageMetadata': {'promptTokenCount': 921, 'candidatesTokenCount': 23, 'totalTokenCount': 999, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 921}], 'thoughtsTokenCount': 55}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'JSF7aY3vM5at4-EP4PyaqA8'}
2026-01-29 14:28:15,265 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='JSF7aY3vM5at4-EP4PyaqA8', created=1769677095, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='. What kind of dosa do you like best?', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=78, prompt_tokens=921, total_tokens=999, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=55, rejected_prediction_tokens=None, text_tokens=23, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=921, image_tokens=None)))
2026-01-29 14:28:15,268 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content='. What kind of dosa do you like best?', role=None, function_call=None, tool_calls=None, audio=None)
2026-01-29 14:28:15,270 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='JSF7aY3vM5at4-EP4PyaqA8', created=1769677095, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='. What kind of dosa do you like best?', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=78, prompt_tokens=921, total_tokens=999, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=55, rejected_prediction_tokens=None, text_tokens=23, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=921, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-29 14:28:15,279 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=False
2026-01-29 14:28:15,279 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=False
2026-01-29 14:28:15,281 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-29 14:28:15,281 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-29 14:28:15,284 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 14:28:15,285 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 14:28:15,286 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:28:15,287 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:28:15,289 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:28:15,292 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:28:15,294 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:28:15,295 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:28:15,296 | DEBUG | LiteLLM | response_cost: 0.0004713
2026-01-29 14:28:15,296 | DEBUG | LiteLLM | response_cost: 0.0004713
2026-01-29 14:28:15,298 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:28:15,299 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0004713
2026-01-29 14:28:15,304 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:28:15,308 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:28:15,308 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:28:15,309 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:28:15,310 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:28:15,312 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-29 14:28:15,313 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GuardrailSpanData object at 0x000002370C7BEF80> with id None
2026-01-29 14:28:15,314 | DEBUG | openai.agents | Resetting current trace
2026-01-29 14:28:15,324 | INFO | session_summary | üß† SUMMARY_START | session_id=65c1e585-bc10-4f5a-9b70-e15f0513bbff
2026-01-29 14:28:15,329 | INFO | session_summary | üìä Unsummarized messages = 18
2026-01-29 14:28:15,341 | INFO | session_summary | üìÑ Existing summary = False
2026-01-29 14:28:15,351 | INFO | session_summary | üßæ Messages fetched = 12
2026-01-29 14:28:15,356 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Thu, 29 Jan 2026 08:58:14 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_abf46489b19413527752625ad31ef3dc'), (b'openai-processing-ms', b'121'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'124'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c5786cdce6da346-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-29 14:28:15,357 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-29 14:28:15,355 | INFO | session_summary | ‚úçÔ∏è Calling LLM for MERGED summary
2026-01-29 14:28:15,358 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-29 14:28:15,359 | DEBUG | httpcore.http11 | response_closed.started
2026-01-29 14:28:15,359 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-29 14:28:15,361 | DEBUG | openai.agents | Exported 8 items
2026-01-29 14:28:15,359 | DEBUG | LiteLLM | 

2026-01-29 14:28:15,364 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-29 14:28:15,364 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-29 14:28:15,366 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-29 14:28:15,367 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-29 14:28:15,366 | DEBUG | LiteLLM | [92mlitellm.completion(model='gemini/gemini-2.5-flash', provider='gemini', messages=[{'role': 'system', 'content': 'You are a precise memory compression engine.'}, {'role': 'user', 'content': '\nYou are a memory compression system.\n\nTASK:\nUpdate the existing conversation summary using the new dialogue below.\n\nRULES:\n- Preserve facts, decisions, preferences, and open tasks\n- Preserve key events and discussion topics\n- Remove chit-chat and repetition\n- Do NOT invent new information\n- Keep the summary concise and factual\n\nEXISTING SUMMARY:\nNone\n\nNEW DIALOGUE:\nUSER: hello\nASSISTANT: Sorry, I can‚Äôt help with that request.\nUSER: hello\nASSISTANT: Sorry, I can‚Äôt help with that request.\nUSER: hello\nASSISTANT: Sorry, I can‚Äôt help with that request.\nUSER: hi\'\nASSISTANT: Hello! How can I help you today?Something went wrong, but I‚Äôm still here. Please try again.\nUSER: hello\nASSISTANT: Hello! How can I help you today?Something went wrong, but I‚Äôm still here. Please try again.\nUSER: heellooo ... acn you tell about photosynthesis\nASSISTANT: Photosynthesis is the process by which green plants, algae, and some bacteria convert light energy into chemical energy. This chemical energy is stored in glucose (sugar), which they use as fuel.\n\nHere\'s a simplified breakdown of the process:\n1.  **Ingredients:** Plants take in carbon dioxide from the air through small pores called stomata, and water from the soil through their roots.\n2.  **Location:** This process primarily occurs in the chloroplasts within plant cells, which contain a green pigment called chlorophyll. Chlorophyll absorbs sunlight.\n3.  **Reaction:** Using the energy from sunlight, chlorophyll facilitates a chemical reaction that combines carbon dioxide and water to produce glucose and oxygen.\n4.  **Outputs:** The plant uses the glucose for energy and growth, and releases oxygen as a byproduct into the atmosphere.\n\nEssentially, it\'s how plants "eat" and produce the oxygen we breathe!Something went wrong, but I‚Äôm still here. Please try again.\n\nOUTPUT:\nUpdated summary only.\n'}], temperature=0.2, max_tokens=400)[0m
2026-01-29 14:28:15,368 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-29 14:28:15,368 | DEBUG | LiteLLM | 

2026-01-29 14:28:15,369 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-29 14:28:15,370 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-29 14:28:15,371 | DEBUG | LiteLLM | SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False
2026-01-29 14:28:15,373 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:28:15,374 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-29 14:28:15,380 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': 0.2, 'top_p': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': 400, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': 'You are a precise memory compression engine.'}, {'role': 'user', 'content': '\nYou are a memory compression system.\n\nTASK:\nUpdate the existing conversation summary using the new dialogue below.\n\nRULES:\n- Preserve facts, decisions, preferences, and open tasks\n- Preserve key events and discussion topics\n- Remove chit-chat and repetition\n- Do NOT invent new information\n- Keep the summary concise and factual\n\nEXISTING SUMMARY:\nNone\n\nNEW DIALOGUE:\nUSER: hello\nASSISTANT: Sorry, I can‚Äôt help with that request.\nUSER: hello\nASSISTANT: Sorry, I can‚Äôt help with that request.\nUSER: hello\nASSISTANT: Sorry, I can‚Äôt help with that request.\nUSER: hi\'\nASSISTANT: Hello! How can I help you today?Something went wrong, but I‚Äôm still here. Please try again.\nUSER: hello\nASSISTANT: Hello! How can I help you today?Something went wrong, but I‚Äôm still here. Please try again.\nUSER: heellooo ... acn you tell about photosynthesis\nASSISTANT: Photosynthesis is the process by which green plants, algae, and some bacteria convert light energy into chemical energy. This chemical energy is stored in glucose (sugar), which they use as fuel.\n\nHere\'s a simplified breakdown of the process:\n1.  **Ingredients:** Plants take in carbon dioxide from the air through small pores called stomata, and water from the soil through their roots.\n2.  **Location:** This process primarily occurs in the chloroplasts within plant cells, which contain a green pigment called chlorophyll. Chlorophyll absorbs sunlight.\n3.  **Reaction:** Using the energy from sunlight, chlorophyll facilitates a chemical reaction that combines carbon dioxide and water to produce glucose and oxygen.\n4.  **Outputs:** The plant uses the glucose for energy and growth, and releases oxygen as a byproduct into the atmosphere.\n\nEssentially, it\'s how plants "eat" and produce the oxygen we breathe!Something went wrong, but I‚Äôm still here. Please try again.\n\nOUTPUT:\nUpdated summary only.\n'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None, 'provider': 'gemini'}
2026-01-29 14:28:15,382 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'temperature': 0.2, 'max_tokens': 400}
2026-01-29 14:28:15,383 | DEBUG | LiteLLM | Final returned optional params: {'temperature': 0.2, 'max_output_tokens': 400, 'provider': 'gemini'}
2026-01-29 14:28:15,384 | DEBUG | LiteLLM | self.optional_params: {'temperature': 0.2, 'max_tokens': 400, 'provider': 'gemini'}
2026-01-29 14:28:15,385 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:28:15,386 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:28:15,389 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '\nYou are a memory compression system.\n\nTASK:\nUpdate the existing conversation summary using the new dialogue below.\n\nRULES:\n- Preserve facts, decisions, preferences, and open tasks\n- Preserve key events and discussion topics\n- Remove chit-chat and repetition\n- Do NOT invent new information\n- Keep the summary concise and factual\n\nEXISTING SUMMARY:\nNone\n\nNEW DIALOGUE:\nUSER: hello\nASSISTANT: Sorry, I can‚Äôt help with that request.\nUSER: hello\nASSISTANT: Sorry, I can‚Äôt help with that request.\nUSER: hello\nASSISTANT: Sorry, I can‚Äôt help with that request.\nUSER: hi\'\nASSISTANT: Hello! How can I help you today?Something went wrong, but I‚Äôm still here. Please try again.\nUSER: hello\nASSISTANT: Hello! How can I help you today?Something went wrong, but I‚Äôm still here. Please try again.\nUSER: heellooo ... acn you tell about photosynthesis\nASSISTANT: Photosynthesis is the process by which green plants, algae, and some bacteria convert light energy into chemical energy. This chemical energy is stored in glucose (sugar), which they use as fuel.\n\nHere\'s a simplified breakdown of the process:\n1.  **Ingredients:** Plants take in carbon dioxide from the air through small pores called stomata, and water from the soil through their roots.\n2.  **Location:** This process primarily occurs in the chloroplasts within plant cells, which contain a green pigment called chlorophyll. Chlorophyll absorbs sunlight.\n3.  **Reaction:** Using the energy from sunlight, chlorophyll facilitates a chemical reaction that combines carbon dioxide and water to produce glucose and oxygen.\n4.  **Outputs:** The plant uses the glucose for energy and growth, and releases oxygen as a byproduct into the atmosphere.\n\nEssentially, it\'s how plants "eat" and produce the oxygen we breathe!Something went wrong, but I‚Äôm still here. Please try again.\n\nOUTPUT:\nUpdated summary only.\n'}]}], 'system_instruction': {'parts': [{'text': 'You are a precise memory compression engine.'}]}, 'generationConfig': {'temperature': 0.2, 'max_output_tokens': 400}}'
[0m

2026-01-29 14:28:15,394 | DEBUG | httpcore.connection | connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None
2026-01-29 14:28:15,431 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023702814F10>
2026-01-29 14:28:15,432 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x000002370C1163C0> server_hostname='generativelanguage.googleapis.com' timeout=600.0
2026-01-29 14:28:15,447 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000237028149A0>
2026-01-29 14:28:15,448 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-29 14:28:15,449 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-29 14:28:15,449 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-29 14:28:15,451 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-29 14:28:15,452 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-29 14:28:15,945 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Thu, 29 Jan 2026 08:58:15 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_a2fc49b19b6d1babdf680e4036468a1e'), (b'openai-processing-ms', b'247'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'250'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c5786d09b1ca346-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-29 14:28:15,947 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-29 14:28:15,947 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-29 14:28:15,948 | DEBUG | httpcore.http11 | response_closed.started
2026-01-29 14:28:15,948 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-29 14:28:15,950 | DEBUG | openai.agents | Exported 3 items
2026-01-29 14:28:18,344 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Thu, 29 Jan 2026 08:58:17 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=2843'), (b'Alt-Svc', b'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])
2026-01-29 14:28:18,346 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-29 14:28:18,349 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-29 14:28:18,349 | DEBUG | httpcore.http11 | response_closed.started
2026-01-29 14:28:18,350 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-29 14:28:18,350 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "The user asked about photosynthesis. The assistant explained that photosynthesis is the"
          }
        ],
        "role": "model"
      },
      "finishReason": "MAX_TOKENS",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 447,
    "candidatesTokenCount": 13,
    "totalTokenCount": 843,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 447
      }
    ],
    "thoughtsTokenCount": 383
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "KSF7adCAD_PUqfkPweCIuAI"
}



2026-01-29 14:28:18,355 | INFO | LiteLLM | Wrapper: Completed Call, calling success_handler
2026-01-29 14:28:18,357 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 14:28:18,359 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-29 14:28:18,360 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:28:18,362 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 14:28:18,363 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:28:18,364 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:28:18,367 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:28:18,368 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:28:18,369 | DEBUG | LiteLLM | response_cost: 0.0011241000000000003
2026-01-29 14:28:18,369 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:28:18,370 | DEBUG | LiteLLM | response_cost: 0.0011241000000000003
2026-01-29 14:28:18,370 | INFO | session_summary | üÜï CREATING NEW SUMMARY ROW
2026-01-29 14:28:18,371 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:28:18,374 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:28:18,387 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:28:18,397 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-29 14:28:18,404 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:28:18,407 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:28:18,414 | INFO | session_summary | ‚úÖ SUMMARY_MERGED_SUCCESSFULLY
2026-01-29 14:28:18,418 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:28:18,425 | INFO | chat | ‚úÖ Stream complete | tokens=2 | time=8.11s
2026-01-29 14:29:41,986 | INFO | chat | üì© Chat request | user_id=3 | request_id=427926a3-9467-4995-a2c0-b6059b3b06b0
2026-01-29 14:29:41,988 | INFO | chat | üßë User message | tell me the weather of mumbai now
2026-01-29 14:29:42,020 | INFO | chat | ü§ñ Stream started | session_id=65c1e585-bc10-4f5a-9b70-e15f0513bbff | request_id=427926a3-9467-4995-a2c0-b6059b3b06b0
2026-01-29 14:29:42,022 | INFO | orchestrator | üî• Orchestrator started
2026-01-29 14:29:42,023 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_023a1c954b7c42029f74322c7a90a840
2026-01-29 14:29:42,024 | DEBUG | openai.agents | Setting current trace: trace_023a1c954b7c42029f74322c7a90a840
2026-01-29 14:29:42,025 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000002370C7B95D0> with id None
2026-01-29 14:29:42,026 | DEBUG | openai.agents | Running agent RouterAgent (turn 1)
2026-01-29 14:29:42,028 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GuardrailSpanData object at 0x000002370C7AF9C0> with id None
2026-01-29 14:29:42,029 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000002370C78F460> with id None
2026-01-29 14:29:42,030 | DEBUG | openai.agents | Calling LLM
2026-01-29 14:29:42,031 | DEBUG | LiteLLM | 

2026-01-29 14:29:42,031 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-29 14:29:42,032 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'tell me the weather of mumbai now'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-29 14:29:42,033 | DEBUG | LiteLLM | 

2026-01-29 14:29:42,035 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-29 14:29:42,036 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-29 14:29:42,037 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-29 14:29:42,041 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:29:42,042 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-29 14:29:42,044 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'tell me the weather of mumbai now'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-29 14:29:42,045 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-29 14:29:42,045 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-29 14:29:42,047 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-29 14:29:42,048 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:29:42,050 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:29:42,051 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'tell me the weather of mumbai now'}]}], 'system_instruction': {'parts': [{'text': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n'}]}, 'generationConfig': {}}'
[0m

2026-01-29 14:29:42,328 | DEBUG | httpcore.connection | close.started
2026-01-29 14:29:42,330 | DEBUG | httpcore.connection | close.complete
2026-01-29 14:29:42,331 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-29 14:29:42,390 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002370C7AB8B0>
2026-01-29 14:29:42,391 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000023700CEDBC0> server_hostname='api.openai.com' timeout=5.0
2026-01-29 14:29:42,408 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002370C7AA680>
2026-01-29 14:29:42,409 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-29 14:29:42,410 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-29 14:29:42,411 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-29 14:29:42,412 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-29 14:29:42,413 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-29 14:29:43,440 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Thu, 29 Jan 2026 08:59:42 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_a8e72c16d6b04686f437a632e3ed4e63'), (b'openai-processing-ms', b'109'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'112'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c5788f0ad86867b-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-29 14:29:43,442 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-29 14:29:43,443 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-29 14:29:43,443 | DEBUG | httpcore.http11 | response_closed.started
2026-01-29 14:29:43,446 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-29 14:29:43,447 | DEBUG | openai.agents | Exported 2 items
2026-01-29 14:29:43,718 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\"use_memory\": false, \"use_tool\": true, \"tool_name\": \"weather\", \"tool_arguments\": {}, \"intent\": \"none\", \"memory_key\": null}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 492,
    "candidatesTokenCount": 41,
    "totalTokenCount": 611,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 492
      }
    ],
    "thoughtsTokenCount": 78
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "fiF7aZOiKauFg8UPjOe9kA8"
}



2026-01-29 14:29:43,722 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-29 14:29:43,724 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 14:29:43,724 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-29 14:29:43,725 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:29:43,726 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 14:29:43,727 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:29:43,728 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:29:43,730 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:29:43,731 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:29:43,732 | DEBUG | LiteLLM | response_cost: 0.00044510000000000003
2026-01-29 14:29:43,734 | DEBUG | openai.agents | Received model response
2026-01-29 14:29:43,733 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:29:43,735 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x000002370C8D5EA0>>
2026-01-29 14:29:43,736 | DEBUG | LiteLLM | response_cost: 0.00044510000000000003
2026-01-29 14:29:43,737 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-29 14:29:43,738 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:29:43,741 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-29 14:29:43,741 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-29 14:29:43,742 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:29:43,743 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-29 14:29:43,744 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:29:43,745 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:29:43,746 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:29:43,748 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:29:43,749 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-29 14:29:43,750 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:29:43,750 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:29:43,753 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:29:43,754 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:29:43,756 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:29:43,756 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-29 14:29:43,757 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:29:43,758 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.00044510000000000003
2026-01-29 14:29:43,761 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:29:43,763 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:29:43,764 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:29:43,767 | DEBUG | openai.agents | Resetting current trace
2026-01-29 14:29:43,839 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_86dc8449f91e4edd9f3814abb8ca1943
2026-01-29 14:29:43,839 | DEBUG | openai.agents | Setting current trace: trace_86dc8449f91e4edd9f3814abb8ca1943
2026-01-29 14:29:43,840 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000002370C7A7F60> with id None
2026-01-29 14:29:43,840 | DEBUG | openai.agents | Running agent ToolAgent (turn 1)
2026-01-29 14:29:43,843 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000002370C78EB00> with id None
2026-01-29 14:29:43,843 | DEBUG | openai.agents | Calling LLM
2026-01-29 14:29:43,843 | DEBUG | LiteLLM | 

2026-01-29 14:29:43,844 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-29 14:29:43,845 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a Tool Agent.\n \nYour task is to decide whether the user\'s query requires a tool.\n \nAvailable tools:\n- weather ‚Üí for weather-related queries (city-based)\n- calculator ‚Üí for mathematical calculations\n- web_search ‚Üí for latest information, prices, news, current events\n- browser ‚Üí for visiting, fetching, or summarizing a webpage\n \nRules:\n- You MUST output valid JSON only.\n- If a tool is required, return:\n  { "tool": "<tool_name>", "arguments": { ... } }\n- If no tool is required, return:\n  { "tool": "none", "arguments": {} }\n \nSTRICT RULES:\n- Do NOT answer the user.\n- Do NOT explain.\n- Do NOT include markdown.\n- Do NOT include extra keys.\n- Do NOT wrap JSON in text.\n- Output JSON ONLY.\n \nExamples:\n \nUser: What is the weather in Delhi today?\nOutput:\n{ "tool": "weather", "arguments": { "city": "Delhi" } }\n \nUser: Calculate (10 + 5) * 2\nOutput:\n{ "tool": "calculator", "arguments": { "expression": "(10 + 5) * 2" } }\n \nUser: Explain machine learning\nOutput:\n{ "tool": "none", "arguments": {} }\n ', 'role': 'system'}, {'role': 'user', 'content': 'tell me the weather of mumbai now'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-29 14:29:43,846 | DEBUG | LiteLLM | 

2026-01-29 14:29:43,847 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-29 14:29:43,848 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-29 14:29:43,849 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-29 14:29:43,853 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:29:43,854 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-29 14:29:43,857 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Tool Agent.\n \nYour task is to decide whether the user\'s query requires a tool.\n \nAvailable tools:\n- weather ‚Üí for weather-related queries (city-based)\n- calculator ‚Üí for mathematical calculations\n- web_search ‚Üí for latest information, prices, news, current events\n- browser ‚Üí for visiting, fetching, or summarizing a webpage\n \nRules:\n- You MUST output valid JSON only.\n- If a tool is required, return:\n  { "tool": "<tool_name>", "arguments": { ... } }\n- If no tool is required, return:\n  { "tool": "none", "arguments": {} }\n \nSTRICT RULES:\n- Do NOT answer the user.\n- Do NOT explain.\n- Do NOT include markdown.\n- Do NOT include extra keys.\n- Do NOT wrap JSON in text.\n- Output JSON ONLY.\n \nExamples:\n \nUser: What is the weather in Delhi today?\nOutput:\n{ "tool": "weather", "arguments": { "city": "Delhi" } }\n \nUser: Calculate (10 + 5) * 2\nOutput:\n{ "tool": "calculator", "arguments": { "expression": "(10 + 5) * 2" } }\n \nUser: Explain machine learning\nOutput:\n{ "tool": "none", "arguments": {} }\n ', 'role': 'system'}, {'role': 'user', 'content': 'tell me the weather of mumbai now'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-29 14:29:43,860 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-29 14:29:43,861 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-29 14:29:43,862 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-29 14:29:43,862 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:29:43,863 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:29:43,866 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'tell me the weather of mumbai now'}]}], 'system_instruction': {'parts': [{'text': 'You are a Tool Agent.\n \nYour task is to decide whether the user\'s query requires a tool.\n \nAvailable tools:\n- weather ‚Üí for weather-related queries (city-based)\n- calculator ‚Üí for mathematical calculations\n- web_search ‚Üí for latest information, prices, news, current events\n- browser ‚Üí for visiting, fetching, or summarizing a webpage\n \nRules:\n- You MUST output valid JSON only.\n- If a tool is required, return:\n  { "tool": "<tool_name>", "arguments": { ... } }\n- If no tool is required, return:\n  { "tool": "none", "arguments": {} }\n \nSTRICT RULES:\n- Do NOT answer the user.\n- Do NOT explain.\n- Do NOT include markdown.\n- Do NOT include extra keys.\n- Do NOT wrap JSON in text.\n- Output JSON ONLY.\n \nExamples:\n \nUser: What is the weather in Delhi today?\nOutput:\n{ "tool": "weather", "arguments": { "city": "Delhi" } }\n \nUser: Calculate (10 + 5) * 2\nOutput:\n{ "tool": "calculator", "arguments": { "expression": "(10 + 5) * 2" } }\n \nUser: Explain machine learning\nOutput:\n{ "tool": "none", "arguments": {} }\n '}]}, 'generationConfig': {}}'
[0m

2026-01-29 14:29:44,762 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\"tool\": \"weather\", \"arguments\": {\"city\": \"mumbai\"}}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 319,
    "candidatesTokenCount": 16,
    "totalTokenCount": 376,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 319
      }
    ],
    "thoughtsTokenCount": 41
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "fyF7afSeLK3Eg8UPsvWA2Qw"
}



2026-01-29 14:29:44,765 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-29 14:29:44,769 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 14:29:44,769 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-29 14:29:44,770 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:29:44,771 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 14:29:44,772 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:29:44,775 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:29:44,776 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:29:44,777 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:29:44,778 | DEBUG | LiteLLM | response_cost: 0.00023820000000000002
2026-01-29 14:29:44,780 | DEBUG | openai.agents | Received model response
2026-01-29 14:29:44,779 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:29:44,781 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x000002370C1CA050>>
2026-01-29 14:29:44,782 | DEBUG | LiteLLM | response_cost: 0.00023820000000000002
2026-01-29 14:29:44,783 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-29 14:29:44,784 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:29:44,786 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-29 14:29:44,785 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-29 14:29:44,787 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:29:44,788 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-29 14:29:44,793 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:29:44,794 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:29:44,795 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:29:44,797 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:29:44,799 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-29 14:29:44,799 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:29:44,800 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:29:44,802 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:29:44,803 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:29:44,806 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:29:44,809 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-29 14:29:44,811 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:29:44,812 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.00023820000000000002
2026-01-29 14:29:44,814 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:29:44,815 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:29:44,816 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:29:44,820 | DEBUG | openai.agents | Resetting current trace
2026-01-29 14:29:44,825 | DEBUG | urllib3.connectionpool | Starting new HTTPS connection (1): api.openweathermap.org:443
2026-01-29 14:29:45,054 | DEBUG | urllib3.connectionpool | https://api.openweathermap.org:443 "GET /data/2.5/weather?q=mumbai&appid=1b9bd663a5cceef8c4355bfe45209f86&units=metric HTTP/1.1" 200 494
2026-01-29 14:29:45,059 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_04f77446867147deb641ea0f184064cf
2026-01-29 14:29:45,061 | DEBUG | openai.agents | Setting current trace: trace_04f77446867147deb641ea0f184064cf
2026-01-29 14:29:45,061 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000002370C7BA340> with id None
2026-01-29 14:29:45,063 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000002370C7CC400> with id None
2026-01-29 14:29:45,065 | DEBUG | openai.agents | Calling LLM
2026-01-29 14:29:45,066 | DEBUG | LiteLLM | 

2026-01-29 14:29:45,067 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-29 14:29:45,068 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "system", "content": "Conversation summary:\\nThe user asked about photosynthesis. The assistant explained that photosynthesis is the"}, {"role": "assistant", "content": "I\'m really sorry to hear you\'re feeling this way. Please know that you\'re not alone and there are people who want to help.\\n\\nIf you are in immediate danger, please call your local emergency services.\\n\\nYou can also reach out to a crisis hotline for support. Here are some resources:\\n\\n*   **National Suicide Prevention Lifeline (US):** Call or text 988\\n*   **Crisis Text Line (US):** Text HOME to 741741\\n*   **The Samaritans (UK):** Call 116 123\\n*   **Your local mental health services**\\n\\nPlease reach out to one of these resources, or talk to a trusted friend, family member, or mental health professional. There is help available."}, {"role": "user", "content": "i want to get high fever"}, {"role": "assistant", "content": "I cannot help you with that. Intentionally trying to get a high fever can be dangerous and harmful to your health. A high fever is often a sign of illness and can lead to serious complications.\\n\\nIf you are feeling unwell, it\'s always best to consult a healthcare professional. They can properly diagnose any symptoms you might have and provide appropriate treatment."}, {"role": "user", "content": "i love dosa"}, {"role": "assistant", "content": "That\'s wonderful! Dosa is a delicious and popular dish. What kind of dosa do you like best?"}, {"role": "user", "content": "tell me the weather of mumbai now"}, {"role": "user", "content": "tell me the weather of mumbai now"}], "memory": [], "tool_context": {"coord": {"lon": 72.8479, "lat": 19.0144}, "weather": [{"id": 711, "main": "Smoke", "description": "smoke", "icon": "50d"}], "base": "stations", "main": {"temp": 31.99, "feels_like": 32.82, "temp_min": 28.94, "temp_max": 31.99, "pressure": 1012, "humidity": 43, "sea_level": 1012, "grnd_level": 1012}, "visibility": 3000, "wind": {"speed": 4.12, "deg": 270}, "clouds": {"all": 5}, "dt": 1769676682, "sys": {"type": 1, "id": 9052, "country": "IN", "sunrise": 1769651014, "sunset": 1769691574}, "timezone": 19800, "id": 1275339, "name": "Mumbai", "cod": 200}}'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=True, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-29 14:29:45,070 | DEBUG | LiteLLM | 

2026-01-29 14:29:45,071 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-29 14:29:45,072 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-29 14:29:45,073 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-29 14:29:45,076 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:29:45,077 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-29 14:29:45,079 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "system", "content": "Conversation summary:\\nThe user asked about photosynthesis. The assistant explained that photosynthesis is the"}, {"role": "assistant", "content": "I\'m really sorry to hear you\'re feeling this way. Please know that you\'re not alone and there are people who want to help.\\n\\nIf you are in immediate danger, please call your local emergency services.\\n\\nYou can also reach out to a crisis hotline for support. Here are some resources:\\n\\n*   **National Suicide Prevention Lifeline (US):** Call or text 988\\n*   **Crisis Text Line (US):** Text HOME to 741741\\n*   **The Samaritans (UK):** Call 116 123\\n*   **Your local mental health services**\\n\\nPlease reach out to one of these resources, or talk to a trusted friend, family member, or mental health professional. There is help available."}, {"role": "user", "content": "i want to get high fever"}, {"role": "assistant", "content": "I cannot help you with that. Intentionally trying to get a high fever can be dangerous and harmful to your health. A high fever is often a sign of illness and can lead to serious complications.\\n\\nIf you are feeling unwell, it\'s always best to consult a healthcare professional. They can properly diagnose any symptoms you might have and provide appropriate treatment."}, {"role": "user", "content": "i love dosa"}, {"role": "assistant", "content": "That\'s wonderful! Dosa is a delicious and popular dish. What kind of dosa do you like best?"}, {"role": "user", "content": "tell me the weather of mumbai now"}, {"role": "user", "content": "tell me the weather of mumbai now"}], "memory": [], "tool_context": {"coord": {"lon": 72.8479, "lat": 19.0144}, "weather": [{"id": 711, "main": "Smoke", "description": "smoke", "icon": "50d"}], "base": "stations", "main": {"temp": 31.99, "feels_like": 32.82, "temp_min": 28.94, "temp_max": 31.99, "pressure": 1012, "humidity": 43, "sea_level": 1012, "grnd_level": 1012}, "visibility": 3000, "wind": {"speed": 4.12, "deg": 270}, "clouds": {"all": 5}, "dt": 1769676682, "sys": {"type": 1, "id": 9052, "country": "IN", "sunrise": 1769651014, "sunset": 1769691574}, "timezone": 19800, "id": 1275339, "name": "Mumbai", "cod": 200}}'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-29 14:29:45,081 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': True}
2026-01-29 14:29:45,082 | DEBUG | LiteLLM | Final returned optional params: {'stream': True}
2026-01-29 14:29:45,083 | DEBUG | LiteLLM | self.optional_params: {'stream': True}
2026-01-29 14:29:45,084 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:29:45,088 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:29:45,090 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?key=*****=sse \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "system", "content": "Conversation summary:\\nThe user asked about photosynthesis. The assistant explained that photosynthesis is the"}, {"role": "assistant", "content": "I\'m really sorry to hear you\'re feeling this way. Please know that you\'re not alone and there are people who want to help.\\n\\nIf you are in immediate danger, please call your local emergency services.\\n\\nYou can also reach out to a crisis hotline for support. Here are some resources:\\n\\n*   **National Suicide Prevention Lifeline (US):** Call or text 988\\n*   **Crisis Text Line (US):** Text HOME to 741741\\n*   **The Samaritans (UK):** Call 116 123\\n*   **Your local mental health services**\\n\\nPlease reach out to one of these resources, or talk to a trusted friend, family member, or mental health professional. There is help available."}, {"role": "user", "content": "i want to get high fever"}, {"role": "assistant", "content": "I cannot help you with that. Intentionally trying to get a high fever can be dangerous and harmful to your health. A high fever is often a sign of illness and can lead to serious complications.\\n\\nIf you are feeling unwell, it\'s always best to consult a healthcare professional. They can properly diagnose any symptoms you might have and provide appropriate treatment."}, {"role": "user", "content": "i love dosa"}, {"role": "assistant", "content": "That\'s wonderful! Dosa is a delicious and popular dish. What kind of dosa do you like best?"}, {"role": "user", "content": "tell me the weather of mumbai now"}, {"role": "user", "content": "tell me the weather of mumbai now"}], "memory": [], "tool_context": {"coord": {"lon": 72.8479, "lat": 19.0144}, "weather": [{"id": 711, "main": "Smoke", "description": "smoke", "icon": "50d"}], "base": "stations", "main": {"temp": 31.99, "feels_like": 32.82, "temp_min": 28.94, "temp_max": 31.99, "pressure": 1012, "humidity": 43, "sea_level": 1012, "grnd_level": 1012}, "visibility": 3000, "wind": {"speed": 4.12, "deg": 270}, "clouds": {"all": 5}, "dt": 1769676682, "sys": {"type": 1, "id": 9052, "country": "IN", "sunrise": 1769651014, "sunset": 1769691574}, "timezone": 19800, "id": 1275339, "name": "Mumbai", "cod": 200}}'}]}], 'system_instruction': {'parts': [{'text': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.'}]}, 'generationConfig': {}}'
[0m

2026-01-29 14:29:45,092 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 14:29:45,093 | DEBUG | LiteLLM | messages in token_counter: None, text in token_counter: 
2026-01-29 14:29:45,096 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:29:45,097 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:29:45,097 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:29:45,099 | DEBUG | LiteLLM | response_cost: 0.0
2026-01-29 14:29:47,226 | DEBUG | LiteLLM | RAW RESPONSE:
first stream response received


2026-01-29 14:29:47,228 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': 'The current weather in Mumbai is smoke with a temperature of 31.99¬∞C, feeling like 32.82¬∞C. The humidity is 43%, wind speed is 4'}], 'role': 'model'}, 'index': 0}], 'usageMetadata': {'promptTokenCount': 914, 'candidatesTokenCount': 43, 'totalTokenCount': 1128, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 914}], 'thoughtsTokenCount': 171}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'gSF7aefYDryp4-EPnqC8-Q8'}
2026-01-29 14:29:47,230 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='gSF7aefYDryp4-EPnqC8-Q8', created=1769677187, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='The current weather in Mumbai is smoke with a temperature of 31.99¬∞C, feeling like 32.82¬∞C. The humidity is 43%, wind speed is 4', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=214, prompt_tokens=914, total_tokens=1128, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=171, rejected_prediction_tokens=None, text_tokens=43, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=914, image_tokens=None)))
2026-01-29 14:29:47,234 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content='The current weather in Mumbai is smoke with a temperature of 31.99¬∞C, feeling like 32.82¬∞C. The humidity is 43%, wind speed is 4', role='assistant', function_call=None, tool_calls=None, audio=None)
2026-01-29 14:29:47,235 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='gSF7aefYDryp4-EPnqC8-Q8', created=1769677187, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='The current weather in Mumbai is smoke with a temperature of 31.99¬∞C, feeling like 32.82¬∞C. The humidity is 43%, wind speed is 4', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=214, prompt_tokens=914, total_tokens=1128, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=171, rejected_prediction_tokens=None, text_tokens=43, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=914, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-29 14:29:47,238 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': '.12 m/s, and cloudiness is 5%.'}], 'role': 'model'}, 'finishReason': 'STOP', 'index': 0}], 'usageMetadata': {'promptTokenCount': 914, 'candidatesTokenCount': 57, 'totalTokenCount': 1142, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 914}], 'thoughtsTokenCount': 171}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'gSF7aefYDryp4-EPnqC8-Q8'}
2026-01-29 14:29:47,240 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='gSF7aefYDryp4-EPnqC8-Q8', created=1769677187, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='.12 m/s, and cloudiness is 5%.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=228, prompt_tokens=914, total_tokens=1142, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=171, rejected_prediction_tokens=None, text_tokens=57, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=914, image_tokens=None)))
2026-01-29 14:29:47,243 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content='.12 m/s, and cloudiness is 5%.', role=None, function_call=None, tool_calls=None, audio=None)
2026-01-29 14:29:47,243 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='gSF7aefYDryp4-EPnqC8-Q8', created=1769677187, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='.12 m/s, and cloudiness is 5%.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=228, prompt_tokens=914, total_tokens=1142, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=171, rejected_prediction_tokens=None, text_tokens=57, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=914, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-29 14:29:47,247 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=False
2026-01-29 14:29:47,247 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=False
2026-01-29 14:29:47,248 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-29 14:29:47,249 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-29 14:29:47,250 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 14:29:47,251 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 14:29:47,252 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:29:47,253 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:29:47,255 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:29:47,257 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:29:47,259 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:29:47,259 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:29:47,260 | DEBUG | LiteLLM | response_cost: 0.0008442
2026-01-29 14:29:47,261 | DEBUG | LiteLLM | response_cost: 0.0008442
2026-01-29 14:29:47,263 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:29:47,263 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0008442
2026-01-29 14:29:47,264 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:29:47,265 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:29:47,266 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:29:47,268 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:29:47,279 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:29:47,280 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-29 14:29:47,283 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GuardrailSpanData object at 0x000002370C863340> with id None
2026-01-29 14:29:47,284 | DEBUG | openai.agents | Resetting current trace
2026-01-29 14:29:47,293 | INFO | session_summary | üß† SUMMARY_START | session_id=65c1e585-bc10-4f5a-9b70-e15f0513bbff
2026-01-29 14:29:47,300 | INFO | session_summary | üìä Unsummarized messages = 14
2026-01-29 14:29:47,302 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-29 14:29:47,305 | INFO | chat | ‚úÖ Stream complete | tokens=2 | time=5.32s
2026-01-29 14:29:48,534 | DEBUG | httpcore.connection | close.started
2026-01-29 14:29:48,535 | DEBUG | httpcore.connection | close.complete
2026-01-29 14:29:48,535 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-29 14:29:48,552 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002370C7A8B20>
2026-01-29 14:29:48,554 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000023700CEDBC0> server_hostname='api.openai.com' timeout=5.0
2026-01-29 14:29:48,582 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002370C7A81C0>
2026-01-29 14:29:48,583 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-29 14:29:48,584 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-29 14:29:48,584 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-29 14:29:48,585 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-29 14:29:48,585 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-29 14:29:49,766 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Thu, 29 Jan 2026 08:59:48 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_4b41bd05fa91e572eacbad62b738590d'), (b'openai-processing-ms', b'256'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'259'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c5789173c7891bc-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-29 14:29:49,768 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-29 14:29:49,769 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-29 14:29:49,769 | DEBUG | httpcore.http11 | response_closed.started
2026-01-29 14:29:49,771 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-29 14:29:49,771 | DEBUG | openai.agents | Exported 9 items
2026-01-29 14:30:06,814 | INFO | chat | üì© Chat request | user_id=3 | request_id=c21a45b6-b2fe-409e-a090-0ae87ed4f91f
2026-01-29 14:30:06,816 | INFO | chat | üßë User message | what is my favourite color?
2026-01-29 14:30:06,846 | INFO | chat | ü§ñ Stream started | session_id=65c1e585-bc10-4f5a-9b70-e15f0513bbff | request_id=c21a45b6-b2fe-409e-a090-0ae87ed4f91f
2026-01-29 14:30:06,847 | INFO | orchestrator | üî• Orchestrator started
2026-01-29 14:30:06,850 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_2f3a4a69f31e43ecb8c0de31e72b6225
2026-01-29 14:30:06,850 | DEBUG | openai.agents | Setting current trace: trace_2f3a4a69f31e43ecb8c0de31e72b6225
2026-01-29 14:30:06,852 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000002370C7A74C0> with id None
2026-01-29 14:30:06,852 | DEBUG | openai.agents | Running agent RouterAgent (turn 1)
2026-01-29 14:30:06,853 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GuardrailSpanData object at 0x000002370C7BC180> with id None
2026-01-29 14:30:06,855 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000002370CF12260> with id None
2026-01-29 14:30:06,856 | DEBUG | openai.agents | Calling LLM
2026-01-29 14:30:06,857 | DEBUG | LiteLLM | 

2026-01-29 14:30:06,859 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-29 14:30:06,860 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'what is my favourite color?'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-29 14:30:06,863 | DEBUG | LiteLLM | 

2026-01-29 14:30:06,866 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-29 14:30:06,868 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-29 14:30:06,869 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-29 14:30:06,871 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:30:06,873 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-29 14:30:06,874 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'what is my favourite color?'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-29 14:30:06,876 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-29 14:30:06,877 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-29 14:30:06,878 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-29 14:30:06,878 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:30:06,881 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:30:06,884 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'what is my favourite color?'}]}], 'system_instruction': {'parts': [{'text': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n'}]}, 'generationConfig': {}}'
[0m

2026-01-29 14:30:08,494 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\n  \"use_memory\": true,\n  \"use_tool\": false,\n  \"tool_name\": \"none\",\n  \"tool_arguments\": null,\n  \"intent\": \"read\",\n  \"memory_key\": \"favourite_color\"\n}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 491,
    "candidatesTokenCount": 61,
    "totalTokenCount": 610,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 491
      }
    ],
    "thoughtsTokenCount": 58
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "lyF7acnbGJGNg8UPlcXciQ8"
}



2026-01-29 14:30:08,496 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-29 14:30:08,498 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 14:30:08,500 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-29 14:30:08,501 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:30:08,502 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 14:30:08,503 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:30:08,504 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:30:08,506 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:30:08,507 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:30:08,508 | DEBUG | LiteLLM | response_cost: 0.0004448
2026-01-29 14:30:08,511 | DEBUG | openai.agents | Received model response
2026-01-29 14:30:08,510 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:30:08,512 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x000002370C8D6BF0>>
2026-01-29 14:30:08,513 | DEBUG | LiteLLM | response_cost: 0.0004448
2026-01-29 14:30:08,514 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-29 14:30:08,515 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:30:08,517 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-29 14:30:08,515 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-29 14:30:08,517 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:30:08,518 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-29 14:30:08,520 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:30:08,521 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:30:08,526 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:30:08,527 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:30:08,528 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-29 14:30:08,529 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:30:08,531 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:30:08,532 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:30:08,533 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:30:08,536 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:30:08,537 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-29 14:30:08,538 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:30:08,539 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0004448
2026-01-29 14:30:08,543 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:30:08,544 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:30:08,545 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:30:08,548 | DEBUG | openai.agents | Resetting current trace
2026-01-29 14:30:08,616 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_760a02627b3943d49b3ac073471b0bc2
2026-01-29 14:30:08,617 | DEBUG | openai.agents | Setting current trace: trace_760a02627b3943d49b3ac073471b0bc2
2026-01-29 14:30:08,618 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000002370C7B9EE0> with id None
2026-01-29 14:30:08,620 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000002370C7CEE00> with id None
2026-01-29 14:30:08,620 | DEBUG | openai.agents | Calling LLM
2026-01-29 14:30:08,621 | DEBUG | LiteLLM | 

2026-01-29 14:30:08,623 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-29 14:30:08,627 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "system", "content": "Conversation summary:\\nThe user asked about photosynthesis. The assistant explained that photosynthesis is the"}, {"role": "assistant", "content": "I cannot help you with that. Intentionally trying to get a high fever can be dangerous and harmful to your health. A high fever is often a sign of illness and can lead to serious complications.\\n\\nIf you are feeling unwell, it\'s always best to consult a healthcare professional. They can properly diagnose any symptoms you might have and provide appropriate treatment."}, {"role": "user", "content": "i love dosa"}, {"role": "assistant", "content": "That\'s wonderful! Dosa is a delicious and popular dish. What kind of dosa do you like best?"}, {"role": "user", "content": "tell me the weather of mumbai now"}, {"role": "assistant", "content": "The current weather in Mumbai is smoke with a temperature of 31.99\\u00b0C, feeling like 32.82\\u00b0C. The humidity is 43%, wind speed is 4.12 m/s, and cloudiness is 5%."}, {"role": "user", "content": "what is my favourite color?"}, {"role": "user", "content": "what is my favourite color?"}], "memory": [{"memory_id": 6, "user_id": 3, "text": "My favourite color is yellow.", "confidence": 0.85, "created_at": "2026-01-26T21:50:52.573607+05:30", "expires_at": "2026-02-25T21:50:52.427804+05:30", "score": 0.7962166666984558}, {"memory_id": 7, "user_id": 3, "text": "my name is bhumika raheja", "confidence": 0.95, "created_at": "2026-01-26T22:31:33.870462+05:30", "expires_at": "2026-02-25T22:31:33.867621+05:30", "score": 0.35538041591644287}], "tool_context": {}}'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=True, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-29 14:30:08,628 | DEBUG | LiteLLM | 

2026-01-29 14:30:08,629 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-29 14:30:08,630 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-29 14:30:08,631 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-29 14:30:08,635 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:30:08,636 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-29 14:30:08,637 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "system", "content": "Conversation summary:\\nThe user asked about photosynthesis. The assistant explained that photosynthesis is the"}, {"role": "assistant", "content": "I cannot help you with that. Intentionally trying to get a high fever can be dangerous and harmful to your health. A high fever is often a sign of illness and can lead to serious complications.\\n\\nIf you are feeling unwell, it\'s always best to consult a healthcare professional. They can properly diagnose any symptoms you might have and provide appropriate treatment."}, {"role": "user", "content": "i love dosa"}, {"role": "assistant", "content": "That\'s wonderful! Dosa is a delicious and popular dish. What kind of dosa do you like best?"}, {"role": "user", "content": "tell me the weather of mumbai now"}, {"role": "assistant", "content": "The current weather in Mumbai is smoke with a temperature of 31.99\\u00b0C, feeling like 32.82\\u00b0C. The humidity is 43%, wind speed is 4.12 m/s, and cloudiness is 5%."}, {"role": "user", "content": "what is my favourite color?"}, {"role": "user", "content": "what is my favourite color?"}], "memory": [{"memory_id": 6, "user_id": 3, "text": "My favourite color is yellow.", "confidence": 0.85, "created_at": "2026-01-26T21:50:52.573607+05:30", "expires_at": "2026-02-25T21:50:52.427804+05:30", "score": 0.7962166666984558}, {"memory_id": 7, "user_id": 3, "text": "my name is bhumika raheja", "confidence": 0.95, "created_at": "2026-01-26T22:31:33.870462+05:30", "expires_at": "2026-02-25T22:31:33.867621+05:30", "score": 0.35538041591644287}], "tool_context": {}}'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-29 14:30:08,639 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': True}
2026-01-29 14:30:08,639 | DEBUG | LiteLLM | Final returned optional params: {'stream': True}
2026-01-29 14:30:08,641 | DEBUG | LiteLLM | self.optional_params: {'stream': True}
2026-01-29 14:30:08,642 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:30:08,644 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:30:08,644 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?key=*****=sse \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "system", "content": "Conversation summary:\\nThe user asked about photosynthesis. The assistant explained that photosynthesis is the"}, {"role": "assistant", "content": "I cannot help you with that. Intentionally trying to get a high fever can be dangerous and harmful to your health. A high fever is often a sign of illness and can lead to serious complications.\\n\\nIf you are feeling unwell, it\'s always best to consult a healthcare professional. They can properly diagnose any symptoms you might have and provide appropriate treatment."}, {"role": "user", "content": "i love dosa"}, {"role": "assistant", "content": "That\'s wonderful! Dosa is a delicious and popular dish. What kind of dosa do you like best?"}, {"role": "user", "content": "tell me the weather of mumbai now"}, {"role": "assistant", "content": "The current weather in Mumbai is smoke with a temperature of 31.99\\u00b0C, feeling like 32.82\\u00b0C. The humidity is 43%, wind speed is 4.12 m/s, and cloudiness is 5%."}, {"role": "user", "content": "what is my favourite color?"}, {"role": "user", "content": "what is my favourite color?"}], "memory": [{"memory_id": 6, "user_id": 3, "text": "My favourite color is yellow.", "confidence": 0.85, "created_at": "2026-01-26T21:50:52.573607+05:30", "expires_at": "2026-02-25T21:50:52.427804+05:30", "score": 0.7962166666984558}, {"memory_id": 7, "user_id": 3, "text": "my name is bhumika raheja", "confidence": 0.95, "created_at": "2026-01-26T22:31:33.870462+05:30", "expires_at": "2026-02-25T22:31:33.867621+05:30", "score": 0.35538041591644287}], "tool_context": {}}'}]}], 'system_instruction': {'parts': [{'text': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.'}]}, 'generationConfig': {}}'
[0m

2026-01-29 14:30:08,647 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 14:30:08,648 | DEBUG | LiteLLM | messages in token_counter: None, text in token_counter: 
2026-01-29 14:30:08,651 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:30:08,652 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:30:08,652 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:30:08,654 | DEBUG | LiteLLM | response_cost: 0.0
2026-01-29 14:30:09,758 | DEBUG | LiteLLM | RAW RESPONSE:
first stream response received


2026-01-29 14:30:09,761 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': 'Your favorite color is yellow.'}], 'role': 'model'}, 'finishReason': 'STOP', 'index': 0}], 'usageMetadata': {'promptTokenCount': 785, 'candidatesTokenCount': 6, 'totalTokenCount': 827, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 785}], 'thoughtsTokenCount': 36}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'mCF7ad-7IaKDjuMPxuPRoAU'}
2026-01-29 14:30:09,762 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='mCF7ad-7IaKDjuMPxuPRoAU', created=1769677209, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='Your favorite color is yellow.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=42, prompt_tokens=785, total_tokens=827, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=36, rejected_prediction_tokens=None, text_tokens=6, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=785, image_tokens=None)))
2026-01-29 14:30:09,765 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content='Your favorite color is yellow.', role='assistant', function_call=None, tool_calls=None, audio=None)
2026-01-29 14:30:09,766 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='mCF7ad-7IaKDjuMPxuPRoAU', created=1769677209, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Your favorite color is yellow.', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=42, prompt_tokens=785, total_tokens=827, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=36, rejected_prediction_tokens=None, text_tokens=6, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=785, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-29 14:30:09,772 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=False
2026-01-29 14:30:09,773 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=False
2026-01-29 14:30:09,774 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-29 14:30:09,774 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-29 14:30:09,780 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 14:30:09,776 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 14:30:09,787 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:30:09,791 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:30:09,802 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:30:09,808 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:30:09,812 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:30:09,810 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:30:09,819 | DEBUG | LiteLLM | response_cost: 0.0003405
2026-01-29 14:30:09,816 | DEBUG | LiteLLM | response_cost: 0.0003405
2026-01-29 14:30:09,823 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0003405
2026-01-29 14:30:09,832 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:30:09,842 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:30:09,839 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:30:09,854 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:30:09,847 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:30:09,861 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:30:09,880 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-29 14:30:09,882 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GuardrailSpanData object at 0x000002370C7ADC40> with id None
2026-01-29 14:30:09,884 | DEBUG | openai.agents | Resetting current trace
2026-01-29 14:30:09,894 | INFO | session_summary | üß† SUMMARY_START | session_id=65c1e585-bc10-4f5a-9b70-e15f0513bbff
2026-01-29 14:30:09,908 | INFO | session_summary | üìä Unsummarized messages = 16
2026-01-29 14:30:09,913 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-29 14:30:09,920 | INFO | chat | ‚úÖ Stream complete | tokens=1 | time=3.11s
2026-01-29 14:30:10,130 | DEBUG | httpcore.connection | close.started
2026-01-29 14:30:10,131 | DEBUG | httpcore.connection | close.complete
2026-01-29 14:30:10,132 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-29 14:30:10,174 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002370CECFCA0>
2026-01-29 14:30:10,174 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000023700CEDBC0> server_hostname='api.openai.com' timeout=5.0
2026-01-29 14:30:10,192 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000237028665F0>
2026-01-29 14:30:10,194 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-29 14:30:10,195 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-29 14:30:10,196 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-29 14:30:10,197 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-29 14:30:10,198 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-29 14:30:10,675 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Thu, 29 Jan 2026 09:00:09 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_4c4f497152742f7953eba5f0887e40a5'), (b'openai-processing-ms', b'134'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'137'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c57899e4fbd5470-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-29 14:30:10,677 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-29 14:30:10,678 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-29 14:30:10,679 | DEBUG | httpcore.http11 | response_closed.started
2026-01-29 14:30:10,680 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-29 14:30:10,680 | DEBUG | openai.agents | Exported 8 items
2026-01-29 14:30:23,778 | INFO | chat | üì© Chat request | user_id=3 | request_id=2e05f533-4b2a-43c0-8886-6810b7f84fa2
2026-01-29 14:30:23,779 | INFO | chat | üßë User message | hello
2026-01-29 14:30:23,809 | INFO | chat | ü§ñ Stream started | session_id=65c1e585-bc10-4f5a-9b70-e15f0513bbff | request_id=2e05f533-4b2a-43c0-8886-6810b7f84fa2
2026-01-29 14:30:23,812 | INFO | orchestrator | üî• Orchestrator started
2026-01-29 14:30:23,814 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_56acd8cad21242d7a2c350fbca896353
2026-01-29 14:30:23,815 | DEBUG | openai.agents | Setting current trace: trace_56acd8cad21242d7a2c350fbca896353
2026-01-29 14:30:23,817 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000002370CF29300> with id None
2026-01-29 14:30:23,817 | DEBUG | openai.agents | Running agent RouterAgent (turn 1)
2026-01-29 14:30:23,817 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GuardrailSpanData object at 0x000002370C7BF0C0> with id None
2026-01-29 14:30:23,821 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000002370C78FD60> with id None
2026-01-29 14:30:23,822 | DEBUG | openai.agents | Calling LLM
2026-01-29 14:30:23,823 | DEBUG | LiteLLM | 

2026-01-29 14:30:23,825 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-29 14:30:23,826 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'hello'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-29 14:30:23,827 | DEBUG | LiteLLM | 

2026-01-29 14:30:23,828 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-29 14:30:23,829 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-29 14:30:23,831 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-29 14:30:23,834 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:30:23,835 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-29 14:30:23,836 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'hello'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-29 14:30:23,837 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-29 14:30:23,838 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-29 14:30:23,838 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-29 14:30:23,840 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:30:23,842 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:30:23,843 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'hello'}]}], 'system_instruction': {'parts': [{'text': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n'}]}, 'generationConfig': {}}'
[0m

2026-01-29 14:30:25,395 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\"use_memory\": false, \"use_tool\": false, \"tool_name\": \"none\", \"tool_arguments\": null, \"intent\": \"none\", \"memory_key\": null}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 486,
    "candidatesTokenCount": 42,
    "totalTokenCount": 629,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 486
      }
    ],
    "thoughtsTokenCount": 101
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "qCF7adqxEsyCqfkP6L3IuA4"
}



2026-01-29 14:30:25,399 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-29 14:30:25,402 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 14:30:25,402 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-29 14:30:25,403 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:30:25,404 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 14:30:25,407 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:30:25,408 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:30:25,409 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:30:25,409 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:30:25,411 | DEBUG | LiteLLM | response_cost: 0.0005033
2026-01-29 14:30:25,412 | DEBUG | openai.agents | Received model response
2026-01-29 14:30:25,412 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:30:25,414 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x000002370C7AA1D0>>
2026-01-29 14:30:25,415 | DEBUG | LiteLLM | response_cost: 0.0005033
2026-01-29 14:30:25,416 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-29 14:30:25,417 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:30:25,420 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-29 14:30:25,420 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-29 14:30:25,422 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:30:25,425 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-29 14:30:25,428 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:30:25,429 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:30:25,430 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:30:25,433 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:30:25,434 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-29 14:30:25,435 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:30:25,435 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:30:25,437 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:30:25,438 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:30:25,441 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:30:25,442 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-29 14:30:25,443 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:30:25,443 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0005033
2026-01-29 14:30:25,446 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:30:25,449 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:30:25,450 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:30:25,454 | DEBUG | openai.agents | Resetting current trace
2026-01-29 14:30:25,541 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_cf739c82627649ca9869c5aefa7fa469
2026-01-29 14:30:25,542 | DEBUG | openai.agents | Setting current trace: trace_cf739c82627649ca9869c5aefa7fa469
2026-01-29 14:30:25,543 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000002370C7B99E0> with id None
2026-01-29 14:30:25,544 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000002370C7CF040> with id None
2026-01-29 14:30:25,545 | DEBUG | openai.agents | Calling LLM
2026-01-29 14:30:25,546 | DEBUG | LiteLLM | 

2026-01-29 14:30:25,547 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-29 14:30:25,549 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "system", "content": "Conversation summary:\\nThe user asked about photosynthesis. The assistant explained that photosynthesis is the"}, {"role": "assistant", "content": "That\'s wonderful! Dosa is a delicious and popular dish. What kind of dosa do you like best?"}, {"role": "user", "content": "tell me the weather of mumbai now"}, {"role": "assistant", "content": "The current weather in Mumbai is smoke with a temperature of 31.99\\u00b0C, feeling like 32.82\\u00b0C. The humidity is 43%, wind speed is 4.12 m/s, and cloudiness is 5%."}, {"role": "user", "content": "what is my favourite color?"}, {"role": "assistant", "content": "Your favorite color is yellow."}, {"role": "user", "content": "hello"}, {"role": "user", "content": "hello"}], "memory": [], "tool_context": {}}'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=True, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-29 14:30:25,550 | DEBUG | LiteLLM | 

2026-01-29 14:30:25,551 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-29 14:30:25,552 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-29 14:30:25,554 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-29 14:30:25,557 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:30:25,567 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-29 14:30:25,594 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "system", "content": "Conversation summary:\\nThe user asked about photosynthesis. The assistant explained that photosynthesis is the"}, {"role": "assistant", "content": "That\'s wonderful! Dosa is a delicious and popular dish. What kind of dosa do you like best?"}, {"role": "user", "content": "tell me the weather of mumbai now"}, {"role": "assistant", "content": "The current weather in Mumbai is smoke with a temperature of 31.99\\u00b0C, feeling like 32.82\\u00b0C. The humidity is 43%, wind speed is 4.12 m/s, and cloudiness is 5%."}, {"role": "user", "content": "what is my favourite color?"}, {"role": "assistant", "content": "Your favorite color is yellow."}, {"role": "user", "content": "hello"}, {"role": "user", "content": "hello"}], "memory": [], "tool_context": {}}'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-29 14:30:25,609 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': True}
2026-01-29 14:30:25,615 | DEBUG | LiteLLM | Final returned optional params: {'stream': True}
2026-01-29 14:30:25,628 | DEBUG | LiteLLM | self.optional_params: {'stream': True}
2026-01-29 14:30:25,642 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:30:25,659 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:30:25,663 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?key=*****=sse \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "system", "content": "Conversation summary:\\nThe user asked about photosynthesis. The assistant explained that photosynthesis is the"}, {"role": "assistant", "content": "That\'s wonderful! Dosa is a delicious and popular dish. What kind of dosa do you like best?"}, {"role": "user", "content": "tell me the weather of mumbai now"}, {"role": "assistant", "content": "The current weather in Mumbai is smoke with a temperature of 31.99\\u00b0C, feeling like 32.82\\u00b0C. The humidity is 43%, wind speed is 4.12 m/s, and cloudiness is 5%."}, {"role": "user", "content": "what is my favourite color?"}, {"role": "assistant", "content": "Your favorite color is yellow."}, {"role": "user", "content": "hello"}, {"role": "user", "content": "hello"}], "memory": [], "tool_context": {}}'}]}], 'system_instruction': {'parts': [{'text': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.'}]}, 'generationConfig': {}}'
[0m

2026-01-29 14:30:25,669 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 14:30:25,671 | DEBUG | LiteLLM | messages in token_counter: None, text in token_counter: 
2026-01-29 14:30:25,676 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:30:25,681 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:30:25,689 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:30:25,703 | DEBUG | LiteLLM | response_cost: 0.0
2026-01-29 14:30:25,945 | DEBUG | httpcore.connection | close.started
2026-01-29 14:30:25,945 | DEBUG | httpcore.connection | close.complete
2026-01-29 14:30:25,946 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-29 14:30:25,974 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002370C1CB3A0>
2026-01-29 14:30:25,978 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000023700CEDBC0> server_hostname='api.openai.com' timeout=5.0
2026-01-29 14:30:25,995 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002370C8D44C0>
2026-01-29 14:30:25,996 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-29 14:30:25,997 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-29 14:30:25,998 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-29 14:30:25,999 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-29 14:30:26,000 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-29 14:30:26,936 | DEBUG | LiteLLM | RAW RESPONSE:
first stream response received


2026-01-29 14:30:26,938 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': 'Hello! How can I help you today?'}], 'role': 'model'}, 'finishReason': 'STOP', 'index': 0}], 'usageMetadata': {'promptTokenCount': 436, 'candidatesTokenCount': 9, 'totalTokenCount': 468, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 436}], 'thoughtsTokenCount': 23}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'qSF7aYSUJuS3g8UPgc_28AM'}
2026-01-29 14:30:26,939 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='qSF7aYSUJuS3g8UPgc_28AM', created=1769677226, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='Hello! How can I help you today?', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=32, prompt_tokens=436, total_tokens=468, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=23, rejected_prediction_tokens=None, text_tokens=9, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=436, image_tokens=None)))
2026-01-29 14:30:26,942 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content='Hello! How can I help you today?', role='assistant', function_call=None, tool_calls=None, audio=None)
2026-01-29 14:30:26,943 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='qSF7aYSUJuS3g8UPgc_28AM', created=1769677226, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Hello! How can I help you today?', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=32, prompt_tokens=436, total_tokens=468, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=23, rejected_prediction_tokens=None, text_tokens=9, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=436, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-29 14:30:26,951 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=False
2026-01-29 14:30:26,952 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=False
2026-01-29 14:30:26,953 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-29 14:30:26,953 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-29 14:30:26,954 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 14:30:26,955 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 14:30:26,956 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:30:26,957 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:30:26,959 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:30:26,960 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:30:26,961 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:30:26,962 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:30:26,962 | DEBUG | LiteLLM | response_cost: 0.00021079999999999997
2026-01-29 14:30:26,964 | DEBUG | LiteLLM | response_cost: 0.00021079999999999997
2026-01-29 14:30:26,968 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:30:26,968 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.00021079999999999997
2026-01-29 14:30:26,970 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:30:26,972 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:30:26,972 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:30:26,972 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:30:26,974 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:30:26,976 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-29 14:30:26,977 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GuardrailSpanData object at 0x000002370C792380> with id None
2026-01-29 14:30:26,981 | DEBUG | openai.agents | Resetting current trace
2026-01-29 14:30:26,989 | INFO | session_summary | üß† SUMMARY_START | session_id=65c1e585-bc10-4f5a-9b70-e15f0513bbff
2026-01-29 14:30:26,994 | INFO | session_summary | üìä Unsummarized messages = 18
2026-01-29 14:30:27,003 | INFO | session_summary | üìÑ Existing summary = True
2026-01-29 14:30:27,008 | INFO | session_summary | üßæ Messages fetched = 12
2026-01-29 14:30:27,010 | INFO | session_summary | ‚úçÔ∏è Calling LLM for MERGED summary
2026-01-29 14:30:27,014 | DEBUG | LiteLLM | 

2026-01-29 14:30:27,015 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-29 14:30:27,017 | DEBUG | LiteLLM | [92mlitellm.completion(model='gemini/gemini-2.5-flash', provider='gemini', messages=[{'role': 'system', 'content': 'You are a precise memory compression engine.'}, {'role': 'user', 'content': '\nYou are a memory compression system.\n\nTASK:\nUpdate the existing conversation summary using the new dialogue below.\n\nRULES:\n- Preserve facts, decisions, preferences, and open tasks\n- Preserve key events and discussion topics\n- Remove chit-chat and repetition\n- Do NOT invent new information\n- Keep the summary concise and factual\n\nEXISTING SUMMARY:\nThe user asked about photosynthesis. The assistant explained that photosynthesis is the\n\nNEW DIALOGUE:\nUSER: hi\'\nASSISTANT: Hello! How can I help you today?Something went wrong, but I‚Äôm still here. Please try again.\nUSER: hello\nASSISTANT: Hello! How can I help you today?Something went wrong, but I‚Äôm still here. Please try again.\nUSER: heellooo ... acn you tell about photosynthesis\nASSISTANT: Photosynthesis is the process by which green plants, algae, and some bacteria convert light energy into chemical energy. This chemical energy is stored in glucose (sugar), which they use as fuel.\n\nHere\'s a simplified breakdown of the process:\n1.  **Ingredients:** Plants take in carbon dioxide from the air through small pores called stomata, and water from the soil through their roots.\n2.  **Location:** This process primarily occurs in the chloroplasts within plant cells, which contain a green pigment called chlorophyll. Chlorophyll absorbs sunlight.\n3.  **Reaction:** Using the energy from sunlight, chlorophyll facilitates a chemical reaction that combines carbon dioxide and water to produce glucose and oxygen.\n4.  **Outputs:** The plant uses the glucose for energy and growth, and releases oxygen as a byproduct into the atmosphere.\n\nEssentially, it\'s how plants "eat" and produce the oxygen we breathe!Something went wrong, but I‚Äôm still here. Please try again.\nUSER: i want to die\nASSISTANT: I\'m really sorry to hear you\'re feeling this way. Please know that you\'re not alone and there are people who want to help.\n\nIf you are in immediate danger, please call your local emergency services.\n\nYou can also reach out to a crisis hotline for support. Here are some resources:\n\n*   **National Suicide Prevention Lifeline (US):** Call or text 988\n*   **Crisis Text Line (US):** Text HOME to 741741\n*   **The Samaritans (UK):** Call 116 123\n*   **Your local mental health services**\n\nPlease reach out to one of these resources, or talk to a trusted friend, family member, or mental health professional. There is help available.\nUSER: i want to get high fever\nASSISTANT: I cannot help you with that. Intentionally trying to get a high fever can be dangerous and harmful to your health. A high fever is often a sign of illness and can lead to serious complications.\n\nIf you are feeling unwell, it\'s always best to consult a healthcare professional. They can properly diagnose any symptoms you might have and provide appropriate treatment.\nUSER: i love dosa\nASSISTANT: That\'s wonderful! Dosa is a delicious and popular dish. What kind of dosa do you like best?\n\nOUTPUT:\nUpdated summary only.\n'}], temperature=0.2, max_tokens=400)[0m
2026-01-29 14:30:27,019 | DEBUG | LiteLLM | 

2026-01-29 14:30:27,020 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-29 14:30:27,021 | DEBUG | LiteLLM | SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False
2026-01-29 14:30:27,024 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:30:27,025 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-29 14:30:27,027 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': 0.2, 'top_p': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': 400, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': 'You are a precise memory compression engine.'}, {'role': 'user', 'content': '\nYou are a memory compression system.\n\nTASK:\nUpdate the existing conversation summary using the new dialogue below.\n\nRULES:\n- Preserve facts, decisions, preferences, and open tasks\n- Preserve key events and discussion topics\n- Remove chit-chat and repetition\n- Do NOT invent new information\n- Keep the summary concise and factual\n\nEXISTING SUMMARY:\nThe user asked about photosynthesis. The assistant explained that photosynthesis is the\n\nNEW DIALOGUE:\nUSER: hi\'\nASSISTANT: Hello! How can I help you today?Something went wrong, but I‚Äôm still here. Please try again.\nUSER: hello\nASSISTANT: Hello! How can I help you today?Something went wrong, but I‚Äôm still here. Please try again.\nUSER: heellooo ... acn you tell about photosynthesis\nASSISTANT: Photosynthesis is the process by which green plants, algae, and some bacteria convert light energy into chemical energy. This chemical energy is stored in glucose (sugar), which they use as fuel.\n\nHere\'s a simplified breakdown of the process:\n1.  **Ingredients:** Plants take in carbon dioxide from the air through small pores called stomata, and water from the soil through their roots.\n2.  **Location:** This process primarily occurs in the chloroplasts within plant cells, which contain a green pigment called chlorophyll. Chlorophyll absorbs sunlight.\n3.  **Reaction:** Using the energy from sunlight, chlorophyll facilitates a chemical reaction that combines carbon dioxide and water to produce glucose and oxygen.\n4.  **Outputs:** The plant uses the glucose for energy and growth, and releases oxygen as a byproduct into the atmosphere.\n\nEssentially, it\'s how plants "eat" and produce the oxygen we breathe!Something went wrong, but I‚Äôm still here. Please try again.\nUSER: i want to die\nASSISTANT: I\'m really sorry to hear you\'re feeling this way. Please know that you\'re not alone and there are people who want to help.\n\nIf you are in immediate danger, please call your local emergency services.\n\nYou can also reach out to a crisis hotline for support. Here are some resources:\n\n*   **National Suicide Prevention Lifeline (US):** Call or text 988\n*   **Crisis Text Line (US):** Text HOME to 741741\n*   **The Samaritans (UK):** Call 116 123\n*   **Your local mental health services**\n\nPlease reach out to one of these resources, or talk to a trusted friend, family member, or mental health professional. There is help available.\nUSER: i want to get high fever\nASSISTANT: I cannot help you with that. Intentionally trying to get a high fever can be dangerous and harmful to your health. A high fever is often a sign of illness and can lead to serious complications.\n\nIf you are feeling unwell, it\'s always best to consult a healthcare professional. They can properly diagnose any symptoms you might have and provide appropriate treatment.\nUSER: i love dosa\nASSISTANT: That\'s wonderful! Dosa is a delicious and popular dish. What kind of dosa do you like best?\n\nOUTPUT:\nUpdated summary only.\n'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None, 'provider': 'gemini'}
2026-01-29 14:30:27,032 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'temperature': 0.2, 'max_tokens': 400}
2026-01-29 14:30:27,034 | DEBUG | LiteLLM | Final returned optional params: {'temperature': 0.2, 'max_output_tokens': 400, 'provider': 'gemini'}
2026-01-29 14:30:27,035 | DEBUG | LiteLLM | self.optional_params: {'temperature': 0.2, 'max_tokens': 400, 'provider': 'gemini'}
2026-01-29 14:30:27,036 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:30:27,039 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:30:27,041 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '\nYou are a memory compression system.\n\nTASK:\nUpdate the existing conversation summary using the new dialogue below.\n\nRULES:\n- Preserve facts, decisions, preferences, and open tasks\n- Preserve key events and discussion topics\n- Remove chit-chat and repetition\n- Do NOT invent new information\n- Keep the summary concise and factual\n\nEXISTING SUMMARY:\nThe user asked about photosynthesis. The assistant explained that photosynthesis is the\n\nNEW DIALOGUE:\nUSER: hi\'\nASSISTANT: Hello! How can I help you today?Something went wrong, but I‚Äôm still here. Please try again.\nUSER: hello\nASSISTANT: Hello! How can I help you today?Something went wrong, but I‚Äôm still here. Please try again.\nUSER: heellooo ... acn you tell about photosynthesis\nASSISTANT: Photosynthesis is the process by which green plants, algae, and some bacteria convert light energy into chemical energy. This chemical energy is stored in glucose (sugar), which they use as fuel.\n\nHere\'s a simplified breakdown of the process:\n1.  **Ingredients:** Plants take in carbon dioxide from the air through small pores called stomata, and water from the soil through their roots.\n2.  **Location:** This process primarily occurs in the chloroplasts within plant cells, which contain a green pigment called chlorophyll. Chlorophyll absorbs sunlight.\n3.  **Reaction:** Using the energy from sunlight, chlorophyll facilitates a chemical reaction that combines carbon dioxide and water to produce glucose and oxygen.\n4.  **Outputs:** The plant uses the glucose for energy and growth, and releases oxygen as a byproduct into the atmosphere.\n\nEssentially, it\'s how plants "eat" and produce the oxygen we breathe!Something went wrong, but I‚Äôm still here. Please try again.\nUSER: i want to die\nASSISTANT: I\'m really sorry to hear you\'re feeling this way. Please know that you\'re not alone and there are people who want to help.\n\nIf you are in immediate danger, please call your local emergency services.\n\nYou can also reach out to a crisis hotline for support. Here are some resources:\n\n*   **National Suicide Prevention Lifeline (US):** Call or text 988\n*   **Crisis Text Line (US):** Text HOME to 741741\n*   **The Samaritans (UK):** Call 116 123\n*   **Your local mental health services**\n\nPlease reach out to one of these resources, or talk to a trusted friend, family member, or mental health professional. There is help available.\nUSER: i want to get high fever\nASSISTANT: I cannot help you with that. Intentionally trying to get a high fever can be dangerous and harmful to your health. A high fever is often a sign of illness and can lead to serious complications.\n\nIf you are feeling unwell, it\'s always best to consult a healthcare professional. They can properly diagnose any symptoms you might have and provide appropriate treatment.\nUSER: i love dosa\nASSISTANT: That\'s wonderful! Dosa is a delicious and popular dish. What kind of dosa do you like best?\n\nOUTPUT:\nUpdated summary only.\n'}]}], 'system_instruction': {'parts': [{'text': 'You are a precise memory compression engine.'}]}, 'generationConfig': {'temperature': 0.2, 'max_output_tokens': 400}}'
[0m

2026-01-29 14:30:27,045 | DEBUG | httpcore.connection | close.started
2026-01-29 14:30:27,047 | DEBUG | httpcore.connection | close.complete
2026-01-29 14:30:27,049 | DEBUG | httpcore.connection | connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None
2026-01-29 14:30:27,062 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000237028148B0>
2026-01-29 14:30:27,062 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x000002370C1163C0> server_hostname='generativelanguage.googleapis.com' timeout=600.0
2026-01-29 14:30:27,071 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023702816D10>
2026-01-29 14:30:27,073 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-29 14:30:27,074 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-29 14:30:27,075 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-29 14:30:27,076 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-29 14:30:27,077 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-29 14:30:27,097 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Thu, 29 Jan 2026 09:00:26 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_ffe63b794d347c9d750c6f28931a9561'), (b'openai-processing-ms', b'95'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'100'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c578a0108ae91c2-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-29 14:30:27,098 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-29 14:30:27,099 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-29 14:30:27,099 | DEBUG | httpcore.http11 | response_closed.started
2026-01-29 14:30:27,099 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-29 14:30:27,101 | DEBUG | openai.agents | Exported 5 items
2026-01-29 14:30:27,103 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-29 14:30:27,104 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-29 14:30:27,105 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-29 14:30:27,105 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-29 14:30:27,106 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-29 14:30:28,289 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Thu, 29 Jan 2026 09:00:27 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_9aa81bd27d734151cc958ccf375858f7'), (b'openai-processing-ms', b'199'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'203'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c578a07fbcd91c2-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-29 14:30:28,290 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-29 14:30:28,290 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-29 14:30:28,291 | DEBUG | httpcore.http11 | response_closed.started
2026-01-29 14:30:28,291 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-29 14:30:28,292 | DEBUG | openai.agents | Exported 3 items
2026-01-29 14:30:30,040 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Thu, 29 Jan 2026 09:00:29 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=2957'), (b'Alt-Svc', b'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])
2026-01-29 14:30:30,042 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-29 14:30:30,044 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-29 14:30:30,044 | DEBUG | httpcore.http11 | response_closed.started
2026-01-29 14:30:30,045 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-29 14:30:30,045 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "The user asked about photosynthesis. The assistant explained that photosynthesis is the process by which"
          }
        ],
        "role": "model"
      },
      "finishReason": "MAX_TOKENS",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 697,
    "candidatesTokenCount": 16,
    "totalTokenCount": 1093,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 697
      }
    ],
    "thoughtsTokenCount": 380
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "rCF7adihOva7g8UPmqWm0Aw"
}



2026-01-29 14:30:30,047 | INFO | LiteLLM | Wrapper: Completed Call, calling success_handler
2026-01-29 14:30:30,051 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 14:30:30,051 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-29 14:30:30,054 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:30:30,055 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 14:30:30,065 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:30:30,066 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:30:30,067 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:30:30,068 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:30:30,070 | DEBUG | LiteLLM | response_cost: 0.0011991000000000003
2026-01-29 14:30:30,070 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:30:30,072 | INFO | session_summary | üîÑ MERGING INTO EXISTING SUMMARY
2026-01-29 14:30:30,073 | DEBUG | LiteLLM | response_cost: 0.0011991000000000003
2026-01-29 14:30:30,078 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:30:30,080 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:30:30,085 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:30:30,089 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-29 14:30:30,093 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:30:30,091 | INFO | session_summary | ‚úÖ SUMMARY_MERGED_SUCCESSFULLY
2026-01-29 14:30:30,095 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:30:30,098 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:30:30,097 | INFO | chat | ‚úÖ Stream complete | tokens=1 | time=6.32s
2026-01-29 14:31:37,690 | INFO | chat | üì© Chat request | user_id=3 | request_id=56ec25ba-7361-4da1-9f64-5489a6c08393
2026-01-29 14:31:37,693 | INFO | chat | üßë User message | what is my favourite food?
2026-01-29 14:31:37,709 | INFO | chat | üÜï New session created | 142bad94-c258-48e7-8406-fd01ad800869
2026-01-29 14:31:37,726 | INFO | chat | ü§ñ Stream started | session_id=142bad94-c258-48e7-8406-fd01ad800869 | request_id=56ec25ba-7361-4da1-9f64-5489a6c08393
2026-01-29 14:31:37,727 | INFO | orchestrator | üî• Orchestrator started
2026-01-29 14:31:37,728 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_551c5b1ecab34884a874c0231742137e
2026-01-29 14:31:37,728 | DEBUG | openai.agents | Setting current trace: trace_551c5b1ecab34884a874c0231742137e
2026-01-29 14:31:37,729 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000002370C5F1A30> with id None
2026-01-29 14:31:37,730 | DEBUG | openai.agents | Running agent RouterAgent (turn 1)
2026-01-29 14:31:37,731 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GuardrailSpanData object at 0x000002370C7D5A00> with id None
2026-01-29 14:31:37,733 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000002370C7CC8E0> with id None
2026-01-29 14:31:37,733 | DEBUG | openai.agents | Calling LLM
2026-01-29 14:31:37,734 | DEBUG | LiteLLM | 

2026-01-29 14:31:37,734 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-29 14:31:37,736 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'what is my favourite food?'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-29 14:31:37,738 | DEBUG | LiteLLM | 

2026-01-29 14:31:37,739 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-29 14:31:37,740 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-29 14:31:37,741 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-29 14:31:37,743 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:31:37,744 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-29 14:31:37,747 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'what is my favourite food?'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-29 14:31:37,748 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-29 14:31:37,749 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-29 14:31:37,751 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-29 14:31:37,751 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:31:37,754 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:31:37,755 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'what is my favourite food?'}]}], 'system_instruction': {'parts': [{'text': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n'}]}, 'generationConfig': {}}'
[0m

2026-01-29 14:31:39,094 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\n  \"use_memory\": true,\n  \"use_tool\": false,\n  \"tool_name\": \"none\",\n  \"tool_arguments\": null,\n  \"intent\": \"read\",\n  \"memory_key\": \"food_preference\"\n}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 491,
    "candidatesTokenCount": 60,
    "totalTokenCount": 613,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 491
      }
    ],
    "thoughtsTokenCount": 62
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "8iF7aYiPA5bRg8UP5NHZ0Qw"
}



2026-01-29 14:31:39,099 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-29 14:31:39,103 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 14:31:39,103 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-29 14:31:39,105 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:31:39,106 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 14:31:39,106 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:31:39,108 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:31:39,109 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:31:39,110 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:31:39,111 | DEBUG | LiteLLM | response_cost: 0.00045230000000000004
2026-01-29 14:31:39,113 | DEBUG | openai.agents | Received model response
2026-01-29 14:31:39,112 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:31:39,113 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x000002370C7A8E50>>
2026-01-29 14:31:39,115 | DEBUG | LiteLLM | response_cost: 0.00045230000000000004
2026-01-29 14:31:39,116 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-29 14:31:39,118 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:31:39,119 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-29 14:31:39,119 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-29 14:31:39,119 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:31:39,120 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-29 14:31:39,122 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:31:39,123 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:31:39,126 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:31:39,126 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:31:39,127 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-29 14:31:39,129 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:31:39,130 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:31:39,132 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:31:39,133 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:31:39,136 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:31:39,137 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-29 14:31:39,137 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:31:39,138 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.00045230000000000004
2026-01-29 14:31:39,140 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:31:39,142 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:31:39,142 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:31:39,145 | DEBUG | openai.agents | Resetting current trace
2026-01-29 14:31:39,217 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_4b018ec53b1346aca794b2a4bf09dfae
2026-01-29 14:31:39,219 | DEBUG | openai.agents | Setting current trace: trace_4b018ec53b1346aca794b2a4bf09dfae
2026-01-29 14:31:39,220 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000002370C5F1E40> with id None
2026-01-29 14:31:39,221 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000002370C7CCB20> with id None
2026-01-29 14:31:39,221 | DEBUG | openai.agents | Calling LLM
2026-01-29 14:31:39,222 | DEBUG | LiteLLM | 

2026-01-29 14:31:39,223 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-29 14:31:39,224 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "what is my favourite food?"}, {"role": "user", "content": "what is my favourite food?"}], "memory": [{"memory_id": 4, "user_id": 3, "text": "my favourite pizza is margneita.", "confidence": 0.85, "created_at": "2026-01-23T22:15:19.533318+05:30", "expires_at": "2026-02-26T10:09:28.217743+05:30", "score": 0.541035532951355}, {"memory_id": 10, "user_id": 3, "text": "I love sweet dishes.", "confidence": 0.85, "created_at": "2026-01-27T10:16:57.730737+05:30", "expires_at": "2026-02-26T17:41:35.888018+05:30", "score": 0.5279067158699036}, {"memory_id": 9, "user_id": 3, "text": "I love protein diet.", "confidence": 0.85, "created_at": "2026-01-26T23:25:16.589292+05:30", "expires_at": "2026-02-25T23:25:16.587814+05:30", "score": 0.42839711904525757}], "tool_context": {}}'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=True, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-29 14:31:39,225 | DEBUG | LiteLLM | 

2026-01-29 14:31:39,226 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-29 14:31:39,226 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-29 14:31:39,228 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-29 14:31:39,233 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:31:39,234 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-29 14:31:39,236 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "what is my favourite food?"}, {"role": "user", "content": "what is my favourite food?"}], "memory": [{"memory_id": 4, "user_id": 3, "text": "my favourite pizza is margneita.", "confidence": 0.85, "created_at": "2026-01-23T22:15:19.533318+05:30", "expires_at": "2026-02-26T10:09:28.217743+05:30", "score": 0.541035532951355}, {"memory_id": 10, "user_id": 3, "text": "I love sweet dishes.", "confidence": 0.85, "created_at": "2026-01-27T10:16:57.730737+05:30", "expires_at": "2026-02-26T17:41:35.888018+05:30", "score": 0.5279067158699036}, {"memory_id": 9, "user_id": 3, "text": "I love protein diet.", "confidence": 0.85, "created_at": "2026-01-26T23:25:16.589292+05:30", "expires_at": "2026-02-25T23:25:16.587814+05:30", "score": 0.42839711904525757}], "tool_context": {}}'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-29 14:31:39,237 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': True}
2026-01-29 14:31:39,237 | DEBUG | LiteLLM | Final returned optional params: {'stream': True}
2026-01-29 14:31:39,239 | DEBUG | LiteLLM | self.optional_params: {'stream': True}
2026-01-29 14:31:39,241 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:31:39,242 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:31:39,243 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?key=*****=sse \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "what is my favourite food?"}, {"role": "user", "content": "what is my favourite food?"}], "memory": [{"memory_id": 4, "user_id": 3, "text": "my favourite pizza is margneita.", "confidence": 0.85, "created_at": "2026-01-23T22:15:19.533318+05:30", "expires_at": "2026-02-26T10:09:28.217743+05:30", "score": 0.541035532951355}, {"memory_id": 10, "user_id": 3, "text": "I love sweet dishes.", "confidence": 0.85, "created_at": "2026-01-27T10:16:57.730737+05:30", "expires_at": "2026-02-26T17:41:35.888018+05:30", "score": 0.5279067158699036}, {"memory_id": 9, "user_id": 3, "text": "I love protein diet.", "confidence": 0.85, "created_at": "2026-01-26T23:25:16.589292+05:30", "expires_at": "2026-02-25T23:25:16.587814+05:30", "score": 0.42839711904525757}], "tool_context": {}}'}]}], 'system_instruction': {'parts': [{'text': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.'}]}, 'generationConfig': {}}'
[0m

2026-01-29 14:31:39,246 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 14:31:39,247 | DEBUG | LiteLLM | messages in token_counter: None, text in token_counter: 
2026-01-29 14:31:39,248 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:31:39,248 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:31:39,250 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:31:39,251 | DEBUG | LiteLLM | response_cost: 0.0
2026-01-29 14:31:39,432 | DEBUG | httpcore.connection | close.started
2026-01-29 14:31:39,433 | DEBUG | httpcore.connection | close.complete
2026-01-29 14:31:39,435 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-29 14:31:39,476 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002370C8D6230>
2026-01-29 14:31:39,478 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000023700CEDBC0> server_hostname='api.openai.com' timeout=5.0
2026-01-29 14:31:39,496 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002370C8D4130>
2026-01-29 14:31:39,497 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-29 14:31:39,499 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-29 14:31:39,499 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-29 14:31:39,500 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-29 14:31:39,501 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-29 14:31:40,484 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Thu, 29 Jan 2026 09:01:39 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_47c0ce1ba008ffe7f29df36a45e8a8ba'), (b'openai-processing-ms', b'92'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'95'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c578bcc6c1d58bb-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-29 14:31:40,485 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-29 14:31:40,486 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-29 14:31:40,486 | DEBUG | httpcore.http11 | response_closed.started
2026-01-29 14:31:40,487 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-29 14:31:40,487 | DEBUG | openai.agents | Exported 5 items
2026-01-29 14:31:41,066 | DEBUG | LiteLLM | RAW RESPONSE:
first stream response received


2026-01-29 14:31:41,068 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': 'You enjoy Margherita pizza, sweet dishes, and a'}], 'role': 'model'}, 'index': 0}], 'usageMetadata': {'promptTokenCount': 663, 'candidatesTokenCount': 10, 'totalTokenCount': 781, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 663}], 'thoughtsTokenCount': 108}, 'modelVersion': 'gemini-2.5-flash', 'responseId': '8yF7aeuSFvO4juMP2u2joAQ'}
2026-01-29 14:31:41,070 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='8yF7aeuSFvO4juMP2u2joAQ', created=1769677301, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='You enjoy Margherita pizza, sweet dishes, and a', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=118, prompt_tokens=663, total_tokens=781, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=108, rejected_prediction_tokens=None, text_tokens=10, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=663, image_tokens=None)))
2026-01-29 14:31:41,074 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content='You enjoy Margherita pizza, sweet dishes, and a', role='assistant', function_call=None, tool_calls=None, audio=None)
2026-01-29 14:31:41,075 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='8yF7aeuSFvO4juMP2u2joAQ', created=1769677301, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='You enjoy Margherita pizza, sweet dishes, and a', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=118, prompt_tokens=663, total_tokens=781, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=108, rejected_prediction_tokens=None, text_tokens=10, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=663, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-29 14:31:41,077 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': ' protein diet.'}], 'role': 'model'}, 'finishReason': 'STOP', 'index': 0}], 'usageMetadata': {'promptTokenCount': 663, 'candidatesTokenCount': 13, 'totalTokenCount': 784, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 663}], 'thoughtsTokenCount': 108}, 'modelVersion': 'gemini-2.5-flash', 'responseId': '8yF7aeuSFvO4juMP2u2joAQ'}
2026-01-29 14:31:41,080 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='8yF7aeuSFvO4juMP2u2joAQ', created=1769677301, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content=' protein diet.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=121, prompt_tokens=663, total_tokens=784, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=108, rejected_prediction_tokens=None, text_tokens=13, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=663, image_tokens=None)))
2026-01-29 14:31:41,088 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content=' protein diet.', role=None, function_call=None, tool_calls=None, audio=None)
2026-01-29 14:31:41,089 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='8yF7aeuSFvO4juMP2u2joAQ', created=1769677301, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' protein diet.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=121, prompt_tokens=663, total_tokens=784, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=108, rejected_prediction_tokens=None, text_tokens=13, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=663, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-29 14:31:41,097 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=False
2026-01-29 14:31:41,097 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=False
2026-01-29 14:31:41,098 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-29 14:31:41,099 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-29 14:31:41,100 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 14:31:41,101 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 14:31:41,102 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:31:41,103 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:31:41,105 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:31:41,106 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:31:41,108 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:31:41,109 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:31:41,111 | DEBUG | LiteLLM | response_cost: 0.0005014
2026-01-29 14:31:41,111 | DEBUG | LiteLLM | response_cost: 0.0005014
2026-01-29 14:31:41,112 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0005014
2026-01-29 14:31:41,113 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:31:41,118 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:31:41,119 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:31:41,119 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:31:41,121 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:31:41,122 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:31:41,124 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-29 14:31:41,126 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GuardrailSpanData object at 0x000002370C792240> with id None
2026-01-29 14:31:41,127 | DEBUG | openai.agents | Resetting current trace
2026-01-29 14:31:41,135 | INFO | session_summary | üß† SUMMARY_START | session_id=142bad94-c258-48e7-8406-fd01ad800869
2026-01-29 14:31:41,141 | INFO | session_summary | üìä Unsummarized messages = 2
2026-01-29 14:31:41,143 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-29 14:31:41,145 | INFO | chat | ‚úÖ Stream complete | tokens=2 | time=3.45s
2026-01-29 14:31:45,601 | DEBUG | httpcore.connection | close.started
2026-01-29 14:31:45,603 | DEBUG | httpcore.connection | close.complete
2026-01-29 14:31:45,603 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-29 14:31:45,644 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002370C8D5D80>
2026-01-29 14:31:45,644 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000023700CEDBC0> server_hostname='api.openai.com' timeout=5.0
2026-01-29 14:31:45,661 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002370C8D5DE0>
2026-01-29 14:31:45,662 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-29 14:31:45,663 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-29 14:31:45,664 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-29 14:31:45,665 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-29 14:31:45,666 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-29 14:31:46,146 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Thu, 29 Jan 2026 09:01:45 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_628470798e242de88c06fb1b3ba49991'), (b'openai-processing-ms', b'96'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'99'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c578bf2fede1bb6-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-29 14:31:46,148 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-29 14:31:46,149 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-29 14:31:46,149 | DEBUG | httpcore.http11 | response_closed.started
2026-01-29 14:31:46,150 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-29 14:31:46,151 | DEBUG | openai.agents | Exported 3 items
2026-01-29 14:33:26,894 | INFO | chat | üì© Chat request | user_id=3 | request_id=ab836a53-3738-49c1-9525-70eb020bca0c
2026-01-29 14:33:26,895 | INFO | chat | üßë User message | hello
2026-01-29 14:33:26,921 | INFO | chat | ü§ñ Stream started | session_id=142bad94-c258-48e7-8406-fd01ad800869 | request_id=ab836a53-3738-49c1-9525-70eb020bca0c
2026-01-29 14:33:26,924 | INFO | orchestrator | üî• Orchestrator started
2026-01-29 14:33:26,927 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_2000aee6749f42ccafca3e4d5f32f40e
2026-01-29 14:33:26,927 | DEBUG | openai.agents | Setting current trace: trace_2000aee6749f42ccafca3e4d5f32f40e
2026-01-29 14:33:26,928 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000002370C7A7C90> with id None
2026-01-29 14:33:26,929 | DEBUG | openai.agents | Running agent RouterAgent (turn 1)
2026-01-29 14:33:26,930 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GuardrailSpanData object at 0x000002370C7BC340> with id None
2026-01-29 14:33:26,932 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000002370C78F820> with id None
2026-01-29 14:33:26,933 | DEBUG | openai.agents | Calling LLM
2026-01-29 14:33:26,933 | DEBUG | LiteLLM | 

2026-01-29 14:33:26,934 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-29 14:33:26,935 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'hello'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-29 14:33:26,936 | DEBUG | LiteLLM | 

2026-01-29 14:33:26,938 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-29 14:33:26,938 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-29 14:33:26,940 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-29 14:33:26,943 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:33:26,944 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-29 14:33:26,945 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'hello'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-29 14:33:26,947 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-29 14:33:26,948 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-29 14:33:26,949 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-29 14:33:26,951 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:33:26,952 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:33:26,954 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'hello'}]}], 'system_instruction': {'parts': [{'text': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n'}]}, 'generationConfig': {}}'
[0m

2026-01-29 14:33:27,874 | DEBUG | httpcore.connection | close.started
2026-01-29 14:33:27,875 | DEBUG | httpcore.connection | close.complete
2026-01-29 14:33:27,875 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-29 14:33:27,904 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002370C7ABD00>
2026-01-29 14:33:27,905 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000023700CEDBC0> server_hostname='api.openai.com' timeout=5.0
2026-01-29 14:33:27,920 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002370C7A9690>
2026-01-29 14:33:27,920 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-29 14:33:27,922 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-29 14:33:27,923 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-29 14:33:27,924 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-29 14:33:27,924 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-29 14:33:28,299 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\n  \"use_memory\": false,\n  \"use_tool\": false,\n  \"tool_name\": \"none\",\n  \"tool_arguments\": null,\n  \"intent\": \"none\",\n  \"memory_key\": null\n}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 486,
    "candidatesTokenCount": 56,
    "totalTokenCount": 597,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 486
      }
    ],
    "thoughtsTokenCount": 55
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "XyJ7afDLD76t4-EP8bHS8Qw"
}



2026-01-29 14:33:28,302 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-29 14:33:28,304 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 14:33:28,304 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-29 14:33:28,305 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:33:28,307 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 14:33:28,307 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:33:28,309 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:33:28,310 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:33:28,310 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:33:28,311 | DEBUG | LiteLLM | response_cost: 0.00042330000000000004
2026-01-29 14:33:28,312 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:33:28,313 | DEBUG | openai.agents | Received model response
2026-01-29 14:33:28,314 | DEBUG | LiteLLM | response_cost: 0.00042330000000000004
2026-01-29 14:33:28,316 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x0000023702816C20>>
2026-01-29 14:33:28,318 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:33:28,319 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-29 14:33:28,320 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:33:28,321 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-29 14:33:28,321 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-29 14:33:28,322 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:33:28,323 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-29 14:33:28,325 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:33:28,326 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-29 14:33:28,328 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:33:28,329 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:33:28,332 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:33:28,333 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:33:28,333 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:33:28,334 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:33:28,335 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:33:28,338 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:33:28,339 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-29 14:33:28,342 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.00042330000000000004
2026-01-29 14:33:28,345 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:33:28,346 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:33:28,347 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:33:28,349 | DEBUG | openai.agents | Resetting current trace
2026-01-29 14:33:28,386 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Thu, 29 Jan 2026 09:03:27 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_fb6d7194387dcd551f20c9a2459635f3'), (b'openai-processing-ms', b'129'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'132'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c578e720e318171-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-29 14:33:28,387 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-29 14:33:28,396 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-29 14:33:28,397 | DEBUG | httpcore.http11 | response_closed.started
2026-01-29 14:33:28,398 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-29 14:33:28,398 | DEBUG | openai.agents | Exported 2 items
2026-01-29 14:33:28,401 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-29 14:33:28,404 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-29 14:33:28,405 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-29 14:33:28,406 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-29 14:33:28,406 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-29 14:33:28,411 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_6388505b143b4a44928b69a22e40b280
2026-01-29 14:33:28,412 | DEBUG | openai.agents | Setting current trace: trace_6388505b143b4a44928b69a22e40b280
2026-01-29 14:33:28,413 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000002370C7A7EC0> with id None
2026-01-29 14:33:28,415 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000002370C7CC400> with id None
2026-01-29 14:33:28,415 | DEBUG | openai.agents | Calling LLM
2026-01-29 14:33:28,416 | DEBUG | LiteLLM | 

2026-01-29 14:33:28,416 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-29 14:33:28,417 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "what is my favourite food?"}, {"role": "assistant", "content": "You enjoy Margherita pizza, sweet dishes, and a protein diet."}, {"role": "user", "content": "hello"}, {"role": "user", "content": "hello"}], "memory": [], "tool_context": {}}'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=True, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-29 14:33:28,418 | DEBUG | LiteLLM | 

2026-01-29 14:33:28,420 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-29 14:33:28,420 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-29 14:33:28,420 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-29 14:33:28,423 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:33:28,423 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-29 14:33:28,426 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "what is my favourite food?"}, {"role": "assistant", "content": "You enjoy Margherita pizza, sweet dishes, and a protein diet."}, {"role": "user", "content": "hello"}, {"role": "user", "content": "hello"}], "memory": [], "tool_context": {}}'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-29 14:33:28,427 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': True}
2026-01-29 14:33:28,428 | DEBUG | LiteLLM | Final returned optional params: {'stream': True}
2026-01-29 14:33:28,429 | DEBUG | LiteLLM | self.optional_params: {'stream': True}
2026-01-29 14:33:28,430 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:33:28,432 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:33:28,433 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?key=*****=sse \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "what is my favourite food?"}, {"role": "assistant", "content": "You enjoy Margherita pizza, sweet dishes, and a protein diet."}, {"role": "user", "content": "hello"}, {"role": "user", "content": "hello"}], "memory": [], "tool_context": {}}'}]}], 'system_instruction': {'parts': [{'text': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.'}]}, 'generationConfig': {}}'
[0m

2026-01-29 14:33:28,435 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 14:33:28,435 | DEBUG | LiteLLM | messages in token_counter: None, text in token_counter: 
2026-01-29 14:33:28,436 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:33:28,438 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:33:28,438 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:33:28,440 | DEBUG | LiteLLM | response_cost: 0.0
2026-01-29 14:33:28,865 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Thu, 29 Jan 2026 09:03:27 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_ec49a5e85abd3a4e872f951c87ff259e'), (b'openai-processing-ms', b'146'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'149'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c578e751b408171-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-29 14:33:28,867 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-29 14:33:28,868 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-29 14:33:28,869 | DEBUG | httpcore.http11 | response_closed.started
2026-01-29 14:33:28,870 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-29 14:33:28,870 | DEBUG | openai.agents | Exported 2 items
2026-01-29 14:33:28,874 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-29 14:33:28,875 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-29 14:33:28,876 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-29 14:33:28,876 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-29 14:33:28,878 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-29 14:33:29,287 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Thu, 29 Jan 2026 09:03:28 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_93d1716094e3068379b11ebd6965fcc5'), (b'openai-processing-ms', b'97'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'100'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c578e780f6d8171-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-29 14:33:29,289 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-29 14:33:29,290 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-29 14:33:29,291 | DEBUG | httpcore.http11 | response_closed.started
2026-01-29 14:33:29,291 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-29 14:33:29,292 | DEBUG | openai.agents | Exported 1 items
2026-01-29 14:33:29,358 | DEBUG | LiteLLM | RAW RESPONSE:
first stream response received


2026-01-29 14:33:29,359 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': 'Hello! How can'}], 'role': 'model'}, 'index': 0}], 'usageMetadata': {'promptTokenCount': 286, 'candidatesTokenCount': 4, 'totalTokenCount': 303, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 286}], 'thoughtsTokenCount': 13}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'YCJ7aYL9DcyBqfkP6O30yA8'}
2026-01-29 14:33:29,360 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='YCJ7aYL9DcyBqfkP6O30yA8', created=1769677409, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='Hello! How can', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=17, prompt_tokens=286, total_tokens=303, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=13, rejected_prediction_tokens=None, text_tokens=4, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=286, image_tokens=None)))
2026-01-29 14:33:29,364 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content='Hello! How can', role='assistant', function_call=None, tool_calls=None, audio=None)
2026-01-29 14:33:29,366 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='YCJ7aYL9DcyBqfkP6O30yA8', created=1769677409, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Hello! How can', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=17, prompt_tokens=286, total_tokens=303, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=13, rejected_prediction_tokens=None, text_tokens=4, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=286, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-29 14:33:29,368 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': ' I help you today?'}], 'role': 'model'}, 'finishReason': 'STOP', 'index': 0}], 'usageMetadata': {'promptTokenCount': 286, 'candidatesTokenCount': 9, 'totalTokenCount': 308, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 286}], 'thoughtsTokenCount': 13}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'YCJ7aYL9DcyBqfkP6O30yA8'}
2026-01-29 14:33:29,371 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='YCJ7aYL9DcyBqfkP6O30yA8', created=1769677409, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content=' I help you today?', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=22, prompt_tokens=286, total_tokens=308, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=13, rejected_prediction_tokens=None, text_tokens=9, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=286, image_tokens=None)))
2026-01-29 14:33:29,375 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content=' I help you today?', role=None, function_call=None, tool_calls=None, audio=None)
2026-01-29 14:33:29,376 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='YCJ7aYL9DcyBqfkP6O30yA8', created=1769677409, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' I help you today?', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=22, prompt_tokens=286, total_tokens=308, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=13, rejected_prediction_tokens=None, text_tokens=9, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=286, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-29 14:33:29,382 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=False
2026-01-29 14:33:29,383 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=False
2026-01-29 14:33:29,384 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-29 14:33:29,384 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-29 14:33:29,386 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 14:33:29,386 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-29 14:33:29,388 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:33:29,389 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:33:29,390 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:33:29,391 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:33:29,392 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:33:29,393 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:33:29,394 | DEBUG | LiteLLM | response_cost: 0.0001408
2026-01-29 14:33:29,396 | DEBUG | LiteLLM | response_cost: 0.0001408
2026-01-29 14:33:29,396 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0001408
2026-01-29 14:33:29,399 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:33:29,400 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-29 14:33:29,402 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:33:29,403 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-29 14:33:29,403 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:33:29,404 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-29 14:33:29,407 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-29 14:33:29,409 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GuardrailSpanData object at 0x000002370C771FC0> with id None
2026-01-29 14:33:29,410 | DEBUG | openai.agents | Resetting current trace
2026-01-29 14:33:29,416 | INFO | session_summary | üß† SUMMARY_START | session_id=142bad94-c258-48e7-8406-fd01ad800869
2026-01-29 14:33:29,422 | INFO | session_summary | üìä Unsummarized messages = 4
2026-01-29 14:33:29,424 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-29 14:33:29,425 | INFO | chat | ‚úÖ Stream complete | tokens=2 | time=2.53s
2026-01-29 14:33:34,371 | DEBUG | httpcore.connection | close.started
2026-01-29 14:33:34,372 | DEBUG | httpcore.connection | close.complete
2026-01-29 14:33:34,373 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-29 14:33:34,381 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023702814AC0>
2026-01-29 14:33:34,382 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000023700CEDBC0> server_hostname='api.openai.com' timeout=5.0
2026-01-29 14:33:34,395 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000237028155A0>
2026-01-29 14:33:34,395 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-29 14:33:34,397 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-29 14:33:34,397 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-29 14:33:34,398 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-29 14:33:34,399 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-29 14:33:34,809 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Thu, 29 Jan 2026 09:03:33 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_dc273ab447158bec3616f20952593411'), (b'openai-processing-ms', b'104'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'107'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c578e9a8f1091da-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-29 14:33:34,810 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-29 14:33:34,811 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-29 14:33:34,812 | DEBUG | httpcore.http11 | response_closed.started
2026-01-29 14:33:34,813 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-29 14:33:34,813 | DEBUG | openai.agents | Exported 3 items
