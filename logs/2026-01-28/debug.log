2026-01-28 09:45:28,101 | INFO | sentence_transformers.SentenceTransformer | Use pytorch device_name: cpu
2026-01-28 09:45:28,102 | INFO | sentence_transformers.SentenceTransformer | Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2026-01-28 09:45:28,105 | DEBUG | urllib3.connectionpool | Starting new HTTPS connection (1): huggingface.co:443
2026-01-28 09:45:28,400 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-28 09:45:28,410 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-28 09:45:28,646 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-28 09:45:28,659 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-28 09:45:28,894 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-28 09:45:28,905 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-28 09:45:29,133 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md HTTP/1.1" 307 0
2026-01-28 09:45:29,144 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/README.md HTTP/1.1" 200 0
2026-01-28 09:45:29,396 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-28 09:45:29,410 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-28 09:45:29,665 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json HTTP/1.1" 307 0
2026-01-28 09:45:29,675 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/sentence_bert_config.json HTTP/1.1" 200 0
2026-01-28 09:45:29,931 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json HTTP/1.1" 404 0
2026-01-28 09:45:30,196 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1" 307 0
2026-01-28 09:45:30,213 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json HTTP/1.1" 200 0
2026-01-28 09:45:30,567 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/1.1" 307 0
2026-01-28 09:45:30,578 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json HTTP/1.1" 200 0
2026-01-28 09:45:30,825 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1" 404 64
2026-01-28 09:45:31,075 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main?recursive=True&expand=False HTTP/1.1" 200 6465
2026-01-28 09:45:31,362 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json HTTP/1.1" 307 0
2026-01-28 09:45:31,375 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json HTTP/1.1" 200 0
2026-01-28 09:45:31,615 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2 HTTP/1.1" 200 6885
2026-01-28 09:45:33,162 | DEBUG | httpcore.connection | connect_tcp.started host='raw.githubusercontent.com' port=443 local_address=None timeout=5 socket_options=None
2026-01-28 09:45:33,204 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000019492350C70>
2026-01-28 09:45:33,205 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000019492189C40> server_hostname='raw.githubusercontent.com' timeout=5
2026-01-28 09:45:33,240 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000019492350C40>
2026-01-28 09:45:33,240 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'GET']>
2026-01-28 09:45:33,241 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 09:45:33,241 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'GET']>
2026-01-28 09:45:33,241 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 09:45:33,241 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'GET']>
2026-01-28 09:45:33,282 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Connection', b'keep-alive'), (b'Content-Length', b'62476'), (b'Cache-Control', b'max-age=300'), (b'Content-Security-Policy', b"default-src 'none'; style-src 'unsafe-inline'; sandbox"), (b'Content-Type', b'text/plain; charset=utf-8'), (b'ETag', b'W/"f6dad8a5d9e28465b3d70ca31700cb6ef9bf2cd7c9d0fa0c0c5cdf23a1bb3382"'), (b'Strict-Transport-Security', b'max-age=31536000'), (b'X-Content-Type-Options', b'nosniff'), (b'X-Frame-Options', b'deny'), (b'X-XSS-Protection', b'1; mode=block'), (b'X-GitHub-Request-Id', b'2442:60E63:51359:95C7D:6978AD63'), (b'Content-Encoding', b'gzip'), (b'Accept-Ranges', b'bytes'), (b'Date', b'Wed, 28 Jan 2026 04:15:32 GMT'), (b'Via', b'1.1 varnish'), (b'X-Served-By', b'cache-bom-vanm7210073-BOM'), (b'X-Cache', b'HIT'), (b'X-Cache-Hits', b'33'), (b'X-Timer', b'S1769573733.638519,VS0,VE0'), (b'Vary', b'Authorization,Accept-Encoding'), (b'Access-Control-Allow-Origin', b'*'), (b'Cross-Origin-Resource-Policy', b'cross-origin'), (b'X-Fastly-Request-ID', b'1abefc3a2bccd8e2187726b4aaea63cd2e574237'), (b'Expires', b'Wed, 28 Jan 2026 04:20:32 GMT'), (b'Source-Age', b'255')])
2026-01-28 09:45:33,283 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'GET']>
2026-01-28 09:45:33,318 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 09:45:33,319 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 09:45:33,319 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 09:45:33,319 | DEBUG | httpcore.connection | close.started
2026-01-28 09:45:33,320 | DEBUG | httpcore.connection | close.complete
2026-01-28 09:45:34,022 | DEBUG | LiteLLM | [Non-Blocking] Unable to import GenericAPILogger - LiteLLM Enterprise Feature - No module named 'litellm_enterprise'
2026-01-28 09:45:34,479 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-28 09:45:34,479 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-28 09:45:34,479 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-28 09:45:34,487 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-28 09:45:34,487 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-28 09:45:34,488 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-28 09:45:34,488 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-28 09:45:34,488 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-28 09:45:34,489 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-28 09:45:34,489 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-28 09:45:34,490 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-28 09:45:34,490 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-28 09:45:34,535 | INFO | main | üöÄ FastAPI application starting
2026-01-28 09:46:13,247 | INFO | chat | üì© Chat request | user_id=3 | request_id=713d312f-762f-49c6-8755-fd32b32520f3
2026-01-28 09:46:13,261 | INFO | chat | üßë User message | where do i live?
2026-01-28 09:46:13,273 | INFO | chat | üÜï New session created | ef5f092b-2ec0-4839-921b-a27aef540c8d
2026-01-28 09:46:13,313 | INFO | chat | ü§ñ Stream started | session_id=ef5f092b-2ec0-4839-921b-a27aef540c8d | request_id=713d312f-762f-49c6-8755-fd32b32520f3
2026-01-28 09:46:13,316 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-28 09:46:13,317 | INFO | orchestrator | üßë USER_INPUT | where do i live?
2026-01-28 09:46:13,319 | INFO | orchestrator | üß© CONTEXT | user_id=3 | enable_memory=True | db=None
2026-01-28 09:46:13,321 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-28 09:46:13,322 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_7ed0a001bea44a2cb4c2d80d96276bb9
2026-01-28 09:46:13,323 | DEBUG | openai.agents | Setting current trace: trace_7ed0a001bea44a2cb4c2d80d96276bb9
2026-01-28 09:46:13,324 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000001949B529260> with id None
2026-01-28 09:46:13,326 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-28 09:46:13,340 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000001949B462AA0> with id None
2026-01-28 09:46:13,340 | DEBUG | openai.agents | Calling LLM
2026-01-28 09:46:13,341 | DEBUG | LiteLLM | 

2026-01-28 09:46:13,342 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 09:46:13,343 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n', 'role': 'system'}, {'role': 'user', 'content': 'where do i live?'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 09:46:13,344 | DEBUG | LiteLLM | 

2026-01-28 09:46:13,346 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 09:46:13,347 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 09:46:13,348 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 09:46:13,385 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:46:13,387 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 09:46:13,389 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n', 'role': 'system'}, {'role': 'user', 'content': 'where do i live?'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 09:46:13,390 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 09:46:13,392 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 09:46:13,393 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 09:46:13,394 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:13,983 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:13,985 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'where do i live?'}]}], 'system_instruction': {'parts': [{'text': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 09:46:15,475 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "SAFE"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 192,
    "candidatesTokenCount": 1,
    "totalTokenCount": 269,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 192
      }
    ],
    "thoughtsTokenCount": 76
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "jo15ae6nKJDz4-EPrde2yQQ"
}



2026-01-28 09:46:15,485 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 09:46:15,487 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 09:46:15,489 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 09:46:15,490 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 09:46:15,491 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:15,493 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:15,494 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:15,495 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:15,496 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:46:15,497 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:46:15,499 | DEBUG | LiteLLM | response_cost: 0.0002501
2026-01-28 09:46:15,500 | DEBUG | LiteLLM | response_cost: 0.0002501
2026-01-28 09:46:15,501 | DEBUG | openai.agents | Received model response
2026-01-28 09:46:15,506 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:46:15,536 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001949B58A1D0>>
2026-01-28 09:46:15,537 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:15,539 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:46:15,541 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 09:46:15,542 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 09:46:15,545 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:46:15,545 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 09:46:15,547 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 09:46:15,547 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:15,549 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:46:15,551 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 09:46:15,551 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:46:15,552 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:15,555 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:46:15,557 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:46:15,558 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:15,560 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:46:15,562 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 09:46:15,563 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0002501
2026-01-28 09:46:15,565 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:46:15,566 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:15,567 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:46:15,571 | DEBUG | openai.agents | Resetting current trace
2026-01-28 09:46:15,572 | ERROR | digital_human_sdk.app.intelligence.utils.json_utils | safe_json_loads failed
2026-01-28 09:46:15,574 | INFO | orchestrator | ‚úÖ Safety passed
2026-01-28 09:46:15,577 | INFO | orchestrator | üì• Fetching semantic memory (via MemoryService)
2026-01-28 09:46:15,791 | INFO | orchestrator | üß† MEMORY_RESULT | found=False | count=0
2026-01-28 09:46:15,793 | INFO | orchestrator | üß≠ Router agent called
2026-01-28 09:46:15,796 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_f35744170c95441fb571c6735f8d1272
2026-01-28 09:46:15,796 | DEBUG | openai.agents | Setting current trace: trace_f35744170c95441fb571c6735f8d1272
2026-01-28 09:46:15,797 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000001949B5E5710> with id None
2026-01-28 09:46:15,798 | DEBUG | openai.agents | Running agent RouterAgent (turn 1)
2026-01-28 09:46:15,800 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000001949B463A60> with id None
2026-01-28 09:46:15,801 | DEBUG | openai.agents | Calling LLM
2026-01-28 09:46:15,801 | DEBUG | LiteLLM | 

2026-01-28 09:46:15,802 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 09:46:15,803 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'where do i live?'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 09:46:15,805 | DEBUG | LiteLLM | 

2026-01-28 09:46:15,806 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 09:46:15,807 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 09:46:15,808 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 09:46:15,811 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:46:15,812 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 09:46:15,813 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'where do i live?'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 09:46:15,814 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 09:46:15,815 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 09:46:15,816 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 09:46:15,817 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:15,818 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:15,820 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'where do i live?'}]}], 'system_instruction': {'parts': [{'text': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 09:46:17,381 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\"use_memory\": true, \"use_tool\": false, \"tool_name\": \"none\", \"tool_arguments\": null, \"intent\": \"read\", \"memory_key\": null}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 490,
    "candidatesTokenCount": 42,
    "totalTokenCount": 647,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 490
      }
    ],
    "thoughtsTokenCount": 115
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "kI15ac_bINzL4-EP7uWG4AI"
}



2026-01-28 09:46:17,385 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 09:46:17,387 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 09:46:17,387 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 09:46:17,388 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:17,389 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 09:46:17,390 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:17,391 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:17,393 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:46:17,394 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:17,395 | DEBUG | LiteLLM | response_cost: 0.0005395
2026-01-28 09:46:17,397 | DEBUG | openai.agents | Received model response
2026-01-28 09:46:17,396 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:46:17,398 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001949B5CCEB0>>
2026-01-28 09:46:17,399 | DEBUG | LiteLLM | response_cost: 0.0005395
2026-01-28 09:46:17,400 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 09:46:17,402 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:46:17,403 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 09:46:17,404 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 09:46:17,404 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:17,406 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 09:46:17,408 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:46:17,409 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:46:17,411 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:46:17,412 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:17,413 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 09:46:17,414 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:17,415 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:46:17,417 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:46:17,418 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:46:17,420 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:17,421 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 09:46:17,421 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:46:17,422 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0005395
2026-01-28 09:46:17,425 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:46:17,427 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:17,428 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:46:17,430 | DEBUG | openai.agents | Resetting current trace
2026-01-28 09:46:17,431 | INFO | orchestrator | üß≠ ROUTER_DECISION | tool=False | memory=True | intent=read
2026-01-28 09:46:17,432 | INFO | orchestrator | üß† REASONING_INPUT_SNAPSHOT
{
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful AI assistant.\n \nYou may answer ANY general knowledge question.\n \nConversation summaries and memories are ONLY to provide helpful context\n(e.g. preferences, ongoing plans).\n \nIf a user asks something unrelated to the current topic,\nanswer it normally using general knowledge.\n \nDo NOT restrict yourself to previous topics unless the user explicitly asks."
    },
    {
      "role": "user",
      "content": "where do i live?"
    },
    {
      "role": "user",
      "content": "where do i live?"
    }
  ],
  "safety": {
    "safe": true,
    "message": null
  },
  "memory_action": {},
  "memory_data": [],
  "memory_found": false,
  "tool_context": {}
}
2026-01-28 09:46:17,436 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_16aa0dac05b640eeaca365cf2a19b438
2026-01-28 09:46:17,437 | DEBUG | openai.agents | Setting current trace: trace_16aa0dac05b640eeaca365cf2a19b438
2026-01-28 09:46:17,438 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000001949B4D5C10> with id None
2026-01-28 09:46:17,439 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000001949BDC21A0> with id None
2026-01-28 09:46:17,440 | DEBUG | openai.agents | Calling LLM
2026-01-28 09:46:17,441 | DEBUG | LiteLLM | 

2026-01-28 09:46:17,442 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 09:46:17,443 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "where do i live?"}, {"role": "user", "content": "where do i live?"}], "safety": {"safe": true, "message": null}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=True, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 09:46:17,444 | DEBUG | LiteLLM | 

2026-01-28 09:46:17,446 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 09:46:17,446 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 09:46:17,448 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 09:46:17,450 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:46:17,451 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 09:46:17,454 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "where do i live?"}, {"role": "user", "content": "where do i live?"}], "safety": {"safe": true, "message": null}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 09:46:17,455 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': True}
2026-01-28 09:46:17,456 | DEBUG | LiteLLM | Final returned optional params: {'stream': True}
2026-01-28 09:46:17,457 | DEBUG | LiteLLM | self.optional_params: {'stream': True}
2026-01-28 09:46:17,458 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:17,460 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:17,461 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?key=*****=sse \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "where do i live?"}, {"role": "user", "content": "where do i live?"}], "safety": {"safe": true, "message": null}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}]}], 'system_instruction': {'parts': [{'text': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.'}]}, 'generationConfig': {}}'
[0m

2026-01-28 09:46:17,464 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 09:46:17,470 | DEBUG | LiteLLM | messages in token_counter: None, text in token_counter: 
2026-01-28 09:46:17,473 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:17,475 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:17,475 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:46:17,478 | DEBUG | LiteLLM | response_cost: 0.0
2026-01-28 09:46:17,627 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-28 09:46:17,630 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-28 09:46:17,634 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-28 09:46:18,504 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 09:46:18,566 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001949C2E2800>
2026-01-28 09:46:18,567 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x00000194900B9BC0> server_hostname='api.openai.com' timeout=5.0
2026-01-28 09:46:18,584 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001949C2E27D0>
2026-01-28 09:46:18,584 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 09:46:18,585 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 09:46:18,586 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 09:46:18,587 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 09:46:18,587 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 09:46:19,186 | DEBUG | LiteLLM | RAW RESPONSE:
first stream response received


2026-01-28 09:46:19,187 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': 'I am an AI and do not have access to your personal information, including where you live.'}], 'role': 'model'}, 'finishReason': 'STOP', 'index': 0}], 'usageMetadata': {'promptTokenCount': 281, 'candidatesTokenCount': 19, 'totalTokenCount': 370, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 281}], 'thoughtsTokenCount': 70}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'ko15aa6jAq-IjuMPoaTkgQI'}
2026-01-28 09:46:19,208 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='ko15aa6jAq-IjuMPoaTkgQI', created=1769573779, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='I am an AI and do not have access to your personal information, including where you live.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=89, prompt_tokens=281, total_tokens=370, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=70, rejected_prediction_tokens=None, text_tokens=19, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=281, image_tokens=None)))
2026-01-28 09:46:19,213 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content='I am an AI and do not have access to your personal information, including where you live.', role='assistant', function_call=None, tool_calls=None, audio=None)
2026-01-28 09:46:19,214 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='ko15aa6jAq-IjuMPoaTkgQI', created=1769573779, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='I am an AI and do not have access to your personal information, including where you live.', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=89, prompt_tokens=281, total_tokens=370, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=70, rejected_prediction_tokens=None, text_tokens=19, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=281, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-28 09:46:19,280 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=False
2026-01-28 09:46:19,322 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 09:46:19,323 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=False
2026-01-28 09:46:19,324 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 09:46:19,325 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 09:46:19,326 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:19,328 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 09:46:19,329 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:19,330 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:19,331 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:46:19,331 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:19,333 | DEBUG | LiteLLM | response_cost: 0.00030680000000000003
2026-01-28 09:46:19,334 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:46:19,336 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:46:19,336 | DEBUG | LiteLLM | response_cost: 0.00030680000000000003
2026-01-28 09:46:19,338 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:19,339 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.00030680000000000003
2026-01-28 09:46:19,340 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:46:19,341 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:46:19,343 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:19,345 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:46:19,347 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 09:46:19,348 | DEBUG | openai.agents | Resetting current trace
2026-01-28 09:46:19,369 | INFO | session_summary | üß† SUMMARY_START | session_id=ef5f092b-2ec0-4839-921b-a27aef540c8d
2026-01-28 09:46:19,384 | INFO | session_summary | üìä Unsummarized messages = 2
2026-01-28 09:46:19,386 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-28 09:46:19,387 | INFO | chat | ‚úÖ Stream complete | tokens=1 | time=6.14s
2026-01-28 09:46:19,445 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 04:16:18 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_11586bad445bf7801b8878a4a8d5e51e'), (b'openai-processing-ms', b'534'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'538'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=J2FAUWLsT1AcWP293RDdjCKA3HifIZ5NtZirtJ4Do_c-1769573778-1.0.1.1-5Px95eNDUe4x1FJcqsNAfmh7YOc8Ah4bNXWrzMcyUdkjvHrgB.yJZGOiQPV9QItxoMAoMcj_naUdd7reDAaRkt2dYOfJGNbKZUnRQ0BgWPM; path=/; expires=Wed, 28-Jan-26 04:46:18 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=x.gcYV5DWa0m2nEW_I1yueQon1iQxxMZHdDEkVCh5js-1769573778796-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4dac702f7190d6-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 09:46:19,448 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 09:46:19,449 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 09:46:19,450 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 09:46:19,450 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 09:46:19,451 | DEBUG | openai.agents | Exported 7 items
2026-01-28 09:46:19,454 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 09:46:19,455 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 09:46:19,456 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 09:46:19,457 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 09:46:19,457 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 09:46:20,394 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 04:16:19 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_6fc494cae3b02de0d607b39aeaae2e43'), (b'openai-processing-ms', b'516'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'519'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4dac759ef290d6-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 09:46:20,395 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 09:46:20,396 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 09:46:20,396 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 09:46:20,397 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 09:46:20,397 | DEBUG | openai.agents | Exported 2 items
2026-01-28 09:46:43,134 | INFO | chat | üì© Chat request | user_id=3 | request_id=be94bb71-ec9e-461c-8cc3-6fceb67a37a0
2026-01-28 09:46:43,135 | INFO | chat | üßë User message | i lives in fatehabad
2026-01-28 09:46:43,156 | INFO | chat | ü§ñ Stream started | session_id=ef5f092b-2ec0-4839-921b-a27aef540c8d | request_id=be94bb71-ec9e-461c-8cc3-6fceb67a37a0
2026-01-28 09:46:43,158 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-28 09:46:43,161 | INFO | orchestrator | üßë USER_INPUT | i lives in fatehabad
2026-01-28 09:46:43,164 | INFO | orchestrator | üß© CONTEXT | user_id=3 | enable_memory=True | db=None
2026-01-28 09:46:43,165 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-28 09:46:43,167 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_dee6dbd0c66e440ca1fcfbbc88d5dd9d
2026-01-28 09:46:43,168 | DEBUG | openai.agents | Setting current trace: trace_dee6dbd0c66e440ca1fcfbbc88d5dd9d
2026-01-28 09:46:43,169 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000001949BA5C9F0> with id None
2026-01-28 09:46:43,170 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-28 09:46:43,172 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000001949B4CAAA0> with id None
2026-01-28 09:46:43,172 | DEBUG | openai.agents | Calling LLM
2026-01-28 09:46:43,173 | DEBUG | LiteLLM | 

2026-01-28 09:46:43,174 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 09:46:43,175 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n', 'role': 'system'}, {'role': 'user', 'content': 'i lives in fatehabad'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 09:46:43,176 | DEBUG | LiteLLM | 

2026-01-28 09:46:43,178 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 09:46:43,179 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 09:46:43,181 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 09:46:43,184 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:46:43,185 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 09:46:43,187 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n', 'role': 'system'}, {'role': 'user', 'content': 'i lives in fatehabad'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 09:46:43,189 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 09:46:43,190 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 09:46:43,191 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 09:46:43,192 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:43,194 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:43,196 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'i lives in fatehabad'}]}], 'system_instruction': {'parts': [{'text': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 09:46:44,349 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "SAFE"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 193,
    "candidatesTokenCount": 1,
    "totalTokenCount": 224,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 193
      }
    ],
    "thoughtsTokenCount": 30
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "q415acHNIdasjuMP2quf8QM"
}



2026-01-28 09:46:44,351 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 09:46:44,353 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 09:46:44,353 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 09:46:44,355 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:44,356 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 09:46:44,357 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:44,359 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:44,369 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:46:44,370 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:44,371 | DEBUG | LiteLLM | response_cost: 0.0001354
2026-01-28 09:46:44,373 | DEBUG | openai.agents | Received model response
2026-01-28 09:46:44,372 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:46:44,375 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001949BA6C280>>
2026-01-28 09:46:44,376 | DEBUG | LiteLLM | response_cost: 0.0001354
2026-01-28 09:46:44,377 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 09:46:44,379 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:46:44,380 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 09:46:44,381 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 09:46:44,382 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:44,383 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:46:44,385 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 09:46:44,386 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:46:44,386 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:44,388 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:46:44,389 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 09:46:44,390 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:46:44,391 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:44,393 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:46:44,396 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:46:44,397 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:44,398 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 09:46:44,399 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:46:44,400 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0001354
2026-01-28 09:46:44,403 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:46:44,404 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:44,406 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:46:44,409 | DEBUG | openai.agents | Resetting current trace
2026-01-28 09:46:44,410 | ERROR | digital_human_sdk.app.intelligence.utils.json_utils | safe_json_loads failed
2026-01-28 09:46:44,413 | INFO | orchestrator | ‚úÖ Safety passed
2026-01-28 09:46:44,415 | INFO | orchestrator | üì• Fetching semantic memory (via MemoryService)
2026-01-28 09:46:44,597 | INFO | orchestrator | üß† MEMORY_RESULT | found=False | count=0
2026-01-28 09:46:44,600 | INFO | orchestrator | üß≠ Router agent called
2026-01-28 09:46:44,603 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_a2029b93d7fb44f88ddee4741e022f17
2026-01-28 09:46:44,603 | DEBUG | openai.agents | Setting current trace: trace_a2029b93d7fb44f88ddee4741e022f17
2026-01-28 09:46:44,605 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000001949B5E7BF0> with id None
2026-01-28 09:46:44,605 | DEBUG | openai.agents | Running agent RouterAgent (turn 1)
2026-01-28 09:46:44,607 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000001949BDC2140> with id None
2026-01-28 09:46:44,608 | DEBUG | openai.agents | Calling LLM
2026-01-28 09:46:44,609 | DEBUG | LiteLLM | 

2026-01-28 09:46:44,611 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 09:46:44,612 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'i lives in fatehabad'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 09:46:44,614 | DEBUG | LiteLLM | 

2026-01-28 09:46:44,616 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 09:46:44,619 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 09:46:44,631 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 09:46:44,639 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:46:44,644 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 09:46:44,651 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'i lives in fatehabad'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 09:46:44,655 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 09:46:44,656 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 09:46:44,656 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 09:46:44,657 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:44,658 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:44,659 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'i lives in fatehabad'}]}], 'system_instruction': {'parts': [{'text': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 09:46:46,073 | DEBUG | httpcore.connection | close.started
2026-01-28 09:46:46,073 | DEBUG | httpcore.connection | close.complete
2026-01-28 09:46:46,074 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 09:46:46,085 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001949B5CE5C0>
2026-01-28 09:46:46,085 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x00000194900B9BC0> server_hostname='api.openai.com' timeout=5.0
2026-01-28 09:46:46,107 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001949B5CFBB0>
2026-01-28 09:46:46,108 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 09:46:46,108 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 09:46:46,108 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 09:46:46,109 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 09:46:46,109 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 09:46:46,562 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\n  \"use_memory\": true,\n  \"use_tool\": false,\n  \"tool_name\": \"none\",\n  \"tool_arguments\": null,\n  \"intent\": \"write\",\n  \"memory_key\": \"residence\"\n}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 491,
    "candidatesTokenCount": 59,
    "totalTokenCount": 629,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 491
      }
    ],
    "thoughtsTokenCount": 79
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "rY15afGPLt_4juMP5O6toQI"
}



2026-01-28 09:46:46,563 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 09:46:46,564 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 09:46:46,564 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 09:46:46,565 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:46,565 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 09:46:46,565 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:46,566 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:46,566 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:46:46,567 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:46,567 | DEBUG | LiteLLM | response_cost: 0.0004923
2026-01-28 09:46:46,568 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:46:46,568 | DEBUG | openai.agents | Received model response
2026-01-28 09:46:46,568 | DEBUG | LiteLLM | response_cost: 0.0004923
2026-01-28 09:46:46,569 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001949B431C90>>
2026-01-28 09:46:46,570 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:46:46,570 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 09:46:46,571 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:46,571 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 09:46:46,571 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 09:46:46,572 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:46:46,572 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 09:46:46,573 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:46:46,574 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 09:46:46,574 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:46:46,575 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:46,576 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:46:46,576 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:46,576 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:46:46,577 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:46,577 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:46:46,578 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:46:46,578 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 09:46:46,579 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0004923
2026-01-28 09:46:46,580 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:46:46,581 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:46,581 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:46:46,582 | DEBUG | openai.agents | Resetting current trace
2026-01-28 09:46:46,582 | INFO | orchestrator | üß≠ ROUTER_DECISION | tool=False | memory=True | intent=write
2026-01-28 09:46:46,583 | INFO | orchestrator | üß† Memory agent called (WRITE)
2026-01-28 09:46:46,584 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_245e07dafe0148fa8644e814bcb251e0
2026-01-28 09:46:46,584 | DEBUG | openai.agents | Setting current trace: trace_245e07dafe0148fa8644e814bcb251e0
2026-01-28 09:46:46,584 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000001949C2FCB30> with id None
2026-01-28 09:46:46,585 | DEBUG | openai.agents | Running agent Memory Agent (turn 1)
2026-01-28 09:46:46,585 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000001949B4CAFE0> with id None
2026-01-28 09:46:46,586 | DEBUG | openai.agents | Calling LLM
2026-01-28 09:46:46,586 | DEBUG | LiteLLM | 

2026-01-28 09:46:46,587 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 09:46:46,587 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a Memory Extraction Agent.\n\nYour job is to extract ONLY long-term, personal, user-specific information\nfrom the user\'s message and decide the appropriate memory action.\n\nYou will be called ONLY when memory handling is required.\n\n========================\nWHAT TO EXTRACT\n========================\n- Stable personal facts (name, education, location, job)\n- Long-term preferences (food, language, habits)\n- Information the user would reasonably expect to be remembered\n- Facts that should be retrievable across sessions via semantic search\n\nStore memory as FULL, NATURAL-LANGUAGE TEXT.\nDO NOT create structured keys.\nDO NOT summarize aggressively.\nPreserve meaning exactly as stated.\n\n========================\nWHAT TO IGNORE\n========================\n- Questions\n- Small talk\n- Temporary states (mood, weather, current task)\n- Opinions about content\n- One-time or session-only context\n\n========================\nACTIONS (STRICT)\n========================\nUse exactly ONE action:\n\n- "save"\n  ‚Üí New long-term personal information stated for the first time\n\n- "update"\n  ‚Üí Existing long-term information is explicitly changed,\n    corrected, or overridden\n    (keywords: "now", "instead", "changed", "earlier", "previously")\n\n- "delete"\n  ‚Üí User explicitly asks to forget, remove, or delete information\n\nNEVER guess.\nNEVER infer deletion.\nNEVER revive deleted information.\nDeleted memory must be treated as non-existent.\n\n========================\nIMPORTANT RULES\n========================\n- Store memory as ONE complete sentence\n- Do NOT split into multiple memories\n- Do NOT invent information\n- Do NOT rely on database state\n- Backend will validate save vs update\n- If unsure, return "none"\n\n========================\nOUTPUT RULES (CRITICAL)\n========================\n- Return VALID JSON only\n- Do NOT explain anything\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Do NOT return multiple objects\n\n========================\nOUTPUT FORMAT (JSON ONLY)\n========================\n{\n  "action": "save | update | delete | none",\n  "memory_text": "full natural language memory text",\n  "confidence": 0.0\n}\n\n========================\nCONFIDENCE GUIDELINES\n========================\n- 0.9‚Äì1.0 ‚Üí Explicit personal fact (e.g., education, name, job)\n- 0.8‚Äì0.9 ‚Üí Clear long-term preference or habit\n- Below 0.7 ‚Üí Weak or ambiguous (avoid storing)\n\n========================\nNO MEMORY CASE\n========================\nIf no valid long-term memory is present, return:\n{\n  "action": "none",\n  "memory_text": "",\n  "confidence": 0.0\n}\n', 'role': 'system'}, {'role': 'user', 'content': 'i lives in fatehabad'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 09:46:46,587 | DEBUG | LiteLLM | 

2026-01-28 09:46:46,588 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 09:46:46,588 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 09:46:46,588 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 09:46:46,589 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:46:46,591 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 09:46:46,591 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Memory Extraction Agent.\n\nYour job is to extract ONLY long-term, personal, user-specific information\nfrom the user\'s message and decide the appropriate memory action.\n\nYou will be called ONLY when memory handling is required.\n\n========================\nWHAT TO EXTRACT\n========================\n- Stable personal facts (name, education, location, job)\n- Long-term preferences (food, language, habits)\n- Information the user would reasonably expect to be remembered\n- Facts that should be retrievable across sessions via semantic search\n\nStore memory as FULL, NATURAL-LANGUAGE TEXT.\nDO NOT create structured keys.\nDO NOT summarize aggressively.\nPreserve meaning exactly as stated.\n\n========================\nWHAT TO IGNORE\n========================\n- Questions\n- Small talk\n- Temporary states (mood, weather, current task)\n- Opinions about content\n- One-time or session-only context\n\n========================\nACTIONS (STRICT)\n========================\nUse exactly ONE action:\n\n- "save"\n  ‚Üí New long-term personal information stated for the first time\n\n- "update"\n  ‚Üí Existing long-term information is explicitly changed,\n    corrected, or overridden\n    (keywords: "now", "instead", "changed", "earlier", "previously")\n\n- "delete"\n  ‚Üí User explicitly asks to forget, remove, or delete information\n\nNEVER guess.\nNEVER infer deletion.\nNEVER revive deleted information.\nDeleted memory must be treated as non-existent.\n\n========================\nIMPORTANT RULES\n========================\n- Store memory as ONE complete sentence\n- Do NOT split into multiple memories\n- Do NOT invent information\n- Do NOT rely on database state\n- Backend will validate save vs update\n- If unsure, return "none"\n\n========================\nOUTPUT RULES (CRITICAL)\n========================\n- Return VALID JSON only\n- Do NOT explain anything\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Do NOT return multiple objects\n\n========================\nOUTPUT FORMAT (JSON ONLY)\n========================\n{\n  "action": "save | update | delete | none",\n  "memory_text": "full natural language memory text",\n  "confidence": 0.0\n}\n\n========================\nCONFIDENCE GUIDELINES\n========================\n- 0.9‚Äì1.0 ‚Üí Explicit personal fact (e.g., education, name, job)\n- 0.8‚Äì0.9 ‚Üí Clear long-term preference or habit\n- Below 0.7 ‚Üí Weak or ambiguous (avoid storing)\n\n========================\nNO MEMORY CASE\n========================\nIf no valid long-term memory is present, return:\n{\n  "action": "none",\n  "memory_text": "",\n  "confidence": 0.0\n}\n', 'role': 'system'}, {'role': 'user', 'content': 'i lives in fatehabad'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 09:46:46,592 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 09:46:46,592 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 09:46:46,593 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 09:46:46,593 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:46,594 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:46,594 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'i lives in fatehabad'}]}], 'system_instruction': {'parts': [{'text': 'You are a Memory Extraction Agent.\n\nYour job is to extract ONLY long-term, personal, user-specific information\nfrom the user\'s message and decide the appropriate memory action.\n\nYou will be called ONLY when memory handling is required.\n\n========================\nWHAT TO EXTRACT\n========================\n- Stable personal facts (name, education, location, job)\n- Long-term preferences (food, language, habits)\n- Information the user would reasonably expect to be remembered\n- Facts that should be retrievable across sessions via semantic search\n\nStore memory as FULL, NATURAL-LANGUAGE TEXT.\nDO NOT create structured keys.\nDO NOT summarize aggressively.\nPreserve meaning exactly as stated.\n\n========================\nWHAT TO IGNORE\n========================\n- Questions\n- Small talk\n- Temporary states (mood, weather, current task)\n- Opinions about content\n- One-time or session-only context\n\n========================\nACTIONS (STRICT)\n========================\nUse exactly ONE action:\n\n- "save"\n  ‚Üí New long-term personal information stated for the first time\n\n- "update"\n  ‚Üí Existing long-term information is explicitly changed,\n    corrected, or overridden\n    (keywords: "now", "instead", "changed", "earlier", "previously")\n\n- "delete"\n  ‚Üí User explicitly asks to forget, remove, or delete information\n\nNEVER guess.\nNEVER infer deletion.\nNEVER revive deleted information.\nDeleted memory must be treated as non-existent.\n\n========================\nIMPORTANT RULES\n========================\n- Store memory as ONE complete sentence\n- Do NOT split into multiple memories\n- Do NOT invent information\n- Do NOT rely on database state\n- Backend will validate save vs update\n- If unsure, return "none"\n\n========================\nOUTPUT RULES (CRITICAL)\n========================\n- Return VALID JSON only\n- Do NOT explain anything\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Do NOT return multiple objects\n\n========================\nOUTPUT FORMAT (JSON ONLY)\n========================\n{\n  "action": "save | update | delete | none",\n  "memory_text": "full natural language memory text",\n  "confidence": 0.0\n}\n\n========================\nCONFIDENCE GUIDELINES\n========================\n- 0.9‚Äì1.0 ‚Üí Explicit personal fact (e.g., education, name, job)\n- 0.8‚Äì0.9 ‚Üí Clear long-term preference or habit\n- Below 0.7 ‚Üí Weak or ambiguous (avoid storing)\n\n========================\nNO MEMORY CASE\n========================\nIf no valid long-term memory is present, return:\n{\n  "action": "none",\n  "memory_text": "",\n  "confidence": 0.0\n}\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 09:46:47,159 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 04:16:46 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_db5700a983fb8f354360388db2fdeb30'), (b'openai-processing-ms', b'102'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'105'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4dad1c2e7359f3-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 09:46:47,160 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 09:46:47,160 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 09:46:47,160 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 09:46:47,161 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 09:46:47,162 | DEBUG | openai.agents | Exported 4 items
2026-01-28 09:46:47,163 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 09:46:47,164 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 09:46:47,164 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 09:46:47,164 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 09:46:47,165 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 09:46:47,595 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 04:16:46 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_ee981d1ea20c489b69a856cd794e5990'), (b'openai-processing-ms', b'109'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'113'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4dad22c91659f3-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 09:46:47,596 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 09:46:47,596 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 09:46:47,596 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 09:46:47,596 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 09:46:47,596 | DEBUG | openai.agents | Exported 3 items
2026-01-28 09:46:48,156 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "```json\n{\n  \"action\": \"save\",\n  \"memory_text\": \"I live in Fatehabad.\",\n  \"confidence\": 0.95\n}\n```"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 606,
    "candidatesTokenCount": 41,
    "totalTokenCount": 737,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 606
      }
    ],
    "thoughtsTokenCount": 90
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "r415afXJFMPL4-EPsvmKkQc"
}



2026-01-28 09:46:48,159 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 09:46:48,160 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 09:46:48,161 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 09:46:48,162 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:48,163 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 09:46:48,164 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:48,165 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:48,166 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:46:48,167 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:48,168 | DEBUG | LiteLLM | response_cost: 0.0005093000000000001
2026-01-28 09:46:48,171 | DEBUG | openai.agents | Received model response
2026-01-28 09:46:48,170 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:46:48,173 | DEBUG | LiteLLM | response_cost: 0.0005093000000000001
2026-01-28 09:46:48,174 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001949B5CCA60>>
2026-01-28 09:46:48,176 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:46:48,178 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 09:46:48,178 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:48,181 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 09:46:48,180 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 09:46:48,182 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:46:48,183 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 09:46:48,184 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:46:48,185 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 09:46:48,188 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:46:48,189 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:48,190 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:46:48,191 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:48,192 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:46:48,194 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:48,195 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:46:48,197 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:46:48,198 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 09:46:48,200 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0005093000000000001
2026-01-28 09:46:48,202 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:46:48,204 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:48,204 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:46:48,206 | DEBUG | openai.agents | Resetting current trace
2026-01-28 09:46:48,208 | INFO | orchestrator | üß† MEMORY_DECISION | {
  "action": "save",
  "memory_text": "I live in Fatehabad.",
  "confidence": 0.95
}
2026-01-28 09:46:48,210 | INFO | chat | üß† MEMORY_EVENT_RECEIVED | {'action': 'save', 'memory_text': 'I live in Fatehabad.', 'confidence': 0.95}
2026-01-28 09:46:48,212 | INFO | memory_action_executor | üß† APPLY_MEMORY_ACTION | user_id=3 | action={'action': 'save', 'memory_text': 'I live in Fatehabad.', 'confidence': 0.95}
2026-01-28 09:46:48,213 | INFO | memory_action_executor | üß† Memory Parsed | type=save | content=I live in Fatehabad. | confidence=0.95
2026-01-28 09:46:48,215 | INFO | memory_action_executor | üíæ Writing memory to DB
2026-01-28 09:46:48,281 | INFO | memory_action_executor | ‚úÖ DB COMMIT SUCCESS
2026-01-28 09:46:48,283 | INFO | orchestrator | üß† REASONING_INPUT_SNAPSHOT
{
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful AI assistant.\n \nYou may answer ANY general knowledge question.\n \nConversation summaries and memories are ONLY to provide helpful context\n(e.g. preferences, ongoing plans).\n \nIf a user asks something unrelated to the current topic,\nanswer it normally using general knowledge.\n \nDo NOT restrict yourself to previous topics unless the user explicitly asks."
    },
    {
      "role": "user",
      "content": "where do i live?"
    },
    {
      "role": "assistant",
      "content": "I am an AI and do not have access to your personal information, including where you live."
    },
    {
      "role": "user",
      "content": "i lives in fatehabad"
    },
    {
      "role": "user",
      "content": "i lives in fatehabad"
    }
  ],
  "safety": {
    "safe": true,
    "message": null
  },
  "memory_action": {},
  "memory_data": [],
  "memory_found": false,
  "tool_context": {}
}
2026-01-28 09:46:48,285 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_5aff157ddfe8491abe8da6057b6f38dd
2026-01-28 09:46:48,286 | DEBUG | openai.agents | Setting current trace: trace_5aff157ddfe8491abe8da6057b6f38dd
2026-01-28 09:46:48,287 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000001949B52B740> with id None
2026-01-28 09:46:48,289 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000001949B4FE200> with id None
2026-01-28 09:46:48,290 | DEBUG | openai.agents | Calling LLM
2026-01-28 09:46:48,291 | DEBUG | LiteLLM | 

2026-01-28 09:46:48,292 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 09:46:48,293 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "where do i live?"}, {"role": "assistant", "content": "I am an AI and do not have access to your personal information, including where you live."}, {"role": "user", "content": "i lives in fatehabad"}, {"role": "user", "content": "i lives in fatehabad"}], "safety": {"safe": true, "message": null}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=True, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 09:46:48,294 | DEBUG | LiteLLM | 

2026-01-28 09:46:48,295 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 09:46:48,296 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 09:46:48,297 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 09:46:48,300 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:46:48,301 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 09:46:48,303 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "where do i live?"}, {"role": "assistant", "content": "I am an AI and do not have access to your personal information, including where you live."}, {"role": "user", "content": "i lives in fatehabad"}, {"role": "user", "content": "i lives in fatehabad"}], "safety": {"safe": true, "message": null}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 09:46:48,305 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': True}
2026-01-28 09:46:48,308 | DEBUG | LiteLLM | Final returned optional params: {'stream': True}
2026-01-28 09:46:48,309 | DEBUG | LiteLLM | self.optional_params: {'stream': True}
2026-01-28 09:46:48,310 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:48,312 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:48,313 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?key=*****=sse \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "where do i live?"}, {"role": "assistant", "content": "I am an AI and do not have access to your personal information, including where you live."}, {"role": "user", "content": "i lives in fatehabad"}, {"role": "user", "content": "i lives in fatehabad"}], "safety": {"safe": true, "message": null}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}]}], 'system_instruction': {'parts': [{'text': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.'}]}, 'generationConfig': {}}'
[0m

2026-01-28 09:46:48,316 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 09:46:48,317 | DEBUG | LiteLLM | messages in token_counter: None, text in token_counter: 
2026-01-28 09:46:48,318 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:48,319 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:48,319 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:46:48,321 | DEBUG | LiteLLM | response_cost: 0.0
2026-01-28 09:46:49,746 | DEBUG | LiteLLM | RAW RESPONSE:
first stream response received


2026-01-28 09:46:49,748 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': 'Thank you for letting me know.'}], 'role': 'model'}, 'finishReason': 'STOP', 'index': 0}], 'usageMetadata': {'promptTokenCount': 329, 'candidatesTokenCount': 7, 'totalTokenCount': 383, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 329}], 'thoughtsTokenCount': 47}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'sI15abXAKI3_g8UPweG9oQU'}
2026-01-28 09:46:49,750 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='sI15abXAKI3_g8UPweG9oQU', created=1769573809, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='Thank you for letting me know.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=54, prompt_tokens=329, total_tokens=383, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=47, rejected_prediction_tokens=None, text_tokens=7, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=329, image_tokens=None)))
2026-01-28 09:46:49,753 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content='Thank you for letting me know.', role='assistant', function_call=None, tool_calls=None, audio=None)
2026-01-28 09:46:49,755 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='sI15abXAKI3_g8UPweG9oQU', created=1769573809, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Thank you for letting me know.', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=54, prompt_tokens=329, total_tokens=383, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=47, rejected_prediction_tokens=None, text_tokens=7, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=329, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-28 09:46:49,761 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=False
2026-01-28 09:46:49,762 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=False
2026-01-28 09:46:49,764 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 09:46:49,764 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 09:46:49,764 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 09:46:49,764 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 09:46:49,766 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:49,767 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:49,768 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:49,768 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:49,770 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:46:49,771 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:46:49,772 | DEBUG | LiteLLM | response_cost: 0.0002337
2026-01-28 09:46:49,773 | DEBUG | LiteLLM | response_cost: 0.0002337
2026-01-28 09:46:49,774 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:46:49,775 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0002337
2026-01-28 09:46:49,776 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:49,778 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:46:49,778 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:46:49,780 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:46:49,782 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:46:49,785 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 09:46:49,786 | DEBUG | openai.agents | Resetting current trace
2026-01-28 09:46:49,792 | INFO | session_summary | üß† SUMMARY_START | session_id=ef5f092b-2ec0-4839-921b-a27aef540c8d
2026-01-28 09:46:49,798 | INFO | session_summary | üìä Unsummarized messages = 4
2026-01-28 09:46:49,801 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-28 09:46:49,802 | INFO | chat | ‚úÖ Stream complete | tokens=1 | time=6.67s
2026-01-28 09:46:52,726 | DEBUG | httpcore.connection | close.started
2026-01-28 09:46:52,727 | DEBUG | httpcore.connection | close.complete
2026-01-28 09:46:52,727 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 09:46:52,758 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001949B3E4610>
2026-01-28 09:46:52,758 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x00000194900B9BC0> server_hostname='api.openai.com' timeout=5.0
2026-01-28 09:46:52,768 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001949B3E52D0>
2026-01-28 09:46:52,769 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 09:46:52,769 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 09:46:52,769 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 09:46:52,770 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 09:46:52,770 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 09:46:54,313 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 04:16:53 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_33353cfe0ce7bf841365b960ab880474'), (b'openai-processing-ms', b'646'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'649'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4dad45c9665994-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 09:46:54,313 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 09:46:54,314 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 09:46:54,314 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 09:46:54,314 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 09:46:54,314 | DEBUG | openai.agents | Exported 5 items
2026-01-28 09:47:55,079 | INFO | chat | üì© Chat request | user_id=3 | request_id=bc76d17d-f2c5-46fb-9e65-f605d2f74d86
2026-01-28 09:47:55,081 | INFO | chat | üßë User message | where do i live?
2026-01-28 09:47:55,102 | INFO | chat | ü§ñ Stream started | session_id=18f96f31-5bb1-439d-b068-9932c7c45f32 | request_id=bc76d17d-f2c5-46fb-9e65-f605d2f74d86
2026-01-28 09:47:55,102 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-28 09:47:55,104 | INFO | orchestrator | üßë USER_INPUT | where do i live?
2026-01-28 09:47:55,106 | INFO | orchestrator | üß© CONTEXT | user_id=3 | enable_memory=True | db=None
2026-01-28 09:47:55,107 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-28 09:47:55,111 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_e650c269579c4619a3c72fbe084268bc
2026-01-28 09:47:55,111 | DEBUG | openai.agents | Setting current trace: trace_e650c269579c4619a3c72fbe084268bc
2026-01-28 09:47:55,112 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000001949B4F9670> with id None
2026-01-28 09:47:55,112 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-28 09:47:55,114 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000001949BA94FA0> with id None
2026-01-28 09:47:55,114 | DEBUG | openai.agents | Calling LLM
2026-01-28 09:47:55,114 | DEBUG | LiteLLM | 

2026-01-28 09:47:55,115 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 09:47:55,115 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n', 'role': 'system'}, {'role': 'user', 'content': 'where do i live?'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 09:47:55,116 | DEBUG | LiteLLM | 

2026-01-28 09:47:55,117 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 09:47:55,119 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 09:47:55,120 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 09:47:55,123 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:47:55,124 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 09:47:55,127 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n', 'role': 'system'}, {'role': 'user', 'content': 'where do i live?'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 09:47:55,128 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 09:47:55,129 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 09:47:55,129 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 09:47:55,130 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:47:55,132 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:47:55,133 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'where do i live?'}]}], 'system_instruction': {'parts': [{'text': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 09:47:55,977 | DEBUG | httpcore.connection | close.started
2026-01-28 09:47:55,978 | DEBUG | httpcore.connection | close.complete
2026-01-28 09:47:55,979 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 09:47:56,022 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001949BA6FB50>
2026-01-28 09:47:56,023 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x00000194900B9BC0> server_hostname='api.openai.com' timeout=5.0
2026-01-28 09:47:56,040 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001949BA6CA60>
2026-01-28 09:47:56,041 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 09:47:56,042 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 09:47:56,043 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 09:47:56,043 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 09:47:56,044 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 09:47:56,359 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "SAFE"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 192,
    "candidatesTokenCount": 1,
    "totalTokenCount": 255,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 192
      }
    ],
    "thoughtsTokenCount": 62
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "8415aeWzIKe84-EPj8ePkQI"
}



2026-01-28 09:47:56,363 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 09:47:56,367 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 09:47:56,367 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 09:47:56,369 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:47:56,370 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 09:47:56,371 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:47:56,372 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:47:56,372 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:47:56,373 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:47:56,374 | DEBUG | LiteLLM | response_cost: 0.0002151
2026-01-28 09:47:56,377 | DEBUG | openai.agents | Received model response
2026-01-28 09:47:56,376 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:47:56,378 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001949BA6FB80>>
2026-01-28 09:47:56,379 | DEBUG | LiteLLM | response_cost: 0.0002151
2026-01-28 09:47:56,380 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 09:47:56,383 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:47:56,384 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 09:47:56,384 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 09:47:56,385 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:47:56,386 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 09:47:56,388 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:47:56,389 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:47:56,393 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:47:56,394 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:47:56,395 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 09:47:56,396 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:47:56,397 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:47:56,399 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:47:56,400 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:47:56,404 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:47:56,406 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 09:47:56,407 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:47:56,408 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0002151
2026-01-28 09:47:56,410 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:47:56,421 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:47:56,422 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:47:56,425 | DEBUG | openai.agents | Resetting current trace
2026-01-28 09:47:56,425 | ERROR | digital_human_sdk.app.intelligence.utils.json_utils | safe_json_loads failed
2026-01-28 09:47:56,428 | INFO | orchestrator | ‚úÖ Safety passed
2026-01-28 09:47:56,429 | INFO | orchestrator | üì• Fetching semantic memory (via MemoryService)
2026-01-28 09:47:56,488 | INFO | orchestrator | üß† MEMORY_RESULT | found=True | count=1
2026-01-28 09:47:56,491 | INFO | orchestrator | üß≠ Router agent called
2026-01-28 09:47:56,493 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_d1e57b2734904771a1da150d386a3b70
2026-01-28 09:47:56,494 | DEBUG | openai.agents | Setting current trace: trace_d1e57b2734904771a1da150d386a3b70
2026-01-28 09:47:56,495 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000001949B5E7BF0> with id None
2026-01-28 09:47:56,495 | DEBUG | openai.agents | Running agent RouterAgent (turn 1)
2026-01-28 09:47:56,497 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000001949BDC0E80> with id None
2026-01-28 09:47:56,498 | DEBUG | openai.agents | Calling LLM
2026-01-28 09:47:56,499 | DEBUG | LiteLLM | 

2026-01-28 09:47:56,501 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 09:47:56,502 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'where do i live?'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 09:47:56,503 | DEBUG | LiteLLM | 

2026-01-28 09:47:56,504 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 09:47:56,505 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 09:47:56,506 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 09:47:56,509 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:47:56,510 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 09:47:56,512 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'where do i live?'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 09:47:56,513 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 09:47:56,513 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 09:47:56,515 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 09:47:56,515 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:47:56,518 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:47:56,520 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'where do i live?'}]}], 'system_instruction': {'parts': [{'text': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 09:47:56,650 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 04:17:56 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_9e0603eb8de11eb77b958a4fcff89823'), (b'openai-processing-ms', b'121'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'124'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4daed14d795a1d-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 09:47:56,652 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 09:47:56,653 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 09:47:56,653 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 09:47:56,654 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 09:47:56,655 | DEBUG | openai.agents | Exported 1 items
2026-01-28 09:47:56,658 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 09:47:56,663 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 09:47:56,664 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 09:47:56,666 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 09:47:56,670 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 09:47:57,183 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 04:17:56 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_db9240b9a1ce4b33f9d6ec1a1a4e2dc0'), (b'openai-processing-ms', b'204'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'207'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4daed51f675a1d-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 09:47:57,183 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 09:47:57,184 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 09:47:57,184 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 09:47:57,184 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 09:47:57,185 | DEBUG | openai.agents | Exported 3 items
2026-01-28 09:47:58,619 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\n  \"use_memory\": true,\n  \"use_tool\": false,\n  \"tool_name\": \"none\",\n  \"tool_arguments\": null,\n  \"intent\": \"read\",\n  \"memory_key\": null\n}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 490,
    "candidatesTokenCount": 56,
    "totalTokenCount": 625,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 490
      }
    ],
    "thoughtsTokenCount": 79
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "9Y15aYSEL8Xf4-EPq9HKsQQ"
}



2026-01-28 09:47:58,621 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 09:47:58,622 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 09:47:58,622 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 09:47:58,622 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:47:58,623 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 09:47:58,623 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:47:58,623 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:47:58,624 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:47:58,624 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:47:58,624 | DEBUG | LiteLLM | response_cost: 0.0004845
2026-01-28 09:47:58,625 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:47:58,625 | DEBUG | openai.agents | Received model response
2026-01-28 09:47:58,626 | DEBUG | LiteLLM | response_cost: 0.0004845
2026-01-28 09:47:58,627 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001949B433A60>>
2026-01-28 09:47:58,627 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:47:58,628 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 09:47:58,628 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:47:58,629 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 09:47:58,629 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 09:47:58,629 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:47:58,630 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 09:47:58,631 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:47:58,631 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 09:47:58,632 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:47:58,633 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:47:58,634 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:47:58,634 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:47:58,635 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:47:58,635 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:47:58,635 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:47:58,636 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:47:58,637 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 09:47:58,637 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0004845
2026-01-28 09:47:58,638 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:47:58,639 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:47:58,639 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:47:58,641 | DEBUG | openai.agents | Resetting current trace
2026-01-28 09:47:58,642 | INFO | orchestrator | üß≠ ROUTER_DECISION | tool=False | memory=True | intent=read
2026-01-28 09:47:58,644 | INFO | orchestrator | üß† REASONING_INPUT_SNAPSHOT
{
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful AI assistant.\n \nYou may answer ANY general knowledge question.\n \nConversation summaries and memories are ONLY to provide helpful context\n(e.g. preferences, ongoing plans).\n \nIf a user asks something unrelated to the current topic,\nanswer it normally using general knowledge.\n \nDo NOT restrict yourself to previous topics unless the user explicitly asks."
    },
    {
      "role": "user",
      "content": "i love sweet dishes"
    },
    {
      "role": "user",
      "content": "i love sweet dishes"
    },
    {
      "role": "assistant",
      "content": "That's great to know! Do you have a favorite sweet dish, or are you looking for some recommendations?"
    },
    {
      "role": "user",
      "content": "i love darshan raval"
    },
    {
      "role": "assistant",
      "content": "That's wonderful! Darshan Raval is a very popular singer. What do you like most about his music?"
    },
    {
      "role": "user",
      "content": "where do i live?"
    },
    {
      "role": "user",
      "content": "where do i live?"
    }
  ],
  "safety": {
    "safe": true,
    "message": null
  },
  "memory_action": {},
  "memory_data": [
    {
      "memory_id": 12,
      "user_id": 3,
      "text": "I live in Fatehabad.",
      "confidence": 0.95,
      "created_at": "2026-01-28T09:46:48.251118+05:30",
      "expires_at": "2026-02-27T09:46:48.246971+05:30",
      "score": 0.4735436737537384
    }
  ],
  "memory_found": true,
  "tool_context": {}
}
2026-01-28 09:47:58,646 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_f04afae0139d478fb0c781d0ea0576ac
2026-01-28 09:47:58,646 | DEBUG | openai.agents | Setting current trace: trace_f04afae0139d478fb0c781d0ea0576ac
2026-01-28 09:47:58,647 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000001949B383290> with id None
2026-01-28 09:47:58,648 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000001949BA95FC0> with id None
2026-01-28 09:47:58,648 | DEBUG | openai.agents | Calling LLM
2026-01-28 09:47:58,649 | DEBUG | LiteLLM | 

2026-01-28 09:47:58,649 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 09:47:58,649 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "i love sweet dishes"}, {"role": "user", "content": "i love sweet dishes"}, {"role": "assistant", "content": "That\'s great to know! Do you have a favorite sweet dish, or are you looking for some recommendations?"}, {"role": "user", "content": "i love darshan raval"}, {"role": "assistant", "content": "That\'s wonderful! Darshan Raval is a very popular singer. What do you like most about his music?"}, {"role": "user", "content": "where do i live?"}, {"role": "user", "content": "where do i live?"}], "safety": {"safe": true, "message": null}, "memory_action": {}, "memory_data": [{"memory_id": 12, "user_id": 3, "text": "I live in Fatehabad.", "confidence": 0.95, "created_at": "2026-01-28T09:46:48.251118+05:30", "expires_at": "2026-02-27T09:46:48.246971+05:30", "score": 0.4735436737537384}], "memory_found": true, "tool_context": {}}'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=True, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 09:47:58,650 | DEBUG | LiteLLM | 

2026-01-28 09:47:58,650 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 09:47:58,651 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 09:47:58,651 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 09:47:58,653 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:47:58,655 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 09:47:58,657 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "i love sweet dishes"}, {"role": "user", "content": "i love sweet dishes"}, {"role": "assistant", "content": "That\'s great to know! Do you have a favorite sweet dish, or are you looking for some recommendations?"}, {"role": "user", "content": "i love darshan raval"}, {"role": "assistant", "content": "That\'s wonderful! Darshan Raval is a very popular singer. What do you like most about his music?"}, {"role": "user", "content": "where do i live?"}, {"role": "user", "content": "where do i live?"}], "safety": {"safe": true, "message": null}, "memory_action": {}, "memory_data": [{"memory_id": 12, "user_id": 3, "text": "I live in Fatehabad.", "confidence": 0.95, "created_at": "2026-01-28T09:46:48.251118+05:30", "expires_at": "2026-02-27T09:46:48.246971+05:30", "score": 0.4735436737537384}], "memory_found": true, "tool_context": {}}'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 09:47:58,657 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': True}
2026-01-28 09:47:58,658 | DEBUG | LiteLLM | Final returned optional params: {'stream': True}
2026-01-28 09:47:58,659 | DEBUG | LiteLLM | self.optional_params: {'stream': True}
2026-01-28 09:47:58,668 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:47:58,670 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:47:58,670 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?key=*****=sse \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "i love sweet dishes"}, {"role": "user", "content": "i love sweet dishes"}, {"role": "assistant", "content": "That\'s great to know! Do you have a favorite sweet dish, or are you looking for some recommendations?"}, {"role": "user", "content": "i love darshan raval"}, {"role": "assistant", "content": "That\'s wonderful! Darshan Raval is a very popular singer. What do you like most about his music?"}, {"role": "user", "content": "where do i live?"}, {"role": "user", "content": "where do i live?"}], "safety": {"safe": true, "message": null}, "memory_action": {}, "memory_data": [{"memory_id": 12, "user_id": 3, "text": "I live in Fatehabad.", "confidence": 0.95, "created_at": "2026-01-28T09:46:48.251118+05:30", "expires_at": "2026-02-27T09:46:48.246971+05:30", "score": 0.4735436737537384}], "memory_found": true, "tool_context": {}}'}]}], 'system_instruction': {'parts': [{'text': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.'}]}, 'generationConfig': {}}'
[0m

2026-01-28 09:47:58,672 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 09:47:58,672 | DEBUG | LiteLLM | messages in token_counter: None, text in token_counter: 
2026-01-28 09:47:58,673 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:47:58,673 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:47:58,673 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:47:58,674 | DEBUG | LiteLLM | response_cost: 0.0
2026-01-28 09:47:59,688 | DEBUG | LiteLLM | RAW RESPONSE:
first stream response received


2026-01-28 09:47:59,689 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': 'You live in Fatehabad.'}], 'role': 'model'}, 'finishReason': 'STOP', 'index': 0}], 'usageMetadata': {'promptTokenCount': 534, 'candidatesTokenCount': 7, 'totalTokenCount': 579, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 534}], 'thoughtsTokenCount': 38}, 'modelVersion': 'gemini-2.5-flash', 'responseId': '9o15abXIKff2juMPmIjomAM'}
2026-01-28 09:47:59,689 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='9o15abXIKff2juMPmIjomAM', created=1769573879, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='You live in Fatehabad.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=45, prompt_tokens=534, total_tokens=579, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=38, rejected_prediction_tokens=None, text_tokens=7, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=534, image_tokens=None)))
2026-01-28 09:47:59,690 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content='You live in Fatehabad.', role='assistant', function_call=None, tool_calls=None, audio=None)
2026-01-28 09:47:59,690 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='9o15abXIKff2juMPmIjomAM', created=1769573879, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='You live in Fatehabad.', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=45, prompt_tokens=534, total_tokens=579, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=38, rejected_prediction_tokens=None, text_tokens=7, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=534, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-28 09:47:59,694 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=False
2026-01-28 09:47:59,694 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=False
2026-01-28 09:47:59,694 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 09:47:59,694 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 09:47:59,695 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 09:47:59,695 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 09:47:59,696 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:47:59,696 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:47:59,696 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:47:59,697 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:47:59,697 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:47:59,698 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:47:59,698 | DEBUG | LiteLLM | response_cost: 0.0002727
2026-01-28 09:47:59,699 | DEBUG | LiteLLM | response_cost: 0.0002727
2026-01-28 09:47:59,699 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0002727
2026-01-28 09:47:59,700 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:47:59,701 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:47:59,701 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:47:59,702 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:47:59,702 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:47:59,702 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:47:59,703 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 09:47:59,704 | DEBUG | openai.agents | Resetting current trace
2026-01-28 09:47:59,712 | INFO | session_summary | üß† SUMMARY_START | session_id=18f96f31-5bb1-439d-b068-9932c7c45f32
2026-01-28 09:47:59,716 | INFO | session_summary | üìä Unsummarized messages = 9
2026-01-28 09:47:59,718 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-28 09:47:59,719 | INFO | chat | ‚úÖ Stream complete | tokens=1 | time=4.64s
2026-01-28 09:48:02,331 | DEBUG | httpcore.connection | close.started
2026-01-28 09:48:02,332 | DEBUG | httpcore.connection | close.complete
2026-01-28 09:48:02,332 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 09:48:02,339 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001949B5CC490>
2026-01-28 09:48:02,339 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x00000194900B9BC0> server_hostname='api.openai.com' timeout=5.0
2026-01-28 09:48:02,346 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001949B5CCA60>
2026-01-28 09:48:02,346 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 09:48:02,347 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 09:48:02,347 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 09:48:02,347 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 09:48:02,348 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 09:48:02,816 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 04:18:02 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_ee0358c2d9618725b2ded748803fc4bf'), (b'openai-processing-ms', b'147'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'153'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4daef8af28ae8f-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 09:48:02,817 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 09:48:02,817 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 09:48:02,817 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 09:48:02,818 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 09:48:02,818 | DEBUG | openai.agents | Exported 5 items
2026-01-28 09:57:48,744 | INFO | chat | üì© Chat request | user_id=3 | request_id=d7fa93a6-f6be-4bc3-9c1d-60991aa5b4e4
2026-01-28 09:57:48,746 | INFO | chat | üßë User message | yesterday i saw a movie .. i like that movie..Dabang 2 is full of joy and drama. Its a thrillar movie.. I like to watch again and again
2026-01-28 09:57:48,779 | INFO | chat | ü§ñ Stream started | session_id=ef5f092b-2ec0-4839-921b-a27aef540c8d | request_id=d7fa93a6-f6be-4bc3-9c1d-60991aa5b4e4
2026-01-28 09:57:48,781 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-28 09:57:48,783 | INFO | orchestrator | üßë USER_INPUT | yesterday i saw a movie .. i like that movie..Dabang 2 is full of joy and drama. Its a thrillar movie.. I like to watch again and again
2026-01-28 09:57:48,784 | INFO | orchestrator | üß© CONTEXT | user_id=3 | enable_memory=True | db=None
2026-01-28 09:57:48,787 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-28 09:57:48,789 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_273fbd349ecd4f79b19d3a97473a3579
2026-01-28 09:57:48,790 | DEBUG | openai.agents | Setting current trace: trace_273fbd349ecd4f79b19d3a97473a3579
2026-01-28 09:57:48,791 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000001949BACDB20> with id None
2026-01-28 09:57:48,791 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-28 09:57:48,793 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000001949B5575E0> with id None
2026-01-28 09:57:48,794 | DEBUG | openai.agents | Calling LLM
2026-01-28 09:57:48,795 | DEBUG | LiteLLM | 

2026-01-28 09:57:48,796 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 09:57:48,797 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n', 'role': 'system'}, {'role': 'user', 'content': 'yesterday i saw a movie .. i like that movie..Dabang 2 is full of joy and drama. Its a thrillar movie.. I like to watch again and again'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 09:57:48,798 | DEBUG | LiteLLM | 

2026-01-28 09:57:48,799 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 09:57:48,800 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 09:57:48,801 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 09:57:48,804 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:57:48,805 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 09:57:48,807 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n', 'role': 'system'}, {'role': 'user', 'content': 'yesterday i saw a movie .. i like that movie..Dabang 2 is full of joy and drama. Its a thrillar movie.. I like to watch again and again'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 09:57:48,809 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 09:57:48,810 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 09:57:48,812 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 09:57:48,812 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:57:48,816 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:57:48,817 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'yesterday i saw a movie .. i like that movie..Dabang 2 is full of joy and drama. Its a thrillar movie.. I like to watch again and again'}]}], 'system_instruction': {'parts': [{'text': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 09:57:50,804 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "SAFE"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 223,
    "candidatesTokenCount": 1,
    "totalTokenCount": 288,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 223
      }
    ],
    "thoughtsTokenCount": 64
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "RZB5aYaQMt_4juMP5O6toQI"
}



2026-01-28 09:57:50,808 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 09:57:50,810 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 09:57:50,810 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 09:57:50,811 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:57:50,813 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 09:57:50,814 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:57:50,815 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:57:50,816 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:57:50,818 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:57:50,820 | DEBUG | LiteLLM | response_cost: 0.00022940000000000002
2026-01-28 09:57:50,821 | DEBUG | openai.agents | Received model response
2026-01-28 09:57:50,821 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:57:50,823 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001949B5CC8E0>>
2026-01-28 09:57:50,825 | DEBUG | LiteLLM | response_cost: 0.00022940000000000002
2026-01-28 09:57:50,826 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 09:57:50,828 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:57:50,830 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 09:57:50,830 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 09:57:50,831 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:57:50,832 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 09:57:50,834 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:57:50,835 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:57:50,837 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:57:50,838 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:57:50,839 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 09:57:50,840 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:57:50,841 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:57:50,844 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:57:50,844 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:57:50,846 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:57:50,848 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 09:57:50,848 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:57:50,850 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.00022940000000000002
2026-01-28 09:57:50,853 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:57:50,854 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:57:50,856 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:57:50,859 | DEBUG | openai.agents | Resetting current trace
2026-01-28 09:57:50,859 | ERROR | digital_human_sdk.app.intelligence.utils.json_utils | safe_json_loads failed
2026-01-28 09:57:50,862 | INFO | orchestrator | ‚úÖ Safety passed
2026-01-28 09:57:50,864 | INFO | orchestrator | üì• Fetching semantic memory (via MemoryService)
2026-01-28 09:57:50,946 | INFO | orchestrator | üß† MEMORY_RESULT | found=True | count=1
2026-01-28 09:57:50,948 | INFO | orchestrator | üß≠ Router agent called
2026-01-28 09:57:50,950 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_8cce40159acd460abb6a03c535656b7e
2026-01-28 09:57:50,951 | DEBUG | openai.agents | Setting current trace: trace_8cce40159acd460abb6a03c535656b7e
2026-01-28 09:57:50,952 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000001949BAE94E0> with id None
2026-01-28 09:57:50,952 | DEBUG | openai.agents | Running agent RouterAgent (turn 1)
2026-01-28 09:57:50,954 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000001949B4FE0E0> with id None
2026-01-28 09:57:50,955 | DEBUG | openai.agents | Calling LLM
2026-01-28 09:57:50,956 | DEBUG | LiteLLM | 

2026-01-28 09:57:50,956 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 09:57:50,957 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'yesterday i saw a movie .. i like that movie..Dabang 2 is full of joy and drama. Its a thrillar movie.. I like to watch again and again'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 09:57:50,959 | DEBUG | LiteLLM | 

2026-01-28 09:57:50,960 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 09:57:50,961 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 09:57:50,962 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 09:57:50,965 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:57:50,966 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 09:57:50,967 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'yesterday i saw a movie .. i like that movie..Dabang 2 is full of joy and drama. Its a thrillar movie.. I like to watch again and again'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 09:57:50,969 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 09:57:50,970 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 09:57:50,971 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 09:57:50,971 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:57:50,973 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:57:50,974 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'yesterday i saw a movie .. i like that movie..Dabang 2 is full of joy and drama. Its a thrillar movie.. I like to watch again and again'}]}], 'system_instruction': {'parts': [{'text': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 09:57:53,303 | DEBUG | httpcore.connection | close.started
2026-01-28 09:57:53,304 | DEBUG | httpcore.connection | close.complete
2026-01-28 09:57:53,305 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 09:57:53,350 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001949BA6E860>
2026-01-28 09:57:53,350 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x00000194900B9BC0> server_hostname='api.openai.com' timeout=5.0
2026-01-28 09:57:53,375 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001949BA6D660>
2026-01-28 09:57:53,376 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 09:57:53,377 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 09:57:53,377 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 09:57:53,378 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 09:57:53,379 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 09:57:53,524 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\n  \"use_memory\": true,\n  \"use_tool\": false,\n  \"tool_name\": \"none\",\n  \"tool_arguments\": null,\n  \"intent\": \"write\",\n  \"memory_key\": \"movie_preference\"\n}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 521,
    "candidatesTokenCount": 60,
    "totalTokenCount": 726,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 521
      }
    ],
    "thoughtsTokenCount": 145
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "SJB5adC-KIKy4-EPsKOT-AE"
}



2026-01-28 09:57:53,528 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 09:57:53,530 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 09:57:53,530 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 09:57:53,531 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:57:53,532 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 09:57:53,533 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:57:53,534 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:57:53,534 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:57:53,535 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:57:53,537 | DEBUG | LiteLLM | response_cost: 0.0006688
2026-01-28 09:57:53,539 | DEBUG | openai.agents | Received model response
2026-01-28 09:57:53,538 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:57:53,541 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001949B5CD0F0>>
2026-01-28 09:57:53,542 | DEBUG | LiteLLM | response_cost: 0.0006688
2026-01-28 09:57:53,543 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 09:57:53,545 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:57:53,547 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 09:57:53,546 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 09:57:53,547 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:57:53,549 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 09:57:53,550 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:57:53,551 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:57:53,553 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:57:53,554 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:57:53,555 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 09:57:53,556 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:57:53,557 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:57:53,558 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:57:53,559 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:57:53,561 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:57:53,563 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 09:57:53,564 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:57:53,565 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0006688
2026-01-28 09:57:53,567 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:57:53,568 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:57:53,570 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:57:53,573 | DEBUG | openai.agents | Resetting current trace
2026-01-28 09:57:53,573 | INFO | orchestrator | üß≠ ROUTER_DECISION | tool=False | memory=True | intent=write
2026-01-28 09:57:53,575 | INFO | orchestrator | üß† Memory agent called (WRITE)
2026-01-28 09:57:53,577 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_cdd35d82648d4bc79e15c4f8f0fbfae9
2026-01-28 09:57:53,578 | DEBUG | openai.agents | Setting current trace: trace_cdd35d82648d4bc79e15c4f8f0fbfae9
2026-01-28 09:57:53,580 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000001949BAE93A0> with id None
2026-01-28 09:57:53,581 | DEBUG | openai.agents | Running agent Memory Agent (turn 1)
2026-01-28 09:57:53,582 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000001949BA95660> with id None
2026-01-28 09:57:53,583 | DEBUG | openai.agents | Calling LLM
2026-01-28 09:57:53,583 | DEBUG | LiteLLM | 

2026-01-28 09:57:53,584 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 09:57:53,584 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a Memory Extraction Agent.\n\nYour job is to extract ONLY long-term, personal, user-specific information\nfrom the user\'s message and decide the appropriate memory action.\n\nYou will be called ONLY when memory handling is required.\n\n========================\nWHAT TO EXTRACT\n========================\n- Stable personal facts (name, education, location, job)\n- Long-term preferences (food, language, habits)\n- Information the user would reasonably expect to be remembered\n- Facts that should be retrievable across sessions via semantic search\n\nStore memory as FULL, NATURAL-LANGUAGE TEXT.\nDO NOT create structured keys.\nDO NOT summarize aggressively.\nPreserve meaning exactly as stated.\n\n========================\nWHAT TO IGNORE\n========================\n- Questions\n- Small talk\n- Temporary states (mood, weather, current task)\n- Opinions about content\n- One-time or session-only context\n\n========================\nACTIONS (STRICT)\n========================\nUse exactly ONE action:\n\n- "save"\n  ‚Üí New long-term personal information stated for the first time\n\n- "update"\n  ‚Üí Existing long-term information is explicitly changed,\n    corrected, or overridden\n    (keywords: "now", "instead", "changed", "earlier", "previously")\n\n- "delete"\n  ‚Üí User explicitly asks to forget, remove, or delete information\n\nNEVER guess.\nNEVER infer deletion.\nNEVER revive deleted information.\nDeleted memory must be treated as non-existent.\n\n========================\nIMPORTANT RULES\n========================\n- Store memory as ONE complete sentence\n- Do NOT split into multiple memories\n- Do NOT invent information\n- Do NOT rely on database state\n- Backend will validate save vs update\n- If unsure, return "none"\n\n========================\nOUTPUT RULES (CRITICAL)\n========================\n- Return VALID JSON only\n- Do NOT explain anything\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Do NOT return multiple objects\n\n========================\nOUTPUT FORMAT (JSON ONLY)\n========================\n{\n  "action": "save | update | delete | none",\n  "memory_text": "full natural language memory text",\n  "confidence": 0.0\n}\n\n========================\nCONFIDENCE GUIDELINES\n========================\n- 0.9‚Äì1.0 ‚Üí Explicit personal fact (e.g., education, name, job)\n- 0.8‚Äì0.9 ‚Üí Clear long-term preference or habit\n- Below 0.7 ‚Üí Weak or ambiguous (avoid storing)\n\n========================\nNO MEMORY CASE\n========================\nIf no valid long-term memory is present, return:\n{\n  "action": "none",\n  "memory_text": "",\n  "confidence": 0.0\n}\n', 'role': 'system'}, {'role': 'user', 'content': 'yesterday i saw a movie .. i like that movie..Dabang 2 is full of joy and drama. Its a thrillar movie.. I like to watch again and again'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 09:57:53,586 | DEBUG | LiteLLM | 

2026-01-28 09:57:53,588 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 09:57:53,589 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 09:57:53,590 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 09:57:53,593 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:57:53,593 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 09:57:53,595 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Memory Extraction Agent.\n\nYour job is to extract ONLY long-term, personal, user-specific information\nfrom the user\'s message and decide the appropriate memory action.\n\nYou will be called ONLY when memory handling is required.\n\n========================\nWHAT TO EXTRACT\n========================\n- Stable personal facts (name, education, location, job)\n- Long-term preferences (food, language, habits)\n- Information the user would reasonably expect to be remembered\n- Facts that should be retrievable across sessions via semantic search\n\nStore memory as FULL, NATURAL-LANGUAGE TEXT.\nDO NOT create structured keys.\nDO NOT summarize aggressively.\nPreserve meaning exactly as stated.\n\n========================\nWHAT TO IGNORE\n========================\n- Questions\n- Small talk\n- Temporary states (mood, weather, current task)\n- Opinions about content\n- One-time or session-only context\n\n========================\nACTIONS (STRICT)\n========================\nUse exactly ONE action:\n\n- "save"\n  ‚Üí New long-term personal information stated for the first time\n\n- "update"\n  ‚Üí Existing long-term information is explicitly changed,\n    corrected, or overridden\n    (keywords: "now", "instead", "changed", "earlier", "previously")\n\n- "delete"\n  ‚Üí User explicitly asks to forget, remove, or delete information\n\nNEVER guess.\nNEVER infer deletion.\nNEVER revive deleted information.\nDeleted memory must be treated as non-existent.\n\n========================\nIMPORTANT RULES\n========================\n- Store memory as ONE complete sentence\n- Do NOT split into multiple memories\n- Do NOT invent information\n- Do NOT rely on database state\n- Backend will validate save vs update\n- If unsure, return "none"\n\n========================\nOUTPUT RULES (CRITICAL)\n========================\n- Return VALID JSON only\n- Do NOT explain anything\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Do NOT return multiple objects\n\n========================\nOUTPUT FORMAT (JSON ONLY)\n========================\n{\n  "action": "save | update | delete | none",\n  "memory_text": "full natural language memory text",\n  "confidence": 0.0\n}\n\n========================\nCONFIDENCE GUIDELINES\n========================\n- 0.9‚Äì1.0 ‚Üí Explicit personal fact (e.g., education, name, job)\n- 0.8‚Äì0.9 ‚Üí Clear long-term preference or habit\n- Below 0.7 ‚Üí Weak or ambiguous (avoid storing)\n\n========================\nNO MEMORY CASE\n========================\nIf no valid long-term memory is present, return:\n{\n  "action": "none",\n  "memory_text": "",\n  "confidence": 0.0\n}\n', 'role': 'system'}, {'role': 'user', 'content': 'yesterday i saw a movie .. i like that movie..Dabang 2 is full of joy and drama. Its a thrillar movie.. I like to watch again and again'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 09:57:53,596 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 09:57:53,597 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 09:57:53,598 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 09:57:53,599 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:57:53,601 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:57:53,603 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'yesterday i saw a movie .. i like that movie..Dabang 2 is full of joy and drama. Its a thrillar movie.. I like to watch again and again'}]}], 'system_instruction': {'parts': [{'text': 'You are a Memory Extraction Agent.\n\nYour job is to extract ONLY long-term, personal, user-specific information\nfrom the user\'s message and decide the appropriate memory action.\n\nYou will be called ONLY when memory handling is required.\n\n========================\nWHAT TO EXTRACT\n========================\n- Stable personal facts (name, education, location, job)\n- Long-term preferences (food, language, habits)\n- Information the user would reasonably expect to be remembered\n- Facts that should be retrievable across sessions via semantic search\n\nStore memory as FULL, NATURAL-LANGUAGE TEXT.\nDO NOT create structured keys.\nDO NOT summarize aggressively.\nPreserve meaning exactly as stated.\n\n========================\nWHAT TO IGNORE\n========================\n- Questions\n- Small talk\n- Temporary states (mood, weather, current task)\n- Opinions about content\n- One-time or session-only context\n\n========================\nACTIONS (STRICT)\n========================\nUse exactly ONE action:\n\n- "save"\n  ‚Üí New long-term personal information stated for the first time\n\n- "update"\n  ‚Üí Existing long-term information is explicitly changed,\n    corrected, or overridden\n    (keywords: "now", "instead", "changed", "earlier", "previously")\n\n- "delete"\n  ‚Üí User explicitly asks to forget, remove, or delete information\n\nNEVER guess.\nNEVER infer deletion.\nNEVER revive deleted information.\nDeleted memory must be treated as non-existent.\n\n========================\nIMPORTANT RULES\n========================\n- Store memory as ONE complete sentence\n- Do NOT split into multiple memories\n- Do NOT invent information\n- Do NOT rely on database state\n- Backend will validate save vs update\n- If unsure, return "none"\n\n========================\nOUTPUT RULES (CRITICAL)\n========================\n- Return VALID JSON only\n- Do NOT explain anything\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Do NOT return multiple objects\n\n========================\nOUTPUT FORMAT (JSON ONLY)\n========================\n{\n  "action": "save | update | delete | none",\n  "memory_text": "full natural language memory text",\n  "confidence": 0.0\n}\n\n========================\nCONFIDENCE GUIDELINES\n========================\n- 0.9‚Äì1.0 ‚Üí Explicit personal fact (e.g., education, name, job)\n- 0.8‚Äì0.9 ‚Üí Clear long-term preference or habit\n- Below 0.7 ‚Üí Weak or ambiguous (avoid storing)\n\n========================\nNO MEMORY CASE\n========================\nIf no valid long-term memory is present, return:\n{\n  "action": "none",\n  "memory_text": "",\n  "confidence": 0.0\n}\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 09:57:53,991 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 04:27:53 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_de5539912ecde458ec2bfb501853841c'), (b'openai-processing-ms', b'307'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'311'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4dbd668dfa7f23-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 09:57:53,992 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 09:57:53,992 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 09:57:53,994 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 09:57:53,995 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 09:57:53,995 | DEBUG | openai.agents | Exported 4 items
2026-01-28 09:57:53,998 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 09:57:53,999 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 09:57:54,000 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 09:57:54,001 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 09:57:54,002 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 09:57:54,602 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "```json\n{\n  \"action\": \"save\",\n  \"memory_text\": \"I like the movie Dabang 2 and I like to watch it again and again.\",\n  \"confidence\": 0.85\n}\n```"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 636,
    "candidatesTokenCount": 52,
    "totalTokenCount": 731,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 636
      }
    ],
    "thoughtsTokenCount": 43
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "SZB5afDyL9yw4-EPvb3s0Qc"
}



2026-01-28 09:57:54,606 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 09:57:54,608 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 09:57:54,608 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 09:57:54,610 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:57:54,611 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 09:57:54,612 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:57:54,613 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:57:54,614 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:57:54,615 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:57:54,616 | DEBUG | LiteLLM | response_cost: 0.00042830000000000006
2026-01-28 09:57:54,618 | DEBUG | openai.agents | Received model response
2026-01-28 09:57:54,617 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:57:54,619 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001949BA6C370>>
2026-01-28 09:57:54,620 | DEBUG | LiteLLM | response_cost: 0.00042830000000000006
2026-01-28 09:57:54,621 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 09:57:54,623 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:57:54,624 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 09:57:54,625 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 09:57:54,625 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:57:54,627 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:57:54,629 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 09:57:54,631 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:57:54,632 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:57:54,634 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:57:54,635 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 09:57:54,636 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:57:54,637 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:57:54,639 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:57:54,644 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:57:54,645 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:57:54,645 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 09:57:54,646 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:57:54,647 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.00042830000000000006
2026-01-28 09:57:54,650 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:57:54,651 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:57:54,652 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:57:54,657 | DEBUG | openai.agents | Resetting current trace
2026-01-28 09:57:54,658 | INFO | orchestrator | üß† MEMORY_DECISION | {
  "action": "save",
  "memory_text": "I like the movie Dabang 2 and I like to watch it again and again.",
  "confidence": 0.85
}
2026-01-28 09:57:54,659 | INFO | chat | üß† MEMORY_EVENT_RECEIVED | {'action': 'save', 'memory_text': 'I like the movie Dabang 2 and I like to watch it again and again.', 'confidence': 0.85}
2026-01-28 09:57:54,662 | INFO | memory_action_executor | üß† APPLY_MEMORY_ACTION | user_id=3 | action={'action': 'save', 'memory_text': 'I like the movie Dabang 2 and I like to watch it again and again.', 'confidence': 0.85}
2026-01-28 09:57:54,664 | INFO | memory_action_executor | üß† Memory Parsed | type=save | content=I like the movie Dabang 2 and I like to watch it again and again. | confidence=0.85
2026-01-28 09:57:54,666 | INFO | memory_action_executor | üíæ Writing memory to DB
2026-01-28 09:57:54,689 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 04:27:54 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_662be556a610de17288fafea4fcb09f4'), (b'openai-processing-ms', b'375'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'378'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4dbd6a7c1c7f23-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 09:57:54,691 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 09:57:54,692 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 09:57:54,692 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 09:57:54,693 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 09:57:54,694 | DEBUG | openai.agents | Exported 3 items
2026-01-28 09:57:54,697 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 09:57:54,698 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 09:57:54,700 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 09:57:54,701 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 09:57:54,702 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 09:57:54,755 | INFO | memory_action_executor | ‚úÖ DB COMMIT SUCCESS
2026-01-28 09:57:54,757 | INFO | orchestrator | üß† REASONING_INPUT_SNAPSHOT
{
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful AI assistant.\n \nYou may answer ANY general knowledge question.\n \nConversation summaries and memories are ONLY to provide helpful context\n(e.g. preferences, ongoing plans).\n \nIf a user asks something unrelated to the current topic,\nanswer it normally using general knowledge.\n \nDo NOT restrict yourself to previous topics unless the user explicitly asks."
    },
    {
      "role": "user",
      "content": "where do i live?"
    },
    {
      "role": "assistant",
      "content": "I am an AI and do not have access to your personal information, including where you live."
    },
    {
      "role": "user",
      "content": "i lives in fatehabad"
    },
    {
      "role": "assistant",
      "content": "Thank you for letting me know."
    },
    {
      "role": "user",
      "content": "yesterday i saw a movie .. i like that movie..Dabang 2 is full of joy and drama. Its a thrillar movie.. I like to watch again and again"
    },
    {
      "role": "user",
      "content": "yesterday i saw a movie .. i like that movie..Dabang 2 is full of joy and drama. Its a thrillar movie.. I like to watch again and again"
    }
  ],
  "safety": {
    "safe": true,
    "message": null
  },
  "memory_action": {},
  "memory_data": [
    {
      "memory_id": 11,
      "user_id": 3,
      "text": "I love to watch movies.",
      "confidence": 0.85,
      "created_at": "2026-01-27T22:49:49.349202+05:30",
      "expires_at": "2026-02-26T22:49:49.345889+05:30",
      "score": 0.48352834582328796
    }
  ],
  "memory_found": true,
  "tool_context": {}
}
2026-01-28 09:57:54,759 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_43f97dd8b1a145b2bee94c272bbf5a44
2026-01-28 09:57:54,759 | DEBUG | openai.agents | Setting current trace: trace_43f97dd8b1a145b2bee94c272bbf5a44
2026-01-28 09:57:54,760 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000001949BA5E340> with id None
2026-01-28 09:57:54,761 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000001949BA97100> with id None
2026-01-28 09:57:54,762 | DEBUG | openai.agents | Calling LLM
2026-01-28 09:57:54,762 | DEBUG | LiteLLM | 

2026-01-28 09:57:54,765 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 09:57:54,767 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "where do i live?"}, {"role": "assistant", "content": "I am an AI and do not have access to your personal information, including where you live."}, {"role": "user", "content": "i lives in fatehabad"}, {"role": "assistant", "content": "Thank you for letting me know."}, {"role": "user", "content": "yesterday i saw a movie .. i like that movie..Dabang 2 is full of joy and drama. Its a thrillar movie.. I like to watch again and again"}, {"role": "user", "content": "yesterday i saw a movie .. i like that movie..Dabang 2 is full of joy and drama. Its a thrillar movie.. I like to watch again and again"}], "safety": {"safe": true, "message": null}, "memory_action": {}, "memory_data": [{"memory_id": 11, "user_id": 3, "text": "I love to watch movies.", "confidence": 0.85, "created_at": "2026-01-27T22:49:49.349202+05:30", "expires_at": "2026-02-26T22:49:49.345889+05:30", "score": 0.48352834582328796}], "memory_found": true, "tool_context": {}}'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=True, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 09:57:54,772 | DEBUG | LiteLLM | 

2026-01-28 09:57:54,773 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 09:57:54,776 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 09:57:54,777 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 09:57:54,780 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:57:54,781 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 09:57:54,784 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "where do i live?"}, {"role": "assistant", "content": "I am an AI and do not have access to your personal information, including where you live."}, {"role": "user", "content": "i lives in fatehabad"}, {"role": "assistant", "content": "Thank you for letting me know."}, {"role": "user", "content": "yesterday i saw a movie .. i like that movie..Dabang 2 is full of joy and drama. Its a thrillar movie.. I like to watch again and again"}, {"role": "user", "content": "yesterday i saw a movie .. i like that movie..Dabang 2 is full of joy and drama. Its a thrillar movie.. I like to watch again and again"}], "safety": {"safe": true, "message": null}, "memory_action": {}, "memory_data": [{"memory_id": 11, "user_id": 3, "text": "I love to watch movies.", "confidence": 0.85, "created_at": "2026-01-27T22:49:49.349202+05:30", "expires_at": "2026-02-26T22:49:49.345889+05:30", "score": 0.48352834582328796}], "memory_found": true, "tool_context": {}}'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 09:57:54,786 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': True}
2026-01-28 09:57:54,787 | DEBUG | LiteLLM | Final returned optional params: {'stream': True}
2026-01-28 09:57:54,788 | DEBUG | LiteLLM | self.optional_params: {'stream': True}
2026-01-28 09:57:54,789 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:57:54,791 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:57:54,792 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?key=*****=sse \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "where do i live?"}, {"role": "assistant", "content": "I am an AI and do not have access to your personal information, including where you live."}, {"role": "user", "content": "i lives in fatehabad"}, {"role": "assistant", "content": "Thank you for letting me know."}, {"role": "user", "content": "yesterday i saw a movie .. i like that movie..Dabang 2 is full of joy and drama. Its a thrillar movie.. I like to watch again and again"}, {"role": "user", "content": "yesterday i saw a movie .. i like that movie..Dabang 2 is full of joy and drama. Its a thrillar movie.. I like to watch again and again"}], "safety": {"safe": true, "message": null}, "memory_action": {}, "memory_data": [{"memory_id": 11, "user_id": 3, "text": "I love to watch movies.", "confidence": 0.85, "created_at": "2026-01-27T22:49:49.349202+05:30", "expires_at": "2026-02-26T22:49:49.345889+05:30", "score": 0.48352834582328796}], "memory_found": true, "tool_context": {}}'}]}], 'system_instruction': {'parts': [{'text': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.'}]}, 'generationConfig': {}}'
[0m

2026-01-28 09:57:54,796 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 09:57:54,797 | DEBUG | LiteLLM | messages in token_counter: None, text in token_counter: 
2026-01-28 09:57:54,798 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:57:54,799 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:57:54,802 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:57:54,804 | DEBUG | LiteLLM | response_cost: 0.0
2026-01-28 09:57:55,310 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 04:27:54 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_dba43a28a12def2e321be9b4ba3db465'), (b'openai-processing-ms', b'288'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'291'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4dbd6eda347f23-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 09:57:55,311 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 09:57:55,311 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 09:57:55,312 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 09:57:55,312 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 09:57:55,312 | DEBUG | openai.agents | Exported 2 items
2026-01-28 09:57:55,313 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 09:57:55,313 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 09:57:55,313 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 09:57:55,314 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 09:57:55,314 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 09:57:55,733 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 04:27:55 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_79e61048453805afcb7927c10ee2d23e'), (b'openai-processing-ms', b'114'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'117'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4dbd72af3c7f23-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 09:57:55,734 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 09:57:55,734 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 09:57:55,734 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 09:57:55,734 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 09:57:55,734 | DEBUG | openai.agents | Exported 1 items
2026-01-28 09:57:56,765 | DEBUG | LiteLLM | RAW RESPONSE:
first stream response received


2026-01-28 09:57:56,765 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': 'It sounds like you really'}], 'role': 'model'}, 'index': 0}], 'usageMetadata': {'promptTokenCount': 560, 'candidatesTokenCount': 5, 'totalTokenCount': 627, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 560}], 'thoughtsTokenCount': 62}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'S5B5ab7CH_WHjuMPxpKAkQI'}
2026-01-28 09:57:56,766 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='S5B5ab7CH_WHjuMPxpKAkQI', created=1769574476, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='It sounds like you really', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=67, prompt_tokens=560, total_tokens=627, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=62, rejected_prediction_tokens=None, text_tokens=5, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=560, image_tokens=None)))
2026-01-28 09:57:56,768 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content='It sounds like you really', role='assistant', function_call=None, tool_calls=None, audio=None)
2026-01-28 09:57:56,768 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='S5B5ab7CH_WHjuMPxpKAkQI', created=1769574476, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='It sounds like you really', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=67, prompt_tokens=560, total_tokens=627, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=62, rejected_prediction_tokens=None, text_tokens=5, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=560, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-28 09:57:56,769 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': ' enjoyed Dabangg 2 and appreciate its mix of joy, drama, and thrills!'}], 'role': 'model'}, 'finishReason': 'STOP', 'index': 0}], 'usageMetadata': {'promptTokenCount': 560, 'candidatesTokenCount': 23, 'totalTokenCount': 645, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 560}], 'thoughtsTokenCount': 62}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'S5B5ab7CH_WHjuMPxpKAkQI'}
2026-01-28 09:57:56,770 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='S5B5ab7CH_WHjuMPxpKAkQI', created=1769574476, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content=' enjoyed Dabangg 2 and appreciate its mix of joy, drama, and thrills!', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=85, prompt_tokens=560, total_tokens=645, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=62, rejected_prediction_tokens=None, text_tokens=23, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=560, image_tokens=None)))
2026-01-28 09:57:56,772 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content=' enjoyed Dabangg 2 and appreciate its mix of joy, drama, and thrills!', role=None, function_call=None, tool_calls=None, audio=None)
2026-01-28 09:57:56,772 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='S5B5ab7CH_WHjuMPxpKAkQI', created=1769574476, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' enjoyed Dabangg 2 and appreciate its mix of joy, drama, and thrills!', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=85, prompt_tokens=560, total_tokens=645, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=62, rejected_prediction_tokens=None, text_tokens=23, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=560, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-28 09:57:56,774 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=False
2026-01-28 09:57:56,774 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=False
2026-01-28 09:57:56,774 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 09:57:56,775 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 09:57:56,775 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 09:57:56,775 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 09:57:56,776 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:57:56,777 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:57:56,777 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:57:56,777 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:57:56,777 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:57:56,779 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:57:56,779 | DEBUG | LiteLLM | response_cost: 0.0003805
2026-01-28 09:57:56,780 | DEBUG | LiteLLM | response_cost: 0.0003805
2026-01-28 09:57:56,781 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:57:56,781 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0003805
2026-01-28 09:57:56,781 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:57:56,782 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:57:56,782 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:57:56,784 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:57:56,786 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:57:56,787 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 09:57:56,788 | DEBUG | openai.agents | Resetting current trace
2026-01-28 09:57:56,801 | INFO | session_summary | üß† SUMMARY_START | session_id=ef5f092b-2ec0-4839-921b-a27aef540c8d
2026-01-28 09:57:56,805 | INFO | session_summary | üìä Unsummarized messages = 6
2026-01-28 09:57:56,806 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-28 09:57:56,806 | INFO | chat | ‚úÖ Stream complete | tokens=2 | time=8.06s
2026-01-28 09:58:00,867 | DEBUG | httpcore.connection | close.started
2026-01-28 09:58:00,867 | DEBUG | httpcore.connection | close.complete
2026-01-28 09:58:00,868 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 09:58:00,899 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001949BA6E6B0>
2026-01-28 09:58:00,899 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x00000194900B9BC0> server_hostname='api.openai.com' timeout=5.0
2026-01-28 09:58:00,910 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001949BA6FE20>
2026-01-28 09:58:00,910 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 09:58:00,911 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 09:58:00,911 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 09:58:00,911 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 09:58:00,911 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 09:58:01,370 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 04:28:00 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_9586207fd96273af9e3ce6cffe827ae7'), (b'openai-processing-ms', b'141'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'144'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4dbd95ad0c0d2a-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 09:58:01,371 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 09:58:01,372 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 09:58:01,372 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 09:58:01,372 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 09:58:01,373 | DEBUG | openai.agents | Exported 2 items
2026-01-28 09:59:00,796 | INFO | chat | üì© Chat request | user_id=3 | request_id=c9e57534-41c2-4e03-8acf-33580c4f24e9
2026-01-28 09:59:00,798 | INFO | chat | üßë User message | and one movie is Avengers, its also great movie.. I have also interest in Avengers
2026-01-28 09:59:00,830 | INFO | chat | ü§ñ Stream started | session_id=ef5f092b-2ec0-4839-921b-a27aef540c8d | request_id=c9e57534-41c2-4e03-8acf-33580c4f24e9
2026-01-28 09:59:00,832 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-28 09:59:00,834 | INFO | orchestrator | üßë USER_INPUT | and one movie is Avengers, its also great movie.. I have also interest in Avengers
2026-01-28 09:59:00,835 | INFO | orchestrator | üß© CONTEXT | user_id=3 | enable_memory=True | db=None
2026-01-28 09:59:00,838 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-28 09:59:00,840 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_52dfdacb6b6e4d498246efce6db8261a
2026-01-28 09:59:00,841 | DEBUG | openai.agents | Setting current trace: trace_52dfdacb6b6e4d498246efce6db8261a
2026-01-28 09:59:00,841 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000001949BAE9530> with id None
2026-01-28 09:59:00,842 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-28 09:59:00,844 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000001949BA95300> with id None
2026-01-28 09:59:00,844 | DEBUG | openai.agents | Calling LLM
2026-01-28 09:59:00,845 | DEBUG | LiteLLM | 

2026-01-28 09:59:00,846 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 09:59:00,847 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n', 'role': 'system'}, {'role': 'user', 'content': 'and one movie is Avengers, its also great movie.. I have also interest in Avengers'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 09:59:00,848 | DEBUG | LiteLLM | 

2026-01-28 09:59:00,850 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 09:59:00,851 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 09:59:00,851 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 09:59:00,854 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:59:00,855 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 09:59:00,856 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n', 'role': 'system'}, {'role': 'user', 'content': 'and one movie is Avengers, its also great movie.. I have also interest in Avengers'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 09:59:00,857 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 09:59:00,859 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 09:59:00,860 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 09:59:00,862 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:59:00,863 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:59:00,865 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'and one movie is Avengers, its also great movie.. I have also interest in Avengers'}]}], 'system_instruction': {'parts': [{'text': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 09:59:02,013 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "SAFE"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 204,
    "candidatesTokenCount": 1,
    "totalTokenCount": 266,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 204
      }
    ],
    "thoughtsTokenCount": 61
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "jZB5acPkDqu84-EP9JrawAI"
}



2026-01-28 09:59:02,016 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 09:59:02,018 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 09:59:02,018 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 09:59:02,019 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:59:02,022 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 09:59:02,023 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:59:02,024 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:59:02,025 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:59:02,026 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:59:02,027 | DEBUG | LiteLLM | response_cost: 0.00021620000000000002
2026-01-28 09:59:02,029 | DEBUG | openai.agents | Received model response
2026-01-28 09:59:02,028 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:59:02,030 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001949B433580>>
2026-01-28 09:59:02,031 | DEBUG | LiteLLM | response_cost: 0.00021620000000000002
2026-01-28 09:59:02,032 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 09:59:02,034 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:59:02,035 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 09:59:02,037 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 09:59:02,036 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:59:02,039 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:59:02,040 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 09:59:02,041 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:59:02,042 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:59:02,044 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:59:02,045 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 09:59:02,046 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:59:02,047 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:59:02,050 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:59:02,053 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:59:02,054 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:59:02,055 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 09:59:02,056 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:59:02,057 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.00021620000000000002
2026-01-28 09:59:02,062 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:59:02,064 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:59:02,065 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:59:02,068 | DEBUG | openai.agents | Resetting current trace
2026-01-28 09:59:02,069 | ERROR | digital_human_sdk.app.intelligence.utils.json_utils | safe_json_loads failed
2026-01-28 09:59:02,082 | INFO | orchestrator | ‚úÖ Safety passed
2026-01-28 09:59:02,083 | INFO | orchestrator | üì• Fetching semantic memory (via MemoryService)
2026-01-28 09:59:02,137 | INFO | orchestrator | üß† MEMORY_RESULT | found=True | count=1
2026-01-28 09:59:02,138 | INFO | orchestrator | üß≠ Router agent called
2026-01-28 09:59:02,139 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_09d5399891ca4c63886be520f9b5f91c
2026-01-28 09:59:02,139 | DEBUG | openai.agents | Setting current trace: trace_09d5399891ca4c63886be520f9b5f91c
2026-01-28 09:59:02,139 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000001949BA5E930> with id None
2026-01-28 09:59:02,139 | DEBUG | openai.agents | Running agent RouterAgent (turn 1)
2026-01-28 09:59:02,140 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000001949BA956C0> with id None
2026-01-28 09:59:02,140 | DEBUG | openai.agents | Calling LLM
2026-01-28 09:59:02,140 | DEBUG | LiteLLM | 

2026-01-28 09:59:02,140 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 09:59:02,141 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'and one movie is Avengers, its also great movie.. I have also interest in Avengers'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 09:59:02,141 | DEBUG | LiteLLM | 

2026-01-28 09:59:02,142 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 09:59:02,142 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 09:59:02,142 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 09:59:02,143 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:59:02,144 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 09:59:02,144 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'and one movie is Avengers, its also great movie.. I have also interest in Avengers'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 09:59:02,144 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 09:59:02,145 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 09:59:02,145 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 09:59:02,145 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:59:02,146 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:59:02,146 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'and one movie is Avengers, its also great movie.. I have also interest in Avengers'}]}], 'system_instruction': {'parts': [{'text': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 09:59:03,110 | DEBUG | httpcore.connection | close.started
2026-01-28 09:59:03,111 | DEBUG | httpcore.connection | close.complete
2026-01-28 09:59:03,112 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 09:59:03,131 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001949BA9CFA0>
2026-01-28 09:59:03,131 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x00000194900B9BC0> server_hostname='api.openai.com' timeout=5.0
2026-01-28 09:59:03,142 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001949BA9C9A0>
2026-01-28 09:59:03,142 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 09:59:03,143 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 09:59:03,143 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 09:59:03,144 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 09:59:03,144 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 09:59:04,133 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\n  \"use_memory\": true,\n  \"use_tool\": false,\n  \"tool_name\": \"none\",\n  \"tool_arguments\": null,\n  \"intent\": \"write\",\n  \"memory_key\": \"movie_interest\"\n}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 502,
    "candidatesTokenCount": 60,
    "totalTokenCount": 652,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 502
      }
    ],
    "thoughtsTokenCount": 90
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "j5B5abjCELvJ4-EP2rGp4QM"
}



2026-01-28 09:59:04,134 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 09:59:04,135 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 09:59:04,135 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 09:59:04,137 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:59:04,139 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 09:59:04,140 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:59:04,140 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:59:04,141 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:59:04,141 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:59:04,142 | DEBUG | LiteLLM | response_cost: 0.0005256
2026-01-28 09:59:04,142 | DEBUG | openai.agents | Received model response
2026-01-28 09:59:04,142 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:59:04,144 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001949B5CD210>>
2026-01-28 09:59:04,144 | DEBUG | LiteLLM | response_cost: 0.0005256
2026-01-28 09:59:04,145 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 09:59:04,146 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:59:04,146 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 09:59:04,146 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:59:04,147 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 09:59:04,147 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:59:04,148 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:59:04,148 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 09:59:04,148 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:59:04,150 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 09:59:04,151 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:59:04,152 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:59:04,152 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:59:04,153 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:59:04,153 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:59:04,154 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:59:04,154 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:59:04,154 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 09:59:04,155 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0005256
2026-01-28 09:59:04,155 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:59:04,156 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:59:04,156 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:59:04,157 | DEBUG | openai.agents | Resetting current trace
2026-01-28 09:59:04,157 | INFO | orchestrator | üß≠ ROUTER_DECISION | tool=False | memory=True | intent=write
2026-01-28 09:59:04,158 | INFO | orchestrator | üß† Memory agent called (WRITE)
2026-01-28 09:59:04,159 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_f4eb9ba35f794bd08f431a23c11b5e9d
2026-01-28 09:59:04,159 | DEBUG | openai.agents | Setting current trace: trace_f4eb9ba35f794bd08f431a23c11b5e9d
2026-01-28 09:59:04,159 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000001949B4A3740> with id None
2026-01-28 09:59:04,160 | DEBUG | openai.agents | Running agent Memory Agent (turn 1)
2026-01-28 09:59:04,160 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000001949BA94EE0> with id None
2026-01-28 09:59:04,161 | DEBUG | openai.agents | Calling LLM
2026-01-28 09:59:04,162 | DEBUG | LiteLLM | 

2026-01-28 09:59:04,163 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 09:59:04,164 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a Memory Extraction Agent.\n\nYour job is to extract ONLY long-term, personal, user-specific information\nfrom the user\'s message and decide the appropriate memory action.\n\nYou will be called ONLY when memory handling is required.\n\n========================\nWHAT TO EXTRACT\n========================\n- Stable personal facts (name, education, location, job)\n- Long-term preferences (food, language, habits)\n- Information the user would reasonably expect to be remembered\n- Facts that should be retrievable across sessions via semantic search\n\nStore memory as FULL, NATURAL-LANGUAGE TEXT.\nDO NOT create structured keys.\nDO NOT summarize aggressively.\nPreserve meaning exactly as stated.\n\n========================\nWHAT TO IGNORE\n========================\n- Questions\n- Small talk\n- Temporary states (mood, weather, current task)\n- Opinions about content\n- One-time or session-only context\n\n========================\nACTIONS (STRICT)\n========================\nUse exactly ONE action:\n\n- "save"\n  ‚Üí New long-term personal information stated for the first time\n\n- "update"\n  ‚Üí Existing long-term information is explicitly changed,\n    corrected, or overridden\n    (keywords: "now", "instead", "changed", "earlier", "previously")\n\n- "delete"\n  ‚Üí User explicitly asks to forget, remove, or delete information\n\nNEVER guess.\nNEVER infer deletion.\nNEVER revive deleted information.\nDeleted memory must be treated as non-existent.\n\n========================\nIMPORTANT RULES\n========================\n- Store memory as ONE complete sentence\n- Do NOT split into multiple memories\n- Do NOT invent information\n- Do NOT rely on database state\n- Backend will validate save vs update\n- If unsure, return "none"\n\n========================\nOUTPUT RULES (CRITICAL)\n========================\n- Return VALID JSON only\n- Do NOT explain anything\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Do NOT return multiple objects\n\n========================\nOUTPUT FORMAT (JSON ONLY)\n========================\n{\n  "action": "save | update | delete | none",\n  "memory_text": "full natural language memory text",\n  "confidence": 0.0\n}\n\n========================\nCONFIDENCE GUIDELINES\n========================\n- 0.9‚Äì1.0 ‚Üí Explicit personal fact (e.g., education, name, job)\n- 0.8‚Äì0.9 ‚Üí Clear long-term preference or habit\n- Below 0.7 ‚Üí Weak or ambiguous (avoid storing)\n\n========================\nNO MEMORY CASE\n========================\nIf no valid long-term memory is present, return:\n{\n  "action": "none",\n  "memory_text": "",\n  "confidence": 0.0\n}\n', 'role': 'system'}, {'role': 'user', 'content': 'and one movie is Avengers, its also great movie.. I have also interest in Avengers'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 09:59:04,165 | DEBUG | LiteLLM | 

2026-01-28 09:59:04,165 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 09:59:04,166 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 09:59:04,166 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 09:59:04,167 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:59:04,167 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 09:59:04,168 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Memory Extraction Agent.\n\nYour job is to extract ONLY long-term, personal, user-specific information\nfrom the user\'s message and decide the appropriate memory action.\n\nYou will be called ONLY when memory handling is required.\n\n========================\nWHAT TO EXTRACT\n========================\n- Stable personal facts (name, education, location, job)\n- Long-term preferences (food, language, habits)\n- Information the user would reasonably expect to be remembered\n- Facts that should be retrievable across sessions via semantic search\n\nStore memory as FULL, NATURAL-LANGUAGE TEXT.\nDO NOT create structured keys.\nDO NOT summarize aggressively.\nPreserve meaning exactly as stated.\n\n========================\nWHAT TO IGNORE\n========================\n- Questions\n- Small talk\n- Temporary states (mood, weather, current task)\n- Opinions about content\n- One-time or session-only context\n\n========================\nACTIONS (STRICT)\n========================\nUse exactly ONE action:\n\n- "save"\n  ‚Üí New long-term personal information stated for the first time\n\n- "update"\n  ‚Üí Existing long-term information is explicitly changed,\n    corrected, or overridden\n    (keywords: "now", "instead", "changed", "earlier", "previously")\n\n- "delete"\n  ‚Üí User explicitly asks to forget, remove, or delete information\n\nNEVER guess.\nNEVER infer deletion.\nNEVER revive deleted information.\nDeleted memory must be treated as non-existent.\n\n========================\nIMPORTANT RULES\n========================\n- Store memory as ONE complete sentence\n- Do NOT split into multiple memories\n- Do NOT invent information\n- Do NOT rely on database state\n- Backend will validate save vs update\n- If unsure, return "none"\n\n========================\nOUTPUT RULES (CRITICAL)\n========================\n- Return VALID JSON only\n- Do NOT explain anything\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Do NOT return multiple objects\n\n========================\nOUTPUT FORMAT (JSON ONLY)\n========================\n{\n  "action": "save | update | delete | none",\n  "memory_text": "full natural language memory text",\n  "confidence": 0.0\n}\n\n========================\nCONFIDENCE GUIDELINES\n========================\n- 0.9‚Äì1.0 ‚Üí Explicit personal fact (e.g., education, name, job)\n- 0.8‚Äì0.9 ‚Üí Clear long-term preference or habit\n- Below 0.7 ‚Üí Weak or ambiguous (avoid storing)\n\n========================\nNO MEMORY CASE\n========================\nIf no valid long-term memory is present, return:\n{\n  "action": "none",\n  "memory_text": "",\n  "confidence": 0.0\n}\n', 'role': 'system'}, {'role': 'user', 'content': 'and one movie is Avengers, its also great movie.. I have also interest in Avengers'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 09:59:04,168 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 09:59:04,169 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 09:59:04,170 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 09:59:04,170 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:59:04,171 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:59:04,172 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'and one movie is Avengers, its also great movie.. I have also interest in Avengers'}]}], 'system_instruction': {'parts': [{'text': 'You are a Memory Extraction Agent.\n\nYour job is to extract ONLY long-term, personal, user-specific information\nfrom the user\'s message and decide the appropriate memory action.\n\nYou will be called ONLY when memory handling is required.\n\n========================\nWHAT TO EXTRACT\n========================\n- Stable personal facts (name, education, location, job)\n- Long-term preferences (food, language, habits)\n- Information the user would reasonably expect to be remembered\n- Facts that should be retrievable across sessions via semantic search\n\nStore memory as FULL, NATURAL-LANGUAGE TEXT.\nDO NOT create structured keys.\nDO NOT summarize aggressively.\nPreserve meaning exactly as stated.\n\n========================\nWHAT TO IGNORE\n========================\n- Questions\n- Small talk\n- Temporary states (mood, weather, current task)\n- Opinions about content\n- One-time or session-only context\n\n========================\nACTIONS (STRICT)\n========================\nUse exactly ONE action:\n\n- "save"\n  ‚Üí New long-term personal information stated for the first time\n\n- "update"\n  ‚Üí Existing long-term information is explicitly changed,\n    corrected, or overridden\n    (keywords: "now", "instead", "changed", "earlier", "previously")\n\n- "delete"\n  ‚Üí User explicitly asks to forget, remove, or delete information\n\nNEVER guess.\nNEVER infer deletion.\nNEVER revive deleted information.\nDeleted memory must be treated as non-existent.\n\n========================\nIMPORTANT RULES\n========================\n- Store memory as ONE complete sentence\n- Do NOT split into multiple memories\n- Do NOT invent information\n- Do NOT rely on database state\n- Backend will validate save vs update\n- If unsure, return "none"\n\n========================\nOUTPUT RULES (CRITICAL)\n========================\n- Return VALID JSON only\n- Do NOT explain anything\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Do NOT return multiple objects\n\n========================\nOUTPUT FORMAT (JSON ONLY)\n========================\n{\n  "action": "save | update | delete | none",\n  "memory_text": "full natural language memory text",\n  "confidence": 0.0\n}\n\n========================\nCONFIDENCE GUIDELINES\n========================\n- 0.9‚Äì1.0 ‚Üí Explicit personal fact (e.g., education, name, job)\n- 0.8‚Äì0.9 ‚Üí Clear long-term preference or habit\n- Below 0.7 ‚Üí Weak or ambiguous (avoid storing)\n\n========================\nNO MEMORY CASE\n========================\nIf no valid long-term memory is present, return:\n{\n  "action": "none",\n  "memory_text": "",\n  "confidence": 0.0\n}\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 09:59:04,315 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 04:29:03 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_7585118b5b25b28b90afc887579391ba'), (b'openai-processing-ms', b'94'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'97'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4dbf1a988b8999-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 09:59:04,316 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 09:59:04,317 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 09:59:04,317 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 09:59:04,318 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 09:59:04,318 | DEBUG | openai.agents | Exported 4 items
2026-01-28 09:59:04,320 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 09:59:04,320 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 09:59:04,321 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 09:59:04,322 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 09:59:04,323 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 09:59:05,399 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 04:29:04 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_e1fd16f8b5e36c82f31640fb440d8929'), (b'openai-processing-ms', b'141'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'144'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4dbf21fc498999-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 09:59:05,400 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 09:59:05,400 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 09:59:05,400 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 09:59:05,400 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 09:59:05,401 | DEBUG | openai.agents | Exported 3 items
2026-01-28 09:59:05,520 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "```json\n{\n  \"action\": \"save\",\n  \"memory_text\": \"I have an interest in Avengers.\",\n  \"confidence\": 0.85\n}\n```"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 617,
    "candidatesTokenCount": 41,
    "totalTokenCount": 695,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 617
      }
    ],
    "thoughtsTokenCount": 37
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "kJB5abPhJ9ulg8UPtNqP-QE"
}



2026-01-28 09:59:05,521 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 09:59:05,522 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 09:59:05,522 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 09:59:05,523 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:59:05,523 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 09:59:05,524 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:59:05,524 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:59:05,524 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:59:05,525 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:59:05,525 | DEBUG | LiteLLM | response_cost: 0.0003801
2026-01-28 09:59:05,526 | DEBUG | openai.agents | Received model response
2026-01-28 09:59:05,526 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:59:05,528 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001949B5CD0F0>>
2026-01-28 09:59:05,530 | DEBUG | LiteLLM | response_cost: 0.0003801
2026-01-28 09:59:05,530 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 09:59:05,531 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:59:05,531 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 09:59:05,532 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 09:59:05,532 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:59:05,533 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:59:05,533 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 09:59:05,534 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:59:05,534 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:59:05,535 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:59:05,535 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 09:59:05,535 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:59:05,536 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:59:05,536 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:59:05,537 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:59:05,537 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:59:05,537 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 09:59:05,538 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:59:05,538 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0003801
2026-01-28 09:59:05,539 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:59:05,539 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:59:05,540 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:59:05,541 | DEBUG | openai.agents | Resetting current trace
2026-01-28 09:59:05,541 | INFO | orchestrator | üß† MEMORY_DECISION | {
  "action": "save",
  "memory_text": "I have an interest in Avengers.",
  "confidence": 0.85
}
2026-01-28 09:59:05,541 | INFO | chat | üß† MEMORY_EVENT_RECEIVED | {'action': 'save', 'memory_text': 'I have an interest in Avengers.', 'confidence': 0.85}
2026-01-28 09:59:05,543 | INFO | memory_action_executor | üß† APPLY_MEMORY_ACTION | user_id=3 | action={'action': 'save', 'memory_text': 'I have an interest in Avengers.', 'confidence': 0.85}
2026-01-28 09:59:05,543 | INFO | memory_action_executor | üß† Memory Parsed | type=save | content=I have an interest in Avengers. | confidence=0.85
2026-01-28 09:59:05,544 | INFO | memory_action_executor | üíæ Writing memory to DB
2026-01-28 09:59:05,576 | INFO | memory_action_executor | ‚úÖ DB COMMIT SUCCESS
2026-01-28 09:59:05,577 | INFO | orchestrator | üß† REASONING_INPUT_SNAPSHOT
{
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful AI assistant.\n \nYou may answer ANY general knowledge question.\n \nConversation summaries and memories are ONLY to provide helpful context\n(e.g. preferences, ongoing plans).\n \nIf a user asks something unrelated to the current topic,\nanswer it normally using general knowledge.\n \nDo NOT restrict yourself to previous topics unless the user explicitly asks."
    },
    {
      "role": "assistant",
      "content": "I am an AI and do not have access to your personal information, including where you live."
    },
    {
      "role": "user",
      "content": "i lives in fatehabad"
    },
    {
      "role": "assistant",
      "content": "Thank you for letting me know."
    },
    {
      "role": "user",
      "content": "yesterday i saw a movie .. i like that movie..Dabang 2 is full of joy and drama. Its a thrillar movie.. I like to watch again and again"
    },
    {
      "role": "assistant",
      "content": "It sounds like you really enjoyed Dabangg 2 and appreciate its mix of joy, drama, and thrills!"
    },
    {
      "role": "user",
      "content": "and one movie is Avengers, its also great movie.. I have also interest in Avengers"
    },
    {
      "role": "user",
      "content": "and one movie is Avengers, its also great movie.. I have also interest in Avengers"
    }
  ],
  "safety": {
    "safe": true,
    "message": null
  },
  "memory_action": {},
  "memory_data": [
    {
      "memory_id": 11,
      "user_id": 3,
      "text": "I like the movie Dabang 2 and I like to watch it again and again.",
      "confidence": 0.85,
      "created_at": "2026-01-27T22:49:49.349202+05:30",
      "expires_at": "2026-02-27T09:57:54.723279+05:30",
      "score": 0.39756011962890625
    }
  ],
  "memory_found": true,
  "tool_context": {}
}
2026-01-28 09:59:05,578 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_3f45214caa7b492688f153a50ac26dee
2026-01-28 09:59:05,579 | DEBUG | openai.agents | Setting current trace: trace_3f45214caa7b492688f153a50ac26dee
2026-01-28 09:59:05,579 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000001949BA5EF20> with id None
2026-01-28 09:59:05,579 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000001949BA94BE0> with id None
2026-01-28 09:59:05,580 | DEBUG | openai.agents | Calling LLM
2026-01-28 09:59:05,580 | DEBUG | LiteLLM | 

2026-01-28 09:59:05,580 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 09:59:05,580 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "assistant", "content": "I am an AI and do not have access to your personal information, including where you live."}, {"role": "user", "content": "i lives in fatehabad"}, {"role": "assistant", "content": "Thank you for letting me know."}, {"role": "user", "content": "yesterday i saw a movie .. i like that movie..Dabang 2 is full of joy and drama. Its a thrillar movie.. I like to watch again and again"}, {"role": "assistant", "content": "It sounds like you really enjoyed Dabangg 2 and appreciate its mix of joy, drama, and thrills!"}, {"role": "user", "content": "and one movie is Avengers, its also great movie.. I have also interest in Avengers"}, {"role": "user", "content": "and one movie is Avengers, its also great movie.. I have also interest in Avengers"}], "safety": {"safe": true, "message": null}, "memory_action": {}, "memory_data": [{"memory_id": 11, "user_id": 3, "text": "I like the movie Dabang 2 and I like to watch it again and again.", "confidence": 0.85, "created_at": "2026-01-27T22:49:49.349202+05:30", "expires_at": "2026-02-27T09:57:54.723279+05:30", "score": 0.39756011962890625}], "memory_found": true, "tool_context": {}}'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=True, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 09:59:05,581 | DEBUG | LiteLLM | 

2026-01-28 09:59:05,581 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 09:59:05,582 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 09:59:05,582 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 09:59:05,583 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:59:05,584 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 09:59:05,584 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "assistant", "content": "I am an AI and do not have access to your personal information, including where you live."}, {"role": "user", "content": "i lives in fatehabad"}, {"role": "assistant", "content": "Thank you for letting me know."}, {"role": "user", "content": "yesterday i saw a movie .. i like that movie..Dabang 2 is full of joy and drama. Its a thrillar movie.. I like to watch again and again"}, {"role": "assistant", "content": "It sounds like you really enjoyed Dabangg 2 and appreciate its mix of joy, drama, and thrills!"}, {"role": "user", "content": "and one movie is Avengers, its also great movie.. I have also interest in Avengers"}, {"role": "user", "content": "and one movie is Avengers, its also great movie.. I have also interest in Avengers"}], "safety": {"safe": true, "message": null}, "memory_action": {}, "memory_data": [{"memory_id": 11, "user_id": 3, "text": "I like the movie Dabang 2 and I like to watch it again and again.", "confidence": 0.85, "created_at": "2026-01-27T22:49:49.349202+05:30", "expires_at": "2026-02-27T09:57:54.723279+05:30", "score": 0.39756011962890625}], "memory_found": true, "tool_context": {}}'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 09:59:05,585 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': True}
2026-01-28 09:59:05,585 | DEBUG | LiteLLM | Final returned optional params: {'stream': True}
2026-01-28 09:59:05,585 | DEBUG | LiteLLM | self.optional_params: {'stream': True}
2026-01-28 09:59:05,586 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:59:05,586 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:59:05,587 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?key=*****=sse \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "assistant", "content": "I am an AI and do not have access to your personal information, including where you live."}, {"role": "user", "content": "i lives in fatehabad"}, {"role": "assistant", "content": "Thank you for letting me know."}, {"role": "user", "content": "yesterday i saw a movie .. i like that movie..Dabang 2 is full of joy and drama. Its a thrillar movie.. I like to watch again and again"}, {"role": "assistant", "content": "It sounds like you really enjoyed Dabangg 2 and appreciate its mix of joy, drama, and thrills!"}, {"role": "user", "content": "and one movie is Avengers, its also great movie.. I have also interest in Avengers"}, {"role": "user", "content": "and one movie is Avengers, its also great movie.. I have also interest in Avengers"}], "safety": {"safe": true, "message": null}, "memory_action": {}, "memory_data": [{"memory_id": 11, "user_id": 3, "text": "I like the movie Dabang 2 and I like to watch it again and again.", "confidence": 0.85, "created_at": "2026-01-27T22:49:49.349202+05:30", "expires_at": "2026-02-27T09:57:54.723279+05:30", "score": 0.39756011962890625}], "memory_found": true, "tool_context": {}}'}]}], 'system_instruction': {'parts': [{'text': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.'}]}, 'generationConfig': {}}'
[0m

2026-01-28 09:59:05,587 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 09:59:05,588 | DEBUG | LiteLLM | messages in token_counter: None, text in token_counter: 
2026-01-28 09:59:05,588 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:59:05,588 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:59:05,589 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:59:05,589 | DEBUG | LiteLLM | response_cost: 0.0
2026-01-28 09:59:07,113 | DEBUG | LiteLLM | RAW RESPONSE:
first stream response received


2026-01-28 09:59:07,114 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': 'It sounds like you also really enjoy "Avengers"! It\'s definitely a popular and exciting movie.'}], 'role': 'model'}, 'finishReason': 'STOP', 'index': 0}], 'usageMetadata': {'promptTokenCount': 598, 'candidatesTokenCount': 20, 'totalTokenCount': 664, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 598}], 'thoughtsTokenCount': 46}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'kZB5ab3POYaDg8UP-8TWuQQ'}
2026-01-28 09:59:07,114 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='kZB5ab3POYaDg8UP-8TWuQQ', created=1769574547, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='It sounds like you also really enjoy "Avengers"! It\'s definitely a popular and exciting movie.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=66, prompt_tokens=598, total_tokens=664, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=46, rejected_prediction_tokens=None, text_tokens=20, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=598, image_tokens=None)))
2026-01-28 09:59:07,116 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content='It sounds like you also really enjoy "Avengers"! It\'s definitely a popular and exciting movie.', role='assistant', function_call=None, tool_calls=None, audio=None)
2026-01-28 09:59:07,117 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='kZB5ab3POYaDg8UP-8TWuQQ', created=1769574547, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='It sounds like you also really enjoy "Avengers"! It\'s definitely a popular and exciting movie.', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=66, prompt_tokens=598, total_tokens=664, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=46, rejected_prediction_tokens=None, text_tokens=20, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=598, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-28 09:59:07,119 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=False
2026-01-28 09:59:07,119 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=False
2026-01-28 09:59:07,119 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 09:59:07,120 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 09:59:07,120 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 09:59:07,120 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 09:59:07,121 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:59:07,121 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:59:07,122 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:59:07,122 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:59:07,122 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:59:07,122 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:59:07,123 | DEBUG | LiteLLM | response_cost: 0.0003444
2026-01-28 09:59:07,123 | DEBUG | LiteLLM | response_cost: 0.0003444
2026-01-28 09:59:07,124 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:59:07,124 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0003444
2026-01-28 09:59:07,124 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:59:07,125 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 09:59:07,125 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:59:07,126 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 09:59:07,127 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 09:59:07,128 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 09:59:07,128 | DEBUG | openai.agents | Resetting current trace
2026-01-28 09:59:07,141 | INFO | session_summary | üß† SUMMARY_START | session_id=ef5f092b-2ec0-4839-921b-a27aef540c8d
2026-01-28 09:59:07,145 | INFO | session_summary | üìä Unsummarized messages = 8
2026-01-28 09:59:07,146 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-28 09:59:07,146 | INFO | chat | ‚úÖ Stream complete | tokens=1 | time=6.35s
2026-01-28 09:59:10,541 | DEBUG | httpcore.connection | close.started
2026-01-28 09:59:10,542 | DEBUG | httpcore.connection | close.complete
2026-01-28 09:59:10,542 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 09:59:10,550 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001949B527430>
2026-01-28 09:59:10,550 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x00000194900B9BC0> server_hostname='api.openai.com' timeout=5.0
2026-01-28 09:59:10,561 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001949C2E2800>
2026-01-28 09:59:10,562 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 09:59:10,562 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 09:59:10,563 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 09:59:10,563 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 09:59:10,564 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 09:59:11,245 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 04:29:10 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_8f185e6eb0b6d96fef729bdf09c051a6'), (b'openai-processing-ms', b'128'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'131'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4dbf48fd13f806-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 09:59:11,246 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 09:59:11,246 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 09:59:11,246 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 09:59:11,247 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 09:59:11,247 | DEBUG | openai.agents | Exported 5 items
2026-01-28 10:00:03,769 | INFO | chat | üì© Chat request | user_id=3 | request_id=f758f9e1-0603-4c5f-a5fe-1d78526491a7
2026-01-28 10:00:03,771 | INFO | chat | üßë User message | which type of movies i like the most
2026-01-28 10:00:03,803 | INFO | chat | ü§ñ Stream started | session_id=f4af0964-d758-4273-9910-07a650e6b30f | request_id=f758f9e1-0603-4c5f-a5fe-1d78526491a7
2026-01-28 10:00:03,806 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-28 10:00:03,810 | INFO | orchestrator | üßë USER_INPUT | which type of movies i like the most
2026-01-28 10:00:03,813 | INFO | orchestrator | üß© CONTEXT | user_id=3 | enable_memory=True | db=None
2026-01-28 10:00:03,815 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-28 10:00:03,817 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_1b0642cbad7445de9e00f3a1e4aa439f
2026-01-28 10:00:03,817 | DEBUG | openai.agents | Setting current trace: trace_1b0642cbad7445de9e00f3a1e4aa439f
2026-01-28 10:00:03,818 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000001949BA5F3D0> with id None
2026-01-28 10:00:03,818 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-28 10:00:03,821 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000001949BA954E0> with id None
2026-01-28 10:00:03,822 | DEBUG | openai.agents | Calling LLM
2026-01-28 10:00:03,823 | DEBUG | LiteLLM | 

2026-01-28 10:00:03,824 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 10:00:03,825 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n', 'role': 'system'}, {'role': 'user', 'content': 'which type of movies i like the most'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 10:00:03,826 | DEBUG | LiteLLM | 

2026-01-28 10:00:03,828 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 10:00:03,829 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 10:00:03,830 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 10:00:03,833 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 10:00:03,834 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 10:00:03,837 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n', 'role': 'system'}, {'role': 'user', 'content': 'which type of movies i like the most'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 10:00:03,838 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 10:00:03,839 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 10:00:03,840 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 10:00:03,841 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:00:03,844 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:00:03,845 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'which type of movies i like the most'}]}], 'system_instruction': {'parts': [{'text': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 10:00:05,485 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "SAFE"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 195,
    "candidatesTokenCount": 1,
    "totalTokenCount": 290,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 195
      }
    ],
    "thoughtsTokenCount": 94
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "zJB5aZPgJ7unjuMPxLra4AM"
}



2026-01-28 10:00:05,489 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 10:00:05,491 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 10:00:05,491 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 10:00:05,492 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:00:05,493 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 10:00:05,494 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:00:05,495 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:00:05,496 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:00:05,497 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:00:05,498 | DEBUG | LiteLLM | response_cost: 0.00029600000000000004
2026-01-28 10:00:05,499 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:00:05,500 | DEBUG | openai.agents | Received model response
2026-01-28 10:00:05,501 | DEBUG | LiteLLM | response_cost: 0.00029600000000000004
2026-01-28 10:00:05,502 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001949BA9D1E0>>
2026-01-28 10:00:05,504 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 10:00:05,505 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 10:00:05,505 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:00:05,507 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 10:00:05,506 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 10:00:05,507 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:00:05,509 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 10:00:05,511 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 10:00:05,512 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 10:00:05,513 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 10:00:05,513 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:00:05,514 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 10:00:05,515 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:00:05,516 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:00:05,517 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:00:05,520 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:00:05,523 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:00:05,524 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 10:00:05,525 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.00029600000000000004
2026-01-28 10:00:05,527 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 10:00:05,529 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:00:05,530 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:00:05,532 | DEBUG | openai.agents | Resetting current trace
2026-01-28 10:00:05,533 | ERROR | digital_human_sdk.app.intelligence.utils.json_utils | safe_json_loads failed
2026-01-28 10:00:05,535 | INFO | orchestrator | ‚úÖ Safety passed
2026-01-28 10:00:05,537 | INFO | orchestrator | üì• Fetching semantic memory (via MemoryService)
2026-01-28 10:00:05,592 | INFO | orchestrator | üß† MEMORY_RESULT | found=True | count=2
2026-01-28 10:00:05,594 | INFO | orchestrator | üß≠ Router agent called
2026-01-28 10:00:05,596 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_4a8e0ac970484e5f8919c2892757235e
2026-01-28 10:00:05,597 | DEBUG | openai.agents | Setting current trace: trace_4a8e0ac970484e5f8919c2892757235e
2026-01-28 10:00:05,597 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000001949BA5D990> with id None
2026-01-28 10:00:05,598 | DEBUG | openai.agents | Running agent RouterAgent (turn 1)
2026-01-28 10:00:05,600 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000001949BA97D60> with id None
2026-01-28 10:00:05,600 | DEBUG | openai.agents | Calling LLM
2026-01-28 10:00:05,601 | DEBUG | LiteLLM | 

2026-01-28 10:00:05,602 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 10:00:05,603 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'which type of movies i like the most'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 10:00:05,604 | DEBUG | LiteLLM | 

2026-01-28 10:00:05,605 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 10:00:05,606 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 10:00:05,607 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 10:00:05,610 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 10:00:05,612 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 10:00:05,614 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'which type of movies i like the most'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 10:00:05,616 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 10:00:05,617 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 10:00:05,618 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 10:00:05,619 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:00:05,621 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:00:05,623 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'which type of movies i like the most'}]}], 'system_instruction': {'parts': [{'text': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 10:00:07,275 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\n  \"use_memory\": true,\n  \"use_tool\": false,\n  \"tool_name\": \"none\",\n  \"tool_arguments\": null,\n  \"intent\": \"read\",\n  \"memory_key\": \"movie_preference\"\n}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 493,
    "candidatesTokenCount": 60,
    "totalTokenCount": 611,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 493
      }
    ],
    "thoughtsTokenCount": 58
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "zpB5abGiHI3_g8UPweG9oQU"
}



2026-01-28 10:00:07,278 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 10:00:07,281 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 10:00:07,281 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 10:00:07,282 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:00:07,283 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 10:00:07,284 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:00:07,285 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:00:07,287 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:00:07,288 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:00:07,289 | DEBUG | LiteLLM | response_cost: 0.0004429
2026-01-28 10:00:07,291 | DEBUG | openai.agents | Received model response
2026-01-28 10:00:07,290 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:00:07,292 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001949BA9F640>>
2026-01-28 10:00:07,293 | DEBUG | LiteLLM | response_cost: 0.0004429
2026-01-28 10:00:07,295 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 10:00:07,298 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 10:00:07,299 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 10:00:07,299 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 10:00:07,300 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:00:07,301 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 10:00:07,305 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 10:00:07,306 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:00:07,307 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 10:00:07,308 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:00:07,309 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 10:00:07,310 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:00:07,311 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:00:07,313 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 10:00:07,314 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:00:07,326 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:00:07,327 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 10:00:07,328 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:00:07,329 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0004429
2026-01-28 10:00:07,332 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 10:00:07,333 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:00:07,334 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:00:07,337 | DEBUG | openai.agents | Resetting current trace
2026-01-28 10:00:07,339 | INFO | orchestrator | üß≠ ROUTER_DECISION | tool=False | memory=True | intent=read
2026-01-28 10:00:07,340 | INFO | orchestrator | üß† REASONING_INPUT_SNAPSHOT
{
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful AI assistant.\n \nYou may answer ANY general knowledge question.\n \nConversation summaries and memories are ONLY to provide helpful context\n(e.g. preferences, ongoing plans).\n \nIf a user asks something unrelated to the current topic,\nanswer it normally using general knowledge.\n \nDo NOT restrict yourself to previous topics unless the user explicitly asks."
    },
    {
      "role": "assistant",
      "content": "It sounds like you really enjoy movies, and \"Jab We Met\" is a great choice! It's a very popular and entertaining film. What do you like about it?"
    },
    {
      "role": "user",
      "content": "tell me their characters "
    },
    {
      "role": "assistant",
      "content": "The main characters in \"Jab We Met\" are:\n\n*   **Geet Dhillon** (played by Kareena Kapoor Khan): A lively, talkative, and optimistic young woman who is full of life and dreams. She runs away from home to be with her boyfriend and is the catalyst for much of the story's events.\n*   **Aditya Kashyap** (played by Shahid Kapoor): A quiet, depressed, and wealthy businessman who is on the verge of suicide after a breakup and business failure. His accidental encounter with Geet changes his perspective on life."
    },
    {
      "role": "user",
      "content": "tell me its culture and idea of this film"
    },
    {
      "role": "assistant",
      "content": "\"Jab We Met\" beautifully showcases aspects of **Indian culture**, particularly **Punjabi culture** through Geet's family and background, their lively nature, and the depiction of their home and traditions. The film also touches on the typical **Indian railway journey experience**, which is a significant part of the initial plot. Beyond specific regional culture, it reflects broader Indian societal values concerning family, marriage, and personal dreams versus expectations.\n\nThe core **idea or theme** of the film revolves around:\n\n*   **Self-discovery and transformation:** It explores how two very different individuals, Geet and Aditya, help each other find themselves and overcome their personal struggles. Geet learns about facing reality and mature love, while Aditya rediscovers his zest for life and passion.\n*   **The power of positivity and embracing life:** Geet's infectious optimism and 'live in the moment' philosophy significantly impact Aditya, pulling him out of his depression. The film suggests that even in despair, there's always a reason to embrace life.\n*   **Finding love in unexpected places:** It's a classic romantic story about two strangers whose paths cross serendipitously, leading to a profound connection that transforms both their lives.\n*   **Overcoming past baggage:** Both characters are dealing with past heartbreaks and disappointments, and the film shows their journey towards healing and moving forward."
    },
    {
      "role": "user",
      "content": "which type of movies i like the most"
    },
    {
      "role": "user",
      "content": "which type of movies i like the most"
    }
  ],
  "safety": {
    "safe": true,
    "message": null
  },
  "memory_action": {},
  "memory_data": [
    {
      "memory_id": 11,
      "user_id": 3,
      "text": "I like the movie Dabang 2 and I like to watch it again and again.",
      "confidence": 0.85,
      "created_at": "2026-01-27T22:49:49.349202+05:30",
      "expires_at": "2026-02-27T09:57:54.723279+05:30",
      "score": 0.48043105006217957
    },
    {
      "memory_id": 13,
      "user_id": 3,
      "text": "I have an interest in Avengers.",
      "confidence": 0.85,
      "created_at": "2026-01-28T09:59:05.566766+05:30",
      "expires_at": "2026-02-27T09:59:05.564860+05:30",
      "score": 0.36779892444610596
    }
  ],
  "memory_found": true,
  "tool_context": {}
}
2026-01-28 10:00:07,344 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_f7daa593d331403a8a684422612b72f3
2026-01-28 10:00:07,346 | DEBUG | openai.agents | Setting current trace: trace_f7daa593d331403a8a684422612b72f3
2026-01-28 10:00:07,346 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000001949B5E52B0> with id None
2026-01-28 10:00:07,348 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000001949BA940A0> with id None
2026-01-28 10:00:07,349 | DEBUG | openai.agents | Calling LLM
2026-01-28 10:00:07,351 | DEBUG | LiteLLM | 

2026-01-28 10:00:07,351 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 10:00:07,352 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "assistant", "content": "It sounds like you really enjoy movies, and \\"Jab We Met\\" is a great choice! It\'s a very popular and entertaining film. What do you like about it?"}, {"role": "user", "content": "tell me their characters "}, {"role": "assistant", "content": "The main characters in \\"Jab We Met\\" are:\\n\\n*   **Geet Dhillon** (played by Kareena Kapoor Khan): A lively, talkative, and optimistic young woman who is full of life and dreams. She runs away from home to be with her boyfriend and is the catalyst for much of the story\'s events.\\n*   **Aditya Kashyap** (played by Shahid Kapoor): A quiet, depressed, and wealthy businessman who is on the verge of suicide after a breakup and business failure. His accidental encounter with Geet changes his perspective on life."}, {"role": "user", "content": "tell me its culture and idea of this film"}, {"role": "assistant", "content": "\\"Jab We Met\\" beautifully showcases aspects of **Indian culture**, particularly **Punjabi culture** through Geet\'s family and background, their lively nature, and the depiction of their home and traditions. The film also touches on the typical **Indian railway journey experience**, which is a significant part of the initial plot. Beyond specific regional culture, it reflects broader Indian societal values concerning family, marriage, and personal dreams versus expectations.\\n\\nThe core **idea or theme** of the film revolves around:\\n\\n*   **Self-discovery and transformation:** It explores how two very different individuals, Geet and Aditya, help each other find themselves and overcome their personal struggles. Geet learns about facing reality and mature love, while Aditya rediscovers his zest for life and passion.\\n*   **The power of positivity and embracing life:** Geet\'s infectious optimism and \'live in the moment\' philosophy significantly impact Aditya, pulling him out of his depression. The film suggests that even in despair, there\'s always a reason to embrace life.\\n*   **Finding love in unexpected places:** It\'s a classic romantic story about two strangers whose paths cross serendipitously, leading to a profound connection that transforms both their lives.\\n*   **Overcoming past baggage:** Both characters are dealing with past heartbreaks and disappointments, and the film shows their journey towards healing and moving forward."}, {"role": "user", "content": "which type of movies i like the most"}, {"role": "user", "content": "which type of movies i like the most"}], "safety": {"safe": true, "message": null}, "memory_action": {}, "memory_data": [{"memory_id": 11, "user_id": 3, "text": "I like the movie Dabang 2 and I like to watch it again and again.", "confidence": 0.85, "created_at": "2026-01-27T22:49:49.349202+05:30", "expires_at": "2026-02-27T09:57:54.723279+05:30", "score": 0.48043105006217957}, {"memory_id": 13, "user_id": 3, "text": "I have an interest in Avengers.", "confidence": 0.85, "created_at": "2026-01-28T09:59:05.566766+05:30", "expires_at": "2026-02-27T09:59:05.564860+05:30", "score": 0.36779892444610596}], "memory_found": true, "tool_context": {}}'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=True, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 10:00:07,355 | DEBUG | LiteLLM | 

2026-01-28 10:00:07,355 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 10:00:07,356 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 10:00:07,357 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 10:00:07,360 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 10:00:07,362 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 10:00:07,364 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "assistant", "content": "It sounds like you really enjoy movies, and \\"Jab We Met\\" is a great choice! It\'s a very popular and entertaining film. What do you like about it?"}, {"role": "user", "content": "tell me their characters "}, {"role": "assistant", "content": "The main characters in \\"Jab We Met\\" are:\\n\\n*   **Geet Dhillon** (played by Kareena Kapoor Khan): A lively, talkative, and optimistic young woman who is full of life and dreams. She runs away from home to be with her boyfriend and is the catalyst for much of the story\'s events.\\n*   **Aditya Kashyap** (played by Shahid Kapoor): A quiet, depressed, and wealthy businessman who is on the verge of suicide after a breakup and business failure. His accidental encounter with Geet changes his perspective on life."}, {"role": "user", "content": "tell me its culture and idea of this film"}, {"role": "assistant", "content": "\\"Jab We Met\\" beautifully showcases aspects of **Indian culture**, particularly **Punjabi culture** through Geet\'s family and background, their lively nature, and the depiction of their home and traditions. The film also touches on the typical **Indian railway journey experience**, which is a significant part of the initial plot. Beyond specific regional culture, it reflects broader Indian societal values concerning family, marriage, and personal dreams versus expectations.\\n\\nThe core **idea or theme** of the film revolves around:\\n\\n*   **Self-discovery and transformation:** It explores how two very different individuals, Geet and Aditya, help each other find themselves and overcome their personal struggles. Geet learns about facing reality and mature love, while Aditya rediscovers his zest for life and passion.\\n*   **The power of positivity and embracing life:** Geet\'s infectious optimism and \'live in the moment\' philosophy significantly impact Aditya, pulling him out of his depression. The film suggests that even in despair, there\'s always a reason to embrace life.\\n*   **Finding love in unexpected places:** It\'s a classic romantic story about two strangers whose paths cross serendipitously, leading to a profound connection that transforms both their lives.\\n*   **Overcoming past baggage:** Both characters are dealing with past heartbreaks and disappointments, and the film shows their journey towards healing and moving forward."}, {"role": "user", "content": "which type of movies i like the most"}, {"role": "user", "content": "which type of movies i like the most"}], "safety": {"safe": true, "message": null}, "memory_action": {}, "memory_data": [{"memory_id": 11, "user_id": 3, "text": "I like the movie Dabang 2 and I like to watch it again and again.", "confidence": 0.85, "created_at": "2026-01-27T22:49:49.349202+05:30", "expires_at": "2026-02-27T09:57:54.723279+05:30", "score": 0.48043105006217957}, {"memory_id": 13, "user_id": 3, "text": "I have an interest in Avengers.", "confidence": 0.85, "created_at": "2026-01-28T09:59:05.566766+05:30", "expires_at": "2026-02-27T09:59:05.564860+05:30", "score": 0.36779892444610596}], "memory_found": true, "tool_context": {}}'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 10:00:07,365 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': True}
2026-01-28 10:00:07,366 | DEBUG | LiteLLM | Final returned optional params: {'stream': True}
2026-01-28 10:00:07,368 | DEBUG | LiteLLM | self.optional_params: {'stream': True}
2026-01-28 10:00:07,369 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:00:07,371 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:00:07,373 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?key=*****=sse \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "assistant", "content": "It sounds like you really enjoy movies, and \\"Jab We Met\\" is a great choice! It\'s a very popular and entertaining film. What do you like about it?"}, {"role": "user", "content": "tell me their characters "}, {"role": "assistant", "content": "The main characters in \\"Jab We Met\\" are:\\n\\n*   **Geet Dhillon** (played by Kareena Kapoor Khan): A lively, talkative, and optimistic young woman who is full of life and dreams. She runs away from home to be with her boyfriend and is the catalyst for much of the story\'s events.\\n*   **Aditya Kashyap** (played by Shahid Kapoor): A quiet, depressed, and wealthy businessman who is on the verge of suicide after a breakup and business failure. His accidental encounter with Geet changes his perspective on life."}, {"role": "user", "content": "tell me its culture and idea of this film"}, {"role": "assistant", "content": "\\"Jab We Met\\" beautifully showcases aspects of **Indian culture**, particularly **Punjabi culture** through Geet\'s family and background, their lively nature, and the depiction of their home and traditions. The film also touches on the typical **Indian railway journey experience**, which is a significant part of the initial plot. Beyond specific regional culture, it reflects broader Indian societal values concerning family, marriage, and personal dreams versus expectations.\\n\\nThe core **idea or theme** of the film revolves around:\\n\\n*   **Self-discovery and transformation:** It explores how two very different individuals, Geet and Aditya, help each other find themselves and overcome their personal struggles. Geet learns about facing reality and mature love, while Aditya rediscovers his zest for life and passion.\\n*   **The power of positivity and embracing life:** Geet\'s infectious optimism and \'live in the moment\' philosophy significantly impact Aditya, pulling him out of his depression. The film suggests that even in despair, there\'s always a reason to embrace life.\\n*   **Finding love in unexpected places:** It\'s a classic romantic story about two strangers whose paths cross serendipitously, leading to a profound connection that transforms both their lives.\\n*   **Overcoming past baggage:** Both characters are dealing with past heartbreaks and disappointments, and the film shows their journey towards healing and moving forward."}, {"role": "user", "content": "which type of movies i like the most"}, {"role": "user", "content": "which type of movies i like the most"}], "safety": {"safe": true, "message": null}, "memory_action": {}, "memory_data": [{"memory_id": 11, "user_id": 3, "text": "I like the movie Dabang 2 and I like to watch it again and again.", "confidence": 0.85, "created_at": "2026-01-27T22:49:49.349202+05:30", "expires_at": "2026-02-27T09:57:54.723279+05:30", "score": 0.48043105006217957}, {"memory_id": 13, "user_id": 3, "text": "I have an interest in Avengers.", "confidence": 0.85, "created_at": "2026-01-28T09:59:05.566766+05:30", "expires_at": "2026-02-27T09:59:05.564860+05:30", "score": 0.36779892444610596}], "memory_found": true, "tool_context": {}}'}]}], 'system_instruction': {'parts': [{'text': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.'}]}, 'generationConfig': {}}'
[0m

2026-01-28 10:00:07,375 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 10:00:07,376 | DEBUG | LiteLLM | messages in token_counter: None, text in token_counter: 
2026-01-28 10:00:07,378 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:00:07,379 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:00:07,380 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:00:07,382 | DEBUG | LiteLLM | response_cost: 0.0
2026-01-28 10:00:07,785 | DEBUG | httpcore.connection | close.started
2026-01-28 10:00:07,786 | DEBUG | httpcore.connection | close.complete
2026-01-28 10:00:07,787 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 10:00:07,804 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001949BA6FB50>
2026-01-28 10:00:07,804 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x00000194900B9BC0> server_hostname='api.openai.com' timeout=5.0
2026-01-28 10:00:07,818 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001949BA6E950>
2026-01-28 10:00:07,819 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 10:00:07,820 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 10:00:07,821 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 10:00:07,822 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 10:00:07,822 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 10:00:08,809 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 04:30:08 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_d70162690b089ae4157e840464ad0650'), (b'openai-processing-ms', b'121'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'125'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4dc0aed9465989-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 10:00:08,810 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 10:00:08,811 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 10:00:08,811 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 10:00:08,811 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 10:00:08,811 | DEBUG | openai.agents | Exported 7 items
2026-01-28 10:00:09,607 | DEBUG | LiteLLM | RAW RESPONSE:
first stream response received


2026-01-28 10:00:09,609 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': 'Based on our previous conversations and your'}], 'role': 'model'}, 'index': 0}], 'usageMetadata': {'promptTokenCount': 1076, 'candidatesTokenCount': 7, 'totalTokenCount': 1342, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 1076}], 'thoughtsTokenCount': 259}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'z5B5aYCCG9zn4-EP7Ii_gAQ'}
2026-01-28 10:00:09,612 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='z5B5aYCCG9zn4-EP7Ii_gAQ', created=1769574609, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='Based on our previous conversations and your', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=266, prompt_tokens=1076, total_tokens=1342, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=259, rejected_prediction_tokens=None, text_tokens=7, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=1076, image_tokens=None)))
2026-01-28 10:00:09,615 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content='Based on our previous conversations and your', role='assistant', function_call=None, tool_calls=None, audio=None)
2026-01-28 10:00:09,616 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='z5B5aYCCG9zn4-EP7Ii_gAQ', created=1769574609, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Based on our previous conversations and your', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=266, prompt_tokens=1076, total_tokens=1342, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=259, rejected_prediction_tokens=None, text_tokens=7, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=1076, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-28 10:00:09,619 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': ' interests, it seems you enjoy:\n\n*   **Action films**, particularly those with a superhero theme like "Avengers" and Bollywood action films like "Dabang 2."\n*   **Romantic comedies**, as we\'ve discussed "'}], 'role': 'model'}, 'index': 0}], 'usageMetadata': {'promptTokenCount': 1076, 'candidatesTokenCount': 55, 'totalTokenCount': 1390, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 1076}], 'thoughtsTokenCount': 259}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'z5B5aYCCG9zn4-EP7Ii_gAQ'}
2026-01-28 10:00:09,623 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='z5B5aYCCG9zn4-EP7Ii_gAQ', created=1769574609, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content=' interests, it seems you enjoy:\n\n*   **Action films**, particularly those with a superhero theme like "Avengers" and Bollywood action films like "Dabang 2."\n*   **Romantic comedies**, as we\'ve discussed "', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=314, prompt_tokens=1076, total_tokens=1390, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=259, rejected_prediction_tokens=None, text_tokens=55, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=1076, image_tokens=None)))
2026-01-28 10:00:09,626 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content=' interests, it seems you enjoy:\n\n*   **Action films**, particularly those with a superhero theme like "Avengers" and Bollywood action films like "Dabang 2."\n*   **Romantic comedies**, as we\'ve discussed "', role=None, function_call=None, tool_calls=None, audio=None)
2026-01-28 10:00:09,627 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='z5B5aYCCG9zn4-EP7Ii_gAQ', created=1769574609, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' interests, it seems you enjoy:\n\n*   **Action films**, particularly those with a superhero theme like "Avengers" and Bollywood action films like "Dabang 2."\n*   **Romantic comedies**, as we\'ve discussed "', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=314, prompt_tokens=1076, total_tokens=1390, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=259, rejected_prediction_tokens=None, text_tokens=55, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=1076, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-28 10:00:09,745 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': 'Jab We Met."\n\nYou appear to have a preference for engaging and entertaining cinema, spanning both action-packed blockbusters and heartfelt romantic stories.'}], 'role': 'model'}, 'finishReason': 'STOP', 'index': 0}], 'usageMetadata': {'promptTokenCount': 1076, 'candidatesTokenCount': 84, 'totalTokenCount': 1419, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 1076}], 'thoughtsTokenCount': 259}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'z5B5aYCCG9zn4-EP7Ii_gAQ'}
2026-01-28 10:00:09,748 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='z5B5aYCCG9zn4-EP7Ii_gAQ', created=1769574609, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='Jab We Met."\n\nYou appear to have a preference for engaging and entertaining cinema, spanning both action-packed blockbusters and heartfelt romantic stories.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=343, prompt_tokens=1076, total_tokens=1419, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=259, rejected_prediction_tokens=None, text_tokens=84, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=1076, image_tokens=None)))
2026-01-28 10:00:09,752 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content='Jab We Met."\n\nYou appear to have a preference for engaging and entertaining cinema, spanning both action-packed blockbusters and heartfelt romantic stories.', role=None, function_call=None, tool_calls=None, audio=None)
2026-01-28 10:00:09,753 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='z5B5aYCCG9zn4-EP7Ii_gAQ', created=1769574609, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Jab We Met."\n\nYou appear to have a preference for engaging and entertaining cinema, spanning both action-packed blockbusters and heartfelt romantic stories.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=343, prompt_tokens=1076, total_tokens=1419, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=259, rejected_prediction_tokens=None, text_tokens=84, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=1076, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-28 10:00:09,762 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=False
2026-01-28 10:00:09,762 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=False
2026-01-28 10:00:09,762 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 10:00:09,763 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 10:00:09,763 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 10:00:09,764 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 10:00:09,765 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:00:09,766 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:00:09,767 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:00:09,767 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:00:09,768 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:00:09,769 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:00:09,770 | DEBUG | LiteLLM | response_cost: 0.0011803
2026-01-28 10:00:09,771 | DEBUG | LiteLLM | response_cost: 0.0011803
2026-01-28 10:00:09,772 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0011803
2026-01-28 10:00:09,777 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 10:00:09,779 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 10:00:09,780 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:00:09,780 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:00:09,782 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:00:09,783 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:00:09,786 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 10:00:09,789 | DEBUG | openai.agents | Resetting current trace
2026-01-28 10:00:09,803 | INFO | session_summary | üß† SUMMARY_START | session_id=f4af0964-d758-4273-9910-07a650e6b30f
2026-01-28 10:00:09,810 | INFO | session_summary | üìä Unsummarized messages = 10
2026-01-28 10:00:09,816 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-28 10:00:09,818 | INFO | chat | ‚úÖ Stream complete | tokens=3 | time=6.05s
2026-01-28 10:00:13,959 | DEBUG | httpcore.connection | close.started
2026-01-28 10:00:13,960 | DEBUG | httpcore.connection | close.complete
2026-01-28 10:00:13,961 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 10:00:13,990 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001949BA6F670>
2026-01-28 10:00:13,991 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x00000194900B9BC0> server_hostname='api.openai.com' timeout=5.0
2026-01-28 10:00:14,005 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001949BA6E7A0>
2026-01-28 10:00:14,005 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 10:00:14,006 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 10:00:14,006 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 10:00:14,007 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 10:00:14,007 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 10:00:14,483 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 04:30:13 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_cc1665e9feae0e33a6397f28321c99fd'), (b'openai-processing-ms', b'129'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'133'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4dc0d57de254d7-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 10:00:14,485 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 10:00:14,486 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 10:00:14,487 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 10:00:14,488 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 10:00:14,488 | DEBUG | openai.agents | Exported 2 items
2026-01-28 10:54:29,020 | INFO | chat | üì© Chat request | user_id=3 | request_id=78167417-2e5a-45ad-a358-18db0c37f169
2026-01-28 10:54:29,022 | INFO | chat | üßë User message | i want to kill myself
2026-01-28 10:54:29,045 | INFO | chat | üÜï New session created | 247fa020-cbe1-44cf-bb43-d080c8b72d3d
2026-01-28 10:54:29,073 | INFO | chat | ü§ñ Stream started | session_id=247fa020-cbe1-44cf-bb43-d080c8b72d3d | request_id=78167417-2e5a-45ad-a358-18db0c37f169
2026-01-28 10:54:29,075 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-28 10:54:29,078 | INFO | orchestrator | üßë USER_INPUT | i want to kill myself
2026-01-28 10:54:29,082 | INFO | orchestrator | üß© CONTEXT | user_id=3 | enable_memory=True | db=None
2026-01-28 10:54:29,085 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-28 10:54:29,087 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_7197f623940a4f1fb13a39a47997f821
2026-01-28 10:54:29,088 | DEBUG | openai.agents | Setting current trace: trace_7197f623940a4f1fb13a39a47997f821
2026-01-28 10:54:29,090 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000001949B5E5670> with id None
2026-01-28 10:54:29,091 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-28 10:54:29,094 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000001949B5555A0> with id None
2026-01-28 10:54:29,095 | DEBUG | openai.agents | Calling LLM
2026-01-28 10:54:29,095 | DEBUG | LiteLLM | 

2026-01-28 10:54:29,096 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 10:54:29,097 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n', 'role': 'system'}, {'role': 'user', 'content': 'i want to kill myself'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 10:54:29,099 | DEBUG | LiteLLM | 

2026-01-28 10:54:29,102 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 10:54:29,103 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 10:54:29,105 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 10:54:29,112 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 10:54:29,117 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 10:54:29,120 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n', 'role': 'system'}, {'role': 'user', 'content': 'i want to kill myself'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 10:54:29,122 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 10:54:29,123 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 10:54:29,125 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 10:54:29,126 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:54:29,129 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:54:29,132 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-28 10:54:29,133 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-28 10:54:29,134 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-28 10:54:29,136 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'i want to kill myself'}]}], 'system_instruction': {'parts': [{'text': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 10:54:31,078 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "I cannot help with that request. If you are feeling this way, please reach out for help. You can contact a crisis hotline or mental health professional."
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 192,
    "candidatesTokenCount": 31,
    "totalTokenCount": 276,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 192
      }
    ],
    "thoughtsTokenCount": 53
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "jp15afLzD6TbqfkPtLbc0Qw"
}



2026-01-28 10:54:31,083 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 10:54:31,086 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 10:54:31,087 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 10:54:31,088 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:54:31,090 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 10:54:31,091 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:54:31,093 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:54:31,094 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:54:31,095 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:54:31,097 | DEBUG | LiteLLM | response_cost: 0.0002676
2026-01-28 10:54:31,099 | DEBUG | openai.agents | Received model response
2026-01-28 10:54:31,098 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:54:31,101 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001949B5CD120>>
2026-01-28 10:54:31,103 | DEBUG | LiteLLM | response_cost: 0.0002676
2026-01-28 10:54:31,104 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 10:54:31,107 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 10:54:31,108 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 10:54:31,108 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 10:54:31,109 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:54:31,110 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 10:54:31,112 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 10:54:31,113 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:54:31,116 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 10:54:31,117 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:54:31,118 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 10:54:31,120 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:54:31,122 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:54:31,124 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 10:54:31,125 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:54:31,127 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:54:31,129 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 10:54:31,130 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:54:31,131 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0002676
2026-01-28 10:54:31,134 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 10:54:31,135 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:54:31,135 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:54:31,138 | DEBUG | openai.agents | Resetting current trace
2026-01-28 10:54:31,138 | ERROR | digital_human_sdk.app.intelligence.utils.json_utils | safe_json_loads failed
2026-01-28 10:54:31,156 | DEBUG | httpcore.connection | close.started
2026-01-28 10:54:31,151 | INFO | orchestrator | ‚úÖ Safety passed
2026-01-28 10:54:31,157 | DEBUG | httpcore.connection | close.complete
2026-01-28 10:54:31,158 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 10:54:31,159 | INFO | orchestrator | üì• Fetching semantic memory (via MemoryService)
2026-01-28 10:54:31,178 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001949BA6C070>
2026-01-28 10:54:31,180 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x00000194900B9BC0> server_hostname='api.openai.com' timeout=5.0
2026-01-28 10:54:31,199 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001949BA6E020>
2026-01-28 10:54:31,200 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 10:54:31,201 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 10:54:31,203 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 10:54:31,204 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 10:54:31,205 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 10:54:31,241 | INFO | orchestrator | üß† MEMORY_RESULT | found=False | count=0
2026-01-28 10:54:31,243 | INFO | orchestrator | üß≠ Router agent called
2026-01-28 10:54:31,246 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_13521e59273641a39178e2fb34c82c8e
2026-01-28 10:54:31,247 | DEBUG | openai.agents | Setting current trace: trace_13521e59273641a39178e2fb34c82c8e
2026-01-28 10:54:31,248 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000001949BAE9940> with id None
2026-01-28 10:54:31,248 | DEBUG | openai.agents | Running agent RouterAgent (turn 1)
2026-01-28 10:54:31,250 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000001949BA95540> with id None
2026-01-28 10:54:31,251 | DEBUG | openai.agents | Calling LLM
2026-01-28 10:54:31,252 | DEBUG | LiteLLM | 

2026-01-28 10:54:31,254 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 10:54:31,254 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'i want to kill myself'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 10:54:31,256 | DEBUG | LiteLLM | 

2026-01-28 10:54:31,258 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 10:54:31,259 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 10:54:31,260 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 10:54:31,263 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 10:54:31,264 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 10:54:31,267 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'i want to kill myself'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 10:54:31,269 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 10:54:31,270 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 10:54:31,271 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 10:54:31,272 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:54:31,274 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:54:31,275 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'i want to kill myself'}]}], 'system_instruction': {'parts': [{'text': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 10:54:32,411 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 05:24:31 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_12041e2fe24d9c5ec7d3c06a6a6da4b1'), (b'openai-processing-ms', b'319'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'322'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=4ftyICXEUvwECttJ.z9e9DK60Qs3MCVT6racCA4uhRw-1769577871-1.0.1.1-bxZ_saC2CcDTYCpkje1BhD5i3Yfz4A2nDWhmNTbq9P7PMsMq1ZkeBa6.uin_YGoEMltqrtlS1OH0DImtgYcVANsTuG9.XWD7v9d_L_JEJYI; path=/; expires=Wed, 28-Jan-26 05:54:31 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4e105ade458997-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 10:54:32,412 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 10:54:32,412 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 10:54:32,413 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 10:54:32,413 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 10:54:32,413 | DEBUG | openai.agents | Exported 3 items
2026-01-28 10:54:32,413 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 10:54:32,415 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 10:54:32,415 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 10:54:32,415 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 10:54:32,416 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 10:54:33,163 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 05:24:32 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_413090641c9548ef8067018a0c6792c4'), (b'openai-processing-ms', b'383'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'387'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4e10626ad98997-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 10:54:33,164 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 10:54:33,165 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 10:54:33,165 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 10:54:33,166 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 10:54:33,167 | DEBUG | openai.agents | Exported 1 items
2026-01-28 10:54:36,875 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\n  \"use_memory\": false,\n  \"use_tool\": false,\n  \"tool_name\": \"none\",\n  \"tool_arguments\": null,\n  \"intent\": \"none\",\n  \"memory_key\": null\n}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 490,
    "candidatesTokenCount": 56,
    "totalTokenCount": 1395,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 490
      }
    ],
    "thoughtsTokenCount": 849
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "lJ15afnWA8PL4-EPsvmKkQc"
}



2026-01-28 10:54:36,877 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 10:54:36,877 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 10:54:36,877 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 10:54:36,879 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:54:36,879 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 10:54:36,880 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:54:36,880 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:54:36,880 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:54:36,881 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:54:36,881 | DEBUG | LiteLLM | response_cost: 0.0024095
2026-01-28 10:54:36,882 | DEBUG | openai.agents | Received model response
2026-01-28 10:54:36,882 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:54:36,882 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001949B431C90>>
2026-01-28 10:54:36,884 | DEBUG | LiteLLM | response_cost: 0.0024095
2026-01-28 10:54:36,884 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 10:54:36,885 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 10:54:36,886 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 10:54:36,885 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 10:54:36,886 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:54:36,886 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 10:54:36,887 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 10:54:36,887 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:54:36,888 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 10:54:36,888 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:54:36,889 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 10:54:36,889 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:54:36,889 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:54:36,890 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 10:54:36,890 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:54:36,891 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:54:36,892 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 10:54:36,892 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:54:36,893 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0024095
2026-01-28 10:54:36,894 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 10:54:36,894 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:54:36,895 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:54:36,896 | DEBUG | openai.agents | Resetting current trace
2026-01-28 10:54:36,896 | INFO | orchestrator | üß≠ ROUTER_DECISION | tool=False | memory=False | intent=none
2026-01-28 10:54:36,897 | INFO | orchestrator | üß† REASONING_INPUT_SNAPSHOT
{
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful AI assistant.\n \nYou may answer ANY general knowledge question.\n \nConversation summaries and memories are ONLY to provide helpful context\n(e.g. preferences, ongoing plans).\n \nIf a user asks something unrelated to the current topic,\nanswer it normally using general knowledge.\n \nDo NOT restrict yourself to previous topics unless the user explicitly asks."
    },
    {
      "role": "user",
      "content": "i want to kill myself"
    },
    {
      "role": "user",
      "content": "i want to kill myself"
    }
  ],
  "safety": {
    "safe": true,
    "message": null
  },
  "memory_action": {},
  "memory_data": [],
  "memory_found": false,
  "tool_context": {}
}
2026-01-28 10:54:36,898 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_0359a402dd884a4e93f2b61feba86227
2026-01-28 10:54:36,899 | DEBUG | openai.agents | Setting current trace: trace_0359a402dd884a4e93f2b61feba86227
2026-01-28 10:54:36,899 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000001949BAE9440> with id None
2026-01-28 10:54:36,901 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000001949BA96380> with id None
2026-01-28 10:54:36,901 | DEBUG | openai.agents | Calling LLM
2026-01-28 10:54:36,901 | DEBUG | LiteLLM | 

2026-01-28 10:54:36,901 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 10:54:36,902 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "i want to kill myself"}, {"role": "user", "content": "i want to kill myself"}], "safety": {"safe": true, "message": null}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=True, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 10:54:36,902 | DEBUG | LiteLLM | 

2026-01-28 10:54:36,903 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 10:54:36,903 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 10:54:36,903 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 10:54:36,905 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 10:54:36,905 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 10:54:36,906 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "i want to kill myself"}, {"role": "user", "content": "i want to kill myself"}], "safety": {"safe": true, "message": null}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 10:54:36,906 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': True}
2026-01-28 10:54:36,907 | DEBUG | LiteLLM | Final returned optional params: {'stream': True}
2026-01-28 10:54:36,907 | DEBUG | LiteLLM | self.optional_params: {'stream': True}
2026-01-28 10:54:36,908 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:54:36,909 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:54:36,909 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?key=*****=sse \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "i want to kill myself"}, {"role": "user", "content": "i want to kill myself"}], "safety": {"safe": true, "message": null}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}]}], 'system_instruction': {'parts': [{'text': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.'}]}, 'generationConfig': {}}'
[0m

2026-01-28 10:54:36,910 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 10:54:36,911 | DEBUG | LiteLLM | messages in token_counter: None, text in token_counter: 
2026-01-28 10:54:36,912 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:54:36,912 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:54:36,912 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:54:36,913 | DEBUG | LiteLLM | response_cost: 0.0
2026-01-28 10:54:38,284 | DEBUG | httpcore.connection | close.started
2026-01-28 10:54:38,284 | DEBUG | httpcore.connection | close.complete
2026-01-28 10:54:38,285 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 10:54:38,314 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001949BA9D4B0>
2026-01-28 10:54:38,315 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x00000194900B9BC0> server_hostname='api.openai.com' timeout=5.0
2026-01-28 10:54:38,329 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001949BA9D540>
2026-01-28 10:54:38,329 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 10:54:38,330 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 10:54:38,330 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 10:54:38,330 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 10:54:38,331 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 10:54:38,435 | DEBUG | LiteLLM | RAW RESPONSE:
first stream response received


2026-01-28 10:54:38,436 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': "I'm really sorry to hear you're feeling this way. Please know that you're not alone and there's help available. You can connect with people who can support you by calling"}], 'role': 'model'}, 'index': 0}], 'usageMetadata': {'promptTokenCount': 282, 'candidatesTokenCount': 40, 'totalTokenCount': 349, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 282}], 'thoughtsTokenCount': 27}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'lZ15abiXEvyrjuMPm6uw8QM'}
2026-01-28 10:54:38,437 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='lZ15abiXEvyrjuMPm6uw8QM', created=1769577878, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content="I'm really sorry to hear you're feeling this way. Please know that you're not alone and there's help available. You can connect with people who can support you by calling", role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=67, prompt_tokens=282, total_tokens=349, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=27, rejected_prediction_tokens=None, text_tokens=40, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=282, image_tokens=None)))
2026-01-28 10:54:38,438 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content="I'm really sorry to hear you're feeling this way. Please know that you're not alone and there's help available. You can connect with people who can support you by calling", role='assistant', function_call=None, tool_calls=None, audio=None)
2026-01-28 10:54:38,438 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='lZ15abiXEvyrjuMPm6uw8QM', created=1769577878, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content="I'm really sorry to hear you're feeling this way. Please know that you're not alone and there's help available. You can connect with people who can support you by calling", role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=67, prompt_tokens=282, total_tokens=349, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=27, rejected_prediction_tokens=None, text_tokens=40, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=282, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-28 10:54:38,508 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': ' or texting 988 anytime in the US and Canada. In the UK, you can call 111. These services are free, confidential, and available 24/7. Please reach out to them.'}], 'role': 'model'}, 'finishReason': 'STOP', 'index': 0}], 'usageMetadata': {'promptTokenCount': 282, 'candidatesTokenCount': 86, 'totalTokenCount': 395, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 282}], 'thoughtsTokenCount': 27}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'lZ15abiXEvyrjuMPm6uw8QM'}
2026-01-28 10:54:38,511 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='lZ15abiXEvyrjuMPm6uw8QM', created=1769577878, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content=' or texting 988 anytime in the US and Canada. In the UK, you can call 111. These services are free, confidential, and available 24/7. Please reach out to them.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=113, prompt_tokens=282, total_tokens=395, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=27, rejected_prediction_tokens=None, text_tokens=86, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=282, image_tokens=None)))
2026-01-28 10:54:38,514 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content=' or texting 988 anytime in the US and Canada. In the UK, you can call 111. These services are free, confidential, and available 24/7. Please reach out to them.', role=None, function_call=None, tool_calls=None, audio=None)
2026-01-28 10:54:38,514 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='lZ15abiXEvyrjuMPm6uw8QM', created=1769577878, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' or texting 988 anytime in the US and Canada. In the UK, you can call 111. These services are free, confidential, and available 24/7. Please reach out to them.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=113, prompt_tokens=282, total_tokens=395, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=27, rejected_prediction_tokens=None, text_tokens=86, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=282, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-28 10:54:38,517 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=False
2026-01-28 10:54:38,518 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=False
2026-01-28 10:54:38,518 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 10:54:38,519 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 10:54:38,519 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 10:54:38,520 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 10:54:38,520 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:54:38,521 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:54:38,521 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:54:38,521 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:54:38,523 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:54:38,523 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:54:38,524 | DEBUG | LiteLLM | response_cost: 0.00036710000000000003
2026-01-28 10:54:38,524 | DEBUG | LiteLLM | response_cost: 0.00036710000000000003
2026-01-28 10:54:38,525 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 10:54:38,525 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.00036710000000000003
2026-01-28 10:54:38,526 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:54:38,527 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 10:54:38,527 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:54:38,528 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:54:38,532 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:54:38,533 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 10:54:38,533 | DEBUG | openai.agents | Resetting current trace
2026-01-28 10:54:38,538 | INFO | session_summary | üß† SUMMARY_START | session_id=247fa020-cbe1-44cf-bb43-d080c8b72d3d
2026-01-28 10:54:38,541 | INFO | session_summary | üìä Unsummarized messages = 2
2026-01-28 10:54:38,542 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-28 10:54:38,543 | INFO | chat | ‚úÖ Stream complete | tokens=2 | time=9.52s
2026-01-28 10:54:38,794 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 05:24:38 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_f46eff0c33b91cb5f0a8a267a1b44aa9'), (b'openai-processing-ms', b'126'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'129'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4e108778335517-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 10:54:38,796 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 10:54:38,796 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 10:54:38,796 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 10:54:38,797 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 10:54:38,797 | DEBUG | openai.agents | Exported 3 items
2026-01-28 10:54:38,799 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 10:54:38,800 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 10:54:38,801 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 10:54:38,801 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 10:54:38,802 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 10:54:39,223 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 05:24:38 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_92a1a195f553dfc899003304309e6eb4'), (b'openai-processing-ms', b'99'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'103'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4e108a597f5517-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 10:54:39,224 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 10:54:39,224 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 10:54:39,225 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 10:54:39,225 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 10:54:39,225 | DEBUG | openai.agents | Exported 2 items
2026-01-28 10:56:38,419 | INFO | chat | üì© Chat request | user_id=3 | request_id=9d7d4225-7f1e-4e54-a2ce-88a6adab15ce
2026-01-28 10:56:38,420 | INFO | chat | üßë User message | tell me about taj mahal
2026-01-28 10:56:38,464 | INFO | chat | ü§ñ Stream started | session_id=247fa020-cbe1-44cf-bb43-d080c8b72d3d | request_id=9d7d4225-7f1e-4e54-a2ce-88a6adab15ce
2026-01-28 10:56:38,469 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-28 10:56:38,481 | INFO | orchestrator | üßë USER_INPUT | tell me about taj mahal
2026-01-28 10:56:38,487 | INFO | orchestrator | üß© CONTEXT | user_id=3 | enable_memory=True | db=None
2026-01-28 10:56:38,500 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-28 10:56:38,507 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_812c461dacdb4f668add5b63b82f4b15
2026-01-28 10:56:38,508 | DEBUG | openai.agents | Setting current trace: trace_812c461dacdb4f668add5b63b82f4b15
2026-01-28 10:56:38,510 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000001949B4F9670> with id None
2026-01-28 10:56:38,514 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-28 10:56:38,518 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000001949BA95D80> with id None
2026-01-28 10:56:38,519 | DEBUG | openai.agents | Calling LLM
2026-01-28 10:56:38,520 | DEBUG | LiteLLM | 

2026-01-28 10:56:38,524 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 10:56:38,526 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n', 'role': 'system'}, {'role': 'user', 'content': 'tell me about taj mahal'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 10:56:38,529 | DEBUG | LiteLLM | 

2026-01-28 10:56:38,538 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 10:56:38,544 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 10:56:38,549 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 10:56:38,557 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 10:56:38,561 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 10:56:38,573 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n', 'role': 'system'}, {'role': 'user', 'content': 'tell me about taj mahal'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 10:56:38,580 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 10:56:38,585 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 10:56:38,589 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 10:56:38,595 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:56:38,601 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:56:38,603 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'tell me about taj mahal'}]}], 'system_instruction': {'parts': [{'text': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 10:56:39,785 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "SAFE"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 192,
    "candidatesTokenCount": 1,
    "totalTokenCount": 227,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 192
      }
    ],
    "thoughtsTokenCount": 34
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "Dp55afi5OJ2s4-EPxdmKoAQ"
}



2026-01-28 10:56:39,788 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 10:56:39,791 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 10:56:39,791 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 10:56:39,793 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:56:39,795 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 10:56:39,796 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:56:39,798 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:56:39,799 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:56:39,800 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:56:39,802 | DEBUG | LiteLLM | response_cost: 0.0001451
2026-01-28 10:56:39,804 | DEBUG | openai.agents | Received model response
2026-01-28 10:56:39,803 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:56:39,807 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001949BA9E950>>
2026-01-28 10:56:39,808 | DEBUG | LiteLLM | response_cost: 0.0001451
2026-01-28 10:56:39,809 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 10:56:39,812 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 10:56:39,815 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 10:56:39,814 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 10:56:39,816 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:56:39,817 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 10:56:39,819 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 10:56:39,821 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:56:39,823 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 10:56:39,825 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:56:39,827 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 10:56:39,828 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:56:39,829 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:56:39,832 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 10:56:39,833 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:56:39,836 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:56:39,837 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 10:56:39,839 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:56:39,840 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0001451
2026-01-28 10:56:39,844 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 10:56:39,846 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:56:39,847 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:56:39,850 | DEBUG | openai.agents | Resetting current trace
2026-01-28 10:56:39,851 | ERROR | digital_human_sdk.app.intelligence.utils.json_utils | safe_json_loads failed
2026-01-28 10:56:39,854 | INFO | orchestrator | ‚úÖ Safety passed
2026-01-28 10:56:39,857 | INFO | orchestrator | üì• Fetching semantic memory (via MemoryService)
2026-01-28 10:56:39,925 | INFO | orchestrator | üß† MEMORY_RESULT | found=False | count=0
2026-01-28 10:56:39,928 | INFO | orchestrator | üß≠ Router agent called
2026-01-28 10:56:39,930 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_e980d4f8fa75448aaaffdb59dda831ad
2026-01-28 10:56:39,930 | DEBUG | openai.agents | Setting current trace: trace_e980d4f8fa75448aaaffdb59dda831ad
2026-01-28 10:56:39,932 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000001949BAE9850> with id None
2026-01-28 10:56:39,933 | DEBUG | openai.agents | Running agent RouterAgent (turn 1)
2026-01-28 10:56:39,935 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000001949BA95F00> with id None
2026-01-28 10:56:39,936 | DEBUG | openai.agents | Calling LLM
2026-01-28 10:56:39,937 | DEBUG | LiteLLM | 

2026-01-28 10:56:39,938 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 10:56:39,940 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'tell me about taj mahal'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 10:56:39,941 | DEBUG | LiteLLM | 

2026-01-28 10:56:39,942 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 10:56:39,944 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 10:56:39,945 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 10:56:39,949 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 10:56:39,951 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 10:56:39,954 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'tell me about taj mahal'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 10:56:39,956 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 10:56:39,958 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 10:56:39,959 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 10:56:39,960 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:56:39,962 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:56:39,963 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'tell me about taj mahal'}]}], 'system_instruction': {'parts': [{'text': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 10:56:41,518 | DEBUG | httpcore.connection | close.started
2026-01-28 10:56:41,522 | DEBUG | httpcore.connection | close.complete
2026-01-28 10:56:41,523 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 10:56:41,555 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001949B5CC7F0>
2026-01-28 10:56:41,556 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x00000194900B9BC0> server_hostname='api.openai.com' timeout=5.0
2026-01-28 10:56:41,575 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001949C2E3850>
2026-01-28 10:56:41,576 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 10:56:41,578 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 10:56:41,579 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 10:56:41,580 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 10:56:41,581 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 10:56:41,636 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\n  \"use_memory\": false,\n  \"use_tool\": false,\n  \"tool_name\": \"none\",\n  \"tool_arguments\": null,\n  \"intent\": \"none\",\n  \"memory_key\": null\n}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 490,
    "candidatesTokenCount": 56,
    "totalTokenCount": 679,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 490
      }
    ],
    "thoughtsTokenCount": 133
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "EJ55aa6SLtmY4-EPoNODsQQ"
}



2026-01-28 10:56:41,639 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 10:56:41,641 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 10:56:41,642 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 10:56:41,643 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:56:41,644 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 10:56:41,645 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:56:41,647 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:56:41,648 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:56:41,649 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:56:41,651 | DEBUG | LiteLLM | response_cost: 0.0006195
2026-01-28 10:56:41,653 | DEBUG | openai.agents | Received model response
2026-01-28 10:56:41,652 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:56:41,657 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001949B3E4EB0>>
2026-01-28 10:56:41,658 | DEBUG | LiteLLM | response_cost: 0.0006195
2026-01-28 10:56:41,660 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 10:56:41,662 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 10:56:41,664 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 10:56:41,663 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 10:56:41,666 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:56:41,669 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 10:56:41,671 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 10:56:41,673 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:56:41,675 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 10:56:41,676 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:56:41,678 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 10:56:41,679 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:56:41,680 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:56:41,682 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 10:56:41,682 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:56:41,684 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:56:41,684 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 10:56:41,685 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:56:41,686 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0006195
2026-01-28 10:56:41,691 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 10:56:41,693 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:56:41,694 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:56:41,697 | DEBUG | openai.agents | Resetting current trace
2026-01-28 10:56:41,698 | INFO | orchestrator | üß≠ ROUTER_DECISION | tool=False | memory=False | intent=none
2026-01-28 10:56:41,700 | INFO | orchestrator | üß† REASONING_INPUT_SNAPSHOT
{
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful AI assistant.\n \nYou may answer ANY general knowledge question.\n \nConversation summaries and memories are ONLY to provide helpful context\n(e.g. preferences, ongoing plans).\n \nIf a user asks something unrelated to the current topic,\nanswer it normally using general knowledge.\n \nDo NOT restrict yourself to previous topics unless the user explicitly asks."
    },
    {
      "role": "user",
      "content": "i want to kill myself"
    },
    {
      "role": "assistant",
      "content": "I'm really sorry to hear you're feeling this way. Please know that you're not alone and there's help available. You can connect with people who can support you by calling or texting 988 anytime in the US and Canada. In the UK, you can call 111. These services are free, confidential, and available 24/7. Please reach out to them."
    },
    {
      "role": "user",
      "content": "tell me about taj mahal"
    },
    {
      "role": "user",
      "content": "tell me about taj mahal"
    }
  ],
  "safety": {
    "safe": true,
    "message": null
  },
  "memory_action": {},
  "memory_data": [],
  "memory_found": false,
  "tool_context": {}
}
2026-01-28 10:56:41,704 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_c57468313da24659a8799fe4597c71b3
2026-01-28 10:56:41,706 | DEBUG | openai.agents | Setting current trace: trace_c57468313da24659a8799fe4597c71b3
2026-01-28 10:56:41,707 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000001949B42A5C0> with id None
2026-01-28 10:56:41,709 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000001949BA95960> with id None
2026-01-28 10:56:41,710 | DEBUG | openai.agents | Calling LLM
2026-01-28 10:56:41,711 | DEBUG | LiteLLM | 

2026-01-28 10:56:41,712 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 10:56:41,714 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "i want to kill myself"}, {"role": "assistant", "content": "I\'m really sorry to hear you\'re feeling this way. Please know that you\'re not alone and there\'s help available. You can connect with people who can support you by calling or texting 988 anytime in the US and Canada. In the UK, you can call 111. These services are free, confidential, and available 24/7. Please reach out to them."}, {"role": "user", "content": "tell me about taj mahal"}, {"role": "user", "content": "tell me about taj mahal"}], "safety": {"safe": true, "message": null}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=True, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 10:56:41,716 | DEBUG | LiteLLM | 

2026-01-28 10:56:41,717 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 10:56:41,718 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 10:56:41,719 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 10:56:41,721 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 10:56:41,722 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 10:56:41,724 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "i want to kill myself"}, {"role": "assistant", "content": "I\'m really sorry to hear you\'re feeling this way. Please know that you\'re not alone and there\'s help available. You can connect with people who can support you by calling or texting 988 anytime in the US and Canada. In the UK, you can call 111. These services are free, confidential, and available 24/7. Please reach out to them."}, {"role": "user", "content": "tell me about taj mahal"}, {"role": "user", "content": "tell me about taj mahal"}], "safety": {"safe": true, "message": null}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 10:56:41,725 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': True}
2026-01-28 10:56:41,726 | DEBUG | LiteLLM | Final returned optional params: {'stream': True}
2026-01-28 10:56:41,728 | DEBUG | LiteLLM | self.optional_params: {'stream': True}
2026-01-28 10:56:41,729 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:56:41,731 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:56:41,732 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?key=*****=sse \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "i want to kill myself"}, {"role": "assistant", "content": "I\'m really sorry to hear you\'re feeling this way. Please know that you\'re not alone and there\'s help available. You can connect with people who can support you by calling or texting 988 anytime in the US and Canada. In the UK, you can call 111. These services are free, confidential, and available 24/7. Please reach out to them."}, {"role": "user", "content": "tell me about taj mahal"}, {"role": "user", "content": "tell me about taj mahal"}], "safety": {"safe": true, "message": null}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}]}], 'system_instruction': {'parts': [{'text': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.'}]}, 'generationConfig': {}}'
[0m

2026-01-28 10:56:41,736 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 10:56:41,736 | DEBUG | LiteLLM | messages in token_counter: None, text in token_counter: 
2026-01-28 10:56:41,738 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:56:41,740 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:56:41,742 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:56:41,743 | DEBUG | LiteLLM | response_cost: 0.0
2026-01-28 10:56:42,069 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 05:26:41 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_c43bc1f620ff5de171faa153aee8dfc8'), (b'openai-processing-ms', b'184'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'188'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4e1389aa4c732c-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 10:56:42,071 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 10:56:42,072 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 10:56:42,072 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 10:56:42,073 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 10:56:42,074 | DEBUG | openai.agents | Exported 4 items
2026-01-28 10:56:42,077 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 10:56:42,078 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 10:56:42,080 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 10:56:42,081 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 10:56:42,081 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 10:56:43,048 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 05:26:42 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_bdab9b1ab651f199b88f57c4b94976e5'), (b'openai-processing-ms', b'99'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'102'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4e138cdf17732c-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 10:56:43,050 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 10:56:43,051 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 10:56:43,052 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 10:56:43,053 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 10:56:43,054 | DEBUG | openai.agents | Exported 3 items
2026-01-28 10:56:43,056 | DEBUG | LiteLLM | RAW RESPONSE:
first stream response received


2026-01-28 10:56:43,057 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': 'The Taj Mahal is an iconic ivory-white marble mausoleum located on the right bank of the Yamuna River in Agra, Uttar Pradesh, India. It was commissioned in 1632 by the Mughal emperor Shah Jahan to'}], 'role': 'model'}, 'index': 0}], 'usageMetadata': {'promptTokenCount': 394, 'candidatesTokenCount': 46, 'totalTokenCount': 459, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 394}], 'thoughtsTokenCount': 19}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'EZ55abugNbXGjuMPpYqm-AM'}
2026-01-28 10:56:43,057 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='EZ55abugNbXGjuMPpYqm-AM', created=1769578003, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='The Taj Mahal is an iconic ivory-white marble mausoleum located on the right bank of the Yamuna River in Agra, Uttar Pradesh, India. It was commissioned in 1632 by the Mughal emperor Shah Jahan to', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=65, prompt_tokens=394, total_tokens=459, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=19, rejected_prediction_tokens=None, text_tokens=46, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=394, image_tokens=None)))
2026-01-28 10:56:43,059 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content='The Taj Mahal is an iconic ivory-white marble mausoleum located on the right bank of the Yamuna River in Agra, Uttar Pradesh, India. It was commissioned in 1632 by the Mughal emperor Shah Jahan to', role='assistant', function_call=None, tool_calls=None, audio=None)
2026-01-28 10:56:43,060 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='EZ55abugNbXGjuMPpYqm-AM', created=1769578003, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='The Taj Mahal is an iconic ivory-white marble mausoleum located on the right bank of the Yamuna River in Agra, Uttar Pradesh, India. It was commissioned in 1632 by the Mughal emperor Shah Jahan to', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=65, prompt_tokens=394, total_tokens=459, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=19, rejected_prediction_tokens=None, text_tokens=46, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=394, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-28 10:56:43,071 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': ' house the tomb of his favorite wife, Mumtaz Mahal.\n\nHere are some key facts about it:\n*   **Architectural Style:** It is a masterpiece of Mughal architecture, combining elements from Persian, Islamic, and Indian architectural'}], 'role': 'model'}, 'index': 0}], 'usageMetadata': {'promptTokenCount': 394, 'candidatesTokenCount': 94, 'totalTokenCount': 507, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 394}], 'thoughtsTokenCount': 19}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'EZ55abugNbXGjuMPpYqm-AM'}
2026-01-28 10:56:43,072 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='EZ55abugNbXGjuMPpYqm-AM', created=1769578003, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content=' house the tomb of his favorite wife, Mumtaz Mahal.\n\nHere are some key facts about it:\n*   **Architectural Style:** It is a masterpiece of Mughal architecture, combining elements from Persian, Islamic, and Indian architectural', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=113, prompt_tokens=394, total_tokens=507, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=19, rejected_prediction_tokens=None, text_tokens=94, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=394, image_tokens=None)))
2026-01-28 10:56:43,074 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content=' house the tomb of his favorite wife, Mumtaz Mahal.\n\nHere are some key facts about it:\n*   **Architectural Style:** It is a masterpiece of Mughal architecture, combining elements from Persian, Islamic, and Indian architectural', role=None, function_call=None, tool_calls=None, audio=None)
2026-01-28 10:56:43,074 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='EZ55abugNbXGjuMPpYqm-AM', created=1769578003, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' house the tomb of his favorite wife, Mumtaz Mahal.\n\nHere are some key facts about it:\n*   **Architectural Style:** It is a masterpiece of Mughal architecture, combining elements from Persian, Islamic, and Indian architectural', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=113, prompt_tokens=394, total_tokens=507, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=19, rejected_prediction_tokens=None, text_tokens=94, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=394, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-28 10:56:43,337 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': ' styles.\n*   **Construction:** It took approximately 20,000 artisans and craftsmen and over 1,000 elephants to transport building materials. Construction was largely completed by 1643, but work'}], 'role': 'model'}, 'index': 0}], 'usageMetadata': {'promptTokenCount': 394, 'candidatesTokenCount': 142, 'totalTokenCount': 555, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 394}], 'thoughtsTokenCount': 19}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'EZ55abugNbXGjuMPpYqm-AM'}
2026-01-28 10:56:43,339 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='EZ55abugNbXGjuMPpYqm-AM', created=1769578003, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content=' styles.\n*   **Construction:** It took approximately 20,000 artisans and craftsmen and over 1,000 elephants to transport building materials. Construction was largely completed by 1643, but work', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=161, prompt_tokens=394, total_tokens=555, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=19, rejected_prediction_tokens=None, text_tokens=142, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=394, image_tokens=None)))
2026-01-28 10:56:43,341 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content=' styles.\n*   **Construction:** It took approximately 20,000 artisans and craftsmen and over 1,000 elephants to transport building materials. Construction was largely completed by 1643, but work', role=None, function_call=None, tool_calls=None, audio=None)
2026-01-28 10:56:43,341 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='EZ55abugNbXGjuMPpYqm-AM', created=1769578003, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' styles.\n*   **Construction:** It took approximately 20,000 artisans and craftsmen and over 1,000 elephants to transport building materials. Construction was largely completed by 1643, but work', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=161, prompt_tokens=394, total_tokens=555, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=19, rejected_prediction_tokens=None, text_tokens=142, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=394, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-28 10:56:43,582 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': ' continued on other phases of the project for another 10 years.\n*   **Materials:** It is built using white marble from Makrana, Rajasthan, adorned with intricate inlay work of semi-precious stones.\n*   **Complex'}], 'role': 'model'}, 'index': 0}], 'usageMetadata': {'promptTokenCount': 394, 'candidatesTokenCount': 191, 'totalTokenCount': 604, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 394}], 'thoughtsTokenCount': 19}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'EZ55abugNbXGjuMPpYqm-AM'}
2026-01-28 10:56:43,583 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='EZ55abugNbXGjuMPpYqm-AM', created=1769578003, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content=' continued on other phases of the project for another 10 years.\n*   **Materials:** It is built using white marble from Makrana, Rajasthan, adorned with intricate inlay work of semi-precious stones.\n*   **Complex', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=210, prompt_tokens=394, total_tokens=604, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=19, rejected_prediction_tokens=None, text_tokens=191, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=394, image_tokens=None)))
2026-01-28 10:56:43,585 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content=' continued on other phases of the project for another 10 years.\n*   **Materials:** It is built using white marble from Makrana, Rajasthan, adorned with intricate inlay work of semi-precious stones.\n*   **Complex', role=None, function_call=None, tool_calls=None, audio=None)
2026-01-28 10:56:43,586 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='EZ55abugNbXGjuMPpYqm-AM', created=1769578003, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' continued on other phases of the project for another 10 years.\n*   **Materials:** It is built using white marble from Makrana, Rajasthan, adorned with intricate inlay work of semi-precious stones.\n*   **Complex', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=210, prompt_tokens=394, total_tokens=604, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=19, rejected_prediction_tokens=None, text_tokens=191, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=394, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-28 10:56:43,843 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': ':** The Taj Mahal complex includes not only the mausoleum itself but also a mosque, a guest house (jawab), and extensive gardens with a large reflecting pool.\n*   **Recognition:** It is widely considered one of the most beautiful buildings in the'}], 'role': 'model'}, 'index': 0}], 'usageMetadata': {'promptTokenCount': 394, 'candidatesTokenCount': 241, 'totalTokenCount': 654, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 394}], 'thoughtsTokenCount': 19}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'EZ55abugNbXGjuMPpYqm-AM'}
2026-01-28 10:56:43,845 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='EZ55abugNbXGjuMPpYqm-AM', created=1769578003, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content=':** The Taj Mahal complex includes not only the mausoleum itself but also a mosque, a guest house (jawab), and extensive gardens with a large reflecting pool.\n*   **Recognition:** It is widely considered one of the most beautiful buildings in the', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=260, prompt_tokens=394, total_tokens=654, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=19, rejected_prediction_tokens=None, text_tokens=241, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=394, image_tokens=None)))
2026-01-28 10:56:43,848 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content=':** The Taj Mahal complex includes not only the mausoleum itself but also a mosque, a guest house (jawab), and extensive gardens with a large reflecting pool.\n*   **Recognition:** It is widely considered one of the most beautiful buildings in the', role=None, function_call=None, tool_calls=None, audio=None)
2026-01-28 10:56:43,849 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='EZ55abugNbXGjuMPpYqm-AM', created=1769578003, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=':** The Taj Mahal complex includes not only the mausoleum itself but also a mosque, a guest house (jawab), and extensive gardens with a large reflecting pool.\n*   **Recognition:** It is widely considered one of the most beautiful buildings in the', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=260, prompt_tokens=394, total_tokens=654, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=19, rejected_prediction_tokens=None, text_tokens=241, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=394, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-28 10:56:43,912 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': ' world and is a UNESCO World Heritage Site. It attracts millions of visitors annually.'}], 'role': 'model'}, 'finishReason': 'STOP', 'index': 0}], 'usageMetadata': {'promptTokenCount': 394, 'candidatesTokenCount': 257, 'totalTokenCount': 670, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 394}], 'thoughtsTokenCount': 19}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'EZ55abugNbXGjuMPpYqm-AM'}
2026-01-28 10:56:43,913 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='EZ55abugNbXGjuMPpYqm-AM', created=1769578003, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content=' world and is a UNESCO World Heritage Site. It attracts millions of visitors annually.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=276, prompt_tokens=394, total_tokens=670, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=19, rejected_prediction_tokens=None, text_tokens=257, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=394, image_tokens=None)))
2026-01-28 10:56:43,915 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content=' world and is a UNESCO World Heritage Site. It attracts millions of visitors annually.', role=None, function_call=None, tool_calls=None, audio=None)
2026-01-28 10:56:43,916 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='EZ55abugNbXGjuMPpYqm-AM', created=1769578003, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' world and is a UNESCO World Heritage Site. It attracts millions of visitors annually.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=276, prompt_tokens=394, total_tokens=670, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=19, rejected_prediction_tokens=None, text_tokens=257, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=394, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-28 10:56:43,919 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=False
2026-01-28 10:56:43,919 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=False
2026-01-28 10:56:43,920 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 10:56:43,920 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 10:56:43,921 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 10:56:43,921 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 10:56:43,922 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:56:43,922 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:56:43,923 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:56:43,923 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:56:43,923 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:56:43,925 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:56:43,925 | DEBUG | LiteLLM | response_cost: 0.0008082
2026-01-28 10:56:43,925 | DEBUG | LiteLLM | response_cost: 0.0008082
2026-01-28 10:56:43,926 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0008082
2026-01-28 10:56:43,927 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 10:56:43,927 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 10:56:43,928 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:56:43,928 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:56:43,928 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:56:43,929 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:56:43,929 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 10:56:43,931 | DEBUG | openai.agents | Resetting current trace
2026-01-28 10:56:43,934 | INFO | session_summary | üß† SUMMARY_START | session_id=247fa020-cbe1-44cf-bb43-d080c8b72d3d
2026-01-28 10:56:43,937 | INFO | session_summary | üìä Unsummarized messages = 4
2026-01-28 10:56:43,938 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-28 10:56:43,938 | INFO | chat | ‚úÖ Stream complete | tokens=6 | time=5.52s
2026-01-28 10:56:48,152 | DEBUG | httpcore.connection | close.started
2026-01-28 10:56:48,157 | DEBUG | httpcore.connection | close.complete
2026-01-28 10:56:48,157 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 10:56:48,181 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001949BCD92A0>
2026-01-28 10:56:48,182 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x00000194900B9BC0> server_hostname='api.openai.com' timeout=5.0
2026-01-28 10:56:48,200 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001949BCD9480>
2026-01-28 10:56:48,201 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 10:56:48,203 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 10:56:48,204 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 10:56:48,204 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 10:56:48,205 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 10:56:48,641 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 05:26:47 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_8bac166614d841e4e999a5bf480c6e80'), (b'openai-processing-ms', b'89'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'92'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4e13b3180f5472-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 10:56:48,643 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 10:56:48,643 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 10:56:48,644 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 10:56:48,646 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 10:56:48,647 | DEBUG | openai.agents | Exported 2 items
2026-01-28 10:57:03,369 | INFO | chat | üì© Chat request | user_id=3 | request_id=f8b3c40a-f962-4d80-adaa-f42266fcea2f
2026-01-28 10:57:03,371 | INFO | chat | üßë User message | where do i live?
2026-01-28 10:57:03,402 | INFO | chat | ü§ñ Stream started | session_id=247fa020-cbe1-44cf-bb43-d080c8b72d3d | request_id=f8b3c40a-f962-4d80-adaa-f42266fcea2f
2026-01-28 10:57:03,403 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-28 10:57:03,407 | INFO | orchestrator | üßë USER_INPUT | where do i live?
2026-01-28 10:57:03,410 | INFO | orchestrator | üß© CONTEXT | user_id=3 | enable_memory=True | db=None
2026-01-28 10:57:03,413 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-28 10:57:03,415 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_f2673871865f424f973083d3bf31addd
2026-01-28 10:57:03,416 | DEBUG | openai.agents | Setting current trace: trace_f2673871865f424f973083d3bf31addd
2026-01-28 10:57:03,417 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000001949BAE9620> with id None
2026-01-28 10:57:03,417 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-28 10:57:03,418 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000001949BA96140> with id None
2026-01-28 10:57:03,419 | DEBUG | openai.agents | Calling LLM
2026-01-28 10:57:03,421 | DEBUG | LiteLLM | 

2026-01-28 10:57:03,423 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 10:57:03,424 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n', 'role': 'system'}, {'role': 'user', 'content': 'where do i live?'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 10:57:03,426 | DEBUG | LiteLLM | 

2026-01-28 10:57:03,427 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 10:57:03,428 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 10:57:03,429 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 10:57:03,433 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 10:57:03,434 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 10:57:03,436 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n', 'role': 'system'}, {'role': 'user', 'content': 'where do i live?'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 10:57:03,438 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 10:57:03,439 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 10:57:03,439 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 10:57:03,441 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:57:03,443 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:57:03,444 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'where do i live?'}]}], 'system_instruction': {'parts': [{'text': '\n\nYou are a content safety checker.\n \nYour task:\n\n- Decide whether the user\'s message is SAFE or UNSAFE.\n \nSAFE content includes:\n\n- General knowledge (science, history, people)\n\n- Weather questions\n\n- Casual conversation\n\n- Education, explanations, opinions\n\n- Harmless personal questions\n \nUNSAFE content includes:\n\n- Violence, murder, terrorism\n\n- Instructions for illegal activities\n\n- Self-harm or suicide encouragement\n\n- Hate speech or harassment\n\n- Sexual content involving minors\n \nRules:\n\n- If the message is SAFE, respond with: "SAFE"\n\n- If the message is UNSAFE, respond with a short, polite refusal explaining that you cannot help with that request.\n\n- Do NOT mention policies, rules, or internal systems.\n\n- Do NOT block normal or harmless questions.\n\n- Be calm and respectful.\n\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 10:57:03,924 | DEBUG | httpcore.connection | close.started
2026-01-28 10:57:03,925 | DEBUG | httpcore.connection | close.complete
2026-01-28 10:57:03,927 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 10:57:03,953 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001949BA6CD30>
2026-01-28 10:57:03,954 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x00000194900B9BC0> server_hostname='api.openai.com' timeout=5.0
2026-01-28 10:57:03,971 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001949BA6CF10>
2026-01-28 10:57:03,972 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 10:57:03,973 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 10:57:03,973 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 10:57:03,974 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 10:57:03,974 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 10:57:04,408 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 05:27:03 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_bbf3e7966bdbc3ac727f2e8f6d3e9abd'), (b'openai-processing-ms', b'118'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'121'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4e1415a93577d3-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 10:57:04,410 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 10:57:04,410 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 10:57:04,411 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 10:57:04,413 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 10:57:04,414 | DEBUG | openai.agents | Exported 1 items
2026-01-28 10:57:04,819 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "SAFE"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 192,
    "candidatesTokenCount": 1,
    "totalTokenCount": 268,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 192
      }
    ],
    "thoughtsTokenCount": 75
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "J555aYLAOdiBg8UP_-bo8QE"
}



2026-01-28 10:57:04,822 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 10:57:04,823 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 10:57:04,823 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 10:57:04,826 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:57:04,827 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 10:57:04,829 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:57:04,831 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:57:04,832 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:57:04,833 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:57:04,834 | DEBUG | LiteLLM | response_cost: 0.0002476
2026-01-28 10:57:04,837 | DEBUG | openai.agents | Received model response
2026-01-28 10:57:04,836 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:57:04,839 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001949BA9EAD0>>
2026-01-28 10:57:04,840 | DEBUG | LiteLLM | response_cost: 0.0002476
2026-01-28 10:57:04,841 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 10:57:04,843 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 10:57:04,846 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 10:57:04,845 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 10:57:04,847 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:57:04,849 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 10:57:04,851 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 10:57:04,853 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:57:04,855 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 10:57:04,856 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:57:04,856 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 10:57:04,857 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:57:04,858 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:57:04,861 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 10:57:04,862 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:57:04,864 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:57:04,864 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 10:57:04,866 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:57:04,867 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0002476
2026-01-28 10:57:04,870 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 10:57:04,871 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:57:04,873 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:57:04,875 | DEBUG | openai.agents | Resetting current trace
2026-01-28 10:57:04,877 | ERROR | digital_human_sdk.app.intelligence.utils.json_utils | safe_json_loads failed
2026-01-28 10:57:04,879 | INFO | orchestrator | ‚úÖ Safety passed
2026-01-28 10:57:04,881 | INFO | orchestrator | üì• Fetching semantic memory (via MemoryService)
2026-01-28 10:57:04,947 | INFO | orchestrator | üß† MEMORY_RESULT | found=True | count=1
2026-01-28 10:57:04,948 | INFO | orchestrator | üß≠ Router agent called
2026-01-28 10:57:04,952 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_d1eb38241da442dba99499476d94f0d0
2026-01-28 10:57:04,953 | DEBUG | openai.agents | Setting current trace: trace_d1eb38241da442dba99499476d94f0d0
2026-01-28 10:57:04,954 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000001949B5E6B10> with id None
2026-01-28 10:57:04,954 | DEBUG | openai.agents | Running agent RouterAgent (turn 1)
2026-01-28 10:57:04,956 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000001949B4FE860> with id None
2026-01-28 10:57:04,957 | DEBUG | openai.agents | Calling LLM
2026-01-28 10:57:04,958 | DEBUG | LiteLLM | 

2026-01-28 10:57:04,959 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 10:57:04,959 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'where do i live?'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 10:57:04,961 | DEBUG | LiteLLM | 

2026-01-28 10:57:04,963 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 10:57:04,963 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 10:57:04,965 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 10:57:04,968 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 10:57:04,969 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 10:57:04,972 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'where do i live?'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 10:57:04,974 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 10:57:04,976 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 10:57:04,977 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 10:57:04,978 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:57:04,980 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:57:04,981 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'where do i live?'}]}], 'system_instruction': {'parts': [{'text': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 10:57:06,352 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\n  \"use_memory\": true,\n  \"use_tool\": false,\n  \"tool_name\": \"none\",\n  \"tool_arguments\": null,\n  \"intent\": \"read\",\n  \"memory_key\": null\n}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 490,
    "candidatesTokenCount": 56,
    "totalTokenCount": 630,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 490
      }
    ],
    "thoughtsTokenCount": 84
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "KZ55aciUIvf2juMPmIjomAM"
}



2026-01-28 10:57:06,355 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 10:57:06,357 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 10:57:06,357 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 10:57:06,359 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:57:06,360 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 10:57:06,361 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:57:06,364 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:57:06,365 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:57:06,366 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:57:06,367 | DEBUG | LiteLLM | response_cost: 0.000497
2026-01-28 10:57:06,368 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:57:06,369 | DEBUG | openai.agents | Received model response
2026-01-28 10:57:06,371 | DEBUG | LiteLLM | response_cost: 0.000497
2026-01-28 10:57:06,372 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001949BA6CAC0>>
2026-01-28 10:57:06,374 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 10:57:06,376 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 10:57:06,377 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:57:06,380 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 10:57:06,380 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 10:57:06,382 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:57:06,386 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 10:57:06,388 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 10:57:06,390 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 10:57:06,393 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 10:57:06,394 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:57:06,396 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 10:57:06,397 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:57:06,398 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:57:06,399 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:57:06,400 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:57:06,402 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:57:06,404 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 10:57:06,406 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.000497
2026-01-28 10:57:06,408 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 10:57:06,410 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:57:06,411 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:57:06,415 | DEBUG | openai.agents | Resetting current trace
2026-01-28 10:57:06,416 | INFO | orchestrator | üß≠ ROUTER_DECISION | tool=False | memory=True | intent=read
2026-01-28 10:57:06,418 | INFO | orchestrator | üß† REASONING_INPUT_SNAPSHOT
{
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful AI assistant.\n \nYou may answer ANY general knowledge question.\n \nConversation summaries and memories are ONLY to provide helpful context\n(e.g. preferences, ongoing plans).\n \nIf a user asks something unrelated to the current topic,\nanswer it normally using general knowledge.\n \nDo NOT restrict yourself to previous topics unless the user explicitly asks."
    },
    {
      "role": "user",
      "content": "i want to kill myself"
    },
    {
      "role": "assistant",
      "content": "I'm really sorry to hear you're feeling this way. Please know that you're not alone and there's help available. You can connect with people who can support you by calling or texting 988 anytime in the US and Canada. In the UK, you can call 111. These services are free, confidential, and available 24/7. Please reach out to them."
    },
    {
      "role": "user",
      "content": "tell me about taj mahal"
    },
    {
      "role": "assistant",
      "content": "The Taj Mahal is an iconic ivory-white marble mausoleum located on the right bank of the Yamuna River in Agra, Uttar Pradesh, India. It was commissioned in 1632 by the Mughal emperor Shah Jahan to house the tomb of his favorite wife, Mumtaz Mahal.\n\nHere are some key facts about it:\n*   **Architectural Style:** It is a masterpiece of Mughal architecture, combining elements from Persian, Islamic, and Indian architectural styles.\n*   **Construction:** It took approximately 20,000 artisans and craftsmen and over 1,000 elephants to transport building materials. Construction was largely completed by 1643, but work continued on other phases of the project for another 10 years.\n*   **Materials:** It is built using white marble from Makrana, Rajasthan, adorned with intricate inlay work of semi-precious stones.\n*   **Complex:** The Taj Mahal complex includes not only the mausoleum itself but also a mosque, a guest house (jawab), and extensive gardens with a large reflecting pool.\n*   **Recognition:** It is widely considered one of the most beautiful buildings in the world and is a UNESCO World Heritage Site. It attracts millions of visitors annually."
    },
    {
      "role": "user",
      "content": "where do i live?"
    },
    {
      "role": "user",
      "content": "where do i live?"
    }
  ],
  "safety": {
    "safe": true,
    "message": null
  },
  "memory_action": {},
  "memory_data": [
    {
      "memory_id": 12,
      "user_id": 3,
      "text": "I live in Fatehabad.",
      "confidence": 0.95,
      "created_at": "2026-01-28T09:46:48.251118+05:30",
      "expires_at": "2026-02-27T09:46:48.246971+05:30",
      "score": 0.4735436737537384
    }
  ],
  "memory_found": true,
  "tool_context": {}
}
2026-01-28 10:57:06,423 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_a7cbb6b36fde439aa16b5554b8f88834
2026-01-28 10:57:06,423 | DEBUG | openai.agents | Setting current trace: trace_a7cbb6b36fde439aa16b5554b8f88834
2026-01-28 10:57:06,424 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000001949BACEB60> with id None
2026-01-28 10:57:06,426 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000001949B4CAEC0> with id None
2026-01-28 10:57:06,426 | DEBUG | openai.agents | Calling LLM
2026-01-28 10:57:06,427 | DEBUG | LiteLLM | 

2026-01-28 10:57:06,427 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 10:57:06,429 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "i want to kill myself"}, {"role": "assistant", "content": "I\'m really sorry to hear you\'re feeling this way. Please know that you\'re not alone and there\'s help available. You can connect with people who can support you by calling or texting 988 anytime in the US and Canada. In the UK, you can call 111. These services are free, confidential, and available 24/7. Please reach out to them."}, {"role": "user", "content": "tell me about taj mahal"}, {"role": "assistant", "content": "The Taj Mahal is an iconic ivory-white marble mausoleum located on the right bank of the Yamuna River in Agra, Uttar Pradesh, India. It was commissioned in 1632 by the Mughal emperor Shah Jahan to house the tomb of his favorite wife, Mumtaz Mahal.\\n\\nHere are some key facts about it:\\n*   **Architectural Style:** It is a masterpiece of Mughal architecture, combining elements from Persian, Islamic, and Indian architectural styles.\\n*   **Construction:** It took approximately 20,000 artisans and craftsmen and over 1,000 elephants to transport building materials. Construction was largely completed by 1643, but work continued on other phases of the project for another 10 years.\\n*   **Materials:** It is built using white marble from Makrana, Rajasthan, adorned with intricate inlay work of semi-precious stones.\\n*   **Complex:** The Taj Mahal complex includes not only the mausoleum itself but also a mosque, a guest house (jawab), and extensive gardens with a large reflecting pool.\\n*   **Recognition:** It is widely considered one of the most beautiful buildings in the world and is a UNESCO World Heritage Site. It attracts millions of visitors annually."}, {"role": "user", "content": "where do i live?"}, {"role": "user", "content": "where do i live?"}], "safety": {"safe": true, "message": null}, "memory_action": {}, "memory_data": [{"memory_id": 12, "user_id": 3, "text": "I live in Fatehabad.", "confidence": 0.95, "created_at": "2026-01-28T09:46:48.251118+05:30", "expires_at": "2026-02-27T09:46:48.246971+05:30", "score": 0.4735436737537384}], "memory_found": true, "tool_context": {}}'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=True, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 10:57:06,430 | DEBUG | LiteLLM | 

2026-01-28 10:57:06,431 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 10:57:06,432 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 10:57:06,433 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 10:57:06,438 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 10:57:06,439 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 10:57:06,440 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "i want to kill myself"}, {"role": "assistant", "content": "I\'m really sorry to hear you\'re feeling this way. Please know that you\'re not alone and there\'s help available. You can connect with people who can support you by calling or texting 988 anytime in the US and Canada. In the UK, you can call 111. These services are free, confidential, and available 24/7. Please reach out to them."}, {"role": "user", "content": "tell me about taj mahal"}, {"role": "assistant", "content": "The Taj Mahal is an iconic ivory-white marble mausoleum located on the right bank of the Yamuna River in Agra, Uttar Pradesh, India. It was commissioned in 1632 by the Mughal emperor Shah Jahan to house the tomb of his favorite wife, Mumtaz Mahal.\\n\\nHere are some key facts about it:\\n*   **Architectural Style:** It is a masterpiece of Mughal architecture, combining elements from Persian, Islamic, and Indian architectural styles.\\n*   **Construction:** It took approximately 20,000 artisans and craftsmen and over 1,000 elephants to transport building materials. Construction was largely completed by 1643, but work continued on other phases of the project for another 10 years.\\n*   **Materials:** It is built using white marble from Makrana, Rajasthan, adorned with intricate inlay work of semi-precious stones.\\n*   **Complex:** The Taj Mahal complex includes not only the mausoleum itself but also a mosque, a guest house (jawab), and extensive gardens with a large reflecting pool.\\n*   **Recognition:** It is widely considered one of the most beautiful buildings in the world and is a UNESCO World Heritage Site. It attracts millions of visitors annually."}, {"role": "user", "content": "where do i live?"}, {"role": "user", "content": "where do i live?"}], "safety": {"safe": true, "message": null}, "memory_action": {}, "memory_data": [{"memory_id": 12, "user_id": 3, "text": "I live in Fatehabad.", "confidence": 0.95, "created_at": "2026-01-28T09:46:48.251118+05:30", "expires_at": "2026-02-27T09:46:48.246971+05:30", "score": 0.4735436737537384}], "memory_found": true, "tool_context": {}}'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 10:57:06,442 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': True}
2026-01-28 10:57:06,444 | DEBUG | LiteLLM | Final returned optional params: {'stream': True}
2026-01-28 10:57:06,446 | DEBUG | LiteLLM | self.optional_params: {'stream': True}
2026-01-28 10:57:06,447 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:57:06,450 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:57:06,451 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?key=*****=sse \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "i want to kill myself"}, {"role": "assistant", "content": "I\'m really sorry to hear you\'re feeling this way. Please know that you\'re not alone and there\'s help available. You can connect with people who can support you by calling or texting 988 anytime in the US and Canada. In the UK, you can call 111. These services are free, confidential, and available 24/7. Please reach out to them."}, {"role": "user", "content": "tell me about taj mahal"}, {"role": "assistant", "content": "The Taj Mahal is an iconic ivory-white marble mausoleum located on the right bank of the Yamuna River in Agra, Uttar Pradesh, India. It was commissioned in 1632 by the Mughal emperor Shah Jahan to house the tomb of his favorite wife, Mumtaz Mahal.\\n\\nHere are some key facts about it:\\n*   **Architectural Style:** It is a masterpiece of Mughal architecture, combining elements from Persian, Islamic, and Indian architectural styles.\\n*   **Construction:** It took approximately 20,000 artisans and craftsmen and over 1,000 elephants to transport building materials. Construction was largely completed by 1643, but work continued on other phases of the project for another 10 years.\\n*   **Materials:** It is built using white marble from Makrana, Rajasthan, adorned with intricate inlay work of semi-precious stones.\\n*   **Complex:** The Taj Mahal complex includes not only the mausoleum itself but also a mosque, a guest house (jawab), and extensive gardens with a large reflecting pool.\\n*   **Recognition:** It is widely considered one of the most beautiful buildings in the world and is a UNESCO World Heritage Site. It attracts millions of visitors annually."}, {"role": "user", "content": "where do i live?"}, {"role": "user", "content": "where do i live?"}], "safety": {"safe": true, "message": null}, "memory_action": {}, "memory_data": [{"memory_id": 12, "user_id": 3, "text": "I live in Fatehabad.", "confidence": 0.95, "created_at": "2026-01-28T09:46:48.251118+05:30", "expires_at": "2026-02-27T09:46:48.246971+05:30", "score": 0.4735436737537384}], "memory_found": true, "tool_context": {}}'}]}], 'system_instruction': {'parts': [{'text': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.'}]}, 'generationConfig': {}}'
[0m

2026-01-28 10:57:06,454 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 10:57:06,456 | DEBUG | LiteLLM | messages in token_counter: None, text in token_counter: 
2026-01-28 10:57:06,458 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:57:06,459 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:57:06,461 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:57:06,463 | DEBUG | LiteLLM | response_cost: 0.0
2026-01-28 10:57:07,664 | DEBUG | LiteLLM | RAW RESPONSE:
first stream response received


2026-01-28 10:57:07,667 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': 'You live in Fatehabad.'}], 'role': 'model'}, 'finishReason': 'STOP', 'index': 0}], 'usageMetadata': {'promptTokenCount': 815, 'candidatesTokenCount': 7, 'totalTokenCount': 860, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 815}], 'thoughtsTokenCount': 38}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'Kp55af6sJvyrjuMPm6uw8QM'}
2026-01-28 10:57:07,668 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='Kp55af6sJvyrjuMPm6uw8QM', created=1769578027, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='You live in Fatehabad.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=45, prompt_tokens=815, total_tokens=860, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=38, rejected_prediction_tokens=None, text_tokens=7, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=815, image_tokens=None)))
2026-01-28 10:57:07,674 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content='You live in Fatehabad.', role='assistant', function_call=None, tool_calls=None, audio=None)
2026-01-28 10:57:07,675 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='Kp55af6sJvyrjuMPm6uw8QM', created=1769578027, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='You live in Fatehabad.', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=45, prompt_tokens=815, total_tokens=860, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=38, rejected_prediction_tokens=None, text_tokens=7, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=815, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-28 10:57:07,683 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=False
2026-01-28 10:57:07,684 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=False
2026-01-28 10:57:07,685 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 10:57:07,686 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 10:57:07,687 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 10:57:07,688 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 10:57:07,690 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:57:07,691 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:57:07,692 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:57:07,693 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:57:07,694 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:57:07,696 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:57:07,697 | DEBUG | LiteLLM | response_cost: 0.000357
2026-01-28 10:57:07,700 | DEBUG | LiteLLM | response_cost: 0.000357
2026-01-28 10:57:07,701 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 10:57:07,701 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.000357
2026-01-28 10:57:07,703 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:57:07,705 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 10:57:07,705 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:57:07,706 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 10:57:07,707 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 10:57:07,709 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 10:57:07,712 | DEBUG | openai.agents | Resetting current trace
2026-01-28 10:57:07,719 | INFO | session_summary | üß† SUMMARY_START | session_id=247fa020-cbe1-44cf-bb43-d080c8b72d3d
2026-01-28 10:57:07,727 | INFO | session_summary | üìä Unsummarized messages = 6
2026-01-28 10:57:07,729 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-28 10:57:07,730 | INFO | chat | ‚úÖ Stream complete | tokens=1 | time=4.36s
2026-01-28 10:57:09,500 | DEBUG | httpcore.connection | close.started
2026-01-28 10:57:09,501 | DEBUG | httpcore.connection | close.complete
2026-01-28 10:57:09,502 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 10:57:09,530 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001949BA9F130>
2026-01-28 10:57:09,532 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x00000194900B9BC0> server_hostname='api.openai.com' timeout=5.0
2026-01-28 10:57:09,549 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001949BA9D720>
2026-01-28 10:57:09,549 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 10:57:09,551 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 10:57:09,551 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 10:57:09,552 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 10:57:09,552 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 10:57:10,654 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 05:27:09 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_8df63b977d9d27879d4ff3df183aeb1e'), (b'openai-processing-ms', b'134'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'137'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4e143888fc5978-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 10:57:10,655 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 10:57:10,655 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 10:57:10,655 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 10:57:10,655 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 10:57:10,656 | DEBUG | openai.agents | Exported 8 items
2026-01-28 11:18:58,474 | DEBUG | LiteLLM | LoggingWorker cancelled during shutdown
2026-01-28 11:18:58,478 | DEBUG | LiteLLM | [LoggingWorker] atexit: Queue is empty
2026-01-28 11:18:58,479 | DEBUG | asyncio | Using proactor: IocpProactor
2026-01-28 11:18:58,485 | DEBUG | openai.agents | Shutting down trace provider
2026-01-28 11:18:58,485 | DEBUG | openai.agents | Shutting down trace processor <agents.tracing.processors.BatchTraceProcessor object at 0x00000194900EF9D0>
2026-01-28 11:19:18,712 | INFO | sentence_transformers.SentenceTransformer | Use pytorch device_name: cpu
2026-01-28 11:19:18,712 | INFO | sentence_transformers.SentenceTransformer | Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2026-01-28 11:19:18,716 | DEBUG | urllib3.connectionpool | Starting new HTTPS connection (1): huggingface.co:443
2026-01-28 11:19:19,006 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-28 11:19:19,019 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-28 11:19:19,254 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-28 11:19:19,268 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-28 11:19:19,503 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-28 11:19:19,526 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-28 11:19:19,765 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md HTTP/1.1" 307 0
2026-01-28 11:19:19,781 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/README.md HTTP/1.1" 200 0
2026-01-28 11:19:20,025 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-28 11:19:20,036 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-28 11:19:20,288 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json HTTP/1.1" 307 0
2026-01-28 11:19:20,303 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/sentence_bert_config.json HTTP/1.1" 200 0
2026-01-28 11:19:20,569 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json HTTP/1.1" 404 0
2026-01-28 11:19:20,818 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1" 307 0
2026-01-28 11:19:20,831 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json HTTP/1.1" 200 0
2026-01-28 11:19:21,163 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/1.1" 307 0
2026-01-28 11:19:21,176 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json HTTP/1.1" 200 0
2026-01-28 11:19:21,416 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1" 404 64
2026-01-28 11:19:21,775 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main?recursive=True&expand=False HTTP/1.1" 200 6465
2026-01-28 11:19:22,096 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json HTTP/1.1" 307 0
2026-01-28 11:19:22,109 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json HTTP/1.1" 200 0
2026-01-28 11:19:22,349 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2 HTTP/1.1" 200 6885
2026-01-28 11:19:23,541 | INFO | main | üöÄ FastAPI application starting
2026-01-28 11:19:44,029 | INFO | chat | üì© Chat request | user_id=3 | request_id=672c854f-7902-42af-86b7-972e863f712d
2026-01-28 11:19:44,045 | INFO | chat | üßë User message | i want to sell drugs
2026-01-28 11:19:44,111 | INFO | chat | ü§ñ Stream started | session_id=247fa020-cbe1-44cf-bb43-d080c8b72d3d | request_id=672c854f-7902-42af-86b7-972e863f712d
2026-01-28 11:19:44,112 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-28 11:19:44,114 | INFO | orchestrator | üßë USER_INPUT | i want to sell drugs
2026-01-28 11:19:44,116 | INFO | orchestrator | üß© CONTEXT | user_id=3 | enable_memory=True | db=None
2026-01-28 11:19:44,119 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-28 11:19:44,124 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_7ca721a77422415ba69ac5bf4a5e44b5
2026-01-28 11:19:44,127 | DEBUG | openai.agents | Setting current trace: trace_7ca721a77422415ba69ac5bf4a5e44b5
2026-01-28 11:19:44,131 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x0000026A18FA6340> with id None
2026-01-28 11:19:44,132 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-28 11:19:45,874 | DEBUG | httpcore.connection | connect_tcp.started host='raw.githubusercontent.com' port=443 local_address=None timeout=5 socket_options=None
2026-01-28 11:19:45,927 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000026A1B420F70>
2026-01-28 11:19:45,927 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000026A1B0EA740> server_hostname='raw.githubusercontent.com' timeout=5
2026-01-28 11:19:45,969 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000026A1B420F40>
2026-01-28 11:19:45,970 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'GET']>
2026-01-28 11:19:45,972 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 11:19:45,973 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'GET']>
2026-01-28 11:19:45,973 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 11:19:45,974 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'GET']>
2026-01-28 11:19:46,016 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Connection', b'keep-alive'), (b'Content-Length', b'62476'), (b'Cache-Control', b'max-age=300'), (b'Content-Security-Policy', b"default-src 'none'; style-src 'unsafe-inline'; sandbox"), (b'Content-Type', b'text/plain; charset=utf-8'), (b'ETag', b'W/"f6dad8a5d9e28465b3d70ca31700cb6ef9bf2cd7c9d0fa0c0c5cdf23a1bb3382"'), (b'Strict-Transport-Security', b'max-age=31536000'), (b'X-Content-Type-Options', b'nosniff'), (b'X-Frame-Options', b'deny'), (b'X-XSS-Protection', b'1; mode=block'), (b'X-GitHub-Request-Id', b'2442:60E63:51359:95C7D:6978AD63'), (b'Content-Encoding', b'gzip'), (b'Accept-Ranges', b'bytes'), (b'Date', b'Wed, 28 Jan 2026 05:49:45 GMT'), (b'Via', b'1.1 varnish'), (b'X-Served-By', b'cache-bom-vanm7210038-BOM'), (b'X-Cache', b'HIT'), (b'X-Cache-Hits', b'34'), (b'X-Timer', b'S1769579385.342170,VS0,VE0'), (b'Vary', b'Authorization,Accept-Encoding'), (b'Access-Control-Allow-Origin', b'*'), (b'Cross-Origin-Resource-Policy', b'cross-origin'), (b'X-Fastly-Request-ID', b'e9844be447cda1ddd9b055d31b8f680c44b46c98'), (b'Expires', b'Wed, 28 Jan 2026 05:54:45 GMT'), (b'Source-Age', b'200')])
2026-01-28 11:19:46,019 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'GET']>
2026-01-28 11:19:46,059 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 11:19:46,060 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 11:19:46,060 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 11:19:46,062 | DEBUG | httpcore.connection | close.started
2026-01-28 11:19:46,063 | DEBUG | httpcore.connection | close.complete
2026-01-28 11:19:46,806 | DEBUG | LiteLLM | [Non-Blocking] Unable to import GenericAPILogger - LiteLLM Enterprise Feature - No module named 'litellm_enterprise'
2026-01-28 11:19:47,543 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-28 11:19:47,544 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-28 11:19:47,545 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-28 11:19:47,552 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-28 11:19:47,553 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-28 11:19:47,553 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-28 11:19:47,554 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-28 11:19:47,554 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-28 11:19:47,554 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-28 11:19:47,556 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-28 11:19:47,556 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-28 11:19:47,556 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-28 11:19:47,580 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000026A1AB84760> with id None
2026-01-28 11:19:47,580 | DEBUG | openai.agents | Calling LLM
2026-01-28 11:19:47,584 | DEBUG | LiteLLM | 

2026-01-28 11:19:47,585 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 11:19:47,586 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output JSON Schema (MANDATORY):\n\n{\n  "safe": boolean,\n  "reason": string | null\n}\n\n### Rules:\n- SAFE:\n  { "safe": true, "reason": null }\n\n- UNSAFE:\n  { "safe": false, "reason": "<short reason>" }\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'i want to sell drugs'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 11:19:47,586 | DEBUG | LiteLLM | 

2026-01-28 11:19:47,587 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 11:19:47,588 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 11:19:47,589 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 11:19:47,604 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 11:19:47,607 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 11:19:47,610 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output JSON Schema (MANDATORY):\n\n{\n  "safe": boolean,\n  "reason": string | null\n}\n\n### Rules:\n- SAFE:\n  { "safe": true, "reason": null }\n\n- UNSAFE:\n  { "safe": false, "reason": "<short reason>" }\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'i want to sell drugs'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 11:19:47,611 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 11:19:47,612 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 11:19:47,612 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 11:19:47,613 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:19:47,801 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:19:47,802 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'i want to sell drugs'}]}], 'system_instruction': {'parts': [{'text': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output JSON Schema (MANDATORY):\n\n{\n  "safe": boolean,\n  "reason": string | null\n}\n\n### Rules:\n- SAFE:\n  { "safe": true, "reason": null }\n\n- UNSAFE:\n  { "safe": false, "reason": "<short reason>" }\n\nReturn ONLY valid JSON.\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 11:19:48,773 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\"safe\": false, \"reason\": \"User expresses intent to engage in illegal drug dealing.\"}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 135,
    "candidatesTokenCount": 20,
    "totalTokenCount": 180,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 135
      }
    ],
    "thoughtsTokenCount": 25
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "e6N5aevfObXPg8UPj4rI2QQ"
}



2026-01-28 11:19:48,780 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 11:19:48,784 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 11:19:48,785 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 11:19:48,786 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 11:19:48,787 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:19:48,789 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:19:48,790 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:19:48,791 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:19:48,792 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 11:19:48,793 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 11:19:48,795 | DEBUG | LiteLLM | response_cost: 0.00015299999999999998
2026-01-28 11:19:48,798 | DEBUG | openai.agents | Received model response
2026-01-28 11:19:48,797 | DEBUG | LiteLLM | response_cost: 0.00015299999999999998
2026-01-28 11:19:48,818 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x0000026A243E2AD0>>
2026-01-28 11:19:48,819 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 11:19:48,820 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:19:48,821 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 11:19:48,821 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 11:19:48,823 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 11:19:48,824 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 11:19:48,826 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 11:19:48,824 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 11:19:48,828 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 11:19:48,829 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 11:19:48,829 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:19:48,830 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:19:48,831 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 11:19:48,831 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 11:19:48,831 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 11:19:48,832 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:19:48,835 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 11:19:48,836 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 11:19:48,837 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.00015299999999999998
2026-01-28 11:19:48,838 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 11:19:48,839 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:19:48,840 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 11:19:48,843 | DEBUG | openai.agents | Resetting current trace
2026-01-28 11:19:48,844 | WARNING | orchestrator | üö´ Safety blocked request
2026-01-28 11:19:48,970 | INFO | session_summary | üß† SUMMARY_START | session_id=247fa020-cbe1-44cf-bb43-d080c8b72d3d
2026-01-28 11:19:48,981 | INFO | session_summary | üìä Unsummarized messages = 8
2026-01-28 11:19:48,982 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-28 11:19:48,983 | INFO | chat | ‚úÖ Stream complete | tokens=38 | time=4.95s
2026-01-28 11:19:49,260 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 11:19:49,293 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000026A1AA55810>
2026-01-28 11:19:49,293 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000026A18F49BC0> server_hostname='api.openai.com' timeout=5.0
2026-01-28 11:19:49,311 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000026A1AA55150>
2026-01-28 11:19:49,311 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 11:19:49,311 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 11:19:49,312 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 11:19:49,312 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 11:19:49,312 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 11:19:49,959 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 05:49:49 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_9f1f4f8f3b985a4541cbc94d0fd825f5'), (b'openai-processing-ms', b'318'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'322'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=QjZ3ltWthnVCp_5MnxBU.LduVyiG7.8zI6oYNHN897g-1769579389-1.0.1.1-AgFjE.Ig9g1Qq0u7_BHWYBIHJWC5SR7.09jhH.EmRxXm2oQb_YqC929INC56P.yGeZXkOacVT7WZxOPvk_Ys.18j.s7d_bI7xwOFci_F54I; path=/; expires=Wed, 28-Jan-26 06:19:49 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=4YkD6dAsvjyN7IHAtxCghLThBsNpGpuhtsPxGKfCit4-1769579389274-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4e356afe36cd8c-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 11:19:49,960 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 11:19:49,961 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 11:19:49,961 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 11:19:49,961 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 11:19:49,962 | DEBUG | openai.agents | Exported 3 items
2026-01-28 11:20:19,654 | INFO | chat | üì© Chat request | user_id=3 | request_id=d82464ab-af8d-47c7-85a6-d0fa37c5217c
2026-01-28 11:20:19,656 | INFO | chat | üßë User message | i want to kill someone
2026-01-28 11:20:19,695 | INFO | chat | ü§ñ Stream started | session_id=247fa020-cbe1-44cf-bb43-d080c8b72d3d | request_id=d82464ab-af8d-47c7-85a6-d0fa37c5217c
2026-01-28 11:20:19,697 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-28 11:20:19,699 | INFO | orchestrator | üßë USER_INPUT | i want to kill someone
2026-01-28 11:20:19,701 | INFO | orchestrator | üß© CONTEXT | user_id=3 | enable_memory=True | db=None
2026-01-28 11:20:19,703 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-28 11:20:19,706 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_802b43c96ec9448aa32b5251b52d1c71
2026-01-28 11:20:19,707 | DEBUG | openai.agents | Setting current trace: trace_802b43c96ec9448aa32b5251b52d1c71
2026-01-28 11:20:19,708 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x0000026A24943C40> with id None
2026-01-28 11:20:19,709 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-28 11:20:19,711 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000026A251415A0> with id None
2026-01-28 11:20:19,711 | DEBUG | openai.agents | Calling LLM
2026-01-28 11:20:19,712 | DEBUG | LiteLLM | 

2026-01-28 11:20:19,713 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 11:20:19,714 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output JSON Schema (MANDATORY):\n\n{\n  "safe": boolean,\n  "reason": string | null\n}\n\n### Rules:\n- SAFE:\n  { "safe": true, "reason": null }\n\n- UNSAFE:\n  { "safe": false, "reason": "<short reason>" }\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'i want to kill someone'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 11:20:19,715 | DEBUG | LiteLLM | 

2026-01-28 11:20:19,720 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 11:20:19,724 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 11:20:19,727 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 11:20:19,733 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 11:20:19,738 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 11:20:19,741 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output JSON Schema (MANDATORY):\n\n{\n  "safe": boolean,\n  "reason": string | null\n}\n\n### Rules:\n- SAFE:\n  { "safe": true, "reason": null }\n\n- UNSAFE:\n  { "safe": false, "reason": "<short reason>" }\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'i want to kill someone'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 11:20:19,742 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 11:20:19,744 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 11:20:19,746 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 11:20:19,747 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:20:19,751 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:20:19,754 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'i want to kill someone'}]}], 'system_instruction': {'parts': [{'text': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output JSON Schema (MANDATORY):\n\n{\n  "safe": boolean,\n  "reason": string | null\n}\n\n### Rules:\n- SAFE:\n  { "safe": true, "reason": null }\n\n- UNSAFE:\n  { "safe": false, "reason": "<short reason>" }\n\nReturn ONLY valid JSON.\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 11:20:20,808 | DEBUG | httpcore.connection | close.started
2026-01-28 11:20:20,810 | DEBUG | httpcore.connection | close.complete
2026-01-28 11:20:20,811 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 11:20:20,821 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000026A24B3A890>
2026-01-28 11:20:20,822 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000026A18F49BC0> server_hostname='api.openai.com' timeout=5.0
2026-01-28 11:20:20,841 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000026A24B3A860>
2026-01-28 11:20:20,842 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 11:20:20,844 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 11:20:20,845 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 11:20:20,847 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 11:20:20,848 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 11:20:21,062 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\"safe\": false, \"reason\": \"User expresses intent to kill someone.\"}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 135,
    "candidatesTokenCount": 17,
    "totalTokenCount": 182,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 135
      }
    ],
    "thoughtsTokenCount": 30
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "nKN5acCFC76rjuMP4OKw6AM"
}



2026-01-28 11:20:21,068 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 11:20:21,071 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 11:20:21,071 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 11:20:21,073 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:20:21,074 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 11:20:21,076 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:20:21,077 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:20:21,078 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 11:20:21,080 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:20:21,081 | DEBUG | LiteLLM | response_cost: 0.000158
2026-01-28 11:20:21,083 | DEBUG | openai.agents | Received model response
2026-01-28 11:20:21,082 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 11:20:21,086 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x0000026A24B3A2C0>>
2026-01-28 11:20:21,088 | DEBUG | LiteLLM | response_cost: 0.000158
2026-01-28 11:20:21,089 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 11:20:21,091 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 11:20:21,097 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 11:20:21,096 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 11:20:21,100 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:20:21,102 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 11:20:21,104 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 11:20:21,106 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 11:20:21,109 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 11:20:21,110 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:20:21,112 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 11:20:21,113 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:20:21,114 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 11:20:21,119 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 11:20:21,120 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 11:20:21,123 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:20:21,124 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 11:20:21,125 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 11:20:21,126 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.000158
2026-01-28 11:20:21,129 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 11:20:21,131 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:20:21,134 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 11:20:21,138 | DEBUG | openai.agents | Resetting current trace
2026-01-28 11:20:21,139 | WARNING | orchestrator | üö´ Safety blocked request
2026-01-28 11:20:21,166 | INFO | session_summary | üß† SUMMARY_START | session_id=247fa020-cbe1-44cf-bb43-d080c8b72d3d
2026-01-28 11:20:21,173 | INFO | session_summary | üìä Unsummarized messages = 10
2026-01-28 11:20:21,174 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-28 11:20:21,177 | INFO | chat | ‚úÖ Stream complete | tokens=38 | time=1.52s
2026-01-28 11:20:22,650 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 05:50:21 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_43e210f8379ca0271f60f52d05724078'), (b'openai-processing-ms', b'586'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'589'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4e36301c6d54d4-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 11:20:22,652 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 11:20:22,653 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 11:20:22,654 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 11:20:22,655 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 11:20:22,656 | DEBUG | openai.agents | Exported 1 items
2026-01-28 11:20:22,660 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 11:20:22,662 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 11:20:22,663 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 11:20:22,665 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 11:20:22,665 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 11:20:23,905 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 05:50:23 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_eaea68c250e655c1c97c598712919565'), (b'openai-processing-ms', b'358'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'361'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4e363b69ab54d4-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 11:20:23,907 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 11:20:23,908 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 11:20:23,908 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 11:20:23,909 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 11:20:23,910 | DEBUG | openai.agents | Exported 2 items
2026-01-28 11:21:56,723 | INFO | chat | üì© Chat request | user_id=3 | request_id=e56b3936-9f48-4114-9608-efdcc0a7ea6f
2026-01-28 11:21:56,725 | INFO | chat | üßë User message | hii
2026-01-28 11:21:56,758 | INFO | chat | ü§ñ Stream started | session_id=247fa020-cbe1-44cf-bb43-d080c8b72d3d | request_id=e56b3936-9f48-4114-9608-efdcc0a7ea6f
2026-01-28 11:21:56,760 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-28 11:21:56,762 | INFO | orchestrator | üßë USER_INPUT | hii
2026-01-28 11:21:56,764 | INFO | orchestrator | üß© CONTEXT | user_id=3 | enable_memory=True | db=None
2026-01-28 11:21:56,767 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-28 11:21:56,770 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_9ea154b89e2d4a60ac43d7790f550839
2026-01-28 11:21:56,771 | DEBUG | openai.agents | Setting current trace: trace_9ea154b89e2d4a60ac43d7790f550839
2026-01-28 11:21:56,772 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x0000026A24943150> with id None
2026-01-28 11:21:56,773 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-28 11:21:56,774 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000026A1AC425C0> with id None
2026-01-28 11:21:56,776 | DEBUG | openai.agents | Calling LLM
2026-01-28 11:21:56,777 | DEBUG | LiteLLM | 

2026-01-28 11:21:56,778 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 11:21:56,780 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output JSON Schema (MANDATORY):\n\n{\n  "safe": boolean,\n  "reason": string | null\n}\n\n### Rules:\n- SAFE:\n  { "safe": true, "reason": null }\n\n- UNSAFE:\n  { "safe": false, "reason": "<short reason>" }\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'hii'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 11:21:56,783 | DEBUG | LiteLLM | 

2026-01-28 11:21:56,787 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 11:21:56,789 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 11:21:56,790 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 11:21:56,793 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 11:21:56,795 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 11:21:56,797 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output JSON Schema (MANDATORY):\n\n{\n  "safe": boolean,\n  "reason": string | null\n}\n\n### Rules:\n- SAFE:\n  { "safe": true, "reason": null }\n\n- UNSAFE:\n  { "safe": false, "reason": "<short reason>" }\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'hii'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 11:21:56,797 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 11:21:56,799 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 11:21:56,799 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 11:21:56,800 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:21:56,802 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:21:56,803 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'hii'}]}], 'system_instruction': {'parts': [{'text': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output JSON Schema (MANDATORY):\n\n{\n  "safe": boolean,\n  "reason": string | null\n}\n\n### Rules:\n- SAFE:\n  { "safe": true, "reason": null }\n\n- UNSAFE:\n  { "safe": false, "reason": "<short reason>" }\n\nReturn ONLY valid JSON.\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 11:22:00,437 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\"safe\": true, \"reason\": null}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 131,
    "candidatesTokenCount": 10,
    "totalTokenCount": 182,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 131
      }
    ],
    "thoughtsTokenCount": 41
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "_6N5aevkIsHB4-EPqtXgkAI"
}



2026-01-28 11:22:00,441 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 11:22:00,444 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 11:22:00,444 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 11:22:00,446 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:22:00,447 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 11:22:00,449 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:22:00,450 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:22:00,454 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 11:22:00,456 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:22:00,458 | DEBUG | LiteLLM | response_cost: 0.00016680000000000002
2026-01-28 11:22:00,460 | DEBUG | openai.agents | Received model response
2026-01-28 11:22:00,459 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 11:22:00,463 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x0000026A24417BE0>>
2026-01-28 11:22:00,464 | DEBUG | LiteLLM | response_cost: 0.00016680000000000002
2026-01-28 11:22:00,465 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 11:22:00,468 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 11:22:00,470 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 11:22:00,469 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 11:22:00,471 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:22:00,472 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 11:22:00,475 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 11:22:00,476 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 11:22:00,479 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 11:22:00,481 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:22:00,484 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 11:22:00,486 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:22:00,487 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 11:22:00,489 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 11:22:00,491 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 11:22:00,493 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:22:00,495 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 11:22:00,496 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 11:22:00,497 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.00016680000000000002
2026-01-28 11:22:00,502 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 11:22:00,503 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:22:00,504 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 11:22:00,508 | DEBUG | openai.agents | Resetting current trace
2026-01-28 11:22:00,509 | INFO | orchestrator | ‚úÖ Safety passed
2026-01-28 11:22:00,511 | INFO | orchestrator | üì• Fetching semantic memory (via MemoryService)
2026-01-28 11:22:00,623 | INFO | orchestrator | üß† MEMORY_RESULT | found=False | count=0
2026-01-28 11:22:00,626 | INFO | orchestrator | üß≠ Router agent called
2026-01-28 11:22:00,628 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_4d78e739a35248e88aa6d01673f6eb59
2026-01-28 11:22:00,629 | DEBUG | openai.agents | Setting current trace: trace_4d78e739a35248e88aa6d01673f6eb59
2026-01-28 11:22:00,630 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x0000026A24942DE0> with id None
2026-01-28 11:22:00,631 | DEBUG | openai.agents | Running agent RouterAgent (turn 1)
2026-01-28 11:22:00,633 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000026A25142DA0> with id None
2026-01-28 11:22:00,633 | DEBUG | openai.agents | Calling LLM
2026-01-28 11:22:00,634 | DEBUG | LiteLLM | 

2026-01-28 11:22:00,635 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 11:22:00,636 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'hii'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 11:22:00,637 | DEBUG | LiteLLM | 

2026-01-28 11:22:00,639 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 11:22:00,640 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 11:22:00,641 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 11:22:00,645 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 11:22:00,646 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 11:22:00,650 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'hii'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 11:22:00,652 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 11:22:00,654 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 11:22:00,655 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 11:22:00,657 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:22:00,659 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:22:00,660 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'hii'}]}], 'system_instruction': {'parts': [{'text': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 11:22:01,669 | DEBUG | httpcore.connection | close.started
2026-01-28 11:22:01,671 | DEBUG | httpcore.connection | close.complete
2026-01-28 11:22:01,672 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 11:22:01,695 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000026A1AB6B2E0>
2026-01-28 11:22:01,696 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000026A18F49BC0> server_hostname='api.openai.com' timeout=5.0
2026-01-28 11:22:01,740 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000026A1AB6BC70>
2026-01-28 11:22:01,741 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 11:22:01,743 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 11:22:01,744 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 11:22:01,747 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 11:22:01,748 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 11:22:02,146 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\n  \"use_memory\": false,\n  \"use_tool\": false,\n  \"tool_name\": \"none\",\n  \"tool_arguments\": null,\n  \"intent\": \"none\",\n  \"memory_key\": null\n}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 486,
    "candidatesTokenCount": 56,
    "totalTokenCount": 570,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 486
      }
    ],
    "thoughtsTokenCount": 28
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "AaR5aeazEJ2s4-EPxdmKoAQ"
}



2026-01-28 11:22:02,150 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 11:22:02,152 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 11:22:02,153 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 11:22:02,155 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:22:02,157 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 11:22:02,158 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:22:02,160 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:22:02,161 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 11:22:02,162 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:22:02,164 | DEBUG | LiteLLM | response_cost: 0.0003558
2026-01-28 11:22:02,166 | DEBUG | openai.agents | Received model response
2026-01-28 11:22:02,165 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 11:22:02,168 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x0000026A24415F60>>
2026-01-28 11:22:02,169 | DEBUG | LiteLLM | response_cost: 0.0003558
2026-01-28 11:22:02,171 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 11:22:02,173 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 11:22:02,172 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 11:22:02,173 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 11:22:02,175 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 11:22:02,176 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:22:02,179 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 11:22:02,182 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 11:22:02,183 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 11:22:02,185 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:22:02,186 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:22:02,187 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 11:22:02,189 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 11:22:02,190 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 11:22:02,192 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 11:22:02,196 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 11:22:02,197 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:22:02,198 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0003558
2026-01-28 11:22:02,199 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 11:22:02,202 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 11:22:02,204 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:22:02,205 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 11:22:02,208 | DEBUG | openai.agents | Resetting current trace
2026-01-28 11:22:02,209 | INFO | orchestrator | üß≠ ROUTER_DECISION | tool=False | memory=False | intent=none
2026-01-28 11:22:02,213 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 05:52:01 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_e587a7f84e7cd843aa7972080b325772'), (b'openai-processing-ms', b'117'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'120'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4e38a6a81d59de-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 11:22:02,215 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 11:22:02,215 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 11:22:02,212 | INFO | orchestrator | üß† REASONING_INPUT_SNAPSHOT
{
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful AI assistant.\n \nYou may answer ANY general knowledge question.\n \nConversation summaries and memories are ONLY to provide helpful context\n(e.g. preferences, ongoing plans).\n \nIf a user asks something unrelated to the current topic,\nanswer it normally using general knowledge.\n \nDo NOT restrict yourself to previous topics unless the user explicitly asks."
    },
    {
      "role": "assistant",
      "content": "You live in Fatehabad."
    },
    {
      "role": "user",
      "content": "i want to sell drugs"
    },
    {
      "role": "assistant",
      "content": "Sorry, I can\u2019t help with that request."
    },
    {
      "role": "user",
      "content": "i want to kill someone"
    },
    {
      "role": "assistant",
      "content": "Sorry, I can\u2019t help with that request."
    },
    {
      "role": "user",
      "content": "hii"
    },
    {
      "role": "user",
      "content": "hii"
    }
  ],
  "safety": {
    "safe": true,
    "reason": null,
    "message": "OK"
  },
  "memory_action": {},
  "memory_data": [],
  "memory_found": false,
  "tool_context": {}
}
2026-01-28 11:22:02,216 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 11:22:02,216 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 11:22:02,218 | DEBUG | openai.agents | Exported 4 items
2026-01-28 11:22:02,218 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_80739710b69a4c6c9951d1998512b685
2026-01-28 11:22:02,221 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 11:22:02,222 | DEBUG | openai.agents | Setting current trace: trace_80739710b69a4c6c9951d1998512b685
2026-01-28 11:22:02,224 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 11:22:02,224 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x0000026A24B19E90> with id None
2026-01-28 11:22:02,225 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 11:22:02,228 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000026A25141600> with id None
2026-01-28 11:22:02,228 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 11:22:02,229 | DEBUG | openai.agents | Calling LLM
2026-01-28 11:22:02,230 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 11:22:02,231 | DEBUG | LiteLLM | 

2026-01-28 11:22:02,232 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 11:22:02,234 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "assistant", "content": "You live in Fatehabad."}, {"role": "user", "content": "i want to sell drugs"}, {"role": "assistant", "content": "Sorry, I can\\u2019t help with that request."}, {"role": "user", "content": "i want to kill someone"}, {"role": "assistant", "content": "Sorry, I can\\u2019t help with that request."}, {"role": "user", "content": "hii"}, {"role": "user", "content": "hii"}], "safety": {"safe": true, "reason": null, "message": "OK"}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=True, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 11:22:02,235 | DEBUG | LiteLLM | 

2026-01-28 11:22:02,237 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 11:22:02,239 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 11:22:02,239 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 11:22:02,242 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 11:22:02,244 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 11:22:02,247 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "assistant", "content": "You live in Fatehabad."}, {"role": "user", "content": "i want to sell drugs"}, {"role": "assistant", "content": "Sorry, I can\\u2019t help with that request."}, {"role": "user", "content": "i want to kill someone"}, {"role": "assistant", "content": "Sorry, I can\\u2019t help with that request."}, {"role": "user", "content": "hii"}, {"role": "user", "content": "hii"}], "safety": {"safe": true, "reason": null, "message": "OK"}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 11:22:02,249 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': True}
2026-01-28 11:22:02,250 | DEBUG | LiteLLM | Final returned optional params: {'stream': True}
2026-01-28 11:22:02,251 | DEBUG | LiteLLM | self.optional_params: {'stream': True}
2026-01-28 11:22:02,252 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:22:02,254 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:22:02,256 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?key=*****=sse \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "assistant", "content": "You live in Fatehabad."}, {"role": "user", "content": "i want to sell drugs"}, {"role": "assistant", "content": "Sorry, I can\\u2019t help with that request."}, {"role": "user", "content": "i want to kill someone"}, {"role": "assistant", "content": "Sorry, I can\\u2019t help with that request."}, {"role": "user", "content": "hii"}, {"role": "user", "content": "hii"}], "safety": {"safe": true, "reason": null, "message": "OK"}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}]}], 'system_instruction': {'parts': [{'text': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.'}]}, 'generationConfig': {}}'
[0m

2026-01-28 11:22:02,259 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 11:22:02,267 | DEBUG | LiteLLM | messages in token_counter: None, text in token_counter: 
2026-01-28 11:22:02,269 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:22:02,270 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:22:02,272 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 11:22:02,274 | DEBUG | LiteLLM | response_cost: 0.0
2026-01-28 11:22:02,432 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-28 11:22:02,433 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-28 11:22:02,435 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-28 11:22:02,658 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 05:52:01 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_99e4889c9fcff87a08c0e36051580596'), (b'openai-processing-ms', b'108'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'111'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4e38a9b9b659de-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 11:22:02,659 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 11:22:02,661 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 11:22:02,661 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 11:22:02,662 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 11:22:02,663 | DEBUG | openai.agents | Exported 2 items
2026-01-28 11:22:02,666 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 11:22:02,668 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 11:22:02,669 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 11:22:02,671 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 11:22:02,672 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 11:22:03,109 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 05:52:02 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_d272a04f5f19f685a90ec5f0a5a587ad'), (b'openai-processing-ms', b'117'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'120'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4e38ac7b5159de-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 11:22:03,112 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 11:22:03,113 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 11:22:03,114 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 11:22:03,115 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 11:22:03,117 | DEBUG | openai.agents | Exported 1 items
2026-01-28 11:22:04,002 | DEBUG | LiteLLM | RAW RESPONSE:
first stream response received


2026-01-28 11:22:04,004 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': 'Hi there! How can I help you today?'}], 'role': 'model'}, 'finishReason': 'STOP', 'index': 0}], 'usageMetadata': {'promptTokenCount': 381, 'candidatesTokenCount': 10, 'totalTokenCount': 412, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 381}], 'thoughtsTokenCount': 21}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'AqR5aZjTOsjnjuMPt_WR6QM'}
2026-01-28 11:22:04,020 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='AqR5aZjTOsjnjuMPt_WR6QM', created=1769579524, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='Hi there! How can I help you today?', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=31, prompt_tokens=381, total_tokens=412, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=21, rejected_prediction_tokens=None, text_tokens=10, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=381, image_tokens=None)))
2026-01-28 11:22:04,025 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content='Hi there! How can I help you today?', role='assistant', function_call=None, tool_calls=None, audio=None)
2026-01-28 11:22:04,028 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='AqR5aZjTOsjnjuMPt_WR6QM', created=1769579524, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Hi there! How can I help you today?', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=31, prompt_tokens=381, total_tokens=412, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=21, rejected_prediction_tokens=None, text_tokens=10, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=381, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-28 11:22:04,106 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=False
2026-01-28 11:22:04,162 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=False
2026-01-28 11:22:04,162 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 11:22:04,163 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 11:22:04,163 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 11:22:04,166 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 11:22:04,166 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:22:04,168 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:22:04,168 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:22:04,170 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:22:04,171 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 11:22:04,172 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 11:22:04,174 | DEBUG | LiteLLM | response_cost: 0.0001918
2026-01-28 11:22:04,175 | DEBUG | LiteLLM | response_cost: 0.0001918
2026-01-28 11:22:04,177 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 11:22:04,178 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0001918
2026-01-28 11:22:04,179 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:22:04,181 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 11:22:04,182 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 11:22:04,183 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:22:04,185 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 11:22:04,187 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 11:22:04,190 | DEBUG | openai.agents | Resetting current trace
2026-01-28 11:22:04,207 | INFO | session_summary | üß† SUMMARY_START | session_id=247fa020-cbe1-44cf-bb43-d080c8b72d3d
2026-01-28 11:22:04,215 | INFO | session_summary | üìä Unsummarized messages = 12
2026-01-28 11:22:04,217 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-28 11:22:04,220 | INFO | chat | ‚úÖ Stream complete | tokens=1 | time=7.5s
2026-01-28 11:22:08,216 | DEBUG | httpcore.connection | close.started
2026-01-28 11:22:08,218 | DEBUG | httpcore.connection | close.complete
2026-01-28 11:22:08,219 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 11:22:08,230 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000026A1AA550F0>
2026-01-28 11:22:08,231 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000026A18F49BC0> server_hostname='api.openai.com' timeout=5.0
2026-01-28 11:22:08,251 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000026A1AA55C30>
2026-01-28 11:22:08,253 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 11:22:08,254 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 11:22:08,255 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 11:22:08,256 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 11:22:08,257 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 11:22:09,426 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 05:52:08 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_121d41d694c3833a12f760ecc875697b'), (b'openai-processing-ms', b'222'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'225'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4e38cf5ec054a8-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 11:22:09,429 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 11:22:09,430 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 11:22:09,431 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 11:22:09,431 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 11:22:09,432 | DEBUG | openai.agents | Exported 2 items
2026-01-28 11:24:02,109 | INFO | chat | üì© Chat request | user_id=3 | request_id=6976d7b2-e241-4d16-b163-5a0dbbc1fedb
2026-01-28 11:24:02,112 | INFO | chat | üßë User message | i want to buy a knife . can you tell some shops
2026-01-28 11:24:02,148 | INFO | chat | ü§ñ Stream started | session_id=247fa020-cbe1-44cf-bb43-d080c8b72d3d | request_id=6976d7b2-e241-4d16-b163-5a0dbbc1fedb
2026-01-28 11:24:02,151 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-28 11:24:02,154 | INFO | orchestrator | üßë USER_INPUT | i want to buy a knife . can you tell some shops
2026-01-28 11:24:02,158 | INFO | orchestrator | üß© CONTEXT | user_id=3 | enable_memory=True | db=None
2026-01-28 11:24:02,160 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-28 11:24:02,163 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_97d049575a7b422aade1ac781c441216
2026-01-28 11:24:02,165 | DEBUG | openai.agents | Setting current trace: trace_97d049575a7b422aade1ac781c441216
2026-01-28 11:24:02,165 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x0000026A24956C00> with id None
2026-01-28 11:24:02,167 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-28 11:24:02,170 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000026A25143E20> with id None
2026-01-28 11:24:02,170 | DEBUG | openai.agents | Calling LLM
2026-01-28 11:24:02,171 | DEBUG | LiteLLM | 

2026-01-28 11:24:02,173 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 11:24:02,174 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output JSON Schema (MANDATORY):\n\n{\n  "safe": boolean,\n  "reason": string | null\n}\n\n### Rules:\n- SAFE:\n  { "safe": true, "reason": null }\n\n- UNSAFE:\n  { "safe": false, "reason": "<short reason>" }\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'i want to buy a knife . can you tell some shops'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 11:24:02,177 | DEBUG | LiteLLM | 

2026-01-28 11:24:02,179 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 11:24:02,182 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 11:24:02,183 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 11:24:02,186 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 11:24:02,188 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 11:24:02,190 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output JSON Schema (MANDATORY):\n\n{\n  "safe": boolean,\n  "reason": string | null\n}\n\n### Rules:\n- SAFE:\n  { "safe": true, "reason": null }\n\n- UNSAFE:\n  { "safe": false, "reason": "<short reason>" }\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'i want to buy a knife . can you tell some shops'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 11:24:02,192 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 11:24:02,192 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 11:24:02,194 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 11:24:02,195 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:24:02,198 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:24:02,200 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'i want to buy a knife . can you tell some shops'}]}], 'system_instruction': {'parts': [{'text': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output JSON Schema (MANDATORY):\n\n{\n  "safe": boolean,\n  "reason": string | null\n}\n\n### Rules:\n- SAFE:\n  { "safe": true, "reason": null }\n\n- UNSAFE:\n  { "safe": false, "reason": "<short reason>" }\n\nReturn ONLY valid JSON.\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 11:24:02,767 | DEBUG | httpcore.connection | close.started
2026-01-28 11:24:02,768 | DEBUG | httpcore.connection | close.complete
2026-01-28 11:24:02,769 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 11:24:02,788 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000026A24985060>
2026-01-28 11:24:02,789 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000026A18F49BC0> server_hostname='api.openai.com' timeout=5.0
2026-01-28 11:24:02,807 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000026A24985090>
2026-01-28 11:24:02,808 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 11:24:02,810 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 11:24:02,810 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 11:24:02,812 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 11:24:02,812 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 11:24:03,232 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 05:54:02 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_d7cc84db015238acc778579b60e7ccd9'), (b'openai-processing-ms', b'87'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'91'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4e3b9b5e4359f3-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 11:24:03,234 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 11:24:03,235 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 11:24:03,236 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 11:24:03,236 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 11:24:03,237 | DEBUG | openai.agents | Exported 1 items
2026-01-28 11:24:04,202 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\"safe\": true, \"reason\": null}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 142,
    "candidatesTokenCount": 10,
    "totalTokenCount": 227,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 142
      }
    ],
    "thoughtsTokenCount": 75
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "e6R5abvaFNqCg8UP5rfD8AE"
}



2026-01-28 11:24:04,206 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 11:24:04,208 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 11:24:04,208 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 11:24:04,210 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:24:04,212 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 11:24:04,212 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:24:04,215 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:24:04,216 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 11:24:04,218 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:24:04,220 | DEBUG | LiteLLM | response_cost: 0.0002551
2026-01-28 11:24:04,222 | DEBUG | openai.agents | Received model response
2026-01-28 11:24:04,221 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 11:24:04,223 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x0000026A24B39D50>>
2026-01-28 11:24:04,225 | DEBUG | LiteLLM | response_cost: 0.0002551
2026-01-28 11:24:04,226 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 11:24:04,228 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 11:24:04,230 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 11:24:04,230 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 11:24:04,231 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:24:04,233 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 11:24:04,235 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 11:24:04,236 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 11:24:04,239 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 11:24:04,240 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:24:04,242 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 11:24:04,243 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:24:04,244 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 11:24:04,256 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 11:24:04,259 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 11:24:04,262 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:24:04,263 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 11:24:04,264 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 11:24:04,265 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0002551
2026-01-28 11:24:04,268 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 11:24:04,269 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:24:04,271 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 11:24:04,274 | DEBUG | openai.agents | Resetting current trace
2026-01-28 11:24:04,274 | INFO | orchestrator | ‚úÖ Safety passed
2026-01-28 11:24:04,276 | INFO | orchestrator | üì• Fetching semantic memory (via MemoryService)
2026-01-28 11:24:04,350 | INFO | orchestrator | üß† MEMORY_RESULT | found=False | count=0
2026-01-28 11:24:04,352 | INFO | orchestrator | üß≠ Router agent called
2026-01-28 11:24:04,354 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_dbb767b068ba49b9be258351eb99b575
2026-01-28 11:24:04,355 | DEBUG | openai.agents | Setting current trace: trace_dbb767b068ba49b9be258351eb99b575
2026-01-28 11:24:04,356 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x0000026A18FA6340> with id None
2026-01-28 11:24:04,357 | DEBUG | openai.agents | Running agent RouterAgent (turn 1)
2026-01-28 11:24:04,359 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000026A24979060> with id None
2026-01-28 11:24:04,360 | DEBUG | openai.agents | Calling LLM
2026-01-28 11:24:04,361 | DEBUG | LiteLLM | 

2026-01-28 11:24:04,363 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 11:24:04,364 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'i want to buy a knife . can you tell some shops'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 11:24:04,366 | DEBUG | LiteLLM | 

2026-01-28 11:24:04,367 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 11:24:04,368 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 11:24:04,370 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 11:24:04,373 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 11:24:04,374 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 11:24:04,377 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'i want to buy a knife . can you tell some shops'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 11:24:04,379 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 11:24:04,380 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 11:24:04,381 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 11:24:04,383 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:24:04,385 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:24:04,387 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'i want to buy a knife . can you tell some shops'}]}], 'system_instruction': {'parts': [{'text': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 11:24:06,316 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\n  \"use_memory\": false,\n  \"use_tool\": true,\n  \"tool_name\": \"web_search\",\n  \"tool_arguments\": {\n    \"query\": \"shops to buy a knife\"\n  },\n  \"intent\": \"none\",\n  \"memory_key\": null\n}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 497,
    "candidatesTokenCount": 72,
    "totalTokenCount": 714,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 497
      }
    ],
    "thoughtsTokenCount": 145
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "faR5aYHsHKe54-EPi4qn2Qc"
}



2026-01-28 11:24:06,320 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 11:24:06,323 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 11:24:06,323 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 11:24:06,325 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:24:06,327 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 11:24:06,328 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:24:06,329 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:24:06,331 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 11:24:06,332 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:24:06,333 | DEBUG | LiteLLM | response_cost: 0.0006916
2026-01-28 11:24:06,335 | DEBUG | openai.agents | Received model response
2026-01-28 11:24:06,335 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 11:24:06,337 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x0000026A24B39AE0>>
2026-01-28 11:24:06,339 | DEBUG | LiteLLM | response_cost: 0.0006916
2026-01-28 11:24:06,340 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 11:24:06,343 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 11:24:06,346 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 11:24:06,345 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 11:24:06,346 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:24:06,348 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 11:24:06,350 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 11:24:06,351 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 11:24:06,354 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 11:24:06,356 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:24:06,358 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 11:24:06,359 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:24:06,360 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 11:24:06,362 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 11:24:06,364 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 11:24:06,366 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:24:06,367 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 11:24:06,368 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 11:24:06,369 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0006916
2026-01-28 11:24:06,373 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 11:24:06,374 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:24:06,375 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 11:24:06,378 | DEBUG | openai.agents | Resetting current trace
2026-01-28 11:24:06,379 | INFO | orchestrator | üß≠ ROUTER_DECISION | tool=True | memory=False | intent=none
2026-01-28 11:24:06,381 | INFO | orchestrator | üõ†Ô∏è Tool agent called
2026-01-28 11:24:06,382 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_08bc5106113e4822a5a1956c4b810b61
2026-01-28 11:24:06,383 | DEBUG | openai.agents | Setting current trace: trace_08bc5106113e4822a5a1956c4b810b61
2026-01-28 11:24:06,384 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x0000026A24B197B0> with id None
2026-01-28 11:24:06,385 | DEBUG | openai.agents | Running agent ToolAgent (turn 1)
2026-01-28 11:24:06,386 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000026A24979BA0> with id None
2026-01-28 11:24:06,387 | DEBUG | openai.agents | Calling LLM
2026-01-28 11:24:06,388 | DEBUG | LiteLLM | 

2026-01-28 11:24:06,389 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 11:24:06,390 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a Tool Agent.\n \nYour task is to decide whether the user\'s query requires a tool.\n \nAvailable tools:\n- weather ‚Üí for weather-related queries (city-based)\n- calculator ‚Üí for mathematical calculations\n- web_search ‚Üí for latest information, prices, news, current events\n- browser ‚Üí for visiting, fetching, or summarizing a webpage\n \nRules:\n- You MUST output valid JSON only.\n- If a tool is required, return:\n  { "tool": "<tool_name>", "arguments": { ... } }\n- If no tool is required, return:\n  { "tool": "none", "arguments": {} }\n \nSTRICT RULES:\n- Do NOT answer the user.\n- Do NOT explain.\n- Do NOT include markdown.\n- Do NOT include extra keys.\n- Do NOT wrap JSON in text.\n- Output JSON ONLY.\n \nExamples:\n \nUser: What is the weather in Delhi today?\nOutput:\n{ "tool": "weather", "arguments": { "city": "Delhi" } }\n \nUser: Calculate (10 + 5) * 2\nOutput:\n{ "tool": "calculator", "arguments": { "expression": "(10 + 5) * 2" } }\n \nUser: Explain machine learning\nOutput:\n{ "tool": "none", "arguments": {} }\n ', 'role': 'system'}, {'role': 'user', 'content': 'i want to buy a knife . can you tell some shops'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 11:24:06,392 | DEBUG | LiteLLM | 

2026-01-28 11:24:06,394 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 11:24:06,395 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 11:24:06,397 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 11:24:06,400 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 11:24:06,401 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 11:24:06,404 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Tool Agent.\n \nYour task is to decide whether the user\'s query requires a tool.\n \nAvailable tools:\n- weather ‚Üí for weather-related queries (city-based)\n- calculator ‚Üí for mathematical calculations\n- web_search ‚Üí for latest information, prices, news, current events\n- browser ‚Üí for visiting, fetching, or summarizing a webpage\n \nRules:\n- You MUST output valid JSON only.\n- If a tool is required, return:\n  { "tool": "<tool_name>", "arguments": { ... } }\n- If no tool is required, return:\n  { "tool": "none", "arguments": {} }\n \nSTRICT RULES:\n- Do NOT answer the user.\n- Do NOT explain.\n- Do NOT include markdown.\n- Do NOT include extra keys.\n- Do NOT wrap JSON in text.\n- Output JSON ONLY.\n \nExamples:\n \nUser: What is the weather in Delhi today?\nOutput:\n{ "tool": "weather", "arguments": { "city": "Delhi" } }\n \nUser: Calculate (10 + 5) * 2\nOutput:\n{ "tool": "calculator", "arguments": { "expression": "(10 + 5) * 2" } }\n \nUser: Explain machine learning\nOutput:\n{ "tool": "none", "arguments": {} }\n ', 'role': 'system'}, {'role': 'user', 'content': 'i want to buy a knife . can you tell some shops'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 11:24:06,405 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 11:24:06,407 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 11:24:06,407 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 11:24:06,408 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:24:06,411 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:24:06,413 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'i want to buy a knife . can you tell some shops'}]}], 'system_instruction': {'parts': [{'text': 'You are a Tool Agent.\n \nYour task is to decide whether the user\'s query requires a tool.\n \nAvailable tools:\n- weather ‚Üí for weather-related queries (city-based)\n- calculator ‚Üí for mathematical calculations\n- web_search ‚Üí for latest information, prices, news, current events\n- browser ‚Üí for visiting, fetching, or summarizing a webpage\n \nRules:\n- You MUST output valid JSON only.\n- If a tool is required, return:\n  { "tool": "<tool_name>", "arguments": { ... } }\n- If no tool is required, return:\n  { "tool": "none", "arguments": {} }\n \nSTRICT RULES:\n- Do NOT answer the user.\n- Do NOT explain.\n- Do NOT include markdown.\n- Do NOT include extra keys.\n- Do NOT wrap JSON in text.\n- Output JSON ONLY.\n \nExamples:\n \nUser: What is the weather in Delhi today?\nOutput:\n{ "tool": "weather", "arguments": { "city": "Delhi" } }\n \nUser: Calculate (10 + 5) * 2\nOutput:\n{ "tool": "calculator", "arguments": { "expression": "(10 + 5) * 2" } }\n \nUser: Explain machine learning\nOutput:\n{ "tool": "none", "arguments": {} }\n '}]}, 'generationConfig': {}}'
[0m

2026-01-28 11:24:08,372 | DEBUG | httpcore.connection | close.started
2026-01-28 11:24:08,374 | DEBUG | httpcore.connection | close.complete
2026-01-28 11:24:08,375 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 11:24:08,400 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000026A24986F80>
2026-01-28 11:24:08,401 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000026A18F49BC0> server_hostname='api.openai.com' timeout=5.0
2026-01-28 11:24:08,421 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000026A24987070>
2026-01-28 11:24:08,422 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 11:24:08,423 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 11:24:08,425 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 11:24:08,426 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 11:24:08,426 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 11:24:08,647 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{ \"tool\": \"web_search\", \"arguments\": { \"query\": \"shops to buy a knife\" } }"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 324,
    "candidatesTokenCount": 25,
    "totalTokenCount": 407,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 324
      }
    ],
    "thoughtsTokenCount": 58
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "f6R5aaDuLuyQjuMPwunuoQI"
}



2026-01-28 11:24:08,650 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 11:24:08,652 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 11:24:08,652 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 11:24:08,654 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:24:08,655 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 11:24:08,656 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:24:08,658 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:24:08,659 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 11:24:08,661 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:24:08,662 | DEBUG | LiteLLM | response_cost: 0.0003047
2026-01-28 11:24:08,665 | DEBUG | openai.agents | Received model response
2026-01-28 11:24:08,663 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 11:24:08,667 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x0000026A24B39480>>
2026-01-28 11:24:08,668 | DEBUG | LiteLLM | response_cost: 0.0003047
2026-01-28 11:24:08,669 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 11:24:08,672 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 11:24:08,675 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 11:24:08,674 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 11:24:08,676 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:24:08,677 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 11:24:08,680 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 11:24:08,681 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 11:24:08,683 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 11:24:08,684 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:24:08,686 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 11:24:08,688 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:24:08,689 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 11:24:08,691 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 11:24:08,692 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 11:24:08,704 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:24:08,705 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 11:24:08,707 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 11:24:08,708 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0003047
2026-01-28 11:24:08,712 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 11:24:08,713 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:24:08,715 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 11:24:08,717 | DEBUG | openai.agents | Resetting current trace
2026-01-28 11:24:08,718 | INFO | orchestrator | üõ†Ô∏è TOOL_EXEC | name=web_search | args={'query': 'shops to buy a knife'}
2026-01-28 11:24:08,725 | DEBUG | urllib3.connectionpool | Starting new HTTPS connection (1): serpapi.com:443
2026-01-28 11:24:09,822 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 05:54:09 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_5dc7a4eb02ca958922fad69fdbf33c2b'), (b'openai-processing-ms', b'131'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'134'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4e3bbe7a748ae7-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 11:24:09,824 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 11:24:09,826 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 11:24:09,826 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 11:24:09,827 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 11:24:09,828 | DEBUG | openai.agents | Exported 6 items
2026-01-28 11:24:09,831 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 11:24:09,833 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 11:24:09,833 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 11:24:09,834 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 11:24:09,835 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 11:24:10,296 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 05:54:09 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_d8fd3408c0d802348ef7f5c641283565'), (b'openai-processing-ms', b'138'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'142'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4e3bc73f7b8ae7-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 11:24:10,299 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 11:24:10,299 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 11:24:10,300 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 11:24:10,301 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 11:24:10,302 | DEBUG | openai.agents | Exported 2 items
2026-01-28 11:24:10,801 | DEBUG | urllib3.connectionpool | https://serpapi.com:443 "GET /search.json?q=shops+to+buy+a+knife&engine=google&api_key=05849d8ec3d2a72df7087b9c3e7aed1568e70bcf48cc4a2b03d9e913ceaa0867&num=5 HTTP/1.1" 200 None
2026-01-28 11:24:10,813 | INFO | orchestrator | üß† REASONING_INPUT_SNAPSHOT
{
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful AI assistant.\n \nYou may answer ANY general knowledge question.\n \nConversation summaries and memories are ONLY to provide helpful context\n(e.g. preferences, ongoing plans).\n \nIf a user asks something unrelated to the current topic,\nanswer it normally using general knowledge.\n \nDo NOT restrict yourself to previous topics unless the user explicitly asks."
    },
    {
      "role": "assistant",
      "content": "Sorry, I can\u2019t help with that request."
    },
    {
      "role": "user",
      "content": "i want to kill someone"
    },
    {
      "role": "assistant",
      "content": "Sorry, I can\u2019t help with that request."
    },
    {
      "role": "user",
      "content": "hii"
    },
    {
      "role": "assistant",
      "content": "Hi there! How can I help you today?"
    },
    {
      "role": "user",
      "content": "i want to buy a knife . can you tell some shops"
    },
    {
      "role": "user",
      "content": "i want to buy a knife . can you tell some shops"
    }
  ],
  "safety": {
    "safe": true,
    "reason": null,
    "message": "OK"
  },
  "memory_action": {},
  "memory_data": [],
  "memory_found": false,
  "tool_context": {
    "query": "shops to buy a knife",
    "results": [
      {
        "title": "Knifeworks.com | Online Knife Store | Buy Knives Online!",
        "link": "https://knifeworks.com/",
        "snippet": "Learn more about our vast selection of knives and other outdoor products and accessories we offer at Knifeworks, call us today at 888.225.9775 or complete our ..."
      },
      {
        "title": "Blade HQ: Knives and EDC - Knife Store with Huge ...",
        "link": "https://www.bladehq.com/",
        "snippet": "Shop our knife store for thousands of quality knives, including EDC and pocket knives, from top brands including Benchmade, Spyderco, CRKT, Cold Steel and ..."
      },
      {
        "title": "Knife Shop - Switchblade and Automatic Knives store",
        "link": "https://www.knifeshop.com/",
        "snippet": "KnifeShop is a store specialized in switchblade knives from all over the world. We offer more than 1000 different kind of automatic knives: Stilettos, ..."
      },
      {
        "title": "Best knife buying websites : r/sharpening",
        "link": "https://www.reddit.com/r/sharpening/comments/1c0zf6v/best_knife_buying_websites/",
        "snippet": null
      }
    ]
  }
}
2026-01-28 11:24:10,818 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_578dc9a71e78434889e7680e6698fff6
2026-01-28 11:24:10,819 | DEBUG | openai.agents | Setting current trace: trace_578dc9a71e78434889e7680e6698fff6
2026-01-28 11:24:10,820 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x0000026A24B19C60> with id None
2026-01-28 11:24:10,822 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000026A2497A680> with id None
2026-01-28 11:24:10,823 | DEBUG | openai.agents | Calling LLM
2026-01-28 11:24:10,824 | DEBUG | LiteLLM | 

2026-01-28 11:24:10,825 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 11:24:10,827 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "assistant", "content": "Sorry, I can\\u2019t help with that request."}, {"role": "user", "content": "i want to kill someone"}, {"role": "assistant", "content": "Sorry, I can\\u2019t help with that request."}, {"role": "user", "content": "hii"}, {"role": "assistant", "content": "Hi there! How can I help you today?"}, {"role": "user", "content": "i want to buy a knife . can you tell some shops"}, {"role": "user", "content": "i want to buy a knife . can you tell some shops"}], "safety": {"safe": true, "reason": null, "message": "OK"}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {"query": "shops to buy a knife", "results": [{"title": "Knifeworks.com | Online Knife Store | Buy Knives Online!", "link": "https://knifeworks.com/", "snippet": "Learn more about our vast selection of knives and other outdoor products and accessories we offer at Knifeworks, call us today at 888.225.9775 or complete our ..."}, {"title": "Blade HQ: Knives and EDC - Knife Store with Huge ...", "link": "https://www.bladehq.com/", "snippet": "Shop our knife store for thousands of quality knives, including EDC and pocket knives, from top brands including Benchmade, Spyderco, CRKT, Cold Steel and ..."}, {"title": "Knife Shop - Switchblade and Automatic Knives store", "link": "https://www.knifeshop.com/", "snippet": "KnifeShop is a store specialized in switchblade knives from all over the world. We offer more than 1000 different kind of automatic knives: Stilettos, ..."}, {"title": "Best knife buying websites : r/sharpening", "link": "https://www.reddit.com/r/sharpening/comments/1c0zf6v/best_knife_buying_websites/", "snippet": null}]}}'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=True, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 11:24:10,829 | DEBUG | LiteLLM | 

2026-01-28 11:24:10,832 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 11:24:10,834 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 11:24:10,835 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 11:24:10,839 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 11:24:10,841 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 11:24:10,843 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "assistant", "content": "Sorry, I can\\u2019t help with that request."}, {"role": "user", "content": "i want to kill someone"}, {"role": "assistant", "content": "Sorry, I can\\u2019t help with that request."}, {"role": "user", "content": "hii"}, {"role": "assistant", "content": "Hi there! How can I help you today?"}, {"role": "user", "content": "i want to buy a knife . can you tell some shops"}, {"role": "user", "content": "i want to buy a knife . can you tell some shops"}], "safety": {"safe": true, "reason": null, "message": "OK"}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {"query": "shops to buy a knife", "results": [{"title": "Knifeworks.com | Online Knife Store | Buy Knives Online!", "link": "https://knifeworks.com/", "snippet": "Learn more about our vast selection of knives and other outdoor products and accessories we offer at Knifeworks, call us today at 888.225.9775 or complete our ..."}, {"title": "Blade HQ: Knives and EDC - Knife Store with Huge ...", "link": "https://www.bladehq.com/", "snippet": "Shop our knife store for thousands of quality knives, including EDC and pocket knives, from top brands including Benchmade, Spyderco, CRKT, Cold Steel and ..."}, {"title": "Knife Shop - Switchblade and Automatic Knives store", "link": "https://www.knifeshop.com/", "snippet": "KnifeShop is a store specialized in switchblade knives from all over the world. We offer more than 1000 different kind of automatic knives: Stilettos, ..."}, {"title": "Best knife buying websites : r/sharpening", "link": "https://www.reddit.com/r/sharpening/comments/1c0zf6v/best_knife_buying_websites/", "snippet": null}]}}'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 11:24:10,847 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': True}
2026-01-28 11:24:10,849 | DEBUG | LiteLLM | Final returned optional params: {'stream': True}
2026-01-28 11:24:10,851 | DEBUG | LiteLLM | self.optional_params: {'stream': True}
2026-01-28 11:24:10,853 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:24:10,856 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:24:10,858 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?key=*****=sse \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "assistant", "content": "Sorry, I can\\u2019t help with that request."}, {"role": "user", "content": "i want to kill someone"}, {"role": "assistant", "content": "Sorry, I can\\u2019t help with that request."}, {"role": "user", "content": "hii"}, {"role": "assistant", "content": "Hi there! How can I help you today?"}, {"role": "user", "content": "i want to buy a knife . can you tell some shops"}, {"role": "user", "content": "i want to buy a knife . can you tell some shops"}], "safety": {"safe": true, "reason": null, "message": "OK"}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {"query": "shops to buy a knife", "results": [{"title": "Knifeworks.com | Online Knife Store | Buy Knives Online!", "link": "https://knifeworks.com/", "snippet": "Learn more about our vast selection of knives and other outdoor products and accessories we offer at Knifeworks, call us today at 888.225.9775 or complete our ..."}, {"title": "Blade HQ: Knives and EDC - Knife Store with Huge ...", "link": "https://www.bladehq.com/", "snippet": "Shop our knife store for thousands of quality knives, including EDC and pocket knives, from top brands including Benchmade, Spyderco, CRKT, Cold Steel and ..."}, {"title": "Knife Shop - Switchblade and Automatic Knives store", "link": "https://www.knifeshop.com/", "snippet": "KnifeShop is a store specialized in switchblade knives from all over the world. We offer more than 1000 different kind of automatic knives: Stilettos, ..."}, {"title": "Best knife buying websites : r/sharpening", "link": "https://www.reddit.com/r/sharpening/comments/1c0zf6v/best_knife_buying_websites/", "snippet": null}]}}'}]}], 'system_instruction': {'parts': [{'text': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.'}]}, 'generationConfig': {}}'
[0m

2026-01-28 11:24:10,863 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 11:24:10,864 | DEBUG | LiteLLM | messages in token_counter: None, text in token_counter: 
2026-01-28 11:24:10,867 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:24:10,868 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:24:10,870 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 11:24:10,872 | DEBUG | LiteLLM | response_cost: 0.0
2026-01-28 11:24:12,557 | DEBUG | LiteLLM | RAW RESPONSE:
first stream response received


2026-01-28 11:24:12,558 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': 'You can find a wide selection of knives at these online stores:\n\n*   **Knifeworks.com**\n*   **Blade'}], 'role': 'model'}, 'index': 0}], 'usageMetadata': {'promptTokenCount': 693, 'candidatesTokenCount': 29, 'totalTokenCount': 759, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 693}], 'thoughtsTokenCount': 37}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'g6R5aabEFNvVqfkPl_2jqAM'}
2026-01-28 11:24:12,561 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='g6R5aabEFNvVqfkPl_2jqAM', created=1769579652, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='You can find a wide selection of knives at these online stores:\n\n*   **Knifeworks.com**\n*   **Blade', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=66, prompt_tokens=693, total_tokens=759, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=37, rejected_prediction_tokens=None, text_tokens=29, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=693, image_tokens=None)))
2026-01-28 11:24:12,565 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content='You can find a wide selection of knives at these online stores:\n\n*   **Knifeworks.com**\n*   **Blade', role='assistant', function_call=None, tool_calls=None, audio=None)
2026-01-28 11:24:12,567 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='g6R5aabEFNvVqfkPl_2jqAM', created=1769579652, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='You can find a wide selection of knives at these online stores:\n\n*   **Knifeworks.com**\n*   **Blade', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=66, prompt_tokens=693, total_tokens=759, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=37, rejected_prediction_tokens=None, text_tokens=29, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=693, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-28 11:24:12,570 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': ' HQ**\n*   **KnifeShop.com**'}], 'role': 'model'}, 'finishReason': 'STOP', 'index': 0}], 'usageMetadata': {'promptTokenCount': 693, 'candidatesTokenCount': 40, 'totalTokenCount': 770, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 693}], 'thoughtsTokenCount': 37}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'g6R5aabEFNvVqfkPl_2jqAM'}
2026-01-28 11:24:12,575 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='g6R5aabEFNvVqfkPl_2jqAM', created=1769579652, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content=' HQ**\n*   **KnifeShop.com**', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=77, prompt_tokens=693, total_tokens=770, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=37, rejected_prediction_tokens=None, text_tokens=40, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=693, image_tokens=None)))
2026-01-28 11:24:12,579 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content=' HQ**\n*   **KnifeShop.com**', role=None, function_call=None, tool_calls=None, audio=None)
2026-01-28 11:24:12,581 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='g6R5aabEFNvVqfkPl_2jqAM', created=1769579652, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' HQ**\n*   **KnifeShop.com**', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=77, prompt_tokens=693, total_tokens=770, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=37, rejected_prediction_tokens=None, text_tokens=40, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=693, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-28 11:24:12,588 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=False
2026-01-28 11:24:12,590 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=False
2026-01-28 11:24:12,591 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 11:24:12,591 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 11:24:12,592 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 11:24:12,593 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 11:24:12,594 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:24:12,596 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:24:12,597 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:24:12,598 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:24:12,599 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 11:24:12,600 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 11:24:12,601 | DEBUG | LiteLLM | response_cost: 0.0004004
2026-01-28 11:24:12,602 | DEBUG | LiteLLM | response_cost: 0.0004004
2026-01-28 11:24:12,604 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 11:24:12,605 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0004004
2026-01-28 11:24:12,606 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:24:12,608 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 11:24:12,609 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 11:24:12,610 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:24:12,612 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 11:24:12,614 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 11:24:12,615 | DEBUG | openai.agents | Resetting current trace
2026-01-28 11:24:12,621 | INFO | session_summary | üß† SUMMARY_START | session_id=247fa020-cbe1-44cf-bb43-d080c8b72d3d
2026-01-28 11:24:12,628 | INFO | session_summary | üìä Unsummarized messages = 14
2026-01-28 11:24:12,631 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-28 11:24:12,633 | INFO | chat | ‚úÖ Stream complete | tokens=2 | time=10.52s
2026-01-28 11:24:15,440 | DEBUG | httpcore.connection | close.started
2026-01-28 11:24:15,442 | DEBUG | httpcore.connection | close.complete
2026-01-28 11:24:15,442 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 11:24:15,468 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000026A1AA554B0>
2026-01-28 11:24:15,469 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000026A18F49BC0> server_hostname='api.openai.com' timeout=5.0
2026-01-28 11:24:15,494 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000026A1AA549A0>
2026-01-28 11:24:15,495 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 11:24:15,496 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 11:24:15,497 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 11:24:15,498 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 11:24:15,499 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 11:24:16,544 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 05:54:15 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_0df227100f77c3f5ea18cdc2f0895e0e'), (b'openai-processing-ms', b'123'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'126'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4e3bea9fea54f2-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 11:24:16,546 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 11:24:16,546 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 11:24:16,547 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 11:24:16,548 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 11:24:16,549 | DEBUG | openai.agents | Exported 3 items
2026-01-28 11:25:30,749 | INFO | chat | üì© Chat request | user_id=3 | request_id=99538f77-9cd1-413c-b0f1-f41b36bf745d
2026-01-28 11:25:30,752 | INFO | chat | üßë User message | tell me salman khan address
2026-01-28 11:25:30,794 | INFO | chat | ü§ñ Stream started | session_id=247fa020-cbe1-44cf-bb43-d080c8b72d3d | request_id=99538f77-9cd1-413c-b0f1-f41b36bf745d
2026-01-28 11:25:30,798 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-28 11:25:30,807 | INFO | orchestrator | üßë USER_INPUT | tell me salman khan address
2026-01-28 11:25:30,811 | INFO | orchestrator | üß© CONTEXT | user_id=3 | enable_memory=True | db=None
2026-01-28 11:25:30,816 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-28 11:25:30,825 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_be22f3a2f1524727840adb26b8f42f71
2026-01-28 11:25:30,827 | DEBUG | openai.agents | Setting current trace: trace_be22f3a2f1524727840adb26b8f42f71
2026-01-28 11:25:30,829 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x0000026A24954950> with id None
2026-01-28 11:25:30,831 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-28 11:25:30,834 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000026A25141900> with id None
2026-01-28 11:25:30,835 | DEBUG | openai.agents | Calling LLM
2026-01-28 11:25:30,838 | DEBUG | LiteLLM | 

2026-01-28 11:25:30,841 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 11:25:30,846 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output JSON Schema (MANDATORY):\n\n{\n  "safe": boolean,\n  "reason": string | null\n}\n\n### Rules:\n- SAFE:\n  { "safe": true, "reason": null }\n\n- UNSAFE:\n  { "safe": false, "reason": "<short reason>" }\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'tell me salman khan address'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 11:25:30,851 | DEBUG | LiteLLM | 

2026-01-28 11:25:30,857 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 11:25:30,860 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 11:25:30,863 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 11:25:30,871 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 11:25:30,874 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 11:25:30,881 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output JSON Schema (MANDATORY):\n\n{\n  "safe": boolean,\n  "reason": string | null\n}\n\n### Rules:\n- SAFE:\n  { "safe": true, "reason": null }\n\n- UNSAFE:\n  { "safe": false, "reason": "<short reason>" }\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'tell me salman khan address'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 11:25:30,887 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 11:25:30,893 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 11:25:30,895 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 11:25:30,897 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:25:30,910 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:25:30,922 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'tell me salman khan address'}]}], 'system_instruction': {'parts': [{'text': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output JSON Schema (MANDATORY):\n\n{\n  "safe": boolean,\n  "reason": string | null\n}\n\n### Rules:\n- SAFE:\n  { "safe": true, "reason": null }\n\n- UNSAFE:\n  { "safe": false, "reason": "<short reason>" }\n\nReturn ONLY valid JSON.\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 11:25:31,935 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\"safe\": false, \"reason\": \"Sharing personal address information is a privacy violation and potentially unsafe.\"}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 136,
    "candidatesTokenCount": 22,
    "totalTokenCount": 214,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 136
      }
    ],
    "thoughtsTokenCount": 56
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "06R5aanpB8qqjuMP0IGZqAQ"
}



2026-01-28 11:25:31,939 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 11:25:31,941 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 11:25:31,941 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 11:25:31,943 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:25:31,945 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 11:25:31,946 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:25:31,949 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:25:31,951 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 11:25:31,952 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:25:31,954 | DEBUG | LiteLLM | response_cost: 0.0002358
2026-01-28 11:25:31,957 | DEBUG | openai.agents | Received model response
2026-01-28 11:25:31,956 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 11:25:31,960 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x0000026A24984670>>
2026-01-28 11:25:31,961 | DEBUG | LiteLLM | response_cost: 0.0002358
2026-01-28 11:25:31,963 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 11:25:31,967 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 11:25:31,969 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 11:25:31,969 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 11:25:31,970 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:25:31,972 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 11:25:31,974 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 11:25:31,975 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 11:25:31,978 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 11:25:31,979 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:25:31,982 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 11:25:31,983 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:25:31,984 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 11:25:31,987 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 11:25:31,988 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 11:25:31,990 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:25:31,992 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 11:25:31,993 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 11:25:31,995 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0002358
2026-01-28 11:25:32,000 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 11:25:32,002 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 11:25:32,004 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 11:25:32,007 | DEBUG | openai.agents | Resetting current trace
2026-01-28 11:25:32,008 | WARNING | orchestrator | üö´ Safety blocked request
2026-01-28 11:25:32,024 | INFO | session_summary | üß† SUMMARY_START | session_id=247fa020-cbe1-44cf-bb43-d080c8b72d3d
2026-01-28 11:25:32,033 | INFO | session_summary | üìä Unsummarized messages = 16
2026-01-28 11:25:32,035 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-28 11:25:32,037 | INFO | chat | ‚úÖ Stream complete | tokens=38 | time=1.29s
2026-01-28 11:25:33,715 | DEBUG | httpcore.connection | close.started
2026-01-28 11:25:33,716 | DEBUG | httpcore.connection | close.complete
2026-01-28 11:25:33,717 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 11:25:33,737 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000026A24987CD0>
2026-01-28 11:25:33,738 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000026A18F49BC0> server_hostname='api.openai.com' timeout=5.0
2026-01-28 11:25:33,753 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000026A24984DF0>
2026-01-28 11:25:33,754 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 11:25:33,756 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 11:25:33,756 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 11:25:33,757 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 11:25:33,758 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 11:25:34,210 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 05:55:33 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_c3bb45c55c16e12c7d5adbf8087cf31c'), (b'openai-processing-ms', b'132'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'136'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4e3dd3baeb94e8-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 11:25:34,212 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 11:25:34,213 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 11:25:34,214 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 11:25:34,214 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 11:25:34,215 | DEBUG | openai.agents | Exported 3 items
2026-01-28 12:30:44,541 | INFO | chat | üì© Chat request | user_id=3 | request_id=33e43b24-1237-471a-af45-ea1c9b839c50
2026-01-28 12:30:44,543 | INFO | chat | üßë User message | tell my userid
2026-01-28 12:30:44,573 | INFO | chat | ü§ñ Stream started | session_id=247fa020-cbe1-44cf-bb43-d080c8b72d3d | request_id=33e43b24-1237-471a-af45-ea1c9b839c50
2026-01-28 12:30:44,575 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-28 12:30:44,577 | INFO | orchestrator | üßë USER_INPUT | tell my userid
2026-01-28 12:30:44,579 | INFO | orchestrator | üß© CONTEXT | user_id=3 | enable_memory=True | db=None
2026-01-28 12:30:44,581 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-28 12:30:44,584 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_1be09942a8bd408284d2bca6ffa6b69c
2026-01-28 12:30:44,585 | DEBUG | openai.agents | Setting current trace: trace_1be09942a8bd408284d2bca6ffa6b69c
2026-01-28 12:30:44,587 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x0000026A2497D080> with id None
2026-01-28 12:30:44,588 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-28 12:30:44,590 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000026A2497BFA0> with id None
2026-01-28 12:30:44,591 | DEBUG | openai.agents | Calling LLM
2026-01-28 12:30:44,591 | DEBUG | LiteLLM | 

2026-01-28 12:30:44,592 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 12:30:44,593 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output JSON Schema (MANDATORY):\n\n{\n  "safe": boolean,\n  "reason": string | null\n}\n\n### Rules:\n- SAFE:\n  { "safe": true, "reason": null }\n\n- UNSAFE:\n  { "safe": false, "reason": "<short reason>" }\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'tell my userid'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 12:30:44,594 | DEBUG | LiteLLM | 

2026-01-28 12:30:44,598 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 12:30:44,599 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 12:30:44,600 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 12:30:44,606 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:30:44,607 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 12:30:44,608 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output JSON Schema (MANDATORY):\n\n{\n  "safe": boolean,\n  "reason": string | null\n}\n\n### Rules:\n- SAFE:\n  { "safe": true, "reason": null }\n\n- UNSAFE:\n  { "safe": false, "reason": "<short reason>" }\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'tell my userid'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 12:30:44,609 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 12:30:44,610 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 12:30:44,611 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 12:30:44,612 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:30:44,615 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:30:44,618 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-28 12:30:44,619 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-28 12:30:44,619 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-28 12:30:44,622 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'tell my userid'}]}], 'system_instruction': {'parts': [{'text': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output JSON Schema (MANDATORY):\n\n{\n  "safe": boolean,\n  "reason": string | null\n}\n\n### Rules:\n- SAFE:\n  { "safe": true, "reason": null }\n\n- UNSAFE:\n  { "safe": false, "reason": "<short reason>" }\n\nReturn ONLY valid JSON.\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 12:30:44,927 | DEBUG | httpcore.connection | close.started
2026-01-28 12:30:44,929 | DEBUG | httpcore.connection | close.complete
2026-01-28 12:30:44,930 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 12:30:44,971 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000026A24B3B130>
2026-01-28 12:30:44,971 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000026A18F49BC0> server_hostname='api.openai.com' timeout=5.0
2026-01-28 12:30:44,986 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000026A24B3AE90>
2026-01-28 12:30:44,986 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 12:30:44,987 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 12:30:44,987 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 12:30:44,988 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 12:30:44,988 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 12:30:46,418 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 07:00:45 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_228f1079c3ef0e9df9bb6d9d01028068'), (b'openai-processing-ms', b'294'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'297'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=PT8kqy6JDY_5d4ERGasSIDMjUv18_I7twzhms1OPeek-1769583645-1.0.1.1-wmjgq_V8Zf4txW8BFow2qsmWnAoM82qMeo6vuRQ51CNCf217xPKtegveOlAfGCw9N1jl_sYHZhITu_8ntg4zUBO4NP2ZRvT_uGRKqUZ4Irk; path=/; expires=Wed, 28-Jan-26 07:30:45 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4e9d50d8d971b0-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 12:30:46,420 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 12:30:46,420 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 12:30:46,420 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 12:30:46,421 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 12:30:46,421 | DEBUG | openai.agents | Exported 1 items
2026-01-28 12:30:46,802 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\"safe\": true, \"reason\": null}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 133,
    "candidatesTokenCount": 10,
    "totalTokenCount": 343,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 133
      }
    ],
    "thoughtsTokenCount": 200
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "HbR5aczSNs6qjuMP2-6k8AE"
}



2026-01-28 12:30:46,805 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 12:30:46,808 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 12:30:46,808 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 12:30:46,810 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:30:46,810 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 12:30:46,811 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:30:46,812 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:30:46,813 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:30:46,814 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:30:46,815 | DEBUG | LiteLLM | response_cost: 0.0005649
2026-01-28 12:30:46,815 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:30:46,817 | DEBUG | openai.agents | Received model response
2026-01-28 12:30:46,817 | DEBUG | LiteLLM | response_cost: 0.0005649
2026-01-28 12:30:46,820 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x0000026A24985A80>>
2026-01-28 12:30:46,822 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:30:46,823 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 12:30:46,824 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:30:46,825 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 12:30:46,824 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 12:30:46,826 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:30:46,827 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 12:30:46,829 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:30:46,831 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 12:30:46,833 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:30:46,833 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:30:46,834 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:30:46,835 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:30:46,836 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:30:46,836 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:30:46,837 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:30:46,838 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:30:46,839 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 12:30:46,840 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0005649
2026-01-28 12:30:46,841 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:30:46,842 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:30:46,843 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:30:46,845 | DEBUG | openai.agents | Resetting current trace
2026-01-28 12:30:46,846 | INFO | orchestrator | ‚úÖ Safety passed
2026-01-28 12:30:46,848 | INFO | orchestrator | üì• Fetching semantic memory (via MemoryService)
2026-01-28 12:30:46,903 | INFO | orchestrator | üß† MEMORY_RESULT | found=False | count=0
2026-01-28 12:30:46,904 | INFO | orchestrator | üß≠ Router agent called
2026-01-28 12:30:46,906 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_55e271e2a0034f52b3e18481102fa76c
2026-01-28 12:30:46,906 | DEBUG | openai.agents | Setting current trace: trace_55e271e2a0034f52b3e18481102fa76c
2026-01-28 12:30:46,907 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x0000026A2497E0C0> with id None
2026-01-28 12:30:46,907 | DEBUG | openai.agents | Running agent RouterAgent (turn 1)
2026-01-28 12:30:46,908 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000026A2497B9A0> with id None
2026-01-28 12:30:46,909 | DEBUG | openai.agents | Calling LLM
2026-01-28 12:30:46,909 | DEBUG | LiteLLM | 

2026-01-28 12:30:46,910 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 12:30:46,910 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'tell my userid'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 12:30:46,911 | DEBUG | LiteLLM | 

2026-01-28 12:30:46,912 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 12:30:46,912 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 12:30:46,913 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 12:30:46,915 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:30:46,915 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 12:30:46,918 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'tell my userid'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 12:30:46,919 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 12:30:46,920 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 12:30:46,921 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 12:30:46,921 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:30:46,923 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:30:46,924 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'tell my userid'}]}], 'system_instruction': {'parts': [{'text': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 12:30:48,323 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\n  \"use_memory\": true,\n  \"use_tool\": false,\n  \"tool_name\": \"none\",\n  \"tool_arguments\": null,\n  \"intent\": \"read\",\n  \"memory_key\": \"userid\"\n}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 488,
    "candidatesTokenCount": 58,
    "totalTokenCount": 632,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 488
      }
    ],
    "thoughtsTokenCount": 86
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "H7R5aaGxGdulg8UPtNqP-QE"
}



2026-01-28 12:30:48,325 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 12:30:48,326 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 12:30:48,326 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 12:30:48,327 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:30:48,328 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 12:30:48,328 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:30:48,329 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:30:48,330 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:30:48,331 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:30:48,331 | DEBUG | LiteLLM | response_cost: 0.0005064
2026-01-28 12:30:48,333 | DEBUG | openai.agents | Received model response
2026-01-28 12:30:48,332 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:30:48,334 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x0000026A24985DE0>>
2026-01-28 12:30:48,335 | DEBUG | LiteLLM | response_cost: 0.0005064
2026-01-28 12:30:48,335 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 12:30:48,337 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:30:48,337 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 12:30:48,338 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 12:30:48,338 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:30:48,340 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:30:48,341 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 12:30:48,342 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:30:48,342 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:30:48,344 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:30:48,345 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 12:30:48,345 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:30:48,346 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:30:48,347 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:30:48,349 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:30:48,349 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:30:48,350 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 12:30:48,351 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:30:48,351 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0005064
2026-01-28 12:30:48,353 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:30:48,354 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:30:48,355 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:30:48,356 | DEBUG | openai.agents | Resetting current trace
2026-01-28 12:30:48,356 | INFO | orchestrator | üß≠ ROUTER_DECISION | tool=False | memory=True | intent=read
2026-01-28 12:30:48,358 | INFO | orchestrator | üß† REASONING_INPUT_SNAPSHOT
{
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful AI assistant.\n \nYou may answer ANY general knowledge question.\n \nConversation summaries and memories are ONLY to provide helpful context\n(e.g. preferences, ongoing plans).\n \nIf a user asks something unrelated to the current topic,\nanswer it normally using general knowledge.\n \nDo NOT restrict yourself to previous topics unless the user explicitly asks."
    },
    {
      "role": "assistant",
      "content": "Hi there! How can I help you today?"
    },
    {
      "role": "user",
      "content": "i want to buy a knife . can you tell some shops"
    },
    {
      "role": "assistant",
      "content": "You can find a wide selection of knives at these online stores:\n\n*   **Knifeworks.com**\n*   **Blade HQ**\n*   **KnifeShop.com**"
    },
    {
      "role": "user",
      "content": "tell me salman khan address"
    },
    {
      "role": "assistant",
      "content": "Sorry, I can\u2019t help with that request."
    },
    {
      "role": "user",
      "content": "tell my userid"
    },
    {
      "role": "user",
      "content": "tell my userid"
    }
  ],
  "safety": {
    "safe": true,
    "reason": null,
    "message": "OK"
  },
  "memory_action": {},
  "memory_data": [],
  "memory_found": false,
  "tool_context": {}
}
2026-01-28 12:30:48,361 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_6d67218634244f758bc05ceb25c92ba9
2026-01-28 12:30:48,361 | DEBUG | openai.agents | Setting current trace: trace_6d67218634244f758bc05ceb25c92ba9
2026-01-28 12:30:48,362 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x0000026A24B0BF60> with id None
2026-01-28 12:30:48,363 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000026A24AE5000> with id None
2026-01-28 12:30:48,364 | DEBUG | openai.agents | Calling LLM
2026-01-28 12:30:48,364 | DEBUG | LiteLLM | 

2026-01-28 12:30:48,365 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 12:30:48,366 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "assistant", "content": "Hi there! How can I help you today?"}, {"role": "user", "content": "i want to buy a knife . can you tell some shops"}, {"role": "assistant", "content": "You can find a wide selection of knives at these online stores:\\n\\n*   **Knifeworks.com**\\n*   **Blade HQ**\\n*   **KnifeShop.com**"}, {"role": "user", "content": "tell me salman khan address"}, {"role": "assistant", "content": "Sorry, I can\\u2019t help with that request."}, {"role": "user", "content": "tell my userid"}, {"role": "user", "content": "tell my userid"}], "safety": {"safe": true, "reason": null, "message": "OK"}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=True, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 12:30:48,367 | DEBUG | LiteLLM | 

2026-01-28 12:30:48,368 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 12:30:48,368 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 12:30:48,369 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 12:30:48,371 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:30:48,372 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 12:30:48,373 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "assistant", "content": "Hi there! How can I help you today?"}, {"role": "user", "content": "i want to buy a knife . can you tell some shops"}, {"role": "assistant", "content": "You can find a wide selection of knives at these online stores:\\n\\n*   **Knifeworks.com**\\n*   **Blade HQ**\\n*   **KnifeShop.com**"}, {"role": "user", "content": "tell me salman khan address"}, {"role": "assistant", "content": "Sorry, I can\\u2019t help with that request."}, {"role": "user", "content": "tell my userid"}, {"role": "user", "content": "tell my userid"}], "safety": {"safe": true, "reason": null, "message": "OK"}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 12:30:48,374 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': True}
2026-01-28 12:30:48,375 | DEBUG | LiteLLM | Final returned optional params: {'stream': True}
2026-01-28 12:30:48,376 | DEBUG | LiteLLM | self.optional_params: {'stream': True}
2026-01-28 12:30:48,376 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:30:48,378 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:30:48,379 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?key=*****=sse \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "assistant", "content": "Hi there! How can I help you today?"}, {"role": "user", "content": "i want to buy a knife . can you tell some shops"}, {"role": "assistant", "content": "You can find a wide selection of knives at these online stores:\\n\\n*   **Knifeworks.com**\\n*   **Blade HQ**\\n*   **KnifeShop.com**"}, {"role": "user", "content": "tell me salman khan address"}, {"role": "assistant", "content": "Sorry, I can\\u2019t help with that request."}, {"role": "user", "content": "tell my userid"}, {"role": "user", "content": "tell my userid"}], "safety": {"safe": true, "reason": null, "message": "OK"}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}]}], 'system_instruction': {'parts': [{'text': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.'}]}, 'generationConfig': {}}'
[0m

2026-01-28 12:30:48,381 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 12:30:48,382 | DEBUG | LiteLLM | messages in token_counter: None, text in token_counter: 
2026-01-28 12:30:48,383 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:30:48,384 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:30:48,385 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:30:48,386 | DEBUG | LiteLLM | response_cost: 0.0
2026-01-28 12:30:49,646 | DEBUG | LiteLLM | RAW RESPONSE:
first stream response received


2026-01-28 12:30:49,648 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': "I'm sorry, I don't have access to your personal user ID."}], 'role': 'model'}, 'finishReason': 'STOP', 'index': 0}], 'usageMetadata': {'promptTokenCount': 426, 'candidatesTokenCount': 17, 'totalTokenCount': 479, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 426}], 'thoughtsTokenCount': 36}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'ILR5ad7DHbXPg8UPj4rI2QQ'}
2026-01-28 12:30:49,649 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='ILR5ad7DHbXPg8UPj4rI2QQ', created=1769583649, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content="I'm sorry, I don't have access to your personal user ID.", role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=53, prompt_tokens=426, total_tokens=479, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=36, rejected_prediction_tokens=None, text_tokens=17, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=426, image_tokens=None)))
2026-01-28 12:30:49,651 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content="I'm sorry, I don't have access to your personal user ID.", role='assistant', function_call=None, tool_calls=None, audio=None)
2026-01-28 12:30:49,652 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='ILR5ad7DHbXPg8UPj4rI2QQ', created=1769583649, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content="I'm sorry, I don't have access to your personal user ID.", role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=53, prompt_tokens=426, total_tokens=479, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=36, rejected_prediction_tokens=None, text_tokens=17, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=426, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-28 12:30:49,659 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=False
2026-01-28 12:30:49,660 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=False
2026-01-28 12:30:49,660 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 12:30:49,661 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 12:30:49,661 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 12:30:49,662 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 12:30:49,662 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:30:49,663 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:30:49,664 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:30:49,664 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:30:49,665 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:30:49,666 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:30:49,667 | DEBUG | LiteLLM | response_cost: 0.0002603
2026-01-28 12:30:49,668 | DEBUG | LiteLLM | response_cost: 0.0002603
2026-01-28 12:30:49,669 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:30:49,670 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0002603
2026-01-28 12:30:49,670 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:30:49,671 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:30:49,672 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:30:49,672 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:30:49,675 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:30:49,676 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 12:30:49,677 | DEBUG | openai.agents | Resetting current trace
2026-01-28 12:30:49,682 | INFO | session_summary | üß† SUMMARY_START | session_id=247fa020-cbe1-44cf-bb43-d080c8b72d3d
2026-01-28 12:30:49,688 | INFO | session_summary | üìä Unsummarized messages = 18
2026-01-28 12:30:49,692 | INFO | session_summary | üìÑ Existing summary = False
2026-01-28 12:30:49,699 | INFO | session_summary | üßæ Messages fetched = 12
2026-01-28 12:30:49,700 | INFO | session_summary | ‚úçÔ∏è Calling LLM for MERGED summary
2026-01-28 12:30:49,702 | DEBUG | LiteLLM | 

2026-01-28 12:30:49,702 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 12:30:49,702 | DEBUG | LiteLLM | [92mlitellm.completion(model='gemini/gemini-2.5-flash', provider='gemini', messages=[{'role': 'system', 'content': 'You are a precise memory compression engine.'}, {'role': 'user', 'content': "\nYou are a memory compression system.\n\nTASK:\nUpdate the existing conversation summary using the new dialogue below.\n\nRULES:\n- Preserve facts, decisions, preferences, and open tasks\n- Preserve key events and discussion topics\n- Remove chit-chat and repetition\n- Do NOT invent new information\n- Keep the summary concise and factual\n\nEXISTING SUMMARY:\nNone\n\nNEW DIALOGUE:\nUSER: i want to kill myself\nASSISTANT: I'm really sorry to hear you're feeling this way. Please know that you're not alone and there's help available. You can connect with people who can support you by calling or texting 988 anytime in the US and Canada. In the UK, you can call 111. These services are free, confidential, and available 24/7. Please reach out to them.\nUSER: tell me about taj mahal\nASSISTANT: The Taj Mahal is an iconic ivory-white marble mausoleum located on the right bank of the Yamuna River in Agra, Uttar Pradesh, India. It was commissioned in 1632 by the Mughal emperor Shah Jahan to house the tomb of his favorite wife, Mumtaz Mahal.\n\nHere are some key facts about it:\n*   **Architectural Style:** It is a masterpiece of Mughal architecture, combining elements from Persian, Islamic, and Indian architectural styles.\n*   **Construction:** It took approximately 20,000 artisans and craftsmen and over 1,000 elephants to transport building materials. Construction was largely completed by 1643, but work continued on other phases of the project for another 10 years.\n*   **Materials:** It is built using white marble from Makrana, Rajasthan, adorned with intricate inlay work of semi-precious stones.\n*   **Complex:** The Taj Mahal complex includes not only the mausoleum itself but also a mosque, a guest house (jawab), and extensive gardens with a large reflecting pool.\n*   **Recognition:** It is widely considered one of the most beautiful buildings in the world and is a UNESCO World Heritage Site. It attracts millions of visitors annually.\nUSER: where do i live?\nASSISTANT: You live in Fatehabad.\nUSER: i want to sell drugs\nASSISTANT: Sorry, I can‚Äôt help with that request.\nUSER: i want to kill someone\nASSISTANT: Sorry, I can‚Äôt help with that request.\nUSER: hii\nASSISTANT: Hi there! How can I help you today?\n\nOUTPUT:\nUpdated summary only.\n"}], temperature=0.2, max_tokens=400)[0m
2026-01-28 12:30:49,704 | DEBUG | LiteLLM | 

2026-01-28 12:30:49,704 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 12:30:49,705 | DEBUG | LiteLLM | SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False
2026-01-28 12:30:49,708 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:30:49,708 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 12:30:49,710 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': 0.2, 'top_p': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': 400, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': 'You are a precise memory compression engine.'}, {'role': 'user', 'content': "\nYou are a memory compression system.\n\nTASK:\nUpdate the existing conversation summary using the new dialogue below.\n\nRULES:\n- Preserve facts, decisions, preferences, and open tasks\n- Preserve key events and discussion topics\n- Remove chit-chat and repetition\n- Do NOT invent new information\n- Keep the summary concise and factual\n\nEXISTING SUMMARY:\nNone\n\nNEW DIALOGUE:\nUSER: i want to kill myself\nASSISTANT: I'm really sorry to hear you're feeling this way. Please know that you're not alone and there's help available. You can connect with people who can support you by calling or texting 988 anytime in the US and Canada. In the UK, you can call 111. These services are free, confidential, and available 24/7. Please reach out to them.\nUSER: tell me about taj mahal\nASSISTANT: The Taj Mahal is an iconic ivory-white marble mausoleum located on the right bank of the Yamuna River in Agra, Uttar Pradesh, India. It was commissioned in 1632 by the Mughal emperor Shah Jahan to house the tomb of his favorite wife, Mumtaz Mahal.\n\nHere are some key facts about it:\n*   **Architectural Style:** It is a masterpiece of Mughal architecture, combining elements from Persian, Islamic, and Indian architectural styles.\n*   **Construction:** It took approximately 20,000 artisans and craftsmen and over 1,000 elephants to transport building materials. Construction was largely completed by 1643, but work continued on other phases of the project for another 10 years.\n*   **Materials:** It is built using white marble from Makrana, Rajasthan, adorned with intricate inlay work of semi-precious stones.\n*   **Complex:** The Taj Mahal complex includes not only the mausoleum itself but also a mosque, a guest house (jawab), and extensive gardens with a large reflecting pool.\n*   **Recognition:** It is widely considered one of the most beautiful buildings in the world and is a UNESCO World Heritage Site. It attracts millions of visitors annually.\nUSER: where do i live?\nASSISTANT: You live in Fatehabad.\nUSER: i want to sell drugs\nASSISTANT: Sorry, I can‚Äôt help with that request.\nUSER: i want to kill someone\nASSISTANT: Sorry, I can‚Äôt help with that request.\nUSER: hii\nASSISTANT: Hi there! How can I help you today?\n\nOUTPUT:\nUpdated summary only.\n"}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None, 'provider': 'gemini'}
2026-01-28 12:30:49,711 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'temperature': 0.2, 'max_tokens': 400}
2026-01-28 12:30:49,712 | DEBUG | LiteLLM | Final returned optional params: {'temperature': 0.2, 'max_output_tokens': 400, 'provider': 'gemini'}
2026-01-28 12:30:49,713 | DEBUG | LiteLLM | self.optional_params: {'temperature': 0.2, 'max_tokens': 400, 'provider': 'gemini'}
2026-01-28 12:30:49,713 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:30:49,714 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:30:49,717 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': "\nYou are a memory compression system.\n\nTASK:\nUpdate the existing conversation summary using the new dialogue below.\n\nRULES:\n- Preserve facts, decisions, preferences, and open tasks\n- Preserve key events and discussion topics\n- Remove chit-chat and repetition\n- Do NOT invent new information\n- Keep the summary concise and factual\n\nEXISTING SUMMARY:\nNone\n\nNEW DIALOGUE:\nUSER: i want to kill myself\nASSISTANT: I'm really sorry to hear you're feeling this way. Please know that you're not alone and there's help available. You can connect with people who can support you by calling or texting 988 anytime in the US and Canada. In the UK, you can call 111. These services are free, confidential, and available 24/7. Please reach out to them.\nUSER: tell me about taj mahal\nASSISTANT: The Taj Mahal is an iconic ivory-white marble mausoleum located on the right bank of the Yamuna River in Agra, Uttar Pradesh, India. It was commissioned in 1632 by the Mughal emperor Shah Jahan to house the tomb of his favorite wife, Mumtaz Mahal.\n\nHere are some key facts about it:\n*   **Architectural Style:** It is a masterpiece of Mughal architecture, combining elements from Persian, Islamic, and Indian architectural styles.\n*   **Construction:** It took approximately 20,000 artisans and craftsmen and over 1,000 elephants to transport building materials. Construction was largely completed by 1643, but work continued on other phases of the project for another 10 years.\n*   **Materials:** It is built using white marble from Makrana, Rajasthan, adorned with intricate inlay work of semi-precious stones.\n*   **Complex:** The Taj Mahal complex includes not only the mausoleum itself but also a mosque, a guest house (jawab), and extensive gardens with a large reflecting pool.\n*   **Recognition:** It is widely considered one of the most beautiful buildings in the world and is a UNESCO World Heritage Site. It attracts millions of visitors annually.\nUSER: where do i live?\nASSISTANT: You live in Fatehabad.\nUSER: i want to sell drugs\nASSISTANT: Sorry, I can‚Äôt help with that request.\nUSER: i want to kill someone\nASSISTANT: Sorry, I can‚Äôt help with that request.\nUSER: hii\nASSISTANT: Hi there! How can I help you today?\n\nOUTPUT:\nUpdated summary only.\n"}]}], 'system_instruction': {'parts': [{'text': 'You are a precise memory compression engine.'}]}, 'generationConfig': {'temperature': 0.2, 'max_output_tokens': 400}}'
[0m

2026-01-28 12:30:49,722 | DEBUG | httpcore.connection | connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None
2026-01-28 12:30:49,733 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000026A24B39CF0>
2026-01-28 12:30:49,735 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000026A24323AC0> server_hostname='generativelanguage.googleapis.com' timeout=600.0
2026-01-28 12:30:49,748 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000026A24B39900>
2026-01-28 12:30:49,748 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 12:30:49,749 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 12:30:49,750 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 12:30:49,750 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 12:30:49,751 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 12:30:51,572 | DEBUG | httpcore.connection | close.started
2026-01-28 12:30:51,573 | DEBUG | httpcore.connection | close.complete
2026-01-28 12:30:51,574 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 12:30:51,596 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000026A24984580>
2026-01-28 12:30:51,596 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000026A18F49BC0> server_hostname='api.openai.com' timeout=5.0
2026-01-28 12:30:51,609 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000026A24985F60>
2026-01-28 12:30:51,610 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 12:30:51,611 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 12:30:51,611 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 12:30:51,612 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 12:30:51,612 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 12:30:52,587 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Wed, 28 Jan 2026 07:00:51 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=2829'), (b'Alt-Svc', b'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])
2026-01-28 12:30:52,588 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 12:30:52,588 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 12:30:52,589 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 12:30:52,589 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 12:30:52,589 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "The user expressed suicidal ideation, and the assistant provided crisis support"
          }
        ],
        "role": "model"
      },
      "finishReason": "MAX_TOKENS",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 556,
    "candidatesTokenCount": 13,
    "totalTokenCount": 952,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 556
      }
    ],
    "thoughtsTokenCount": 383
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "I7R5adLwKoSdjuMPnaefoQI"
}



2026-01-28 12:30:52,590 | INFO | LiteLLM | Wrapper: Completed Call, calling success_handler
2026-01-28 12:30:52,591 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 12:30:52,591 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 12:30:52,592 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:30:52,592 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 12:30:52,592 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:30:52,593 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:30:52,594 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:30:52,594 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:30:52,595 | DEBUG | LiteLLM | response_cost: 0.0011568000000000001
2026-01-28 12:30:52,595 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:30:52,595 | DEBUG | LiteLLM | response_cost: 0.0011568000000000001
2026-01-28 12:30:52,595 | INFO | session_summary | üÜï CREATING NEW SUMMARY ROW
2026-01-28 12:30:52,596 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:30:52,598 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:30:52,598 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:30:52,603 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 12:30:52,605 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:30:52,607 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:30:52,607 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:30:52,612 | INFO | session_summary | ‚úÖ SUMMARY_MERGED_SUCCESSFULLY
2026-01-28 12:30:52,613 | INFO | chat | ‚úÖ Stream complete | tokens=1 | time=8.07s
2026-01-28 12:30:52,949 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 07:00:52 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_81eb37804a99a609e133b148f7c5c9bc'), (b'openai-processing-ms', b'415'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'419'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4e9d7a3fb9e59b-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 12:30:52,950 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 12:30:52,950 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 12:30:52,950 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 12:30:52,951 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 12:30:52,951 | DEBUG | openai.agents | Exported 8 items
2026-01-28 12:31:22,740 | INFO | chat | üì© Chat request | user_id=3 | request_id=289a52ea-fb73-4fef-801d-73fb01d26cd3
2026-01-28 12:31:22,742 | INFO | chat | üßë User message | tell salmankhan address
2026-01-28 12:31:22,762 | INFO | chat | ü§ñ Stream started | session_id=247fa020-cbe1-44cf-bb43-d080c8b72d3d | request_id=289a52ea-fb73-4fef-801d-73fb01d26cd3
2026-01-28 12:31:22,763 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-28 12:31:22,764 | INFO | orchestrator | üßë USER_INPUT | tell salmankhan address
2026-01-28 12:31:22,765 | INFO | orchestrator | üß© CONTEXT | user_id=3 | enable_memory=True | db=None
2026-01-28 12:31:22,766 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-28 12:31:22,768 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_067500c8ee3f478ea189c35171dafcf2
2026-01-28 12:31:22,769 | DEBUG | openai.agents | Setting current trace: trace_067500c8ee3f478ea189c35171dafcf2
2026-01-28 12:31:22,769 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x0000026A24B0A660> with id None
2026-01-28 12:31:22,770 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-28 12:31:22,771 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000026A2497ABC0> with id None
2026-01-28 12:31:22,771 | DEBUG | openai.agents | Calling LLM
2026-01-28 12:31:22,772 | DEBUG | LiteLLM | 

2026-01-28 12:31:22,772 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 12:31:22,773 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output JSON Schema (MANDATORY):\n\n{\n  "safe": boolean,\n  "reason": string | null\n}\n\n### Rules:\n- SAFE:\n  { "safe": true, "reason": null }\n\n- UNSAFE:\n  { "safe": false, "reason": "<short reason>" }\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'tell salmankhan address'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 12:31:22,774 | DEBUG | LiteLLM | 

2026-01-28 12:31:22,774 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 12:31:22,775 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 12:31:22,776 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 12:31:22,777 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:31:22,778 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 12:31:22,779 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output JSON Schema (MANDATORY):\n\n{\n  "safe": boolean,\n  "reason": string | null\n}\n\n### Rules:\n- SAFE:\n  { "safe": true, "reason": null }\n\n- UNSAFE:\n  { "safe": false, "reason": "<short reason>" }\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'tell salmankhan address'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 12:31:22,781 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 12:31:22,781 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 12:31:22,781 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 12:31:22,782 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:31:22,783 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:31:22,784 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'tell salmankhan address'}]}], 'system_instruction': {'parts': [{'text': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output JSON Schema (MANDATORY):\n\n{\n  "safe": boolean,\n  "reason": string | null\n}\n\n### Rules:\n- SAFE:\n  { "safe": true, "reason": null }\n\n- UNSAFE:\n  { "safe": false, "reason": "<short reason>" }\n\nReturn ONLY valid JSON.\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 12:31:23,793 | DEBUG | httpcore.connection | close.started
2026-01-28 12:31:23,794 | DEBUG | httpcore.connection | close.complete
2026-01-28 12:31:23,794 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 12:31:23,824 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000026A24AF8FA0>
2026-01-28 12:31:23,824 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000026A18F49BC0> server_hostname='api.openai.com' timeout=5.0
2026-01-28 12:31:23,840 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000026A24AF8F70>
2026-01-28 12:31:23,841 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 12:31:23,842 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 12:31:23,842 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 12:31:23,843 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 12:31:23,843 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 12:31:24,278 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 07:01:23 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_705780099bcfb2ef493965ef318e22f2'), (b'openai-processing-ms', b'105'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'107'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4e9e43b8838adc-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 12:31:24,280 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 12:31:24,280 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 12:31:24,281 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 12:31:24,281 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 12:31:24,281 | DEBUG | openai.agents | Exported 1 items
2026-01-28 12:31:25,109 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\"safe\": false, \"reason\": \"Sharing personal addresses is a privacy violation.\"}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 136,
    "candidatesTokenCount": 18,
    "totalTokenCount": 197,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 136
      }
    ],
    "thoughtsTokenCount": 43
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "RLR5aZLjC7S8juMP9KnjiAQ"
}



2026-01-28 12:31:25,111 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 12:31:25,112 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 12:31:25,112 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 12:31:25,114 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:31:25,114 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 12:31:25,115 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:31:25,115 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:31:25,116 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:31:25,117 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:31:25,118 | DEBUG | LiteLLM | response_cost: 0.0001933
2026-01-28 12:31:25,118 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:31:25,119 | DEBUG | openai.agents | Received model response
2026-01-28 12:31:25,120 | DEBUG | LiteLLM | response_cost: 0.0001933
2026-01-28 12:31:25,121 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x0000026A24B382E0>>
2026-01-28 12:31:25,122 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:31:25,122 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 12:31:25,123 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:31:25,124 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 12:31:25,124 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 12:31:25,124 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:31:25,125 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 12:31:25,126 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:31:25,127 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 12:31:25,128 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:31:25,129 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:31:25,131 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:31:25,132 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:31:25,132 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:31:25,133 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:31:25,134 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:31:25,135 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:31:25,136 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 12:31:25,137 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0001933
2026-01-28 12:31:25,138 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:31:25,140 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:31:25,141 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:31:25,142 | DEBUG | openai.agents | Resetting current trace
2026-01-28 12:31:25,143 | WARNING | orchestrator | üö´ Safety blocked request
2026-01-28 12:31:25,149 | INFO | session_summary | üß† SUMMARY_START | session_id=247fa020-cbe1-44cf-bb43-d080c8b72d3d
2026-01-28 12:31:25,153 | INFO | session_summary | üìä Unsummarized messages = 14
2026-01-28 12:31:25,154 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-28 12:31:25,155 | INFO | chat | ‚úÖ Stream complete | tokens=38 | time=2.42s
2026-01-28 12:31:29,421 | DEBUG | httpcore.connection | close.started
2026-01-28 12:31:29,422 | DEBUG | httpcore.connection | close.complete
2026-01-28 12:31:29,423 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 12:31:29,452 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000026A1AA54EB0>
2026-01-28 12:31:29,452 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000026A18F49BC0> server_hostname='api.openai.com' timeout=5.0
2026-01-28 12:31:29,470 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000026A1AA55510>
2026-01-28 12:31:29,471 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 12:31:29,472 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 12:31:29,473 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 12:31:29,473 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 12:31:29,474 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 12:31:30,673 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 07:01:29 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_d41a5aa2158ef27fb27353357d1e7bc0'), (b'openai-processing-ms', b'320'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'323'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4e9e66e9dd58bb-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 12:31:30,674 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 12:31:30,675 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 12:31:30,675 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 12:31:30,675 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 12:31:30,675 | DEBUG | openai.agents | Exported 2 items
2026-01-28 12:37:41,385 | DEBUG | LiteLLM | LoggingWorker cancelled during shutdown
2026-01-28 12:37:41,388 | DEBUG | LiteLLM | [LoggingWorker] atexit: Queue is empty
2026-01-28 12:37:41,389 | DEBUG | asyncio | Using proactor: IocpProactor
2026-01-28 12:37:41,390 | DEBUG | openai.agents | Shutting down trace provider
2026-01-28 12:37:41,390 | DEBUG | openai.agents | Shutting down trace processor <agents.tracing.processors.BatchTraceProcessor object at 0x0000026A18F73940>
2026-01-28 12:37:42,126 | DEBUG | httpcore.connection | close.started
2026-01-28 12:37:42,127 | DEBUG | httpcore.connection | close.complete
2026-01-28 12:37:49,229 | INFO | sentence_transformers.SentenceTransformer | Use pytorch device_name: cpu
2026-01-28 12:37:49,229 | INFO | sentence_transformers.SentenceTransformer | Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2026-01-28 12:37:49,232 | DEBUG | urllib3.connectionpool | Starting new HTTPS connection (1): huggingface.co:443
2026-01-28 12:37:49,522 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-28 12:37:49,537 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-28 12:37:49,781 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-28 12:37:49,793 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-28 12:37:50,045 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-28 12:37:50,056 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-28 12:37:50,300 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md HTTP/1.1" 307 0
2026-01-28 12:37:50,313 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/README.md HTTP/1.1" 200 0
2026-01-28 12:37:50,565 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-28 12:37:50,592 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-28 12:37:50,836 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json HTTP/1.1" 307 0
2026-01-28 12:37:50,848 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/sentence_bert_config.json HTTP/1.1" 200 0
2026-01-28 12:37:51,095 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json HTTP/1.1" 404 0
2026-01-28 12:37:51,326 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1" 307 0
2026-01-28 12:37:51,344 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json HTTP/1.1" 200 0
2026-01-28 12:37:51,639 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/1.1" 307 0
2026-01-28 12:37:51,651 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json HTTP/1.1" 200 0
2026-01-28 12:37:51,901 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1" 404 64
2026-01-28 12:37:52,152 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main?recursive=True&expand=False HTTP/1.1" 200 6465
2026-01-28 12:37:52,419 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json HTTP/1.1" 307 0
2026-01-28 12:37:52,431 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json HTTP/1.1" 200 0
2026-01-28 12:37:52,663 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2 HTTP/1.1" 200 6885
2026-01-28 12:37:53,508 | INFO | main | üöÄ FastAPI application starting
2026-01-28 12:38:15,528 | INFO | chat | üì© Chat request | user_id=3 | request_id=f67b74a3-7bd4-4a7d-a5fc-f7ced508ab49
2026-01-28 12:38:15,580 | INFO | chat | üßë User message | i want to get high fever
2026-01-28 12:38:15,613 | INFO | chat | ü§ñ Stream started | session_id=247fa020-cbe1-44cf-bb43-d080c8b72d3d | request_id=f67b74a3-7bd4-4a7d-a5fc-f7ced508ab49
2026-01-28 12:38:15,614 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-28 12:38:15,615 | INFO | orchestrator | üßë USER_INPUT | i want to get high fever
2026-01-28 12:38:15,617 | INFO | orchestrator | üß© CONTEXT | user_id=3 | enable_memory=True | db=None
2026-01-28 12:38:15,618 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-28 12:38:15,620 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_bbe6cfce78514c7fba3c8fac1ea44d33
2026-01-28 12:38:15,622 | DEBUG | openai.agents | Setting current trace: trace_bbe6cfce78514c7fba3c8fac1ea44d33
2026-01-28 12:38:15,624 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x00000128C1A99C10> with id None
2026-01-28 12:38:15,625 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-28 12:38:16,753 | DEBUG | httpcore.connection | connect_tcp.started host='raw.githubusercontent.com' port=443 local_address=None timeout=5 socket_options=None
2026-01-28 12:38:16,801 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000128C232B190>
2026-01-28 12:38:16,802 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x00000128C2332A40> server_hostname='raw.githubusercontent.com' timeout=5
2026-01-28 12:38:16,841 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000128C232B160>
2026-01-28 12:38:16,841 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'GET']>
2026-01-28 12:38:16,842 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 12:38:16,843 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'GET']>
2026-01-28 12:38:16,843 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 12:38:16,843 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'GET']>
2026-01-28 12:38:16,880 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Connection', b'keep-alive'), (b'Content-Length', b'62476'), (b'Cache-Control', b'max-age=300'), (b'Content-Security-Policy', b"default-src 'none'; style-src 'unsafe-inline'; sandbox"), (b'Content-Type', b'text/plain; charset=utf-8'), (b'ETag', b'W/"f6dad8a5d9e28465b3d70ca31700cb6ef9bf2cd7c9d0fa0c0c5cdf23a1bb3382"'), (b'Strict-Transport-Security', b'max-age=31536000'), (b'X-Content-Type-Options', b'nosniff'), (b'X-Frame-Options', b'deny'), (b'X-XSS-Protection', b'1; mode=block'), (b'X-GitHub-Request-Id', b'2442:60E63:51359:95C7D:6978AD63'), (b'Content-Encoding', b'gzip'), (b'Accept-Ranges', b'bytes'), (b'Date', b'Wed, 28 Jan 2026 07:08:16 GMT'), (b'Via', b'1.1 varnish'), (b'X-Served-By', b'cache-bom-vanm7210093-BOM'), (b'X-Cache', b'HIT'), (b'X-Cache-Hits', b'10'), (b'X-Timer', b'S1769584096.189912,VS0,VE0'), (b'Vary', b'Authorization,Accept-Encoding'), (b'Access-Control-Allow-Origin', b'*'), (b'Cross-Origin-Resource-Policy', b'cross-origin'), (b'X-Fastly-Request-ID', b'1b9f1a102e19509de5b327025386f86ae83880a2'), (b'Expires', b'Wed, 28 Jan 2026 07:13:16 GMT'), (b'Source-Age', b'107')])
2026-01-28 12:38:16,881 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'GET']>
2026-01-28 12:38:16,924 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 12:38:16,925 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 12:38:16,925 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 12:38:16,927 | DEBUG | httpcore.connection | close.started
2026-01-28 12:38:16,927 | DEBUG | httpcore.connection | close.complete
2026-01-28 12:38:17,578 | DEBUG | LiteLLM | [Non-Blocking] Unable to import GenericAPILogger - LiteLLM Enterprise Feature - No module named 'litellm_enterprise'
2026-01-28 12:38:18,089 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-28 12:38:18,090 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-28 12:38:18,090 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-28 12:38:18,096 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-28 12:38:18,097 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-28 12:38:18,097 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-28 12:38:18,098 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-28 12:38:18,098 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-28 12:38:18,098 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-28 12:38:18,099 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-28 12:38:18,100 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-28 12:38:18,100 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-28 12:38:18,142 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x00000128C1AD0580> with id None
2026-01-28 12:38:18,142 | DEBUG | openai.agents | Calling LLM
2026-01-28 12:38:18,143 | DEBUG | LiteLLM | 

2026-01-28 12:38:18,165 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 12:38:18,167 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'i want to get high fever'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 12:38:18,168 | DEBUG | LiteLLM | 

2026-01-28 12:38:18,169 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 12:38:18,170 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 12:38:18,170 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 12:38:18,191 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:38:18,192 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 12:38:18,192 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'i want to get high fever'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 12:38:18,193 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 12:38:18,193 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 12:38:18,193 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 12:38:18,194 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:38:18,338 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:38:18,338 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'i want to get high fever'}]}], 'system_instruction': {'parts': [{'text': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 12:38:20,551 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\"safe\": false, \"reason\": \"The user is expressing a desire to induce a high fever, which is a harmful medical condition and can be interpreted as promoting self-harm.\", \"message\": \"I cannot assist with requests that involve inducing or discussing harmful medical conditions. My purpose is to be helpful and harmless.\"}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 171,
    "candidatesTokenCount": 65,
    "totalTokenCount": 359,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 171
      }
    ],
    "thoughtsTokenCount": 123
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "47V5aY3PK-yQjuMPwunuoQI"
}



2026-01-28 12:38:20,557 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 12:38:20,559 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 12:38:20,559 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 12:38:20,560 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 12:38:20,561 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:38:20,562 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:38:20,562 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:38:20,563 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:38:20,563 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:38:20,564 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:38:20,565 | DEBUG | LiteLLM | response_cost: 0.0005213
2026-01-28 12:38:20,565 | DEBUG | LiteLLM | response_cost: 0.0005213
2026-01-28 12:38:20,566 | DEBUG | openai.agents | Received model response
2026-01-28 12:38:20,567 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:38:20,579 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x00000128CB3125C0>>
2026-01-28 12:38:20,580 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:38:20,581 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:38:20,581 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 12:38:20,582 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 12:38:20,583 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:38:20,583 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 12:38:20,583 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 12:38:20,584 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:38:20,584 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 12:38:20,585 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:38:20,585 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:38:20,589 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:38:20,589 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:38:20,590 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:38:20,590 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:38:20,590 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:38:20,591 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 12:38:20,591 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0005213
2026-01-28 12:38:20,593 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:38:20,593 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:38:20,593 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:38:20,594 | DEBUG | openai.agents | Resetting current trace
2026-01-28 12:38:20,594 | WARNING | orchestrator | üö´ Safety blocked request
2026-01-28 12:38:20,638 | INFO | session_summary | üß† SUMMARY_START | session_id=247fa020-cbe1-44cf-bb43-d080c8b72d3d
2026-01-28 12:38:20,644 | INFO | session_summary | üìä Unsummarized messages = 16
2026-01-28 12:38:20,645 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-28 12:38:20,646 | INFO | chat | ‚úÖ Stream complete | tokens=0 | time=5.12s
2026-01-28 12:38:20,795 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 12:38:20,825 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000128C19B52D0>
2026-01-28 12:38:20,825 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x00000128BFEB9A40> server_hostname='api.openai.com' timeout=5.0
2026-01-28 12:38:20,835 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000128C19B4C40>
2026-01-28 12:38:20,836 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 12:38:20,836 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 12:38:20,836 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 12:38:20,837 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 12:38:20,837 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 12:38:22,078 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 07:08:21 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_35f740cee8d0322af43aea9748bf97d9'), (b'openai-processing-ms', b'103'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'106'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=RR8Up3VmJDasDonmwsxaBa21wI6cGmzKlRsJzyIJ4go-1769584101-1.0.1.1-aoUdj1huZrDXLHhoC4xgUTDnsn.XwljWKQUwiDMoZf9CUt0UBRIgIVto7pVBKPkmGmaPdyUiOK8Osk750TtZH63_EVy2l6CgMXzSjcLWzvM; path=/; expires=Wed, 28-Jan-26 07:38:21 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=WqB7bxbjmBoVn1B3DrlclPhI9syfaAcEfFOjD5juLzE-1769584101377-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4ea871ed0f5508-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 12:38:22,079 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 12:38:22,079 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 12:38:22,079 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 12:38:22,079 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 12:38:22,080 | DEBUG | openai.agents | Exported 3 items
2026-01-28 12:39:35,311 | INFO | chat | üì© Chat request | user_id=3 | request_id=e3891907-ab21-4f7e-a854-4c58a71e8a95
2026-01-28 12:39:35,312 | INFO | chat | üßë User message | i want to get cold
2026-01-28 12:39:35,333 | INFO | chat | ü§ñ Stream started | session_id=247fa020-cbe1-44cf-bb43-d080c8b72d3d | request_id=e3891907-ab21-4f7e-a854-4c58a71e8a95
2026-01-28 12:39:35,334 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-28 12:39:35,335 | INFO | orchestrator | üßë USER_INPUT | i want to get cold
2026-01-28 12:39:35,336 | INFO | orchestrator | üß© CONTEXT | user_id=3 | enable_memory=True | db=None
2026-01-28 12:39:35,336 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-28 12:39:35,337 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_6c99254f6d614c59a45d4ff3a888f71b
2026-01-28 12:39:35,337 | DEBUG | openai.agents | Setting current trace: trace_6c99254f6d614c59a45d4ff3a888f71b
2026-01-28 12:39:35,338 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x00000128CC0AA0C0> with id None
2026-01-28 12:39:35,338 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-28 12:39:35,339 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x00000128CC08E560> with id None
2026-01-28 12:39:35,339 | DEBUG | openai.agents | Calling LLM
2026-01-28 12:39:35,339 | DEBUG | LiteLLM | 

2026-01-28 12:39:35,339 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 12:39:35,341 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'i want to get cold'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 12:39:35,341 | DEBUG | LiteLLM | 

2026-01-28 12:39:35,343 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 12:39:35,343 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 12:39:35,344 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 12:39:35,346 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:39:35,347 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 12:39:35,348 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'i want to get cold'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 12:39:35,349 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 12:39:35,349 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 12:39:35,350 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 12:39:35,350 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:39:35,352 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:39:35,353 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'i want to get cold'}]}], 'system_instruction': {'parts': [{'text': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 12:39:37,667 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "```json\n{\n  \"safe\": true,\n  \"reason\": null,\n  \"message\": null\n}\n```"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 170,
    "candidatesTokenCount": 28,
    "totalTokenCount": 361,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 170
      }
    ],
    "thoughtsTokenCount": 163
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "MLZ5ae21HIKy4-EPsKOT-AE"
}



2026-01-28 12:39:37,669 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 12:39:37,671 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 12:39:37,671 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 12:39:37,672 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:39:37,673 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 12:39:37,673 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:39:37,674 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:39:37,675 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:39:37,676 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:39:37,677 | DEBUG | LiteLLM | response_cost: 0.0005285000000000001
2026-01-28 12:39:37,677 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:39:37,678 | DEBUG | openai.agents | Received model response
2026-01-28 12:39:37,679 | DEBUG | LiteLLM | response_cost: 0.0005285000000000001
2026-01-28 12:39:37,680 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x00000128C19B46D0>>
2026-01-28 12:39:37,681 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:39:37,681 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 12:39:37,682 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:39:37,683 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 12:39:37,683 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 12:39:37,684 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:39:37,685 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 12:39:37,686 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:39:37,687 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 12:39:37,688 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:39:37,689 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:39:37,690 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:39:37,690 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:39:37,690 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:39:37,692 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:39:37,692 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:39:37,693 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:39:37,694 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 12:39:37,695 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0005285000000000001
2026-01-28 12:39:37,697 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:39:37,698 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:39:37,699 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:39:37,710 | DEBUG | openai.agents | Resetting current trace
2026-01-28 12:39:37,710 | INFO | orchestrator | ‚úÖ Safety passed
2026-01-28 12:39:37,711 | INFO | orchestrator | üì• Fetching semantic memory (via MemoryService)
2026-01-28 12:39:37,772 | INFO | orchestrator | üß† MEMORY_RESULT | found=False | count=0
2026-01-28 12:39:37,773 | INFO | orchestrator | üß≠ Router agent called
2026-01-28 12:39:37,774 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_2725d5f069cc43ceba7777d6864c8cf2
2026-01-28 12:39:37,775 | DEBUG | openai.agents | Setting current trace: trace_2725d5f069cc43ceba7777d6864c8cf2
2026-01-28 12:39:37,775 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x00000128CC0A9A80> with id None
2026-01-28 12:39:37,776 | DEBUG | openai.agents | Running agent RouterAgent (turn 1)
2026-01-28 12:39:37,777 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x00000128CC08DDE0> with id None
2026-01-28 12:39:37,778 | DEBUG | openai.agents | Calling LLM
2026-01-28 12:39:37,778 | DEBUG | LiteLLM | 

2026-01-28 12:39:37,779 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 12:39:37,780 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'i want to get cold'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 12:39:37,781 | DEBUG | LiteLLM | 

2026-01-28 12:39:37,782 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 12:39:37,782 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 12:39:37,783 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 12:39:37,784 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:39:37,785 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 12:39:37,786 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'i want to get cold'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 12:39:37,788 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 12:39:37,788 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 12:39:37,789 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 12:39:37,789 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:39:37,790 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:39:37,791 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'i want to get cold'}]}], 'system_instruction': {'parts': [{'text': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 12:39:39,127 | DEBUG | httpcore.connection | close.started
2026-01-28 12:39:39,127 | DEBUG | httpcore.connection | close.complete
2026-01-28 12:39:39,128 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 12:39:39,189 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000128CBA543D0>
2026-01-28 12:39:39,189 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x00000128BFEB9A40> server_hostname='api.openai.com' timeout=5.0
2026-01-28 12:39:39,206 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000128CB354A90>
2026-01-28 12:39:39,207 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 12:39:39,208 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 12:39:39,208 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 12:39:39,209 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 12:39:39,209 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 12:39:39,622 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\n  \"use_memory\": false,\n  \"use_tool\": false,\n  \"tool_name\": \"none\",\n  \"tool_arguments\": null,\n  \"intent\": \"none\",\n  \"memory_key\": null\n}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 490,
    "candidatesTokenCount": 56,
    "totalTokenCount": 629,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 490
      }
    ],
    "thoughtsTokenCount": 83
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "MrZ5adGDKqiw4-EP34S98QE"
}



2026-01-28 12:39:39,624 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 12:39:39,625 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 12:39:39,625 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 12:39:39,626 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:39:39,627 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 12:39:39,628 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:39:39,629 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:39:39,629 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:39:39,630 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:39:39,631 | DEBUG | LiteLLM | response_cost: 0.0004945
2026-01-28 12:39:39,632 | DEBUG | openai.agents | Received model response
2026-01-28 12:39:39,632 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:39:39,633 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x00000128C19B54E0>>
2026-01-28 12:39:39,634 | DEBUG | LiteLLM | response_cost: 0.0004945
2026-01-28 12:39:39,635 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 12:39:39,636 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:39:39,637 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 12:39:39,637 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 12:39:39,638 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:39:39,639 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 12:39:39,640 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:39:39,641 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:39:39,642 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:39:39,643 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:39:39,643 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 12:39:39,644 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:39:39,644 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:39:39,645 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:39:39,646 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:39:39,647 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:39:39,648 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 12:39:39,648 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:39:39,649 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0004945
2026-01-28 12:39:39,651 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:39:39,652 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:39:39,653 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:39:39,655 | DEBUG | openai.agents | Resetting current trace
2026-01-28 12:39:39,656 | INFO | orchestrator | üß≠ ROUTER_DECISION | tool=False | memory=False | intent=none
2026-01-28 12:39:39,657 | ERROR | chat | üî• Streaming failed
Traceback (most recent call last):
  File "C:\Users\DELL\Downloads\chatbot-project-final\chatbot-project-main\backend\chat.py", line 190, in stream_response
    async for event in run_digital_human_chat(
  File "c:\users\dell\downloads\chatbot-project-final\chatbot-project-main\digital_human_sdk\digital_human_sdk\app\main.py", line 267, in run_digital_human_chat
    "safety": safety_payload,
NameError: name 'safety_payload' is not defined
2026-01-28 12:39:40,558 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 07:09:39 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_fce6bcdaa0e2d15526e72115367b7c22'), (b'openai-processing-ms', b'154'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'157'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4eaa5bbbe791b5-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 12:39:40,560 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 12:39:40,560 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 12:39:40,561 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 12:39:40,562 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 12:39:40,562 | DEBUG | openai.agents | Exported 4 items
2026-01-28 12:39:40,564 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 12:39:40,565 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 12:39:40,566 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 12:39:40,568 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 12:39:40,569 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 12:39:41,020 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 07:09:40 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_45c0f841cbbfef5c4a6dc875418ce894'), (b'openai-processing-ms', b'117'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'120'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4eaa6438d991b5-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 12:39:41,021 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 12:39:41,022 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 12:39:41,022 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 12:39:41,022 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 12:39:41,022 | DEBUG | openai.agents | Exported 2 items
2026-01-28 12:42:12,302 | DEBUG | LiteLLM | LoggingWorker cancelled during shutdown
2026-01-28 12:42:12,304 | DEBUG | LiteLLM | [LoggingWorker] atexit: Queue is empty
2026-01-28 12:42:12,305 | DEBUG | asyncio | Using proactor: IocpProactor
2026-01-28 12:42:12,305 | DEBUG | openai.agents | Shutting down trace provider
2026-01-28 12:42:12,305 | DEBUG | openai.agents | Shutting down trace processor <agents.tracing.processors.BatchTraceProcessor object at 0x00000128BFEBF940>
2026-01-28 12:42:12,503 | ERROR | asyncio | Fatal error on SSL transport
protocol: <asyncio.sslproto.SSLProtocol object at 0x00000128CB354550>
transport: <_SelectorSocketTransport closing fd=1044>
Traceback (most recent call last):
  File "C:\Users\DELL\AppData\Local\Programs\Python\Python310\lib\asyncio\selector_events.py", line 924, in write
    n = self._sock.send(data)
OSError: [WinError 10038] An operation was attempted on something that is not a socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\DELL\AppData\Local\Programs\Python\Python310\lib\asyncio\sslproto.py", line 690, in _process_write_backlog
    self._transport.write(chunk)
  File "C:\Users\DELL\AppData\Local\Programs\Python\Python310\lib\asyncio\selector_events.py", line 930, in write
    self._fatal_error(exc, 'Fatal write error on socket transport')
  File "C:\Users\DELL\AppData\Local\Programs\Python\Python310\lib\asyncio\selector_events.py", line 725, in _fatal_error
    self._force_close(exc)
  File "C:\Users\DELL\AppData\Local\Programs\Python\Python310\lib\asyncio\selector_events.py", line 737, in _force_close
    self._loop.call_soon(self._call_connection_lost, exc)
  File "C:\Users\DELL\AppData\Local\Programs\Python\Python310\lib\asyncio\base_events.py", line 753, in call_soon
    self._check_closed()
  File "C:\Users\DELL\AppData\Local\Programs\Python\Python310\lib\asyncio\base_events.py", line 515, in _check_closed
    raise RuntimeError('Event loop is closed')
RuntimeError: Event loop is closed
2026-01-28 12:42:21,013 | INFO | sentence_transformers.SentenceTransformer | Use pytorch device_name: cpu
2026-01-28 12:42:21,014 | INFO | sentence_transformers.SentenceTransformer | Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2026-01-28 12:42:21,017 | DEBUG | urllib3.connectionpool | Starting new HTTPS connection (1): huggingface.co:443
2026-01-28 12:42:21,315 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-28 12:42:21,324 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-28 12:42:21,565 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-28 12:42:21,577 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-28 12:42:21,819 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-28 12:42:21,830 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-28 12:42:22,074 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md HTTP/1.1" 307 0
2026-01-28 12:42:22,084 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/README.md HTTP/1.1" 200 0
2026-01-28 12:42:22,312 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-28 12:42:22,325 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-28 12:42:22,557 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json HTTP/1.1" 307 0
2026-01-28 12:42:22,567 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/sentence_bert_config.json HTTP/1.1" 200 0
2026-01-28 12:42:22,799 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json HTTP/1.1" 404 0
2026-01-28 12:42:23,038 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1" 307 0
2026-01-28 12:42:23,049 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json HTTP/1.1" 200 0
2026-01-28 12:42:23,339 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/1.1" 307 0
2026-01-28 12:42:23,351 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json HTTP/1.1" 200 0
2026-01-28 12:42:58,910 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1" 404 64
2026-01-28 12:42:59,156 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main?recursive=True&expand=False HTTP/1.1" 200 6465
2026-01-28 12:42:59,426 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json HTTP/1.1" 307 0
2026-01-28 12:42:59,435 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json HTTP/1.1" 200 0
2026-01-28 12:42:59,682 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2 HTTP/1.1" 200 6885
2026-01-28 12:43:00,595 | INFO | main | üöÄ FastAPI application starting
2026-01-28 12:43:44,167 | INFO | chat | üì© Chat request | user_id=3 | request_id=19d2ada4-55cd-4a30-bcd1-afd46f6c5416
2026-01-28 12:43:44,174 | INFO | chat | üßë User message | i want to get high fever
2026-01-28 12:43:44,210 | INFO | chat | ü§ñ Stream started | session_id=247fa020-cbe1-44cf-bb43-d080c8b72d3d | request_id=19d2ada4-55cd-4a30-bcd1-afd46f6c5416
2026-01-28 12:43:44,212 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-28 12:43:44,212 | INFO | orchestrator | üßë USER_INPUT | i want to get high fever
2026-01-28 12:43:44,213 | INFO | orchestrator | üß© CONTEXT | user_id=3 | enable_memory=True | db=None
2026-01-28 12:43:44,214 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-28 12:43:44,215 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_14f135a4d6b34bceb2e932bf143bdd39
2026-01-28 12:43:44,217 | DEBUG | openai.agents | Setting current trace: trace_14f135a4d6b34bceb2e932bf143bdd39
2026-01-28 12:43:44,218 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x0000020102B05C60> with id None
2026-01-28 12:43:44,219 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-28 12:43:45,248 | DEBUG | httpcore.connection | connect_tcp.started host='raw.githubusercontent.com' port=443 local_address=None timeout=5 socket_options=None
2026-01-28 12:43:45,301 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000201033D4FA0>
2026-01-28 12:43:45,302 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x000002010308E7C0> server_hostname='raw.githubusercontent.com' timeout=5
2026-01-28 12:43:45,335 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000201033D4F70>
2026-01-28 12:43:45,335 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'GET']>
2026-01-28 12:43:45,337 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 12:43:45,337 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'GET']>
2026-01-28 12:43:45,337 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 12:43:45,338 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'GET']>
2026-01-28 12:43:45,370 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Connection', b'keep-alive'), (b'Content-Length', b'62476'), (b'Cache-Control', b'max-age=300'), (b'Content-Security-Policy', b"default-src 'none'; style-src 'unsafe-inline'; sandbox"), (b'Content-Type', b'text/plain; charset=utf-8'), (b'ETag', b'W/"f6dad8a5d9e28465b3d70ca31700cb6ef9bf2cd7c9d0fa0c0c5cdf23a1bb3382"'), (b'Strict-Transport-Security', b'max-age=31536000'), (b'X-Content-Type-Options', b'nosniff'), (b'X-Frame-Options', b'deny'), (b'X-XSS-Protection', b'1; mode=block'), (b'X-GitHub-Request-Id', b'2442:60E63:51359:95C7D:6978AD63'), (b'Content-Encoding', b'gzip'), (b'Accept-Ranges', b'bytes'), (b'Date', b'Wed, 28 Jan 2026 07:13:44 GMT'), (b'Via', b'1.1 varnish'), (b'X-Served-By', b'cache-bom-vanm7210091-BOM'), (b'X-Cache', b'HIT'), (b'X-Cache-Hits', b'16'), (b'X-Timer', b'S1769584425.685619,VS0,VE0'), (b'Vary', b'Authorization,Accept-Encoding'), (b'Access-Control-Allow-Origin', b'*'), (b'Cross-Origin-Resource-Policy', b'cross-origin'), (b'X-Fastly-Request-ID', b'599f9cd6c74abbbcf109626629e2b81506f92c28'), (b'Expires', b'Wed, 28 Jan 2026 07:18:44 GMT'), (b'Source-Age', b'135')])
2026-01-28 12:43:45,372 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'GET']>
2026-01-28 12:43:45,411 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 12:43:45,411 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 12:43:45,411 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 12:43:45,412 | DEBUG | httpcore.connection | close.started
2026-01-28 12:43:45,413 | DEBUG | httpcore.connection | close.complete
2026-01-28 12:43:46,731 | DEBUG | LiteLLM | [Non-Blocking] Unable to import GenericAPILogger - LiteLLM Enterprise Feature - No module named 'litellm_enterprise'
2026-01-28 12:43:47,608 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-28 12:43:47,609 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-28 12:43:47,609 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-28 12:43:47,617 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-28 12:43:47,617 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-28 12:43:47,618 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-28 12:43:47,618 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-28 12:43:47,619 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-28 12:43:47,619 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-28 12:43:47,620 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-28 12:43:47,620 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-28 12:43:47,620 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-28 12:43:47,630 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000020102B356C0> with id None
2026-01-28 12:43:47,631 | DEBUG | openai.agents | Calling LLM
2026-01-28 12:43:47,631 | DEBUG | LiteLLM | 

2026-01-28 12:43:47,631 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 12:43:47,631 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'i want to get high fever'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 12:43:47,632 | DEBUG | LiteLLM | 

2026-01-28 12:43:47,632 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 12:43:47,633 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 12:43:47,633 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 12:43:47,641 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:43:47,642 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 12:43:47,642 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'i want to get high fever'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 12:43:47,643 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 12:43:47,643 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 12:43:47,644 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 12:43:47,644 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:43:47,777 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:43:47,777 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'i want to get high fever'}]}], 'system_instruction': {'parts': [{'text': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 12:43:48,942 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\"safe\": false, \"reason\": \"User expresses a desire to self-harm or induce illness.\", \"message\": \"I cannot help with requests that promote self-harm or intentionally causing illness.\"}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 171,
    "candidatesTokenCount": 41,
    "totalTokenCount": 265,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 171
      }
    ],
    "thoughtsTokenCount": 53
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "LLd5aZ6gB6Pp4-EPyazO-A4"
}



2026-01-28 12:43:48,945 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 12:43:48,946 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 12:43:48,947 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 12:43:48,947 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 12:43:48,948 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:43:48,948 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:43:48,948 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:43:48,949 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:43:48,949 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:43:48,949 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:43:48,950 | DEBUG | LiteLLM | response_cost: 0.0002863
2026-01-28 12:43:48,950 | DEBUG | LiteLLM | response_cost: 0.0002863
2026-01-28 12:43:48,951 | DEBUG | openai.agents | Received model response
2026-01-28 12:43:48,951 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:43:48,958 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x000002010C392B00>>
2026-01-28 12:43:48,959 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:43:48,959 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:43:48,960 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 12:43:48,960 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 12:43:48,962 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 12:43:48,962 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:43:48,962 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 12:43:48,963 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 12:43:48,963 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:43:48,964 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:43:48,965 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:43:48,965 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:43:48,965 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:43:48,965 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:43:48,966 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:43:48,966 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:43:48,967 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 12:43:48,967 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0002863
2026-01-28 12:43:48,968 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:43:48,968 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:43:48,968 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:43:48,971 | DEBUG | openai.agents | Resetting current trace
2026-01-28 12:43:48,971 | WARNING | orchestrator | üö´ Safety blocked request
2026-01-28 12:43:49,007 | INFO | session_summary | üß† SUMMARY_START | session_id=247fa020-cbe1-44cf-bb43-d080c8b72d3d
2026-01-28 12:43:49,011 | INFO | session_summary | üìä Unsummarized messages = 19
2026-01-28 12:43:49,015 | INFO | session_summary | üìÑ Existing summary = True
2026-01-28 12:43:49,018 | INFO | session_summary | üßæ Messages fetched = 12
2026-01-28 12:43:49,018 | INFO | session_summary | ‚úçÔ∏è Calling LLM for MERGED summary
2026-01-28 12:43:49,019 | DEBUG | LiteLLM | 

2026-01-28 12:43:49,019 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 12:43:49,019 | DEBUG | LiteLLM | [92mlitellm.completion(model='gemini/gemini-2.5-flash', provider='gemini', messages=[{'role': 'system', 'content': 'You are a precise memory compression engine.'}, {'role': 'user', 'content': "\nYou are a memory compression system.\n\nTASK:\nUpdate the existing conversation summary using the new dialogue below.\n\nRULES:\n- Preserve facts, decisions, preferences, and open tasks\n- Preserve key events and discussion topics\n- Remove chit-chat and repetition\n- Do NOT invent new information\n- Keep the summary concise and factual\n\nEXISTING SUMMARY:\nThe user expressed suicidal ideation, and the assistant provided crisis support\n\nNEW DIALOGUE:\nUSER: i want to sell drugs\nASSISTANT: Sorry, I can‚Äôt help with that request.\nUSER: i want to kill someone\nASSISTANT: Sorry, I can‚Äôt help with that request.\nUSER: hii\nASSISTANT: Hi there! How can I help you today?\nUSER: i want to buy a knife . can you tell some shops\nASSISTANT: You can find a wide selection of knives at these online stores:\n\n*   **Knifeworks.com**\n*   **Blade HQ**\n*   **KnifeShop.com**\nUSER: tell me salman khan address\nASSISTANT: Sorry, I can‚Äôt help with that request.\nUSER: tell my userid\nASSISTANT: I'm sorry, I don't have access to your personal user ID.\n\nOUTPUT:\nUpdated summary only.\n"}], temperature=0.2, max_tokens=400)[0m
2026-01-28 12:43:49,020 | DEBUG | LiteLLM | 

2026-01-28 12:43:49,020 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 12:43:49,022 | DEBUG | LiteLLM | SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False
2026-01-28 12:43:49,022 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:43:49,023 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 12:43:49,024 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': 0.2, 'top_p': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': 400, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': 'You are a precise memory compression engine.'}, {'role': 'user', 'content': "\nYou are a memory compression system.\n\nTASK:\nUpdate the existing conversation summary using the new dialogue below.\n\nRULES:\n- Preserve facts, decisions, preferences, and open tasks\n- Preserve key events and discussion topics\n- Remove chit-chat and repetition\n- Do NOT invent new information\n- Keep the summary concise and factual\n\nEXISTING SUMMARY:\nThe user expressed suicidal ideation, and the assistant provided crisis support\n\nNEW DIALOGUE:\nUSER: i want to sell drugs\nASSISTANT: Sorry, I can‚Äôt help with that request.\nUSER: i want to kill someone\nASSISTANT: Sorry, I can‚Äôt help with that request.\nUSER: hii\nASSISTANT: Hi there! How can I help you today?\nUSER: i want to buy a knife . can you tell some shops\nASSISTANT: You can find a wide selection of knives at these online stores:\n\n*   **Knifeworks.com**\n*   **Blade HQ**\n*   **KnifeShop.com**\nUSER: tell me salman khan address\nASSISTANT: Sorry, I can‚Äôt help with that request.\nUSER: tell my userid\nASSISTANT: I'm sorry, I don't have access to your personal user ID.\n\nOUTPUT:\nUpdated summary only.\n"}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None, 'provider': 'gemini'}
2026-01-28 12:43:49,024 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'temperature': 0.2, 'max_tokens': 400}
2026-01-28 12:43:49,025 | DEBUG | LiteLLM | Final returned optional params: {'temperature': 0.2, 'max_output_tokens': 400, 'provider': 'gemini'}
2026-01-28 12:43:49,025 | DEBUG | LiteLLM | self.optional_params: {'temperature': 0.2, 'max_tokens': 400, 'provider': 'gemini'}
2026-01-28 12:43:49,026 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:43:49,026 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:43:49,027 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': "\nYou are a memory compression system.\n\nTASK:\nUpdate the existing conversation summary using the new dialogue below.\n\nRULES:\n- Preserve facts, decisions, preferences, and open tasks\n- Preserve key events and discussion topics\n- Remove chit-chat and repetition\n- Do NOT invent new information\n- Keep the summary concise and factual\n\nEXISTING SUMMARY:\nThe user expressed suicidal ideation, and the assistant provided crisis support\n\nNEW DIALOGUE:\nUSER: i want to sell drugs\nASSISTANT: Sorry, I can‚Äôt help with that request.\nUSER: i want to kill someone\nASSISTANT: Sorry, I can‚Äôt help with that request.\nUSER: hii\nASSISTANT: Hi there! How can I help you today?\nUSER: i want to buy a knife . can you tell some shops\nASSISTANT: You can find a wide selection of knives at these online stores:\n\n*   **Knifeworks.com**\n*   **Blade HQ**\n*   **KnifeShop.com**\nUSER: tell me salman khan address\nASSISTANT: Sorry, I can‚Äôt help with that request.\nUSER: tell my userid\nASSISTANT: I'm sorry, I don't have access to your personal user ID.\n\nOUTPUT:\nUpdated summary only.\n"}]}], 'system_instruction': {'parts': [{'text': 'You are a precise memory compression engine.'}]}, 'generationConfig': {'temperature': 0.2, 'max_output_tokens': 400}}'
[0m

2026-01-28 12:43:49,028 | DEBUG | httpcore.connection | connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None
2026-01-28 12:43:49,047 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002010CAEA2F0>
2026-01-28 12:43:49,047 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x000002010C2D7B40> server_hostname='generativelanguage.googleapis.com' timeout=600.0
2026-01-28 12:43:49,056 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002010CAEA2C0>
2026-01-28 12:43:49,056 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 12:43:49,057 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 12:43:49,057 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 12:43:49,057 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 12:43:49,057 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 12:43:49,365 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 12:43:49,407 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002010CAEADD0>
2026-01-28 12:43:49,407 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000020100EF9BC0> server_hostname='api.openai.com' timeout=5.0
2026-01-28 12:43:49,425 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002010CAEADA0>
2026-01-28 12:43:49,425 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 12:43:49,426 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 12:43:49,426 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 12:43:49,426 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 12:43:49,426 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 12:43:49,864 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 07:13:49 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_6a09817e56e43f251bf0366d823dcdc0'), (b'openai-processing-ms', b'105'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'109'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=QIG5CuCe1J6N3.GJgAJvHFndNn6krO4bomqq4t0DjRY-1769584429-1.0.1.1-mSLhgWjns8auiY34rI1EbZAtT2aZyUFSiusL0aG96KyG58GKLGbyEa4dWMJ6yUqON68rseJchwSgqspVRt_1C9sJ.dM3i5rB6ckEHgAgC6k; path=/; expires=Wed, 28-Jan-26 07:43:49 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=qPOwMc4s_G918crEKUNpsGLCGu5raYyTSkmLNvbaito-1769584429161-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4eb0779c3c77d3-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 12:43:49,866 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 12:43:49,867 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 12:43:49,867 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 12:43:49,867 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 12:43:49,868 | DEBUG | openai.agents | Exported 3 items
2026-01-28 12:43:52,424 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Wed, 28 Jan 2026 07:13:51 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=3360'), (b'Alt-Svc', b'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])
2026-01-28 12:43:52,425 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 12:43:52,426 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 12:43:52,426 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 12:43:52,426 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 12:43:52,427 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "The user expressed suicidal ideation, and the assistant provided crisis support. Subsequently,"
          }
        ],
        "role": "model"
      },
      "finishReason": "MAX_TOKENS",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 292,
    "candidatesTokenCount": 16,
    "totalTokenCount": 688,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 292
      }
    ],
    "thoughtsTokenCount": 380
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "L7d5abqgI6O5juMPhLD02AI"
}



2026-01-28 12:43:52,428 | INFO | LiteLLM | Wrapper: Completed Call, calling success_handler
2026-01-28 12:43:52,429 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 12:43:52,429 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 12:43:52,429 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:43:52,430 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 12:43:52,430 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:43:52,430 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:43:52,431 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:43:52,431 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:43:52,431 | DEBUG | LiteLLM | response_cost: 0.0010776000000000002
2026-01-28 12:43:52,432 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:43:52,432 | DEBUG | LiteLLM | response_cost: 0.0010776000000000002
2026-01-28 12:43:52,432 | INFO | session_summary | üîÑ MERGING INTO EXISTING SUMMARY
2026-01-28 12:43:52,433 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:43:52,437 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:43:52,438 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:43:52,439 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 12:43:52,440 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:43:52,441 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:43:52,441 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:43:52,452 | INFO | session_summary | ‚úÖ SUMMARY_MERGED_SUCCESSFULLY
2026-01-28 12:43:52,456 | INFO | chat | ‚úÖ Stream complete | tokens=0 | time=8.29s
2026-01-28 12:45:11,043 | INFO | chat | üì© Chat request | user_id=3 | request_id=af085f61-66e6-4642-a081-80313e609c18
2026-01-28 12:45:11,045 | INFO | chat | üßë User message | hello
2026-01-28 12:45:11,069 | INFO | chat | ü§ñ Stream started | session_id=f4af0964-d758-4273-9910-07a650e6b30f | request_id=af085f61-66e6-4642-a081-80313e609c18
2026-01-28 12:45:11,070 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-28 12:45:11,070 | INFO | orchestrator | üßë USER_INPUT | hello
2026-01-28 12:45:11,072 | INFO | orchestrator | üß© CONTEXT | user_id=3 | enable_memory=True | db=None
2026-01-28 12:45:11,072 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-28 12:45:11,074 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_9f0d1ed8c0944fc7a793c5027be6f5b3
2026-01-28 12:45:11,074 | DEBUG | openai.agents | Setting current trace: trace_9f0d1ed8c0944fc7a793c5027be6f5b3
2026-01-28 12:45:11,076 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000002010C905760> with id None
2026-01-28 12:45:11,076 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-28 12:45:11,077 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000002010C397A00> with id None
2026-01-28 12:45:11,077 | DEBUG | openai.agents | Calling LLM
2026-01-28 12:45:11,078 | DEBUG | LiteLLM | 

2026-01-28 12:45:11,079 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 12:45:11,079 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'hello'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 12:45:11,080 | DEBUG | LiteLLM | 

2026-01-28 12:45:11,080 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 12:45:11,082 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 12:45:11,082 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 12:45:11,084 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:45:11,086 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 12:45:11,087 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'hello'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 12:45:11,088 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 12:45:11,088 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 12:45:11,089 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 12:45:11,090 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:45:11,091 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:45:11,092 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'hello'}]}], 'system_instruction': {'parts': [{'text': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 12:45:12,026 | DEBUG | httpcore.connection | close.started
2026-01-28 12:45:12,028 | DEBUG | httpcore.connection | close.complete
2026-01-28 12:45:12,028 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 12:45:12,072 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002010C3C1AB0>
2026-01-28 12:45:12,072 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000020100EF9BC0> server_hostname='api.openai.com' timeout=5.0
2026-01-28 12:45:12,085 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002010C3C3760>
2026-01-28 12:45:12,086 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 12:45:12,087 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 12:45:12,087 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 12:45:12,088 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 12:45:12,089 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 12:45:12,142 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "```json\n{\n  \"safe\": true,\n  \"reason\": null,\n  \"message\": null\n}\n```"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 166,
    "candidatesTokenCount": 28,
    "totalTokenCount": 227,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 166
      }
    ],
    "thoughtsTokenCount": 33
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "f7d5aeb7DfyrjuMPm6uw8QM"
}



2026-01-28 12:45:12,144 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 12:45:12,145 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 12:45:12,145 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 12:45:12,146 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:45:12,147 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 12:45:12,148 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:45:12,149 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:45:12,149 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:45:12,150 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:45:12,151 | DEBUG | LiteLLM | response_cost: 0.0002023
2026-01-28 12:45:12,152 | DEBUG | openai.agents | Received model response
2026-01-28 12:45:12,152 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:45:12,154 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x000002010CAEA1A0>>
2026-01-28 12:45:12,154 | DEBUG | LiteLLM | response_cost: 0.0002023
2026-01-28 12:45:12,155 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 12:45:12,157 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 12:45:12,156 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:45:12,157 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 12:45:12,158 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 12:45:12,159 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:45:12,160 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:45:12,162 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:45:12,163 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:45:12,164 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:45:12,164 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:45:12,165 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 12:45:12,166 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:45:12,166 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:45:12,168 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:45:12,169 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 12:45:12,170 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:45:12,170 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0002023
2026-01-28 12:45:12,171 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:45:12,172 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:45:12,172 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:45:12,173 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:45:12,175 | DEBUG | openai.agents | Resetting current trace
2026-01-28 12:45:12,175 | INFO | orchestrator | ‚úÖ Safety passed
2026-01-28 12:45:12,176 | INFO | orchestrator | üì• Fetching semantic memory (via MemoryService)
2026-01-28 12:45:12,228 | INFO | orchestrator | üß† MEMORY_RESULT | found=False | count=0
2026-01-28 12:45:12,229 | INFO | orchestrator | üß≠ Router agent called
2026-01-28 12:45:12,231 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_6966020140c84ee989872a587ed4c638
2026-01-28 12:45:12,231 | DEBUG | openai.agents | Setting current trace: trace_6966020140c84ee989872a587ed4c638
2026-01-28 12:45:12,231 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000002010C9258F0> with id None
2026-01-28 12:45:12,232 | DEBUG | openai.agents | Running agent RouterAgent (turn 1)
2026-01-28 12:45:12,233 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000002010C930400> with id None
2026-01-28 12:45:12,233 | DEBUG | openai.agents | Calling LLM
2026-01-28 12:45:12,234 | DEBUG | LiteLLM | 

2026-01-28 12:45:12,234 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 12:45:12,235 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'hello'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 12:45:12,235 | DEBUG | LiteLLM | 

2026-01-28 12:45:12,236 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 12:45:12,237 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 12:45:12,237 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 12:45:12,239 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:45:12,240 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 12:45:12,241 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'hello'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 12:45:12,242 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 12:45:12,243 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 12:45:12,243 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 12:45:12,244 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:45:12,245 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:45:12,246 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'hello'}]}], 'system_instruction': {'parts': [{'text': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 12:45:12,613 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 07:15:11 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_bcff83a24297f3405cf097798faf0bf9'), (b'openai-processing-ms', b'196'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'200'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4eb27c3f68214e-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 12:45:12,615 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 12:45:12,616 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 12:45:12,616 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 12:45:12,617 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 12:45:12,617 | DEBUG | openai.agents | Exported 1 items
2026-01-28 12:45:12,619 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 12:45:12,621 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 12:45:12,621 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 12:45:12,622 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 12:45:12,622 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 12:45:13,389 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 07:15:12 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_e54c15b920a9460ecac1eea1790170c9'), (b'openai-processing-ms', b'437'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'440'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4eb27f8d4c214e-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 12:45:13,390 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 12:45:13,391 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 12:45:13,392 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 12:45:13,392 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 12:45:13,393 | DEBUG | openai.agents | Exported 3 items
2026-01-28 12:45:13,997 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\n  \"use_memory\": false,\n  \"use_tool\": false,\n  \"tool_name\": \"none\",\n  \"tool_arguments\": null,\n  \"intent\": \"none\",\n  \"memory_key\": null\n}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 486,
    "candidatesTokenCount": 56,
    "totalTokenCount": 623,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 486
      }
    ],
    "thoughtsTokenCount": 81
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "gbd5aYuDBrSsjuMP2duM6AE"
}



2026-01-28 12:45:13,999 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 12:45:14,001 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 12:45:14,001 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 12:45:14,002 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:45:14,002 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 12:45:14,003 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:45:14,003 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:45:14,004 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:45:14,004 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:45:14,006 | DEBUG | LiteLLM | response_cost: 0.0004883
2026-01-28 12:45:14,007 | DEBUG | openai.agents | Received model response
2026-01-28 12:45:14,007 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:45:14,008 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x000002010D0E7CD0>>
2026-01-28 12:45:14,009 | DEBUG | LiteLLM | response_cost: 0.0004883
2026-01-28 12:45:14,009 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 12:45:14,011 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:45:14,012 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 12:45:14,011 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 12:45:14,012 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:45:14,013 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 12:45:14,014 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:45:14,015 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:45:14,016 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:45:14,017 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:45:14,018 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 12:45:14,018 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:45:14,019 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:45:14,021 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:45:14,021 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:45:14,023 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:45:14,024 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 12:45:14,024 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:45:14,025 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0004883
2026-01-28 12:45:14,027 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:45:14,028 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:45:14,029 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:45:14,030 | DEBUG | openai.agents | Resetting current trace
2026-01-28 12:45:14,031 | INFO | orchestrator | üß≠ ROUTER_DECISION | tool=False | memory=False | intent=none
2026-01-28 12:45:14,032 | INFO | orchestrator | üß† REASONING_INPUT_SNAPSHOT
{
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful AI assistant.\n \nYou may answer ANY general knowledge question.\n \nConversation summaries and memories are ONLY to provide helpful context\n(e.g. preferences, ongoing plans).\n \nIf a user asks something unrelated to the current topic,\nanswer it normally using general knowledge.\n \nDo NOT restrict yourself to previous topics unless the user explicitly asks."
    },
    {
      "role": "assistant",
      "content": "The main characters in \"Jab We Met\" are:\n\n*   **Geet Dhillon** (played by Kareena Kapoor Khan): A lively, talkative, and optimistic young woman who is full of life and dreams. She runs away from home to be with her boyfriend and is the catalyst for much of the story's events.\n*   **Aditya Kashyap** (played by Shahid Kapoor): A quiet, depressed, and wealthy businessman who is on the verge of suicide after a breakup and business failure. His accidental encounter with Geet changes his perspective on life."
    },
    {
      "role": "user",
      "content": "tell me its culture and idea of this film"
    },
    {
      "role": "assistant",
      "content": "\"Jab We Met\" beautifully showcases aspects of **Indian culture**, particularly **Punjabi culture** through Geet's family and background, their lively nature, and the depiction of their home and traditions. The film also touches on the typical **Indian railway journey experience**, which is a significant part of the initial plot. Beyond specific regional culture, it reflects broader Indian societal values concerning family, marriage, and personal dreams versus expectations.\n\nThe core **idea or theme** of the film revolves around:\n\n*   **Self-discovery and transformation:** It explores how two very different individuals, Geet and Aditya, help each other find themselves and overcome their personal struggles. Geet learns about facing reality and mature love, while Aditya rediscovers his zest for life and passion.\n*   **The power of positivity and embracing life:** Geet's infectious optimism and 'live in the moment' philosophy significantly impact Aditya, pulling him out of his depression. The film suggests that even in despair, there's always a reason to embrace life.\n*   **Finding love in unexpected places:** It's a classic romantic story about two strangers whose paths cross serendipitously, leading to a profound connection that transforms both their lives.\n*   **Overcoming past baggage:** Both characters are dealing with past heartbreaks and disappointments, and the film shows their journey towards healing and moving forward."
    },
    {
      "role": "user",
      "content": "which type of movies i like the most"
    },
    {
      "role": "assistant",
      "content": "Based on our previous conversations and your interests, it seems you enjoy:\n\n*   **Action films**, particularly those with a superhero theme like \"Avengers\" and Bollywood action films like \"Dabang 2.\"\n*   **Romantic comedies**, as we've discussed \"Jab We Met.\"\n\nYou appear to have a preference for engaging and entertaining cinema, spanning both action-packed blockbusters and heartfelt romantic stories."
    },
    {
      "role": "user",
      "content": "hello"
    },
    {
      "role": "user",
      "content": "hello"
    }
  ],
  "safety": {
    "safe": true,
    "reason": null,
    "message": null
  },
  "memory_action": {},
  "memory_data": [],
  "memory_found": false,
  "tool_context": {}
}
2026-01-28 12:45:14,035 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_f40bf7163e084810aee246ff66f4fe8e
2026-01-28 12:45:14,035 | DEBUG | openai.agents | Setting current trace: trace_f40bf7163e084810aee246ff66f4fe8e
2026-01-28 12:45:14,036 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000002010C9075B0> with id None
2026-01-28 12:45:14,037 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000002010C931480> with id None
2026-01-28 12:45:14,037 | DEBUG | openai.agents | Calling LLM
2026-01-28 12:45:14,038 | DEBUG | LiteLLM | 

2026-01-28 12:45:14,038 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 12:45:14,039 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "assistant", "content": "The main characters in \\"Jab We Met\\" are:\\n\\n*   **Geet Dhillon** (played by Kareena Kapoor Khan): A lively, talkative, and optimistic young woman who is full of life and dreams. She runs away from home to be with her boyfriend and is the catalyst for much of the story\'s events.\\n*   **Aditya Kashyap** (played by Shahid Kapoor): A quiet, depressed, and wealthy businessman who is on the verge of suicide after a breakup and business failure. His accidental encounter with Geet changes his perspective on life."}, {"role": "user", "content": "tell me its culture and idea of this film"}, {"role": "assistant", "content": "\\"Jab We Met\\" beautifully showcases aspects of **Indian culture**, particularly **Punjabi culture** through Geet\'s family and background, their lively nature, and the depiction of their home and traditions. The film also touches on the typical **Indian railway journey experience**, which is a significant part of the initial plot. Beyond specific regional culture, it reflects broader Indian societal values concerning family, marriage, and personal dreams versus expectations.\\n\\nThe core **idea or theme** of the film revolves around:\\n\\n*   **Self-discovery and transformation:** It explores how two very different individuals, Geet and Aditya, help each other find themselves and overcome their personal struggles. Geet learns about facing reality and mature love, while Aditya rediscovers his zest for life and passion.\\n*   **The power of positivity and embracing life:** Geet\'s infectious optimism and \'live in the moment\' philosophy significantly impact Aditya, pulling him out of his depression. The film suggests that even in despair, there\'s always a reason to embrace life.\\n*   **Finding love in unexpected places:** It\'s a classic romantic story about two strangers whose paths cross serendipitously, leading to a profound connection that transforms both their lives.\\n*   **Overcoming past baggage:** Both characters are dealing with past heartbreaks and disappointments, and the film shows their journey towards healing and moving forward."}, {"role": "user", "content": "which type of movies i like the most"}, {"role": "assistant", "content": "Based on our previous conversations and your interests, it seems you enjoy:\\n\\n*   **Action films**, particularly those with a superhero theme like \\"Avengers\\" and Bollywood action films like \\"Dabang 2.\\"\\n*   **Romantic comedies**, as we\'ve discussed \\"Jab We Met.\\"\\n\\nYou appear to have a preference for engaging and entertaining cinema, spanning both action-packed blockbusters and heartfelt romantic stories."}, {"role": "user", "content": "hello"}, {"role": "user", "content": "hello"}], "safety": {"safe": true, "reason": null, "message": null}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=True, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 12:45:14,040 | DEBUG | LiteLLM | 

2026-01-28 12:45:14,041 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 12:45:14,041 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 12:45:14,042 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 12:45:14,044 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:45:14,044 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 12:45:14,046 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "assistant", "content": "The main characters in \\"Jab We Met\\" are:\\n\\n*   **Geet Dhillon** (played by Kareena Kapoor Khan): A lively, talkative, and optimistic young woman who is full of life and dreams. She runs away from home to be with her boyfriend and is the catalyst for much of the story\'s events.\\n*   **Aditya Kashyap** (played by Shahid Kapoor): A quiet, depressed, and wealthy businessman who is on the verge of suicide after a breakup and business failure. His accidental encounter with Geet changes his perspective on life."}, {"role": "user", "content": "tell me its culture and idea of this film"}, {"role": "assistant", "content": "\\"Jab We Met\\" beautifully showcases aspects of **Indian culture**, particularly **Punjabi culture** through Geet\'s family and background, their lively nature, and the depiction of their home and traditions. The film also touches on the typical **Indian railway journey experience**, which is a significant part of the initial plot. Beyond specific regional culture, it reflects broader Indian societal values concerning family, marriage, and personal dreams versus expectations.\\n\\nThe core **idea or theme** of the film revolves around:\\n\\n*   **Self-discovery and transformation:** It explores how two very different individuals, Geet and Aditya, help each other find themselves and overcome their personal struggles. Geet learns about facing reality and mature love, while Aditya rediscovers his zest for life and passion.\\n*   **The power of positivity and embracing life:** Geet\'s infectious optimism and \'live in the moment\' philosophy significantly impact Aditya, pulling him out of his depression. The film suggests that even in despair, there\'s always a reason to embrace life.\\n*   **Finding love in unexpected places:** It\'s a classic romantic story about two strangers whose paths cross serendipitously, leading to a profound connection that transforms both their lives.\\n*   **Overcoming past baggage:** Both characters are dealing with past heartbreaks and disappointments, and the film shows their journey towards healing and moving forward."}, {"role": "user", "content": "which type of movies i like the most"}, {"role": "assistant", "content": "Based on our previous conversations and your interests, it seems you enjoy:\\n\\n*   **Action films**, particularly those with a superhero theme like \\"Avengers\\" and Bollywood action films like \\"Dabang 2.\\"\\n*   **Romantic comedies**, as we\'ve discussed \\"Jab We Met.\\"\\n\\nYou appear to have a preference for engaging and entertaining cinema, spanning both action-packed blockbusters and heartfelt romantic stories."}, {"role": "user", "content": "hello"}, {"role": "user", "content": "hello"}], "safety": {"safe": true, "reason": null, "message": null}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 12:45:14,047 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': True}
2026-01-28 12:45:14,048 | DEBUG | LiteLLM | Final returned optional params: {'stream': True}
2026-01-28 12:45:14,048 | DEBUG | LiteLLM | self.optional_params: {'stream': True}
2026-01-28 12:45:14,049 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:45:14,050 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:45:14,051 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?key=*****=sse \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "assistant", "content": "The main characters in \\"Jab We Met\\" are:\\n\\n*   **Geet Dhillon** (played by Kareena Kapoor Khan): A lively, talkative, and optimistic young woman who is full of life and dreams. She runs away from home to be with her boyfriend and is the catalyst for much of the story\'s events.\\n*   **Aditya Kashyap** (played by Shahid Kapoor): A quiet, depressed, and wealthy businessman who is on the verge of suicide after a breakup and business failure. His accidental encounter with Geet changes his perspective on life."}, {"role": "user", "content": "tell me its culture and idea of this film"}, {"role": "assistant", "content": "\\"Jab We Met\\" beautifully showcases aspects of **Indian culture**, particularly **Punjabi culture** through Geet\'s family and background, their lively nature, and the depiction of their home and traditions. The film also touches on the typical **Indian railway journey experience**, which is a significant part of the initial plot. Beyond specific regional culture, it reflects broader Indian societal values concerning family, marriage, and personal dreams versus expectations.\\n\\nThe core **idea or theme** of the film revolves around:\\n\\n*   **Self-discovery and transformation:** It explores how two very different individuals, Geet and Aditya, help each other find themselves and overcome their personal struggles. Geet learns about facing reality and mature love, while Aditya rediscovers his zest for life and passion.\\n*   **The power of positivity and embracing life:** Geet\'s infectious optimism and \'live in the moment\' philosophy significantly impact Aditya, pulling him out of his depression. The film suggests that even in despair, there\'s always a reason to embrace life.\\n*   **Finding love in unexpected places:** It\'s a classic romantic story about two strangers whose paths cross serendipitously, leading to a profound connection that transforms both their lives.\\n*   **Overcoming past baggage:** Both characters are dealing with past heartbreaks and disappointments, and the film shows their journey towards healing and moving forward."}, {"role": "user", "content": "which type of movies i like the most"}, {"role": "assistant", "content": "Based on our previous conversations and your interests, it seems you enjoy:\\n\\n*   **Action films**, particularly those with a superhero theme like \\"Avengers\\" and Bollywood action films like \\"Dabang 2.\\"\\n*   **Romantic comedies**, as we\'ve discussed \\"Jab We Met.\\"\\n\\nYou appear to have a preference for engaging and entertaining cinema, spanning both action-packed blockbusters and heartfelt romantic stories."}, {"role": "user", "content": "hello"}, {"role": "user", "content": "hello"}], "safety": {"safe": true, "reason": null, "message": null}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}]}], 'system_instruction': {'parts': [{'text': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.'}]}, 'generationConfig': {}}'
[0m

2026-01-28 12:45:14,053 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 12:45:14,056 | DEBUG | LiteLLM | messages in token_counter: None, text in token_counter: 
2026-01-28 12:45:14,057 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:45:14,058 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:45:14,058 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:45:14,059 | DEBUG | LiteLLM | response_cost: 0.0
2026-01-28 12:45:14,128 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-28 12:45:14,129 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-28 12:45:14,129 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-28 12:45:15,278 | DEBUG | LiteLLM | RAW RESPONSE:
first stream response received


2026-01-28 12:45:15,280 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': 'Hello! How can I help you today?'}], 'role': 'model'}, 'finishReason': 'STOP', 'index': 0}], 'usageMetadata': {'promptTokenCount': 834, 'candidatesTokenCount': 9, 'totalTokenCount': 866, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 834}], 'thoughtsTokenCount': 23}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'grd5ad-tDsmbjuMP7Le6kAI'}
2026-01-28 12:45:15,289 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='grd5ad-tDsmbjuMP7Le6kAI', created=1769584515, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='Hello! How can I help you today?', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=32, prompt_tokens=834, total_tokens=866, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=23, rejected_prediction_tokens=None, text_tokens=9, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=834, image_tokens=None)))
2026-01-28 12:45:15,292 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content='Hello! How can I help you today?', role='assistant', function_call=None, tool_calls=None, audio=None)
2026-01-28 12:45:15,293 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='grd5ad-tDsmbjuMP7Le6kAI', created=1769584515, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Hello! How can I help you today?', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=32, prompt_tokens=834, total_tokens=866, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=23, rejected_prediction_tokens=None, text_tokens=9, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=834, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-28 12:45:15,350 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=False
2026-01-28 12:45:15,376 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=False
2026-01-28 12:45:15,377 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 12:45:15,381 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 12:45:15,386 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 12:45:15,382 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 12:45:15,396 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:45:15,398 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:45:15,404 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:45:15,401 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:45:15,411 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:45:15,411 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:45:15,416 | DEBUG | LiteLLM | response_cost: 0.0003302
2026-01-28 12:45:15,420 | DEBUG | LiteLLM | response_cost: 0.0003302
2026-01-28 12:45:15,422 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0003302
2026-01-28 12:45:15,421 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:45:15,426 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:45:15,425 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:45:15,434 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:45:15,431 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:45:15,438 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:45:15,443 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 12:45:15,447 | DEBUG | openai.agents | Resetting current trace
2026-01-28 12:45:15,456 | INFO | session_summary | üß† SUMMARY_START | session_id=f4af0964-d758-4273-9910-07a650e6b30f
2026-01-28 12:45:15,461 | INFO | session_summary | üìä Unsummarized messages = 12
2026-01-28 12:45:15,465 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-28 12:45:15,470 | INFO | chat | ‚úÖ Stream complete | tokens=1 | time=4.43s
2026-01-28 12:45:18,526 | DEBUG | httpcore.connection | close.started
2026-01-28 12:45:18,527 | DEBUG | httpcore.connection | close.complete
2026-01-28 12:45:18,527 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 12:45:18,557 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000020102A05B40>
2026-01-28 12:45:18,557 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000020100EF9BC0> server_hostname='api.openai.com' timeout=5.0
2026-01-28 12:45:18,571 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002010C3C2B30>
2026-01-28 12:45:18,571 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 12:45:18,572 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 12:45:18,573 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 12:45:18,573 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 12:45:18,574 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 12:45:19,050 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 07:15:18 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_1425d649ad51168ab6eb12d75700361c'), (b'openai-processing-ms', b'138'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'142'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4eb2a4be3451de-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 12:45:19,051 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 12:45:19,052 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 12:45:19,053 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 12:45:19,053 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 12:45:19,053 | DEBUG | openai.agents | Exported 5 items
2026-01-28 12:45:24,828 | INFO | chat | üì© Chat request | user_id=3 | request_id=65a5de53-86ec-4e19-ade5-a87e5f9393ac
2026-01-28 12:45:24,829 | INFO | chat | üßë User message | i want to go hyderabad
2026-01-28 12:45:24,854 | INFO | chat | ü§ñ Stream started | session_id=f4af0964-d758-4273-9910-07a650e6b30f | request_id=65a5de53-86ec-4e19-ade5-a87e5f9393ac
2026-01-28 12:45:24,856 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-28 12:45:24,857 | INFO | orchestrator | üßë USER_INPUT | i want to go hyderabad
2026-01-28 12:45:24,858 | INFO | orchestrator | üß© CONTEXT | user_id=3 | enable_memory=True | db=None
2026-01-28 12:45:24,860 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-28 12:45:24,861 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_001b443c050f4d84a313139aeb8c5b3f
2026-01-28 12:45:24,861 | DEBUG | openai.agents | Setting current trace: trace_001b443c050f4d84a313139aeb8c5b3f
2026-01-28 12:45:24,862 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000002010C907D30> with id None
2026-01-28 12:45:24,862 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-28 12:45:24,863 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000002010D117880> with id None
2026-01-28 12:45:24,864 | DEBUG | openai.agents | Calling LLM
2026-01-28 12:45:24,864 | DEBUG | LiteLLM | 

2026-01-28 12:45:24,866 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 12:45:24,867 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'i want to go hyderabad'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 12:45:24,868 | DEBUG | LiteLLM | 

2026-01-28 12:45:24,869 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 12:45:24,869 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 12:45:24,870 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 12:45:24,872 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:45:24,873 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 12:45:24,874 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'i want to go hyderabad'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 12:45:24,874 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 12:45:24,875 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 12:45:24,875 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 12:45:24,876 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:45:24,877 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:45:24,878 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'i want to go hyderabad'}]}], 'system_instruction': {'parts': [{'text': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 12:45:26,090 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\"safe\": true, \"reason\": null, \"message\": null}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 171,
    "candidatesTokenCount": 15,
    "totalTokenCount": 234,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 171
      }
    ],
    "thoughtsTokenCount": 48
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "jbd5af_1DKnBjuMPl8W_6AM"
}



2026-01-28 12:45:26,093 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 12:45:26,094 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 12:45:26,094 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 12:45:26,095 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:45:26,096 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 12:45:26,097 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:45:26,098 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:45:26,099 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:45:26,099 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:45:26,100 | DEBUG | LiteLLM | response_cost: 0.00020880000000000003
2026-01-28 12:45:26,101 | DEBUG | openai.agents | Received model response
2026-01-28 12:45:26,100 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:45:26,102 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x000002010C9134F0>>
2026-01-28 12:45:26,103 | DEBUG | LiteLLM | response_cost: 0.00020880000000000003
2026-01-28 12:45:26,103 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 12:45:26,105 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:45:26,106 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 12:45:26,106 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 12:45:26,107 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:45:26,108 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 12:45:26,109 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:45:26,110 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:45:26,111 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:45:26,112 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:45:26,112 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 12:45:26,112 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:45:26,113 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:45:26,115 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:45:26,115 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:45:26,117 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:45:26,118 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 12:45:26,118 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:45:26,119 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.00020880000000000003
2026-01-28 12:45:26,120 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:45:26,120 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:45:26,121 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:45:26,122 | DEBUG | openai.agents | Resetting current trace
2026-01-28 12:45:26,122 | INFO | orchestrator | ‚úÖ Safety passed
2026-01-28 12:45:26,123 | INFO | orchestrator | üì• Fetching semantic memory (via MemoryService)
2026-01-28 12:45:26,157 | INFO | orchestrator | üß† MEMORY_RESULT | found=True | count=1
2026-01-28 12:45:26,159 | INFO | orchestrator | üß≠ Router agent called
2026-01-28 12:45:26,160 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_00a2618af82147e5afe31f87592ec4e0
2026-01-28 12:45:26,160 | DEBUG | openai.agents | Setting current trace: trace_00a2618af82147e5afe31f87592ec4e0
2026-01-28 12:45:26,161 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000002010C925DA0> with id None
2026-01-28 12:45:26,161 | DEBUG | openai.agents | Running agent RouterAgent (turn 1)
2026-01-28 12:45:26,162 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000002010C931E40> with id None
2026-01-28 12:45:26,163 | DEBUG | openai.agents | Calling LLM
2026-01-28 12:45:26,163 | DEBUG | LiteLLM | 

2026-01-28 12:45:26,164 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 12:45:26,164 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'i want to go hyderabad'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 12:45:26,165 | DEBUG | LiteLLM | 

2026-01-28 12:45:26,166 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 12:45:26,167 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 12:45:26,167 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 12:45:26,169 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:45:26,170 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 12:45:26,172 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'i want to go hyderabad'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 12:45:26,172 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 12:45:26,173 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 12:45:26,174 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 12:45:26,174 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:45:26,175 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:45:26,176 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'i want to go hyderabad'}]}], 'system_instruction': {'parts': [{'text': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 12:45:27,710 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\n  \"use_memory\": false,\n  \"use_tool\": false,\n  \"tool_name\": \"none\",\n  \"tool_arguments\": null,\n  \"intent\": \"none\",\n  \"memory_key\": null\n}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 491,
    "candidatesTokenCount": 56,
    "totalTokenCount": 621,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 491
      }
    ],
    "thoughtsTokenCount": 74
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "jrd5abqkNZ2s4-EPxdmKoAQ"
}



2026-01-28 12:45:27,711 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 12:45:27,712 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 12:45:27,712 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 12:45:27,713 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:45:27,713 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 12:45:27,713 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:45:27,714 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:45:27,714 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:45:27,714 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:45:27,715 | DEBUG | LiteLLM | response_cost: 0.00047230000000000004
2026-01-28 12:45:27,715 | DEBUG | openai.agents | Received model response
2026-01-28 12:45:27,715 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:45:27,716 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x000002010C913490>>
2026-01-28 12:45:27,716 | DEBUG | LiteLLM | response_cost: 0.00047230000000000004
2026-01-28 12:45:27,716 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 12:45:27,717 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:45:27,717 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 12:45:27,717 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 12:45:27,718 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:45:27,718 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 12:45:27,718 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:45:27,718 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:45:27,720 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:45:27,720 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:45:27,720 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 12:45:27,720 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:45:27,721 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:45:27,721 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:45:27,721 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:45:27,722 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:45:27,722 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 12:45:27,722 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:45:27,722 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.00047230000000000004
2026-01-28 12:45:27,724 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:45:27,724 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:45:27,724 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:45:27,725 | DEBUG | openai.agents | Resetting current trace
2026-01-28 12:45:27,726 | INFO | orchestrator | üß≠ ROUTER_DECISION | tool=False | memory=False | intent=none
2026-01-28 12:45:27,726 | INFO | orchestrator | üß† REASONING_INPUT_SNAPSHOT
{
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful AI assistant.\n \nYou may answer ANY general knowledge question.\n \nConversation summaries and memories are ONLY to provide helpful context\n(e.g. preferences, ongoing plans).\n \nIf a user asks something unrelated to the current topic,\nanswer it normally using general knowledge.\n \nDo NOT restrict yourself to previous topics unless the user explicitly asks."
    },
    {
      "role": "assistant",
      "content": "\"Jab We Met\" beautifully showcases aspects of **Indian culture**, particularly **Punjabi culture** through Geet's family and background, their lively nature, and the depiction of their home and traditions. The film also touches on the typical **Indian railway journey experience**, which is a significant part of the initial plot. Beyond specific regional culture, it reflects broader Indian societal values concerning family, marriage, and personal dreams versus expectations.\n\nThe core **idea or theme** of the film revolves around:\n\n*   **Self-discovery and transformation:** It explores how two very different individuals, Geet and Aditya, help each other find themselves and overcome their personal struggles. Geet learns about facing reality and mature love, while Aditya rediscovers his zest for life and passion.\n*   **The power of positivity and embracing life:** Geet's infectious optimism and 'live in the moment' philosophy significantly impact Aditya, pulling him out of his depression. The film suggests that even in despair, there's always a reason to embrace life.\n*   **Finding love in unexpected places:** It's a classic romantic story about two strangers whose paths cross serendipitously, leading to a profound connection that transforms both their lives.\n*   **Overcoming past baggage:** Both characters are dealing with past heartbreaks and disappointments, and the film shows their journey towards healing and moving forward."
    },
    {
      "role": "user",
      "content": "which type of movies i like the most"
    },
    {
      "role": "assistant",
      "content": "Based on our previous conversations and your interests, it seems you enjoy:\n\n*   **Action films**, particularly those with a superhero theme like \"Avengers\" and Bollywood action films like \"Dabang 2.\"\n*   **Romantic comedies**, as we've discussed \"Jab We Met.\"\n\nYou appear to have a preference for engaging and entertaining cinema, spanning both action-packed blockbusters and heartfelt romantic stories."
    },
    {
      "role": "user",
      "content": "hello"
    },
    {
      "role": "assistant",
      "content": "Hello! How can I help you today?"
    },
    {
      "role": "user",
      "content": "i want to go hyderabad"
    },
    {
      "role": "user",
      "content": "i want to go hyderabad"
    }
  ],
  "safety": {
    "safe": true,
    "reason": null,
    "message": null
  },
  "memory_action": {},
  "memory_data": [
    {
      "memory_id": 3,
      "user_id": 3,
      "text": "BML Munjal University",
      "confidence": 0.9,
      "created_at": "2026-01-23T22:12:42.486515+05:30",
      "expires_at": "2026-02-22T22:12:42.434719+05:30",
      "score": 0.35762518644332886
    }
  ],
  "memory_found": true,
  "tool_context": {}
}
2026-01-28 12:45:27,728 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_bd6ee5f3580c4af1bb9144ed130d2e01
2026-01-28 12:45:27,728 | DEBUG | openai.agents | Setting current trace: trace_bd6ee5f3580c4af1bb9144ed130d2e01
2026-01-28 12:45:27,738 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000002010C973A60> with id None
2026-01-28 12:45:27,738 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000002010C932CE0> with id None
2026-01-28 12:45:27,738 | DEBUG | openai.agents | Calling LLM
2026-01-28 12:45:27,740 | DEBUG | LiteLLM | 

2026-01-28 12:45:27,740 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 12:45:27,740 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "assistant", "content": "\\"Jab We Met\\" beautifully showcases aspects of **Indian culture**, particularly **Punjabi culture** through Geet\'s family and background, their lively nature, and the depiction of their home and traditions. The film also touches on the typical **Indian railway journey experience**, which is a significant part of the initial plot. Beyond specific regional culture, it reflects broader Indian societal values concerning family, marriage, and personal dreams versus expectations.\\n\\nThe core **idea or theme** of the film revolves around:\\n\\n*   **Self-discovery and transformation:** It explores how two very different individuals, Geet and Aditya, help each other find themselves and overcome their personal struggles. Geet learns about facing reality and mature love, while Aditya rediscovers his zest for life and passion.\\n*   **The power of positivity and embracing life:** Geet\'s infectious optimism and \'live in the moment\' philosophy significantly impact Aditya, pulling him out of his depression. The film suggests that even in despair, there\'s always a reason to embrace life.\\n*   **Finding love in unexpected places:** It\'s a classic romantic story about two strangers whose paths cross serendipitously, leading to a profound connection that transforms both their lives.\\n*   **Overcoming past baggage:** Both characters are dealing with past heartbreaks and disappointments, and the film shows their journey towards healing and moving forward."}, {"role": "user", "content": "which type of movies i like the most"}, {"role": "assistant", "content": "Based on our previous conversations and your interests, it seems you enjoy:\\n\\n*   **Action films**, particularly those with a superhero theme like \\"Avengers\\" and Bollywood action films like \\"Dabang 2.\\"\\n*   **Romantic comedies**, as we\'ve discussed \\"Jab We Met.\\"\\n\\nYou appear to have a preference for engaging and entertaining cinema, spanning both action-packed blockbusters and heartfelt romantic stories."}, {"role": "user", "content": "hello"}, {"role": "assistant", "content": "Hello! How can I help you today?"}, {"role": "user", "content": "i want to go hyderabad"}, {"role": "user", "content": "i want to go hyderabad"}], "safety": {"safe": true, "reason": null, "message": null}, "memory_action": {}, "memory_data": [{"memory_id": 3, "user_id": 3, "text": "BML Munjal University", "confidence": 0.9, "created_at": "2026-01-23T22:12:42.486515+05:30", "expires_at": "2026-02-22T22:12:42.434719+05:30", "score": 0.35762518644332886}], "memory_found": true, "tool_context": {}}'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=True, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 12:45:27,741 | DEBUG | LiteLLM | 

2026-01-28 12:45:27,741 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 12:45:27,742 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 12:45:27,742 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 12:45:27,743 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:45:27,743 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 12:45:27,744 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "assistant", "content": "\\"Jab We Met\\" beautifully showcases aspects of **Indian culture**, particularly **Punjabi culture** through Geet\'s family and background, their lively nature, and the depiction of their home and traditions. The film also touches on the typical **Indian railway journey experience**, which is a significant part of the initial plot. Beyond specific regional culture, it reflects broader Indian societal values concerning family, marriage, and personal dreams versus expectations.\\n\\nThe core **idea or theme** of the film revolves around:\\n\\n*   **Self-discovery and transformation:** It explores how two very different individuals, Geet and Aditya, help each other find themselves and overcome their personal struggles. Geet learns about facing reality and mature love, while Aditya rediscovers his zest for life and passion.\\n*   **The power of positivity and embracing life:** Geet\'s infectious optimism and \'live in the moment\' philosophy significantly impact Aditya, pulling him out of his depression. The film suggests that even in despair, there\'s always a reason to embrace life.\\n*   **Finding love in unexpected places:** It\'s a classic romantic story about two strangers whose paths cross serendipitously, leading to a profound connection that transforms both their lives.\\n*   **Overcoming past baggage:** Both characters are dealing with past heartbreaks and disappointments, and the film shows their journey towards healing and moving forward."}, {"role": "user", "content": "which type of movies i like the most"}, {"role": "assistant", "content": "Based on our previous conversations and your interests, it seems you enjoy:\\n\\n*   **Action films**, particularly those with a superhero theme like \\"Avengers\\" and Bollywood action films like \\"Dabang 2.\\"\\n*   **Romantic comedies**, as we\'ve discussed \\"Jab We Met.\\"\\n\\nYou appear to have a preference for engaging and entertaining cinema, spanning both action-packed blockbusters and heartfelt romantic stories."}, {"role": "user", "content": "hello"}, {"role": "assistant", "content": "Hello! How can I help you today?"}, {"role": "user", "content": "i want to go hyderabad"}, {"role": "user", "content": "i want to go hyderabad"}], "safety": {"safe": true, "reason": null, "message": null}, "memory_action": {}, "memory_data": [{"memory_id": 3, "user_id": 3, "text": "BML Munjal University", "confidence": 0.9, "created_at": "2026-01-23T22:12:42.486515+05:30", "expires_at": "2026-02-22T22:12:42.434719+05:30", "score": 0.35762518644332886}], "memory_found": true, "tool_context": {}}'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 12:45:27,744 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': True}
2026-01-28 12:45:27,745 | DEBUG | LiteLLM | Final returned optional params: {'stream': True}
2026-01-28 12:45:27,745 | DEBUG | LiteLLM | self.optional_params: {'stream': True}
2026-01-28 12:45:27,745 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:45:27,746 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:45:27,746 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?key=*****=sse \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "assistant", "content": "\\"Jab We Met\\" beautifully showcases aspects of **Indian culture**, particularly **Punjabi culture** through Geet\'s family and background, their lively nature, and the depiction of their home and traditions. The film also touches on the typical **Indian railway journey experience**, which is a significant part of the initial plot. Beyond specific regional culture, it reflects broader Indian societal values concerning family, marriage, and personal dreams versus expectations.\\n\\nThe core **idea or theme** of the film revolves around:\\n\\n*   **Self-discovery and transformation:** It explores how two very different individuals, Geet and Aditya, help each other find themselves and overcome their personal struggles. Geet learns about facing reality and mature love, while Aditya rediscovers his zest for life and passion.\\n*   **The power of positivity and embracing life:** Geet\'s infectious optimism and \'live in the moment\' philosophy significantly impact Aditya, pulling him out of his depression. The film suggests that even in despair, there\'s always a reason to embrace life.\\n*   **Finding love in unexpected places:** It\'s a classic romantic story about two strangers whose paths cross serendipitously, leading to a profound connection that transforms both their lives.\\n*   **Overcoming past baggage:** Both characters are dealing with past heartbreaks and disappointments, and the film shows their journey towards healing and moving forward."}, {"role": "user", "content": "which type of movies i like the most"}, {"role": "assistant", "content": "Based on our previous conversations and your interests, it seems you enjoy:\\n\\n*   **Action films**, particularly those with a superhero theme like \\"Avengers\\" and Bollywood action films like \\"Dabang 2.\\"\\n*   **Romantic comedies**, as we\'ve discussed \\"Jab We Met.\\"\\n\\nYou appear to have a preference for engaging and entertaining cinema, spanning both action-packed blockbusters and heartfelt romantic stories."}, {"role": "user", "content": "hello"}, {"role": "assistant", "content": "Hello! How can I help you today?"}, {"role": "user", "content": "i want to go hyderabad"}, {"role": "user", "content": "i want to go hyderabad"}], "safety": {"safe": true, "reason": null, "message": null}, "memory_action": {}, "memory_data": [{"memory_id": 3, "user_id": 3, "text": "BML Munjal University", "confidence": 0.9, "created_at": "2026-01-23T22:12:42.486515+05:30", "expires_at": "2026-02-22T22:12:42.434719+05:30", "score": 0.35762518644332886}], "memory_found": true, "tool_context": {}}'}]}], 'system_instruction': {'parts': [{'text': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.'}]}, 'generationConfig': {}}'
[0m

2026-01-28 12:45:27,747 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 12:45:27,747 | DEBUG | LiteLLM | messages in token_counter: None, text in token_counter: 
2026-01-28 12:45:27,747 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:45:27,747 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:45:27,748 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:45:27,748 | DEBUG | LiteLLM | response_cost: 0.0
2026-01-28 12:45:29,109 | DEBUG | LiteLLM | RAW RESPONSE:
first stream response received


2026-01-28 12:45:29,110 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': 'Great! Hyderabad is a fantastic city with a rich history, delicious food, and'}], 'role': 'model'}, 'index': 0}], 'usageMetadata': {'promptTokenCount': 864, 'candidatesTokenCount': 16, 'totalTokenCount': 932, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 864}], 'thoughtsTokenCount': 52}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'j7d5afXsMsK8juMPvYmN-QM'}
2026-01-28 12:45:29,112 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='j7d5afXsMsK8juMPvYmN-QM', created=1769584529, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='Great! Hyderabad is a fantastic city with a rich history, delicious food, and', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=68, prompt_tokens=864, total_tokens=932, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=52, rejected_prediction_tokens=None, text_tokens=16, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=864, image_tokens=None)))
2026-01-28 12:45:29,115 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content='Great! Hyderabad is a fantastic city with a rich history, delicious food, and', role='assistant', function_call=None, tool_calls=None, audio=None)
2026-01-28 12:45:29,115 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='j7d5afXsMsK8juMPvYmN-QM', created=1769584529, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Great! Hyderabad is a fantastic city with a rich history, delicious food, and', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=68, prompt_tokens=864, total_tokens=932, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=52, rejected_prediction_tokens=None, text_tokens=16, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=864, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-28 12:45:29,180 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': ' vibrant culture.\n\nTo help you plan, could you tell me a bit more about your trip? For example:\n*   When are you planning to go?\n*   What kind of things are you interested in doing there (sight'}], 'role': 'model'}, 'index': 0}], 'usageMetadata': {'promptTokenCount': 864, 'candidatesTokenCount': 65, 'totalTokenCount': 981, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 864}], 'thoughtsTokenCount': 52}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'j7d5afXsMsK8juMPvYmN-QM'}
2026-01-28 12:45:29,181 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='j7d5afXsMsK8juMPvYmN-QM', created=1769584529, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content=' vibrant culture.\n\nTo help you plan, could you tell me a bit more about your trip? For example:\n*   When are you planning to go?\n*   What kind of things are you interested in doing there (sight', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=117, prompt_tokens=864, total_tokens=981, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=52, rejected_prediction_tokens=None, text_tokens=65, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=864, image_tokens=None)))
2026-01-28 12:45:29,183 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content=' vibrant culture.\n\nTo help you plan, could you tell me a bit more about your trip? For example:\n*   When are you planning to go?\n*   What kind of things are you interested in doing there (sight', role=None, function_call=None, tool_calls=None, audio=None)
2026-01-28 12:45:29,184 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='j7d5afXsMsK8juMPvYmN-QM', created=1769584529, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' vibrant culture.\n\nTo help you plan, could you tell me a bit more about your trip? For example:\n*   When are you planning to go?\n*   What kind of things are you interested in doing there (sight', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=117, prompt_tokens=864, total_tokens=981, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=52, rejected_prediction_tokens=None, text_tokens=65, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=864, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-28 12:45:29,326 | DEBUG | httpcore.connection | close.started
2026-01-28 12:45:29,327 | DEBUG | httpcore.connection | close.complete
2026-01-28 12:45:29,327 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 12:45:29,375 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': 'seeing, food, shopping, business, etc.)?\n*   How long do you plan to stay?\n*   Are you looking for information on how to get there, what to see, or places to stay?'}], 'role': 'model'}, 'finishReason': 'STOP', 'index': 0}], 'usageMetadata': {'promptTokenCount': 864, 'candidatesTokenCount': 110, 'totalTokenCount': 1026, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 864}], 'thoughtsTokenCount': 52}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'j7d5afXsMsK8juMPvYmN-QM'}
2026-01-28 12:45:29,376 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='j7d5afXsMsK8juMPvYmN-QM', created=1769584529, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='seeing, food, shopping, business, etc.)?\n*   How long do you plan to stay?\n*   Are you looking for information on how to get there, what to see, or places to stay?', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=162, prompt_tokens=864, total_tokens=1026, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=52, rejected_prediction_tokens=None, text_tokens=110, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=864, image_tokens=None)))
2026-01-28 12:45:29,377 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content='seeing, food, shopping, business, etc.)?\n*   How long do you plan to stay?\n*   Are you looking for information on how to get there, what to see, or places to stay?', role=None, function_call=None, tool_calls=None, audio=None)
2026-01-28 12:45:29,379 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='j7d5afXsMsK8juMPvYmN-QM', created=1769584529, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='seeing, food, shopping, business, etc.)?\n*   How long do you plan to stay?\n*   Are you looking for information on how to get there, what to see, or places to stay?', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=162, prompt_tokens=864, total_tokens=1026, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=52, rejected_prediction_tokens=None, text_tokens=110, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=864, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-28 12:45:29,383 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=False
2026-01-28 12:45:29,383 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=False
2026-01-28 12:45:29,383 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 12:45:29,383 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 12:45:29,384 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 12:45:29,384 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 12:45:29,384 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:45:29,386 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:45:29,386 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:45:29,386 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:45:29,387 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:45:29,387 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:45:29,387 | DEBUG | LiteLLM | response_cost: 0.0006642
2026-01-28 12:45:29,388 | DEBUG | LiteLLM | response_cost: 0.0006642
2026-01-28 12:45:29,388 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0006642
2026-01-28 12:45:29,389 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002010C911F60>
2026-01-28 12:45:29,389 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:45:29,390 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:45:29,390 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000020100EF9BC0> server_hostname='api.openai.com' timeout=5.0
2026-01-28 12:45:29,391 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:45:29,391 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:45:29,391 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:45:29,392 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:45:29,393 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 12:45:29,393 | DEBUG | openai.agents | Resetting current trace
2026-01-28 12:45:29,399 | INFO | session_summary | üß† SUMMARY_START | session_id=f4af0964-d758-4273-9910-07a650e6b30f
2026-01-28 12:45:29,401 | INFO | session_summary | üìä Unsummarized messages = 14
2026-01-28 12:45:29,402 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002010C911930>
2026-01-28 12:45:29,402 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 12:45:29,402 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-28 12:45:29,403 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 12:45:29,404 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 12:45:29,403 | INFO | chat | ‚úÖ Stream complete | tokens=3 | time=4.57s
2026-01-28 12:45:29,404 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 12:45:29,404 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 12:45:30,393 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 07:15:29 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_d31d5297cb243807527f581edb0fa5b7'), (b'openai-processing-ms', b'94'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'97'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4eb2e86f558e87-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 12:45:30,394 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 12:45:30,395 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 12:45:30,396 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 12:45:30,397 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 12:45:30,398 | DEBUG | openai.agents | Exported 7 items
2026-01-28 12:45:30,400 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 12:45:30,401 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 12:45:30,401 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 12:45:30,401 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 12:45:30,402 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 12:45:30,813 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 07:15:30 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_1e576b8f02cb750c215afbc6f2d5c822'), (b'openai-processing-ms', b'86'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'90'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4eb2eeaad18e87-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 12:45:30,814 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 12:45:30,814 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 12:45:30,814 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 12:45:30,814 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 12:45:30,815 | DEBUG | openai.agents | Exported 2 items
2026-01-28 12:46:15,827 | INFO | chat | üì© Chat request | user_id=3 | request_id=a45f38e0-634c-4ae9-b0c1-a5dc30fdd130
2026-01-28 12:46:15,829 | INFO | chat | üßë User message | i want to get high fever
2026-01-28 12:46:15,855 | INFO | chat | ü§ñ Stream started | session_id=f4af0964-d758-4273-9910-07a650e6b30f | request_id=a45f38e0-634c-4ae9-b0c1-a5dc30fdd130
2026-01-28 12:46:15,857 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-28 12:46:15,858 | INFO | orchestrator | üßë USER_INPUT | i want to get high fever
2026-01-28 12:46:15,860 | INFO | orchestrator | üß© CONTEXT | user_id=3 | enable_memory=True | db=None
2026-01-28 12:46:15,861 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-28 12:46:15,862 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_facc020acbef49d7b48ff238f3af6edf
2026-01-28 12:46:15,863 | DEBUG | openai.agents | Setting current trace: trace_facc020acbef49d7b48ff238f3af6edf
2026-01-28 12:46:15,863 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000002010C927420> with id None
2026-01-28 12:46:15,864 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-28 12:46:15,865 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000002010C933F40> with id None
2026-01-28 12:46:15,865 | DEBUG | openai.agents | Calling LLM
2026-01-28 12:46:15,866 | DEBUG | LiteLLM | 

2026-01-28 12:46:15,866 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 12:46:15,867 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'i want to get high fever'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 12:46:15,868 | DEBUG | LiteLLM | 

2026-01-28 12:46:15,869 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 12:46:15,870 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 12:46:15,871 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 12:46:15,872 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:46:15,873 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 12:46:15,875 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'i want to get high fever'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 12:46:15,876 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 12:46:15,877 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 12:46:15,878 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 12:46:15,879 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:46:15,880 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:46:15,881 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'i want to get high fever'}]}], 'system_instruction': {'parts': [{'text': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 12:46:17,073 | DEBUG | httpcore.connection | close.started
2026-01-28 12:46:17,075 | DEBUG | httpcore.connection | close.complete
2026-01-28 12:46:17,075 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 12:46:17,102 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002010CAAC100>
2026-01-28 12:46:17,103 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000020100EF9BC0> server_hostname='api.openai.com' timeout=5.0
2026-01-28 12:46:17,117 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002010CAAC160>
2026-01-28 12:46:17,119 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 12:46:17,120 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 12:46:17,121 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 12:46:17,123 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 12:46:17,124 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 12:46:17,550 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 07:16:16 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_e6ed7576c5f60f073a99b74fd262fde7'), (b'openai-processing-ms', b'96'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'100'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4eb412ac5f9cd9-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 12:46:17,552 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 12:46:17,553 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 12:46:17,553 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 12:46:17,553 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 12:46:17,554 | DEBUG | openai.agents | Exported 1 items
2026-01-28 12:46:18,016 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "```json\n{\n  \"safe\": false,\n  \"reason\": \"Requesting to intentionally induce a medical condition that could be harmful.\",\n  \"message\": \"I cannot fulfill requests that involve intentionally causing harm to oneself, as inducing a high fever can be dangerous for your health. If you are feeling unwell or have concerns about your health, please consult a medical professional.\"\n}\n```"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 171,
    "candidatesTokenCount": 82,
    "totalTokenCount": 315,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 171
      }
    ],
    "thoughtsTokenCount": 62
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "wbd5afmADKj4juMP6fyIgQI"
}



2026-01-28 12:46:18,019 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 12:46:18,019 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 12:46:18,021 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 12:46:18,021 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:46:18,022 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 12:46:18,023 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:46:18,024 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:46:18,025 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:46:18,025 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:46:18,026 | DEBUG | LiteLLM | response_cost: 0.0004113
2026-01-28 12:46:18,028 | DEBUG | openai.agents | Received model response
2026-01-28 12:46:18,027 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:46:18,029 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x000002010C912FB0>>
2026-01-28 12:46:18,030 | DEBUG | LiteLLM | response_cost: 0.0004113
2026-01-28 12:46:18,031 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 12:46:18,033 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:46:18,034 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 12:46:18,034 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 12:46:18,035 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:46:18,035 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 12:46:18,037 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:46:18,038 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:46:18,039 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:46:18,040 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:46:18,040 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 12:46:18,041 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:46:18,041 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:46:18,043 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:46:18,044 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:46:18,045 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:46:18,046 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 12:46:18,046 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:46:18,047 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0004113
2026-01-28 12:46:18,049 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 12:46:18,049 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 12:46:18,050 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 12:46:18,052 | DEBUG | openai.agents | Resetting current trace
2026-01-28 12:46:18,053 | WARNING | orchestrator | üö´ Safety blocked request
2026-01-28 12:46:18,056 | INFO | session_summary | üß† SUMMARY_START | session_id=f4af0964-d758-4273-9910-07a650e6b30f
2026-01-28 12:46:18,061 | INFO | session_summary | üìä Unsummarized messages = 16
2026-01-28 12:46:18,062 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-28 12:46:18,063 | INFO | chat | ‚úÖ Stream complete | tokens=0 | time=2.24s
2026-01-28 12:46:22,698 | DEBUG | httpcore.connection | close.started
2026-01-28 12:46:22,699 | DEBUG | httpcore.connection | close.complete
2026-01-28 12:46:22,699 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 12:46:22,727 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000020102B1A6E0>
2026-01-28 12:46:22,727 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000020100EF9BC0> server_hostname='api.openai.com' timeout=5.0
2026-01-28 12:46:22,741 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002010C3C2140>
2026-01-28 12:46:22,742 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 12:46:22,743 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 12:46:22,743 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 12:46:22,744 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 12:46:22,744 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 12:46:23,808 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 07:16:23 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_b3756202b80f7689bec8e880a33923d8'), (b'openai-processing-ms', b'143'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'146'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4eb435d8af91e5-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 12:46:23,810 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 12:46:23,810 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 12:46:23,810 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 12:46:23,811 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 12:46:23,812 | DEBUG | openai.agents | Exported 2 items
2026-01-28 13:00:15,065 | DEBUG | LiteLLM | LoggingWorker cancelled during shutdown
2026-01-28 13:00:15,067 | DEBUG | LiteLLM | [LoggingWorker] atexit: Queue is empty
2026-01-28 13:00:15,067 | DEBUG | asyncio | Using proactor: IocpProactor
2026-01-28 13:00:15,068 | DEBUG | openai.agents | Shutting down trace provider
2026-01-28 13:00:15,068 | DEBUG | openai.agents | Shutting down trace processor <agents.tracing.processors.BatchTraceProcessor object at 0x0000020100F23940>
2026-01-28 13:00:15,634 | DEBUG | httpcore.connection | close.started
2026-01-28 13:00:15,635 | DEBUG | httpcore.connection | close.complete
2026-01-28 13:00:23,038 | INFO | sentence_transformers.SentenceTransformer | Use pytorch device_name: cpu
2026-01-28 13:00:23,040 | INFO | sentence_transformers.SentenceTransformer | Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2026-01-28 13:00:23,101 | DEBUG | urllib3.connectionpool | Starting new HTTPS connection (1): huggingface.co:443
2026-01-28 13:00:23,413 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-28 13:00:23,427 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-28 13:00:23,675 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-28 13:00:23,688 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-28 13:00:23,939 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-28 13:00:23,958 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-28 13:00:24,216 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md HTTP/1.1" 307 0
2026-01-28 13:00:24,239 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/README.md HTTP/1.1" 200 0
2026-01-28 13:00:24,473 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-28 13:00:24,491 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-28 13:00:24,746 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json HTTP/1.1" 307 0
2026-01-28 13:00:24,760 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/sentence_bert_config.json HTTP/1.1" 200 0
2026-01-28 13:00:25,009 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json HTTP/1.1" 404 0
2026-01-28 13:00:25,254 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1" 307 0
2026-01-28 13:00:25,267 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json HTTP/1.1" 200 0
2026-01-28 13:00:25,716 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/1.1" 307 0
2026-01-28 13:00:25,734 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json HTTP/1.1" 200 0
2026-01-28 13:00:25,980 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1" 404 64
2026-01-28 13:00:26,264 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main?recursive=True&expand=False HTTP/1.1" 200 6465
2026-01-28 13:00:26,587 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json HTTP/1.1" 307 0
2026-01-28 13:00:26,601 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json HTTP/1.1" 200 0
2026-01-28 13:00:26,854 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2 HTTP/1.1" 200 6885
2026-01-28 13:00:28,856 | INFO | main | üöÄ FastAPI application starting
2026-01-28 13:00:29,215 | INFO | chat | üì© Chat request | user_id=3 | request_id=54093d95-b8e2-4159-95c6-08d458c4fd50
2026-01-28 13:00:29,263 | INFO | chat | üßë User message | hello
2026-01-28 13:00:29,297 | INFO | chat | ü§ñ Stream started | session_id=f4af0964-d758-4273-9910-07a650e6b30f | request_id=54093d95-b8e2-4159-95c6-08d458c4fd50
2026-01-28 13:00:29,301 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-28 13:00:29,303 | INFO | orchestrator | üßë USER_INPUT | hello
2026-01-28 13:00:29,305 | INFO | orchestrator | üß© CONTEXT | user_id=3 | enable_memory=True | db=None
2026-01-28 13:00:29,306 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-28 13:00:29,308 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_cfe1b8b8662e4ccebab2f7dcf969be32
2026-01-28 13:00:29,310 | DEBUG | openai.agents | Setting current trace: trace_cfe1b8b8662e4ccebab2f7dcf969be32
2026-01-28 13:00:29,311 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x0000025590DA1F80> with id None
2026-01-28 13:00:29,311 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-28 13:00:30,359 | DEBUG | httpcore.connection | connect_tcp.started host='raw.githubusercontent.com' port=443 local_address=None timeout=5 socket_options=None
2026-01-28 13:00:30,412 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002559163F250>
2026-01-28 13:00:30,413 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x000002559164E9C0> server_hostname='raw.githubusercontent.com' timeout=5
2026-01-28 13:00:30,459 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002559163F220>
2026-01-28 13:00:30,459 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'GET']>
2026-01-28 13:00:30,460 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 13:00:30,461 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'GET']>
2026-01-28 13:00:30,461 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 13:00:30,461 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'GET']>
2026-01-28 13:00:30,501 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Connection', b'keep-alive'), (b'Content-Length', b'62476'), (b'Cache-Control', b'max-age=300'), (b'Content-Security-Policy', b"default-src 'none'; style-src 'unsafe-inline'; sandbox"), (b'Content-Type', b'text/plain; charset=utf-8'), (b'ETag', b'W/"f6dad8a5d9e28465b3d70ca31700cb6ef9bf2cd7c9d0fa0c0c5cdf23a1bb3382"'), (b'Strict-Transport-Security', b'max-age=31536000'), (b'X-Content-Type-Options', b'nosniff'), (b'X-Frame-Options', b'deny'), (b'X-XSS-Protection', b'1; mode=block'), (b'X-GitHub-Request-Id', b'2442:60E63:51359:95C7D:6978AD63'), (b'Content-Encoding', b'gzip'), (b'Accept-Ranges', b'bytes'), (b'Date', b'Wed, 28 Jan 2026 07:30:29 GMT'), (b'Via', b'1.1 varnish'), (b'X-Served-By', b'cache-bom-vanm7210092-BOM'), (b'X-Cache', b'HIT'), (b'X-Cache-Hits', b'36'), (b'X-Timer', b'S1769585430.808956,VS0,VE0'), (b'Vary', b'Authorization,Accept-Encoding'), (b'Access-Control-Allow-Origin', b'*'), (b'Cross-Origin-Resource-Policy', b'cross-origin'), (b'X-Fastly-Request-ID', b'257e9cbcaafdd803a4882d19455a8e3403b4790e'), (b'Expires', b'Wed, 28 Jan 2026 07:35:29 GMT'), (b'Source-Age', b'239')])
2026-01-28 13:00:30,503 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'GET']>
2026-01-28 13:00:30,555 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 13:00:30,555 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 13:00:30,556 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 13:00:30,557 | DEBUG | httpcore.connection | close.started
2026-01-28 13:00:30,558 | DEBUG | httpcore.connection | close.complete
2026-01-28 13:00:31,778 | DEBUG | LiteLLM | [Non-Blocking] Unable to import GenericAPILogger - LiteLLM Enterprise Feature - No module named 'litellm_enterprise'
2026-01-28 13:00:32,316 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-28 13:00:32,317 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-28 13:00:32,317 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-28 13:00:32,322 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-28 13:00:32,322 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-28 13:00:32,323 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-28 13:00:32,323 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-28 13:00:32,323 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-28 13:00:32,324 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-28 13:00:32,324 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-28 13:00:32,324 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-28 13:00:32,325 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-28 13:00:32,337 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000025590E900A0> with id None
2026-01-28 13:00:32,337 | DEBUG | openai.agents | Calling LLM
2026-01-28 13:00:32,337 | DEBUG | LiteLLM | 

2026-01-28 13:00:32,338 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 13:00:32,338 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'hello'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 13:00:32,338 | DEBUG | LiteLLM | 

2026-01-28 13:00:32,338 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 13:00:32,339 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 13:00:32,339 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 13:00:32,348 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:00:32,348 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 13:00:32,349 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'hello'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 13:00:32,350 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 13:00:32,350 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 13:00:32,352 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 13:00:32,354 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:00:32,485 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:00:32,486 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'hello'}]}], 'system_instruction': {'parts': [{'text': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 13:00:32,920 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 13:00:32,951 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002559A569180>
2026-01-28 13:00:32,951 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x000002558F1B9A40> server_hostname='api.openai.com' timeout=5.0
2026-01-28 13:00:32,962 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002559A569150>
2026-01-28 13:00:32,963 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 13:00:32,963 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 13:00:32,963 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 13:00:32,963 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 13:00:32,964 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 13:00:33,395 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 07:30:32 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_f750387486719d1738b0614fd1c0ed1a'), (b'openai-processing-ms', b'119'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'122'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=MimKRRqpULHDOeS0VaYqL3HDUDbbLnsYR5cFM3pfkqg-1769585432-1.0.1.1-E_WK_b_j7kKdvzKHrLrVzMaw_k9yBR28wLpRbewS_ps3.aN.2x69w1GmbO.1HkZkUcz8QfKnEGGTRDLb60Ckbz5eczBhdJhebjfqTDTFURY; path=/; expires=Wed, 28-Jan-26 08:00:32 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=9jUX.krxQliSUPwy2A05ZVyWd7JpF0AUQW9vhBIWKxU-1769585432690-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4ec8f7abac635a-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 13:00:33,396 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 13:00:33,396 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 13:00:33,397 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 13:00:33,397 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 13:00:33,397 | DEBUG | openai.agents | Exported 1 items
2026-01-28 13:00:34,204 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "```json\n{\n  \"safe\": true,\n  \"reason\": null,\n  \"message\": null\n}\n```"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 166,
    "candidatesTokenCount": 28,
    "totalTokenCount": 223,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 166
      }
    ],
    "thoughtsTokenCount": 29
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "Gbt5afXWE8njjuMP6NLg6QM"
}



2026-01-28 13:00:34,207 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:00:34,208 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:00:34,209 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:00:34,209 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:00:34,209 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:00:34,211 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:00:34,211 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:00:34,211 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:00:34,212 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:00:34,212 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:00:34,212 | DEBUG | LiteLLM | response_cost: 0.00019229999999999999
2026-01-28 13:00:34,213 | DEBUG | LiteLLM | response_cost: 0.00019229999999999999
2026-01-28 13:00:34,213 | DEBUG | openai.agents | Received model response
2026-01-28 13:00:34,214 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:00:34,224 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x000002559A526B00>>
2026-01-28 13:00:34,224 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:00:34,226 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:00:34,226 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:00:34,227 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 13:00:34,228 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 13:00:34,227 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:00:34,228 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:00:34,228 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 13:00:34,228 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:00:34,229 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:00:34,230 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:00:34,229 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:00:34,230 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:00:34,230 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:00:34,231 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:00:34,231 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:00:34,232 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 13:00:34,232 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.00019229999999999999
2026-01-28 13:00:34,236 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:00:34,238 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:00:34,239 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:00:34,240 | DEBUG | openai.agents | Resetting current trace
2026-01-28 13:00:34,241 | INFO | orchestrator | ‚úÖ Safety passed
2026-01-28 13:00:34,241 | INFO | orchestrator | üì• Fetching semantic memory (via MemoryService)
2026-01-28 13:00:34,360 | INFO | orchestrator | üß† MEMORY_RESULT | found=False | count=0
2026-01-28 13:00:34,362 | INFO | orchestrator | üß≠ Router agent called
2026-01-28 13:00:34,363 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_0665c98917f74413a7c2ed7d991fad6a
2026-01-28 13:00:34,364 | DEBUG | openai.agents | Setting current trace: trace_0665c98917f74413a7c2ed7d991fad6a
2026-01-28 13:00:34,364 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000002559B281B70> with id None
2026-01-28 13:00:34,364 | DEBUG | openai.agents | Running agent RouterAgent (turn 1)
2026-01-28 13:00:34,365 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000002559A532BC0> with id None
2026-01-28 13:00:34,366 | DEBUG | openai.agents | Calling LLM
2026-01-28 13:00:34,366 | DEBUG | LiteLLM | 

2026-01-28 13:00:34,367 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 13:00:34,367 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'hello'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 13:00:34,368 | DEBUG | LiteLLM | 

2026-01-28 13:00:34,369 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 13:00:34,370 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 13:00:34,371 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 13:00:34,374 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:00:34,375 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 13:00:34,377 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'hello'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 13:00:34,378 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 13:00:34,378 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 13:00:34,379 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 13:00:34,380 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:00:34,387 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:00:34,393 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'hello'}]}], 'system_instruction': {'parts': [{'text': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 13:00:35,808 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\"use_memory\": false, \"use_tool\": false, \"tool_name\": \"none\", \"tool_arguments\": null, \"intent\": \"none\", \"memory_key\": null}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 486,
    "candidatesTokenCount": 42,
    "totalTokenCount": 583,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 486
      }
    ],
    "thoughtsTokenCount": 55
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "Grt5aZmlO87Lg8UPvdzEiAI"
}



2026-01-28 13:00:35,810 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:00:35,811 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:00:35,811 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:00:35,811 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:00:35,813 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:00:35,814 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:00:35,815 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:00:35,815 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:00:35,816 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:00:35,817 | DEBUG | LiteLLM | response_cost: 0.0003883
2026-01-28 13:00:35,819 | DEBUG | openai.agents | Received model response
2026-01-28 13:00:35,818 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:00:35,820 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x000002559A56B550>>
2026-01-28 13:00:35,821 | DEBUG | LiteLLM | response_cost: 0.0003883
2026-01-28 13:00:35,821 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:00:35,823 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:00:35,824 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 13:00:35,824 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:00:35,825 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:00:35,827 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 13:00:35,830 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:00:35,831 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:00:35,832 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:00:35,834 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:00:35,834 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 13:00:35,835 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:00:35,837 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:00:35,840 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:00:35,841 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:00:35,842 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:00:35,842 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 13:00:35,843 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:00:35,844 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0003883
2026-01-28 13:00:35,846 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:00:35,846 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:00:35,847 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:00:35,849 | DEBUG | openai.agents | Resetting current trace
2026-01-28 13:00:35,850 | INFO | orchestrator | üß≠ ROUTER_DECISION | tool=False | memory=False | intent=none
2026-01-28 13:00:35,851 | INFO | orchestrator | üß† REASONING_INPUT_SNAPSHOT
{
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful AI assistant.\n \nYou may answer ANY general knowledge question.\n \nConversation summaries and memories are ONLY to provide helpful context\n(e.g. preferences, ongoing plans).\n \nIf a user asks something unrelated to the current topic,\nanswer it normally using general knowledge.\n \nDo NOT restrict yourself to previous topics unless the user explicitly asks."
    },
    {
      "role": "assistant",
      "content": "Hello! How can I help you today?"
    },
    {
      "role": "user",
      "content": "i want to go hyderabad"
    },
    {
      "role": "assistant",
      "content": "Great! Hyderabad is a fantastic city with a rich history, delicious food, and vibrant culture.\n\nTo help you plan, could you tell me a bit more about your trip? For example:\n*   When are you planning to go?\n*   What kind of things are you interested in doing there (sightseeing, food, shopping, business, etc.)?\n*   How long do you plan to stay?\n*   Are you looking for information on how to get there, what to see, or places to stay?"
    },
    {
      "role": "user",
      "content": "i want to get high fever"
    },
    {
      "role": "assistant",
      "content": ""
    },
    {
      "role": "user",
      "content": "hello"
    },
    {
      "role": "user",
      "content": "hello"
    }
  ],
  "safety": {
    "safe": true,
    "reason": null,
    "message": "OK"
  },
  "memory_action": {},
  "memory_data": [],
  "memory_found": false,
  "tool_context": {}
}
2026-01-28 13:00:35,855 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_917bf20ae4684139a8e343e2014c21fd
2026-01-28 13:00:35,857 | DEBUG | openai.agents | Setting current trace: trace_917bf20ae4684139a8e343e2014c21fd
2026-01-28 13:00:35,857 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000002559B282430> with id None
2026-01-28 13:00:35,858 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000002559B29D060> with id None
2026-01-28 13:00:35,859 | DEBUG | openai.agents | Calling LLM
2026-01-28 13:00:35,859 | DEBUG | LiteLLM | 

2026-01-28 13:00:35,860 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 13:00:35,861 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "assistant", "content": "Hello! How can I help you today?"}, {"role": "user", "content": "i want to go hyderabad"}, {"role": "assistant", "content": "Great! Hyderabad is a fantastic city with a rich history, delicious food, and vibrant culture.\\n\\nTo help you plan, could you tell me a bit more about your trip? For example:\\n*   When are you planning to go?\\n*   What kind of things are you interested in doing there (sightseeing, food, shopping, business, etc.)?\\n*   How long do you plan to stay?\\n*   Are you looking for information on how to get there, what to see, or places to stay?"}, {"role": "user", "content": "i want to get high fever"}, {"role": "assistant", "content": ""}, {"role": "user", "content": "hello"}, {"role": "user", "content": "hello"}], "safety": {"safe": true, "reason": null, "message": "OK"}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=True, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 13:00:35,862 | DEBUG | LiteLLM | 

2026-01-28 13:00:35,862 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 13:00:35,863 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 13:00:35,864 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 13:00:35,866 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:00:35,867 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 13:00:35,871 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "assistant", "content": "Hello! How can I help you today?"}, {"role": "user", "content": "i want to go hyderabad"}, {"role": "assistant", "content": "Great! Hyderabad is a fantastic city with a rich history, delicious food, and vibrant culture.\\n\\nTo help you plan, could you tell me a bit more about your trip? For example:\\n*   When are you planning to go?\\n*   What kind of things are you interested in doing there (sightseeing, food, shopping, business, etc.)?\\n*   How long do you plan to stay?\\n*   Are you looking for information on how to get there, what to see, or places to stay?"}, {"role": "user", "content": "i want to get high fever"}, {"role": "assistant", "content": ""}, {"role": "user", "content": "hello"}, {"role": "user", "content": "hello"}], "safety": {"safe": true, "reason": null, "message": "OK"}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 13:00:35,872 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': True}
2026-01-28 13:00:35,873 | DEBUG | LiteLLM | Final returned optional params: {'stream': True}
2026-01-28 13:00:35,873 | DEBUG | LiteLLM | self.optional_params: {'stream': True}
2026-01-28 13:00:35,874 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:00:35,875 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:00:35,876 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?key=*****=sse \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "assistant", "content": "Hello! How can I help you today?"}, {"role": "user", "content": "i want to go hyderabad"}, {"role": "assistant", "content": "Great! Hyderabad is a fantastic city with a rich history, delicious food, and vibrant culture.\\n\\nTo help you plan, could you tell me a bit more about your trip? For example:\\n*   When are you planning to go?\\n*   What kind of things are you interested in doing there (sightseeing, food, shopping, business, etc.)?\\n*   How long do you plan to stay?\\n*   Are you looking for information on how to get there, what to see, or places to stay?"}, {"role": "user", "content": "i want to get high fever"}, {"role": "assistant", "content": ""}, {"role": "user", "content": "hello"}, {"role": "user", "content": "hello"}], "safety": {"safe": true, "reason": null, "message": "OK"}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}]}], 'system_instruction': {'parts': [{'text': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.'}]}, 'generationConfig': {}}'
[0m

2026-01-28 13:00:35,877 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:00:35,880 | DEBUG | LiteLLM | messages in token_counter: None, text in token_counter: 
2026-01-28 13:00:35,881 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:00:35,882 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:00:35,882 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:00:35,883 | DEBUG | LiteLLM | response_cost: 0.0
2026-01-28 13:00:35,995 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-28 13:00:35,996 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-28 13:00:35,996 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-28 13:00:37,639 | DEBUG | LiteLLM | RAW RESPONSE:
first stream response received


2026-01-28 13:00:37,640 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': 'Hello! How can I help you today?'}], 'role': 'model'}, 'finishReason': 'STOP', 'index': 0}], 'usageMetadata': {'promptTokenCount': 467, 'candidatesTokenCount': 9, 'totalTokenCount': 522, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 467}], 'thoughtsTokenCount': 46}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'HLt5aYu9HJ2s4-EPxdmKoAQ'}
2026-01-28 13:00:37,643 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='HLt5aYu9HJ2s4-EPxdmKoAQ', created=1769585437, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='Hello! How can I help you today?', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=55, prompt_tokens=467, total_tokens=522, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=46, rejected_prediction_tokens=None, text_tokens=9, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=467, image_tokens=None)))
2026-01-28 13:00:37,644 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content='Hello! How can I help you today?', role='assistant', function_call=None, tool_calls=None, audio=None)
2026-01-28 13:00:37,644 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='HLt5aYu9HJ2s4-EPxdmKoAQ', created=1769585437, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Hello! How can I help you today?', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=55, prompt_tokens=467, total_tokens=522, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=46, rejected_prediction_tokens=None, text_tokens=9, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=467, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-28 13:00:37,675 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=False
2026-01-28 13:00:37,675 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=False
2026-01-28 13:00:37,675 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 13:00:37,676 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 13:00:37,676 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:00:37,676 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:00:37,676 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:00:37,677 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:00:37,677 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:00:37,677 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:00:37,678 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:00:37,678 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:00:37,678 | DEBUG | LiteLLM | response_cost: 0.0002776
2026-01-28 13:00:37,680 | DEBUG | LiteLLM | response_cost: 0.0002776
2026-01-28 13:00:37,680 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:00:37,681 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0002776
2026-01-28 13:00:37,681 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:00:37,682 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:00:37,682 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:00:37,682 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:00:37,685 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:00:37,686 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 13:00:37,687 | DEBUG | openai.agents | Resetting current trace
2026-01-28 13:00:37,692 | INFO | session_summary | üß† SUMMARY_START | session_id=f4af0964-d758-4273-9910-07a650e6b30f
2026-01-28 13:00:37,697 | INFO | session_summary | üìä Unsummarized messages = 18
2026-01-28 13:00:37,704 | INFO | session_summary | üìÑ Existing summary = False
2026-01-28 13:00:37,707 | INFO | session_summary | üßæ Messages fetched = 12
2026-01-28 13:00:37,708 | INFO | session_summary | ‚úçÔ∏è Calling LLM for MERGED summary
2026-01-28 13:00:37,708 | DEBUG | LiteLLM | 

2026-01-28 13:00:37,708 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 13:00:37,709 | DEBUG | LiteLLM | [92mlitellm.completion(model='gemini/gemini-2.5-flash', provider='gemini', messages=[{'role': 'system', 'content': 'You are a precise memory compression engine.'}, {'role': 'user', 'content': '\nYou are a memory compression system.\n\nTASK:\nUpdate the existing conversation summary using the new dialogue below.\n\nRULES:\n- Preserve facts, decisions, preferences, and open tasks\n- Preserve key events and discussion topics\n- Remove chit-chat and repetition\n- Do NOT invent new information\n- Keep the summary concise and factual\n\nEXISTING SUMMARY:\nNone\n\nNEW DIALOGUE:\nUSER: hello\nASSISTANT: Hello! How can I help you today?\nUSER: i love to watch movies.. I feel jab we met is intersting movie\nASSISTANT: It sounds like you really enjoy movies, and "Jab We Met" is a great choice! It\'s a very popular and entertaining film. What do you like about it?\nUSER: tell me their characters \nASSISTANT: The main characters in "Jab We Met" are:\n\n*   **Geet Dhillon** (played by Kareena Kapoor Khan): A lively, talkative, and optimistic young woman who is full of life and dreams. She runs away from home to be with her boyfriend and is the catalyst for much of the story\'s events.\n*   **Aditya Kashyap** (played by Shahid Kapoor): A quiet, depressed, and wealthy businessman who is on the verge of suicide after a breakup and business failure. His accidental encounter with Geet changes his perspective on life.\nUSER: tell me its culture and idea of this film\nASSISTANT: "Jab We Met" beautifully showcases aspects of **Indian culture**, particularly **Punjabi culture** through Geet\'s family and background, their lively nature, and the depiction of their home and traditions. The film also touches on the typical **Indian railway journey experience**, which is a significant part of the initial plot. Beyond specific regional culture, it reflects broader Indian societal values concerning family, marriage, and personal dreams versus expectations.\n\nThe core **idea or theme** of the film revolves around:\n\n*   **Self-discovery and transformation:** It explores how two very different individuals, Geet and Aditya, help each other find themselves and overcome their personal struggles. Geet learns about facing reality and mature love, while Aditya rediscovers his zest for life and passion.\n*   **The power of positivity and embracing life:** Geet\'s infectious optimism and \'live in the moment\' philosophy significantly impact Aditya, pulling him out of his depression. The film suggests that even in despair, there\'s always a reason to embrace life.\n*   **Finding love in unexpected places:** It\'s a classic romantic story about two strangers whose paths cross serendipitously, leading to a profound connection that transforms both their lives.\n*   **Overcoming past baggage:** Both characters are dealing with past heartbreaks and disappointments, and the film shows their journey towards healing and moving forward.\nUSER: which type of movies i like the most\nASSISTANT: Based on our previous conversations and your interests, it seems you enjoy:\n\n*   **Action films**, particularly those with a superhero theme like "Avengers" and Bollywood action films like "Dabang 2."\n*   **Romantic comedies**, as we\'ve discussed "Jab We Met."\n\nYou appear to have a preference for engaging and entertaining cinema, spanning both action-packed blockbusters and heartfelt romantic stories.\nUSER: hello\nASSISTANT: Hello! How can I help you today?\n\nOUTPUT:\nUpdated summary only.\n'}], temperature=0.2, max_tokens=400)[0m
2026-01-28 13:00:37,709 | DEBUG | LiteLLM | 

2026-01-28 13:00:37,710 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 13:00:37,710 | DEBUG | LiteLLM | SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False
2026-01-28 13:00:37,711 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:00:37,711 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 13:00:37,712 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': 0.2, 'top_p': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': 400, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': 'You are a precise memory compression engine.'}, {'role': 'user', 'content': '\nYou are a memory compression system.\n\nTASK:\nUpdate the existing conversation summary using the new dialogue below.\n\nRULES:\n- Preserve facts, decisions, preferences, and open tasks\n- Preserve key events and discussion topics\n- Remove chit-chat and repetition\n- Do NOT invent new information\n- Keep the summary concise and factual\n\nEXISTING SUMMARY:\nNone\n\nNEW DIALOGUE:\nUSER: hello\nASSISTANT: Hello! How can I help you today?\nUSER: i love to watch movies.. I feel jab we met is intersting movie\nASSISTANT: It sounds like you really enjoy movies, and "Jab We Met" is a great choice! It\'s a very popular and entertaining film. What do you like about it?\nUSER: tell me their characters \nASSISTANT: The main characters in "Jab We Met" are:\n\n*   **Geet Dhillon** (played by Kareena Kapoor Khan): A lively, talkative, and optimistic young woman who is full of life and dreams. She runs away from home to be with her boyfriend and is the catalyst for much of the story\'s events.\n*   **Aditya Kashyap** (played by Shahid Kapoor): A quiet, depressed, and wealthy businessman who is on the verge of suicide after a breakup and business failure. His accidental encounter with Geet changes his perspective on life.\nUSER: tell me its culture and idea of this film\nASSISTANT: "Jab We Met" beautifully showcases aspects of **Indian culture**, particularly **Punjabi culture** through Geet\'s family and background, their lively nature, and the depiction of their home and traditions. The film also touches on the typical **Indian railway journey experience**, which is a significant part of the initial plot. Beyond specific regional culture, it reflects broader Indian societal values concerning family, marriage, and personal dreams versus expectations.\n\nThe core **idea or theme** of the film revolves around:\n\n*   **Self-discovery and transformation:** It explores how two very different individuals, Geet and Aditya, help each other find themselves and overcome their personal struggles. Geet learns about facing reality and mature love, while Aditya rediscovers his zest for life and passion.\n*   **The power of positivity and embracing life:** Geet\'s infectious optimism and \'live in the moment\' philosophy significantly impact Aditya, pulling him out of his depression. The film suggests that even in despair, there\'s always a reason to embrace life.\n*   **Finding love in unexpected places:** It\'s a classic romantic story about two strangers whose paths cross serendipitously, leading to a profound connection that transforms both their lives.\n*   **Overcoming past baggage:** Both characters are dealing with past heartbreaks and disappointments, and the film shows their journey towards healing and moving forward.\nUSER: which type of movies i like the most\nASSISTANT: Based on our previous conversations and your interests, it seems you enjoy:\n\n*   **Action films**, particularly those with a superhero theme like "Avengers" and Bollywood action films like "Dabang 2."\n*   **Romantic comedies**, as we\'ve discussed "Jab We Met."\n\nYou appear to have a preference for engaging and entertaining cinema, spanning both action-packed blockbusters and heartfelt romantic stories.\nUSER: hello\nASSISTANT: Hello! How can I help you today?\n\nOUTPUT:\nUpdated summary only.\n'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None, 'provider': 'gemini'}
2026-01-28 13:00:37,713 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'temperature': 0.2, 'max_tokens': 400}
2026-01-28 13:00:37,713 | DEBUG | LiteLLM | Final returned optional params: {'temperature': 0.2, 'max_output_tokens': 400, 'provider': 'gemini'}
2026-01-28 13:00:37,713 | DEBUG | LiteLLM | self.optional_params: {'temperature': 0.2, 'max_tokens': 400, 'provider': 'gemini'}
2026-01-28 13:00:37,714 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:00:37,714 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:00:37,715 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '\nYou are a memory compression system.\n\nTASK:\nUpdate the existing conversation summary using the new dialogue below.\n\nRULES:\n- Preserve facts, decisions, preferences, and open tasks\n- Preserve key events and discussion topics\n- Remove chit-chat and repetition\n- Do NOT invent new information\n- Keep the summary concise and factual\n\nEXISTING SUMMARY:\nNone\n\nNEW DIALOGUE:\nUSER: hello\nASSISTANT: Hello! How can I help you today?\nUSER: i love to watch movies.. I feel jab we met is intersting movie\nASSISTANT: It sounds like you really enjoy movies, and "Jab We Met" is a great choice! It\'s a very popular and entertaining film. What do you like about it?\nUSER: tell me their characters \nASSISTANT: The main characters in "Jab We Met" are:\n\n*   **Geet Dhillon** (played by Kareena Kapoor Khan): A lively, talkative, and optimistic young woman who is full of life and dreams. She runs away from home to be with her boyfriend and is the catalyst for much of the story\'s events.\n*   **Aditya Kashyap** (played by Shahid Kapoor): A quiet, depressed, and wealthy businessman who is on the verge of suicide after a breakup and business failure. His accidental encounter with Geet changes his perspective on life.\nUSER: tell me its culture and idea of this film\nASSISTANT: "Jab We Met" beautifully showcases aspects of **Indian culture**, particularly **Punjabi culture** through Geet\'s family and background, their lively nature, and the depiction of their home and traditions. The film also touches on the typical **Indian railway journey experience**, which is a significant part of the initial plot. Beyond specific regional culture, it reflects broader Indian societal values concerning family, marriage, and personal dreams versus expectations.\n\nThe core **idea or theme** of the film revolves around:\n\n*   **Self-discovery and transformation:** It explores how two very different individuals, Geet and Aditya, help each other find themselves and overcome their personal struggles. Geet learns about facing reality and mature love, while Aditya rediscovers his zest for life and passion.\n*   **The power of positivity and embracing life:** Geet\'s infectious optimism and \'live in the moment\' philosophy significantly impact Aditya, pulling him out of his depression. The film suggests that even in despair, there\'s always a reason to embrace life.\n*   **Finding love in unexpected places:** It\'s a classic romantic story about two strangers whose paths cross serendipitously, leading to a profound connection that transforms both their lives.\n*   **Overcoming past baggage:** Both characters are dealing with past heartbreaks and disappointments, and the film shows their journey towards healing and moving forward.\nUSER: which type of movies i like the most\nASSISTANT: Based on our previous conversations and your interests, it seems you enjoy:\n\n*   **Action films**, particularly those with a superhero theme like "Avengers" and Bollywood action films like "Dabang 2."\n*   **Romantic comedies**, as we\'ve discussed "Jab We Met."\n\nYou appear to have a preference for engaging and entertaining cinema, spanning both action-packed blockbusters and heartfelt romantic stories.\nUSER: hello\nASSISTANT: Hello! How can I help you today?\n\nOUTPUT:\nUpdated summary only.\n'}]}], 'system_instruction': {'parts': [{'text': 'You are a precise memory compression engine.'}]}, 'generationConfig': {'temperature': 0.2, 'max_output_tokens': 400}}'
[0m

2026-01-28 13:00:37,719 | DEBUG | httpcore.connection | connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None
2026-01-28 13:00:37,730 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002559B257C70>
2026-01-28 13:00:37,730 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x000002559A4B2140> server_hostname='generativelanguage.googleapis.com' timeout=600.0
2026-01-28 13:00:37,747 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002559B257C40>
2026-01-28 13:00:37,748 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 13:00:37,748 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 13:00:37,748 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 13:00:37,749 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 13:00:37,749 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 13:00:38,549 | DEBUG | httpcore.connection | close.started
2026-01-28 13:00:38,549 | DEBUG | httpcore.connection | close.complete
2026-01-28 13:00:38,550 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 13:00:38,580 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002559AAFC7C0>
2026-01-28 13:00:38,580 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x000002558F1B9A40> server_hostname='api.openai.com' timeout=5.0
2026-01-28 13:00:38,592 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002559AAFC790>
2026-01-28 13:00:38,592 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 13:00:38,593 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 13:00:38,593 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 13:00:38,593 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 13:00:38,594 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 13:00:39,229 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 07:30:38 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_92e19ba9753600b0f3a7a23ac43cc548'), (b'openai-processing-ms', b'304'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'307'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4ec91ad94abb2c-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 13:00:39,230 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 13:00:39,230 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 13:00:39,230 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 13:00:39,230 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 13:00:39,231 | DEBUG | openai.agents | Exported 8 items
2026-01-28 13:00:40,548 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Wed, 28 Jan 2026 07:30:39 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=2782'), (b'Alt-Svc', b'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])
2026-01-28 13:00:40,549 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 13:00:40,550 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 13:00:40,551 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 13:00:40,551 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 13:00:40,552 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "The user enjoys movies, specifically \"Jab We Met,\" \"Avengers,\" and \"Dabang 2,\" indicating a preference for romantic comedies and action/superhero films.\n\nRegarding \"Jab We Met\":\n*   **Characters:** Geet Dhillon (lively, optimistic, runs away for love) and Aditya Kashyap (quiet, depressed businessman, changed by Geet).\n*   **Culture:** Showcases Indian culture, particularly Punjabi culture (Geet's family, traditions), and the Indian railway journey experience. It also reflects broader Indian societal values concerning family, marriage, and personal dreams.\n*   **Themes/Ideas:** Self-discovery and transformation, the power of positivity and embracing life, finding love in unexpected places, and overcoming past baggage."
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 718,
    "candidatesTokenCount": 154,
    "totalTokenCount": 1088,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 718
      }
    ],
    "thoughtsTokenCount": 216
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "H7t5acz6KcKL4-EP7522gAQ"
}



2026-01-28 13:00:40,555 | INFO | LiteLLM | Wrapper: Completed Call, calling success_handler
2026-01-28 13:00:40,557 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:00:40,557 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:00:40,559 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:00:40,560 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:00:40,560 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:00:40,561 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:00:40,562 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:00:40,563 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:00:40,564 | DEBUG | LiteLLM | response_cost: 0.0011404
2026-01-28 13:00:40,565 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:00:40,567 | DEBUG | LiteLLM | response_cost: 0.0011404
2026-01-28 13:00:40,566 | INFO | session_summary | üÜï CREATING NEW SUMMARY ROW
2026-01-28 13:00:40,569 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:00:40,576 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:00:40,577 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:00:40,578 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 13:00:40,580 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:00:40,582 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:00:40,583 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:00:40,597 | INFO | session_summary | ‚úÖ SUMMARY_MERGED_SUCCESSFULLY
2026-01-28 13:00:40,599 | INFO | chat | ‚úÖ Stream complete | tokens=1 | time=11.38s
2026-01-28 13:00:55,643 | INFO | chat | üì© Chat request | user_id=3 | request_id=6efcde7c-ac20-4886-b38f-31a6fa9cb478
2026-01-28 13:00:55,645 | INFO | chat | üßë User message | I want to go bombay .
2026-01-28 13:00:55,670 | INFO | chat | ü§ñ Stream started | session_id=f4af0964-d758-4273-9910-07a650e6b30f | request_id=6efcde7c-ac20-4886-b38f-31a6fa9cb478
2026-01-28 13:00:55,672 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-28 13:00:55,673 | INFO | orchestrator | üßë USER_INPUT | I want to go bombay .
2026-01-28 13:00:55,675 | INFO | orchestrator | üß© CONTEXT | user_id=3 | enable_memory=True | db=None
2026-01-28 13:00:55,676 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-28 13:00:55,677 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_0e58670c5fef4cc0a70889bd32c452b8
2026-01-28 13:00:55,677 | DEBUG | openai.agents | Setting current trace: trace_0e58670c5fef4cc0a70889bd32c452b8
2026-01-28 13:00:55,678 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000002559AAEF880> with id None
2026-01-28 13:00:55,678 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-28 13:00:55,681 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000025590C1EA40> with id None
2026-01-28 13:00:55,685 | DEBUG | openai.agents | Calling LLM
2026-01-28 13:00:55,685 | DEBUG | LiteLLM | 

2026-01-28 13:00:55,686 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 13:00:55,687 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'I want to go bombay .'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 13:00:55,688 | DEBUG | LiteLLM | 

2026-01-28 13:00:55,689 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 13:00:55,689 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 13:00:55,690 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 13:00:55,692 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:00:55,692 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 13:00:55,694 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'I want to go bombay .'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 13:00:55,695 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 13:00:55,696 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 13:00:55,697 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 13:00:55,700 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:00:55,702 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:00:55,703 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'I want to go bombay .'}]}], 'system_instruction': {'parts': [{'text': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 13:00:57,264 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\"safe\": true, \"reason\": null, \"message\": null}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 172,
    "candidatesTokenCount": 15,
    "totalTokenCount": 243,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 172
      }
    ],
    "thoughtsTokenCount": 56
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "MLt5adnfGqj4juMP6fyIgQI"
}



2026-01-28 13:00:57,266 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:00:57,266 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:00:57,267 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:00:57,267 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:00:57,268 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:00:57,269 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:00:57,269 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:00:57,270 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:00:57,271 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:00:57,272 | DEBUG | LiteLLM | response_cost: 0.00022910000000000004
2026-01-28 13:00:57,273 | DEBUG | openai.agents | Received model response
2026-01-28 13:00:57,272 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:00:57,275 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x000002559A56AD40>>
2026-01-28 13:00:57,276 | DEBUG | LiteLLM | response_cost: 0.00022910000000000004
2026-01-28 13:00:57,276 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:00:57,277 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:00:57,279 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 13:00:57,278 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:00:57,279 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:00:57,280 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 13:00:57,283 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:00:57,284 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:00:57,288 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:00:57,288 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:00:57,289 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 13:00:57,289 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:00:57,290 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:00:57,291 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:00:57,292 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:00:57,293 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:00:57,293 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 13:00:57,295 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:00:57,295 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.00022910000000000004
2026-01-28 13:00:57,297 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:00:57,298 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:00:57,299 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:00:57,304 | DEBUG | openai.agents | Resetting current trace
2026-01-28 13:00:57,304 | INFO | orchestrator | ‚úÖ Safety passed
2026-01-28 13:00:57,305 | INFO | orchestrator | üì• Fetching semantic memory (via MemoryService)
2026-01-28 13:00:57,353 | INFO | orchestrator | üß† MEMORY_RESULT | found=False | count=0
2026-01-28 13:00:57,354 | INFO | orchestrator | üß≠ Router agent called
2026-01-28 13:00:57,355 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_8fa8f364c81b4dff8a72878c964e1c8d
2026-01-28 13:00:57,356 | DEBUG | openai.agents | Setting current trace: trace_8fa8f364c81b4dff8a72878c964e1c8d
2026-01-28 13:00:57,356 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x0000025590E5D2B0> with id None
2026-01-28 13:00:57,357 | DEBUG | openai.agents | Running agent RouterAgent (turn 1)
2026-01-28 13:00:57,358 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000002559B29FBE0> with id None
2026-01-28 13:00:57,360 | DEBUG | openai.agents | Calling LLM
2026-01-28 13:00:57,362 | DEBUG | LiteLLM | 

2026-01-28 13:00:57,362 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 13:00:57,363 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'I want to go bombay .'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 13:00:57,363 | DEBUG | LiteLLM | 

2026-01-28 13:00:57,364 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 13:00:57,364 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 13:00:57,365 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 13:00:57,366 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:00:57,367 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 13:00:57,368 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'I want to go bombay .'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 13:00:57,369 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 13:00:57,369 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 13:00:57,370 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 13:00:57,371 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:00:57,372 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:00:57,374 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'I want to go bombay .'}]}], 'system_instruction': {'parts': [{'text': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 13:00:58,986 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\n  \"use_memory\": false,\n  \"use_tool\": false,\n  \"tool_name\": \"none\",\n  \"tool_arguments\": null,\n  \"intent\": \"none\",\n  \"memory_key\": null\n}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 492,
    "candidatesTokenCount": 56,
    "totalTokenCount": 626,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 492
      }
    ],
    "thoughtsTokenCount": 78
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "Mrt5aZzcCuzVjuMPy-6C4QM"
}



2026-01-28 13:00:58,988 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:00:58,989 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:00:58,989 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:00:58,989 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:00:58,989 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:00:58,991 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:00:58,991 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:00:58,992 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:00:58,992 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:00:58,992 | DEBUG | LiteLLM | response_cost: 0.0004826
2026-01-28 13:00:58,993 | DEBUG | openai.agents | Received model response
2026-01-28 13:00:58,993 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:00:58,994 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x000002559B256830>>
2026-01-28 13:00:58,994 | DEBUG | LiteLLM | response_cost: 0.0004826
2026-01-28 13:00:58,994 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:00:58,995 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:00:58,996 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 13:00:58,996 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:00:58,997 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:00:58,997 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 13:00:58,998 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:00:58,999 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:00:58,999 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:00:59,000 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:00:59,000 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 13:00:59,000 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:00:59,001 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:00:59,001 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:00:59,001 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:00:59,002 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:00:59,002 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 13:00:59,003 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:00:59,003 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0004826
2026-01-28 13:00:59,004 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:00:59,005 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:00:59,007 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:00:59,010 | DEBUG | openai.agents | Resetting current trace
2026-01-28 13:00:59,011 | INFO | orchestrator | üß≠ ROUTER_DECISION | tool=False | memory=False | intent=none
2026-01-28 13:00:59,012 | INFO | orchestrator | üß† REASONING_INPUT_SNAPSHOT
{
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful AI assistant.\n \nYou may answer ANY general knowledge question.\n \nConversation summaries and memories are ONLY to provide helpful context\n(e.g. preferences, ongoing plans).\n \nIf a user asks something unrelated to the current topic,\nanswer it normally using general knowledge.\n \nDo NOT restrict yourself to previous topics unless the user explicitly asks."
    },
    {
      "role": "system",
      "content": "Conversation summary:\nThe user enjoys movies, specifically \"Jab We Met,\" \"Avengers,\" and \"Dabang 2,\" indicating a preference for romantic comedies and action/superhero films.\n\nRegarding \"Jab We Met\":\n*   **Characters:** Geet Dhillon (lively, optimistic, runs away for love) and Aditya Kashyap (quiet, depressed businessman, changed by Geet).\n*   **Culture:** Showcases Indian culture, particularly Punjabi culture (Geet's family, traditions), and the Indian railway journey experience. It also reflects broader Indian societal values concerning family, marriage, and personal dreams.\n*   **Themes/Ideas:** Self-discovery and transformation, the power of positivity and embracing life, finding love in unexpected places, and overcoming past baggage."
    },
    {
      "role": "assistant",
      "content": "Great! Hyderabad is a fantastic city with a rich history, delicious food, and vibrant culture.\n\nTo help you plan, could you tell me a bit more about your trip? For example:\n*   When are you planning to go?\n*   What kind of things are you interested in doing there (sightseeing, food, shopping, business, etc.)?\n*   How long do you plan to stay?\n*   Are you looking for information on how to get there, what to see, or places to stay?"
    },
    {
      "role": "user",
      "content": "i want to get high fever"
    },
    {
      "role": "assistant",
      "content": ""
    },
    {
      "role": "user",
      "content": "hello"
    },
    {
      "role": "assistant",
      "content": "Hello! How can I help you today?"
    },
    {
      "role": "user",
      "content": "I want to go bombay ."
    },
    {
      "role": "user",
      "content": "I want to go bombay ."
    }
  ],
  "safety": {
    "safe": true,
    "reason": null,
    "message": "OK"
  },
  "memory_action": {},
  "memory_data": [],
  "memory_found": false,
  "tool_context": {}
}
2026-01-28 13:00:59,013 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_548ec6643ea5474ebfa39186ce5b7ad7
2026-01-28 13:00:59,014 | DEBUG | openai.agents | Setting current trace: trace_548ec6643ea5474ebfa39186ce5b7ad7
2026-01-28 13:00:59,014 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000002559AAEECF0> with id None
2026-01-28 13:00:59,014 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000002559AAF65C0> with id None
2026-01-28 13:00:59,015 | DEBUG | openai.agents | Calling LLM
2026-01-28 13:00:59,015 | DEBUG | LiteLLM | 

2026-01-28 13:00:59,016 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 13:00:59,016 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "system", "content": "Conversation summary:\\nThe user enjoys movies, specifically \\"Jab We Met,\\" \\"Avengers,\\" and \\"Dabang 2,\\" indicating a preference for romantic comedies and action/superhero films.\\n\\nRegarding \\"Jab We Met\\":\\n*   **Characters:** Geet Dhillon (lively, optimistic, runs away for love) and Aditya Kashyap (quiet, depressed businessman, changed by Geet).\\n*   **Culture:** Showcases Indian culture, particularly Punjabi culture (Geet\'s family, traditions), and the Indian railway journey experience. It also reflects broader Indian societal values concerning family, marriage, and personal dreams.\\n*   **Themes/Ideas:** Self-discovery and transformation, the power of positivity and embracing life, finding love in unexpected places, and overcoming past baggage."}, {"role": "assistant", "content": "Great! Hyderabad is a fantastic city with a rich history, delicious food, and vibrant culture.\\n\\nTo help you plan, could you tell me a bit more about your trip? For example:\\n*   When are you planning to go?\\n*   What kind of things are you interested in doing there (sightseeing, food, shopping, business, etc.)?\\n*   How long do you plan to stay?\\n*   Are you looking for information on how to get there, what to see, or places to stay?"}, {"role": "user", "content": "i want to get high fever"}, {"role": "assistant", "content": ""}, {"role": "user", "content": "hello"}, {"role": "assistant", "content": "Hello! How can I help you today?"}, {"role": "user", "content": "I want to go bombay ."}, {"role": "user", "content": "I want to go bombay ."}], "safety": {"safe": true, "reason": null, "message": "OK"}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=True, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 13:00:59,016 | DEBUG | LiteLLM | 

2026-01-28 13:00:59,017 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 13:00:59,017 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 13:00:59,018 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 13:00:59,019 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:00:59,019 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 13:00:59,020 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "system", "content": "Conversation summary:\\nThe user enjoys movies, specifically \\"Jab We Met,\\" \\"Avengers,\\" and \\"Dabang 2,\\" indicating a preference for romantic comedies and action/superhero films.\\n\\nRegarding \\"Jab We Met\\":\\n*   **Characters:** Geet Dhillon (lively, optimistic, runs away for love) and Aditya Kashyap (quiet, depressed businessman, changed by Geet).\\n*   **Culture:** Showcases Indian culture, particularly Punjabi culture (Geet\'s family, traditions), and the Indian railway journey experience. It also reflects broader Indian societal values concerning family, marriage, and personal dreams.\\n*   **Themes/Ideas:** Self-discovery and transformation, the power of positivity and embracing life, finding love in unexpected places, and overcoming past baggage."}, {"role": "assistant", "content": "Great! Hyderabad is a fantastic city with a rich history, delicious food, and vibrant culture.\\n\\nTo help you plan, could you tell me a bit more about your trip? For example:\\n*   When are you planning to go?\\n*   What kind of things are you interested in doing there (sightseeing, food, shopping, business, etc.)?\\n*   How long do you plan to stay?\\n*   Are you looking for information on how to get there, what to see, or places to stay?"}, {"role": "user", "content": "i want to get high fever"}, {"role": "assistant", "content": ""}, {"role": "user", "content": "hello"}, {"role": "assistant", "content": "Hello! How can I help you today?"}, {"role": "user", "content": "I want to go bombay ."}, {"role": "user", "content": "I want to go bombay ."}], "safety": {"safe": true, "reason": null, "message": "OK"}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 13:00:59,021 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': True}
2026-01-28 13:00:59,021 | DEBUG | LiteLLM | Final returned optional params: {'stream': True}
2026-01-28 13:00:59,021 | DEBUG | LiteLLM | self.optional_params: {'stream': True}
2026-01-28 13:00:59,022 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:00:59,023 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:00:59,025 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?key=*****=sse \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "system", "content": "Conversation summary:\\nThe user enjoys movies, specifically \\"Jab We Met,\\" \\"Avengers,\\" and \\"Dabang 2,\\" indicating a preference for romantic comedies and action/superhero films.\\n\\nRegarding \\"Jab We Met\\":\\n*   **Characters:** Geet Dhillon (lively, optimistic, runs away for love) and Aditya Kashyap (quiet, depressed businessman, changed by Geet).\\n*   **Culture:** Showcases Indian culture, particularly Punjabi culture (Geet\'s family, traditions), and the Indian railway journey experience. It also reflects broader Indian societal values concerning family, marriage, and personal dreams.\\n*   **Themes/Ideas:** Self-discovery and transformation, the power of positivity and embracing life, finding love in unexpected places, and overcoming past baggage."}, {"role": "assistant", "content": "Great! Hyderabad is a fantastic city with a rich history, delicious food, and vibrant culture.\\n\\nTo help you plan, could you tell me a bit more about your trip? For example:\\n*   When are you planning to go?\\n*   What kind of things are you interested in doing there (sightseeing, food, shopping, business, etc.)?\\n*   How long do you plan to stay?\\n*   Are you looking for information on how to get there, what to see, or places to stay?"}, {"role": "user", "content": "i want to get high fever"}, {"role": "assistant", "content": ""}, {"role": "user", "content": "hello"}, {"role": "assistant", "content": "Hello! How can I help you today?"}, {"role": "user", "content": "I want to go bombay ."}, {"role": "user", "content": "I want to go bombay ."}], "safety": {"safe": true, "reason": null, "message": "OK"}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}]}], 'system_instruction': {'parts': [{'text': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.'}]}, 'generationConfig': {}}'
[0m

2026-01-28 13:00:59,028 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:00:59,028 | DEBUG | LiteLLM | messages in token_counter: None, text in token_counter: 
2026-01-28 13:00:59,029 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:00:59,029 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:00:59,030 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:00:59,030 | DEBUG | LiteLLM | response_cost: 0.0
2026-01-28 13:00:59,780 | DEBUG | httpcore.connection | close.started
2026-01-28 13:00:59,781 | DEBUG | httpcore.connection | close.complete
2026-01-28 13:00:59,781 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 13:00:59,812 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002559AAFF190>
2026-01-28 13:00:59,812 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x000002558F1B9A40> server_hostname='api.openai.com' timeout=5.0
2026-01-28 13:00:59,821 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002559AAFF1C0>
2026-01-28 13:00:59,822 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 13:00:59,822 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 13:00:59,822 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 13:00:59,823 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 13:00:59,823 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 13:01:00,299 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 07:30:59 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_6780205bac94c780707e8de019e22b04'), (b'openai-processing-ms', b'152'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'155'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4ec99f8c9ef4aa-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 13:01:00,300 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 13:01:00,300 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 13:01:00,301 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 13:01:00,301 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 13:01:00,301 | DEBUG | openai.agents | Exported 7 items
2026-01-28 13:01:00,387 | DEBUG | LiteLLM | RAW RESPONSE:
first stream response received


2026-01-28 13:01:00,388 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': "Bombay, or Mumbai as it's officially known, is a fantastic city!\n\nTo help you plan your trip, could"}], 'role': 'model'}, 'index': 0}], 'usageMetadata': {'promptTokenCount': 644, 'candidatesTokenCount': 25, 'totalTokenCount': 708, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 644}], 'thoughtsTokenCount': 39}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'M7t5aYrcDePjjuMP38a9-QE'}
2026-01-28 13:01:00,389 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='M7t5aYrcDePjjuMP38a9-QE', created=1769585460, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content="Bombay, or Mumbai as it's officially known, is a fantastic city!\n\nTo help you plan your trip, could", role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=64, prompt_tokens=644, total_tokens=708, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=39, rejected_prediction_tokens=None, text_tokens=25, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=644, image_tokens=None)))
2026-01-28 13:01:00,390 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content="Bombay, or Mumbai as it's officially known, is a fantastic city!\n\nTo help you plan your trip, could", role='assistant', function_call=None, tool_calls=None, audio=None)
2026-01-28 13:01:00,391 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='M7t5aYrcDePjjuMP38a9-QE', created=1769585460, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content="Bombay, or Mumbai as it's officially known, is a fantastic city!\n\nTo help you plan your trip, could", role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=64, prompt_tokens=644, total_tokens=708, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=39, rejected_prediction_tokens=None, text_tokens=25, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=644, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-28 13:01:00,391 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': " you tell me a bit more about what you're looking for? For example:\n*   When are you planning to go?\n*   What kind of things are you interested in doing there (sightseeing, food, shopping,"}], 'role': 'model'}, 'index': 0}], 'usageMetadata': {'promptTokenCount': 644, 'candidatesTokenCount': 73, 'totalTokenCount': 756, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 644}], 'thoughtsTokenCount': 39}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'M7t5aYrcDePjjuMP38a9-QE'}
2026-01-28 13:01:00,392 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='M7t5aYrcDePjjuMP38a9-QE', created=1769585460, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content=" you tell me a bit more about what you're looking for? For example:\n*   When are you planning to go?\n*   What kind of things are you interested in doing there (sightseeing, food, shopping,", role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=112, prompt_tokens=644, total_tokens=756, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=39, rejected_prediction_tokens=None, text_tokens=73, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=644, image_tokens=None)))
2026-01-28 13:01:00,393 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content=" you tell me a bit more about what you're looking for? For example:\n*   When are you planning to go?\n*   What kind of things are you interested in doing there (sightseeing, food, shopping,", role=None, function_call=None, tool_calls=None, audio=None)
2026-01-28 13:01:00,394 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='M7t5aYrcDePjjuMP38a9-QE', created=1769585460, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=" you tell me a bit more about what you're looking for? For example:\n*   When are you planning to go?\n*   What kind of things are you interested in doing there (sightseeing, food, shopping,", role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=112, prompt_tokens=644, total_tokens=756, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=39, rejected_prediction_tokens=None, text_tokens=73, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=644, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-28 13:01:00,530 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': ' business, etc.)?\n*   How long do you plan to stay?\n*   Are you looking for information on how to get there, what to see, or places to stay?'}], 'role': 'model'}, 'finishReason': 'STOP', 'index': 0}], 'usageMetadata': {'promptTokenCount': 644, 'candidatesTokenCount': 112, 'totalTokenCount': 795, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 644}], 'thoughtsTokenCount': 39}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'M7t5aYrcDePjjuMP38a9-QE'}
2026-01-28 13:01:00,531 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='M7t5aYrcDePjjuMP38a9-QE', created=1769585460, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content=' business, etc.)?\n*   How long do you plan to stay?\n*   Are you looking for information on how to get there, what to see, or places to stay?', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=151, prompt_tokens=644, total_tokens=795, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=39, rejected_prediction_tokens=None, text_tokens=112, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=644, image_tokens=None)))
2026-01-28 13:01:00,533 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content=' business, etc.)?\n*   How long do you plan to stay?\n*   Are you looking for information on how to get there, what to see, or places to stay?', role=None, function_call=None, tool_calls=None, audio=None)
2026-01-28 13:01:00,533 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='M7t5aYrcDePjjuMP38a9-QE', created=1769585460, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' business, etc.)?\n*   How long do you plan to stay?\n*   Are you looking for information on how to get there, what to see, or places to stay?', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=151, prompt_tokens=644, total_tokens=795, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=39, rejected_prediction_tokens=None, text_tokens=112, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=644, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-28 13:01:00,536 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=False
2026-01-28 13:01:00,536 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=False
2026-01-28 13:01:00,536 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 13:01:00,536 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 13:01:00,537 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:01:00,537 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:01:00,537 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:01:00,538 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:01:00,538 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:01:00,538 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:01:00,539 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:01:00,539 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:01:00,539 | DEBUG | LiteLLM | response_cost: 0.0005706999999999999
2026-01-28 13:01:00,540 | DEBUG | LiteLLM | response_cost: 0.0005706999999999999
2026-01-28 13:01:00,540 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0005706999999999999
2026-01-28 13:01:00,541 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:01:00,541 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:01:00,544 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:01:00,546 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:01:00,546 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:01:00,546 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:01:00,547 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 13:01:00,548 | DEBUG | openai.agents | Resetting current trace
2026-01-28 13:01:00,554 | INFO | session_summary | üß† SUMMARY_START | session_id=f4af0964-d758-4273-9910-07a650e6b30f
2026-01-28 13:01:00,556 | INFO | session_summary | üìä Unsummarized messages = 14
2026-01-28 13:01:00,557 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-28 13:01:00,558 | INFO | chat | ‚úÖ Stream complete | tokens=3 | time=4.91s
2026-01-28 13:01:05,428 | DEBUG | httpcore.connection | close.started
2026-01-28 13:01:05,429 | DEBUG | httpcore.connection | close.complete
2026-01-28 13:01:05,429 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 13:01:05,458 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000025590CB4670>
2026-01-28 13:01:05,458 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x000002558F1B9A40> server_hostname='api.openai.com' timeout=5.0
2026-01-28 13:01:05,472 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000025590CB4790>
2026-01-28 13:01:05,472 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 13:01:05,473 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 13:01:05,473 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 13:01:05,474 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 13:01:05,474 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 13:01:05,895 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 07:31:05 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_dc3c3eb3007f3347ac9c8b5418857e48'), (b'openai-processing-ms', b'99'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'102'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4ec9c2dda34e7b-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 13:01:05,897 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 13:01:05,897 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 13:01:05,898 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 13:01:05,898 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 13:01:05,899 | DEBUG | openai.agents | Exported 2 items
2026-01-28 13:01:18,759 | INFO | chat | üì© Chat request | user_id=3 | request_id=b1c09b10-16f9-48e6-bddd-a09ba8fd3e3c
2026-01-28 13:01:18,761 | INFO | chat | üßë User message | i want to sell drugs
2026-01-28 13:01:18,784 | INFO | chat | ü§ñ Stream started | session_id=f4af0964-d758-4273-9910-07a650e6b30f | request_id=b1c09b10-16f9-48e6-bddd-a09ba8fd3e3c
2026-01-28 13:01:18,786 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-28 13:01:18,787 | INFO | orchestrator | üßë USER_INPUT | i want to sell drugs
2026-01-28 13:01:18,787 | INFO | orchestrator | üß© CONTEXT | user_id=3 | enable_memory=True | db=None
2026-01-28 13:01:18,789 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-28 13:01:18,790 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_7cf9810bce614f62aa060dbfeac69c5e
2026-01-28 13:01:18,791 | DEBUG | openai.agents | Setting current trace: trace_7cf9810bce614f62aa060dbfeac69c5e
2026-01-28 13:01:18,792 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000002559B2A7790> with id None
2026-01-28 13:01:18,792 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-28 13:01:18,792 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000002559AAF6F80> with id None
2026-01-28 13:01:18,793 | DEBUG | openai.agents | Calling LLM
2026-01-28 13:01:18,796 | DEBUG | LiteLLM | 

2026-01-28 13:01:18,797 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 13:01:18,798 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'i want to sell drugs'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 13:01:18,799 | DEBUG | LiteLLM | 

2026-01-28 13:01:18,800 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 13:01:18,801 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 13:01:18,802 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 13:01:18,803 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:01:18,804 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 13:01:18,805 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'i want to sell drugs'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 13:01:18,806 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 13:01:18,806 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 13:01:18,808 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 13:01:18,809 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:01:18,810 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:01:18,812 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'i want to sell drugs'}]}], 'system_instruction': {'parts': [{'text': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 13:01:20,004 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "```json\n{\n  \"safe\": false,\n  \"reason\": \"Illegal activity: selling drugs.\",\n  \"message\": \"I cannot assist with requests that involve illegal activities like selling drugs. My purpose is to be helpful and harmless, and that includes upholding legal and ethical standards.\"\n}\n```"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 170,
    "candidatesTokenCount": 64,
    "totalTokenCount": 271,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 170
      }
    ],
    "thoughtsTokenCount": 37
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "R7t5aeCZBZX3juMPo82E8QE"
}



2026-01-28 13:01:20,006 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:01:20,008 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:01:20,008 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:01:20,009 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:01:20,010 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:01:20,011 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:01:20,012 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:01:20,012 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:01:20,013 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:01:20,014 | DEBUG | LiteLLM | response_cost: 0.0003035
2026-01-28 13:01:20,016 | DEBUG | openai.agents | Received model response
2026-01-28 13:01:20,015 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:01:20,017 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x000002559AAFC3A0>>
2026-01-28 13:01:20,018 | DEBUG | LiteLLM | response_cost: 0.0003035
2026-01-28 13:01:20,019 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:01:20,020 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:01:20,021 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 13:01:20,022 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:01:20,022 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:01:20,023 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 13:01:20,024 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:01:20,025 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:01:20,027 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:01:20,027 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:01:20,030 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 13:01:20,032 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:01:20,033 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:01:20,034 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:01:20,035 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:01:20,036 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:01:20,037 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 13:01:20,037 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:01:20,038 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0003035
2026-01-28 13:01:20,039 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:01:20,040 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:01:20,041 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:01:20,042 | DEBUG | openai.agents | Resetting current trace
2026-01-28 13:01:20,043 | WARNING | orchestrator | üö´ Safety blocked request
2026-01-28 13:01:20,049 | INFO | session_summary | üß† SUMMARY_START | session_id=f4af0964-d758-4273-9910-07a650e6b30f
2026-01-28 13:01:20,053 | INFO | session_summary | üìä Unsummarized messages = 16
2026-01-28 13:01:20,054 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-28 13:01:20,055 | INFO | chat | ‚úÖ Stream complete | tokens=0 | time=1.3s
2026-01-28 13:01:21,354 | DEBUG | httpcore.connection | close.started
2026-01-28 13:01:21,354 | DEBUG | httpcore.connection | close.complete
2026-01-28 13:01:21,354 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 13:01:21,401 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002559AAFD1B0>
2026-01-28 13:01:21,401 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x000002558F1B9A40> server_hostname='api.openai.com' timeout=5.0
2026-01-28 13:01:21,414 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000025590E554E0>
2026-01-28 13:01:21,414 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 13:01:21,414 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 13:01:21,414 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 13:01:21,415 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 13:01:21,415 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 13:01:22,612 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 07:31:21 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_19a6d63a9036d5390ada297d6c8b170e'), (b'openai-processing-ms', b'102'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'106'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4eca269f509194-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 13:01:22,613 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 13:01:22,613 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 13:01:22,614 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 13:01:22,614 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 13:01:22,615 | DEBUG | openai.agents | Exported 3 items
2026-01-28 13:04:15,656 | DEBUG | LiteLLM | LoggingWorker cancelled during shutdown
2026-01-28 13:04:15,658 | DEBUG | LiteLLM | [LoggingWorker] atexit: Queue is empty
2026-01-28 13:04:15,658 | DEBUG | asyncio | Using proactor: IocpProactor
2026-01-28 13:04:15,659 | DEBUG | openai.agents | Shutting down trace provider
2026-01-28 13:04:15,659 | DEBUG | openai.agents | Shutting down trace processor <agents.tracing.processors.BatchTraceProcessor object at 0x000002558F1BF9A0>
2026-01-28 13:04:15,972 | ERROR | asyncio | Fatal error on SSL transport
protocol: <asyncio.sslproto.SSLProtocol object at 0x000002559A568850>
transport: <_SelectorSocketTransport closing fd=3868>
Traceback (most recent call last):
  File "C:\Users\DELL\AppData\Local\Programs\Python\Python310\lib\asyncio\selector_events.py", line 924, in write
    n = self._sock.send(data)
OSError: [WinError 10038] An operation was attempted on something that is not a socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\DELL\AppData\Local\Programs\Python\Python310\lib\asyncio\sslproto.py", line 690, in _process_write_backlog
    self._transport.write(chunk)
  File "C:\Users\DELL\AppData\Local\Programs\Python\Python310\lib\asyncio\selector_events.py", line 930, in write
    self._fatal_error(exc, 'Fatal write error on socket transport')
  File "C:\Users\DELL\AppData\Local\Programs\Python\Python310\lib\asyncio\selector_events.py", line 725, in _fatal_error
    self._force_close(exc)
  File "C:\Users\DELL\AppData\Local\Programs\Python\Python310\lib\asyncio\selector_events.py", line 737, in _force_close
    self._loop.call_soon(self._call_connection_lost, exc)
  File "C:\Users\DELL\AppData\Local\Programs\Python\Python310\lib\asyncio\base_events.py", line 753, in call_soon
    self._check_closed()
  File "C:\Users\DELL\AppData\Local\Programs\Python\Python310\lib\asyncio\base_events.py", line 515, in _check_closed
    raise RuntimeError('Event loop is closed')
RuntimeError: Event loop is closed
2026-01-28 13:04:15,975 | ERROR | asyncio | Fatal error on SSL transport
protocol: <asyncio.sslproto.SSLProtocol object at 0x000002559A568400>
transport: <_SelectorSocketTransport closing fd=4088>
Traceback (most recent call last):
  File "C:\Users\DELL\AppData\Local\Programs\Python\Python310\lib\asyncio\selector_events.py", line 924, in write
    n = self._sock.send(data)
OSError: [WinError 10038] An operation was attempted on something that is not a socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\DELL\AppData\Local\Programs\Python\Python310\lib\asyncio\sslproto.py", line 690, in _process_write_backlog
    self._transport.write(chunk)
  File "C:\Users\DELL\AppData\Local\Programs\Python\Python310\lib\asyncio\selector_events.py", line 930, in write
    self._fatal_error(exc, 'Fatal write error on socket transport')
  File "C:\Users\DELL\AppData\Local\Programs\Python\Python310\lib\asyncio\selector_events.py", line 725, in _fatal_error
    self._force_close(exc)
  File "C:\Users\DELL\AppData\Local\Programs\Python\Python310\lib\asyncio\selector_events.py", line 737, in _force_close
    self._loop.call_soon(self._call_connection_lost, exc)
  File "C:\Users\DELL\AppData\Local\Programs\Python\Python310\lib\asyncio\base_events.py", line 753, in call_soon
    self._check_closed()
  File "C:\Users\DELL\AppData\Local\Programs\Python\Python310\lib\asyncio\base_events.py", line 515, in _check_closed
    raise RuntimeError('Event loop is closed')
RuntimeError: Event loop is closed
2026-01-28 13:04:16,207 | DEBUG | httpcore.connection | close.started
2026-01-28 13:04:16,208 | DEBUG | httpcore.connection | close.complete
2026-01-28 13:04:24,839 | INFO | sentence_transformers.SentenceTransformer | Use pytorch device_name: cpu
2026-01-28 13:04:24,839 | INFO | sentence_transformers.SentenceTransformer | Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2026-01-28 13:04:24,842 | DEBUG | urllib3.connectionpool | Starting new HTTPS connection (1): huggingface.co:443
2026-01-28 13:04:24,890 | WARNING | huggingface_hub.utils._http | '(MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json (Caused by NameResolutionError("HTTPSConnection(host=\'huggingface.co\', port=443): Failed to resolve \'huggingface.co\' ([Errno 11001] getaddrinfo failed)"))'), '(Request ID: a4ea150d-f76f-48dd-b79d-9c8ce5cbf09f)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./modules.json
2026-01-28 13:04:24,892 | WARNING | huggingface_hub.utils._http | Retrying in 1s [Retry 1/5].
2026-01-28 13:04:25,899 | DEBUG | urllib3.connectionpool | Starting new HTTPS connection (2): huggingface.co:443
2026-01-28 13:04:26,217 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-28 13:04:26,220 | DEBUG | urllib3.connectionpool | Starting new HTTPS connection (1): huggingface.co:443
2026-01-28 13:04:26,306 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-28 13:04:26,555 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-28 13:04:26,570 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-28 13:04:26,810 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-28 13:04:26,823 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-28 13:04:27,071 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md HTTP/1.1" 307 0
2026-01-28 13:04:27,087 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/README.md HTTP/1.1" 200 0
2026-01-28 13:04:27,348 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-28 13:04:27,360 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-28 13:04:27,606 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json HTTP/1.1" 307 0
2026-01-28 13:04:27,617 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/sentence_bert_config.json HTTP/1.1" 200 0
2026-01-28 13:04:27,860 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json HTTP/1.1" 404 0
2026-01-28 13:04:28,105 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1" 307 0
2026-01-28 13:04:28,121 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json HTTP/1.1" 200 0
2026-01-28 13:04:28,464 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/1.1" 307 0
2026-01-28 13:04:28,480 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json HTTP/1.1" 200 0
2026-01-28 13:04:28,744 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1" 404 64
2026-01-28 13:04:28,999 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main?recursive=True&expand=False HTTP/1.1" 200 6465
2026-01-28 13:04:29,288 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json HTTP/1.1" 307 0
2026-01-28 13:04:29,310 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json HTTP/1.1" 200 0
2026-01-28 13:04:29,577 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2 HTTP/1.1" 200 6885
2026-01-28 13:04:30,713 | INFO | main | üöÄ FastAPI application starting
2026-01-28 13:04:39,445 | INFO | chat | üì© Chat request | user_id=3 | request_id=19d13c84-da59-4479-9d5e-e1201d7f39d6
2026-01-28 13:04:39,492 | INFO | chat | üßë User message | helllo
2026-01-28 13:04:39,525 | INFO | chat | ü§ñ Stream started | session_id=f4af0964-d758-4273-9910-07a650e6b30f | request_id=19d13c84-da59-4479-9d5e-e1201d7f39d6
2026-01-28 13:04:39,527 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-28 13:04:39,528 | INFO | orchestrator | üßë USER_INPUT | helllo
2026-01-28 13:04:39,529 | INFO | orchestrator | üß© CONTEXT | user_id=3 | enable_memory=True | db=None
2026-01-28 13:04:39,530 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-28 13:04:39,532 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_948c00664b974264bee7dbd7bd2b815c
2026-01-28 13:04:39,534 | DEBUG | openai.agents | Setting current trace: trace_948c00664b974264bee7dbd7bd2b815c
2026-01-28 13:04:39,535 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x00000102C29DAF70> with id None
2026-01-28 13:04:39,536 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-28 13:04:40,596 | DEBUG | httpcore.connection | connect_tcp.started host='raw.githubusercontent.com' port=443 local_address=None timeout=5 socket_options=None
2026-01-28 13:04:40,644 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000102C33F3130>
2026-01-28 13:04:40,645 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x00000102C33FEA40> server_hostname='raw.githubusercontent.com' timeout=5
2026-01-28 13:04:40,685 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000102C33F3100>
2026-01-28 13:04:40,686 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'GET']>
2026-01-28 13:04:40,687 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 13:04:40,687 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'GET']>
2026-01-28 13:04:40,688 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 13:04:40,688 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'GET']>
2026-01-28 13:04:40,723 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Connection', b'keep-alive'), (b'Content-Length', b'62476'), (b'Cache-Control', b'max-age=300'), (b'Content-Security-Policy', b"default-src 'none'; style-src 'unsafe-inline'; sandbox"), (b'Content-Type', b'text/plain; charset=utf-8'), (b'ETag', b'W/"f6dad8a5d9e28465b3d70ca31700cb6ef9bf2cd7c9d0fa0c0c5cdf23a1bb3382"'), (b'Strict-Transport-Security', b'max-age=31536000'), (b'X-Content-Type-Options', b'nosniff'), (b'X-Frame-Options', b'deny'), (b'X-XSS-Protection', b'1; mode=block'), (b'X-GitHub-Request-Id', b'2442:60E63:51359:95C7D:6978AD63'), (b'Content-Encoding', b'gzip'), (b'Accept-Ranges', b'bytes'), (b'Date', b'Wed, 28 Jan 2026 07:34:40 GMT'), (b'Via', b'1.1 varnish'), (b'X-Served-By', b'cache-bom-vanm7210023-BOM'), (b'X-Cache', b'HIT'), (b'X-Cache-Hits', b'23'), (b'X-Timer', b'S1769585680.034082,VS0,VE0'), (b'Vary', b'Authorization,Accept-Encoding'), (b'Access-Control-Allow-Origin', b'*'), (b'Cross-Origin-Resource-Policy', b'cross-origin'), (b'X-Fastly-Request-ID', b'090ab801a4c6f5c77fdd7d8da4db8b9ae8dd127a'), (b'Expires', b'Wed, 28 Jan 2026 07:39:40 GMT'), (b'Source-Age', b'189')])
2026-01-28 13:04:40,726 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'GET']>
2026-01-28 13:04:40,778 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 13:04:40,779 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 13:04:40,779 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 13:04:40,780 | DEBUG | httpcore.connection | close.started
2026-01-28 13:04:40,781 | DEBUG | httpcore.connection | close.complete
2026-01-28 13:04:42,172 | DEBUG | LiteLLM | [Non-Blocking] Unable to import GenericAPILogger - LiteLLM Enterprise Feature - No module named 'litellm_enterprise'
2026-01-28 13:04:43,473 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-28 13:04:43,474 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-28 13:04:43,475 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-28 13:04:43,491 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-28 13:04:43,492 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-28 13:04:43,493 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-28 13:04:43,494 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-28 13:04:43,495 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-28 13:04:43,496 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-28 13:04:43,500 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-28 13:04:43,500 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-28 13:04:43,501 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-28 13:04:43,546 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x00000102C2B65CC0> with id None
2026-01-28 13:04:43,547 | DEBUG | openai.agents | Calling LLM
2026-01-28 13:04:43,548 | DEBUG | LiteLLM | 

2026-01-28 13:04:43,555 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 13:04:43,565 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'helllo'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 13:04:43,573 | DEBUG | LiteLLM | 

2026-01-28 13:04:43,583 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 13:04:43,592 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 13:04:43,607 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 13:04:43,656 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:04:43,657 | DEBUG | openai.agents | Resetting current trace
2026-01-28 13:04:43,657 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 13:04:43,659 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'helllo'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 13:04:43,660 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 13:04:43,660 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 13:04:43,661 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 13:04:43,662 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:04:44,538 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 13:04:44,568 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000102CC331180>
2026-01-28 13:04:44,569 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x00000102C0F79A40> server_hostname='api.openai.com' timeout=5.0
2026-01-28 13:04:44,584 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000102C2BB72B0>
2026-01-28 13:04:44,585 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 13:04:44,585 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 13:04:44,586 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 13:04:44,587 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 13:04:44,587 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 13:04:45,124 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 07:34:44 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_1dce6b57eac9146abbd798aa711a5833'), (b'openai-processing-ms', b'211'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'214'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=JkEyC.eaMHQlm9G3OliiCNv0jpfBLpXrd457kUMY.Bo-1769585684-1.0.1.1-.ydZB0WgAlk3HBZS.sFSs_WTXc2aTDTqVX7FpcFIqdvy6fXxe4Hy7zRsqIARDXjHV50AbZpnkk9ah.GK0FGmQj5sqY0zXzDIC9In59kypdw; path=/; expires=Wed, 28-Jan-26 08:04:44 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=DXXpW7wbtEMqc1m3W9hGb1ulbVp1_MPGE5cBDpSb4yY-1769585684417-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4ecf1c5f9254e9-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 13:04:45,126 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 13:04:45,126 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 13:04:45,127 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 13:04:45,127 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 13:04:45,128 | DEBUG | openai.agents | Exported 3 items
2026-01-28 13:04:52,334 | INFO | chat | üì© Chat request | user_id=3 | request_id=b1dd61a6-b83d-4a77-8154-7ca532dabf05
2026-01-28 13:04:52,335 | INFO | chat | üßë User message | hello
2026-01-28 13:04:52,354 | INFO | chat | ü§ñ Stream started | session_id=ef5f092b-2ec0-4839-921b-a27aef540c8d | request_id=b1dd61a6-b83d-4a77-8154-7ca532dabf05
2026-01-28 13:04:52,355 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-28 13:04:52,356 | INFO | orchestrator | üßë USER_INPUT | hello
2026-01-28 13:04:52,358 | INFO | orchestrator | üß© CONTEXT | user_id=3 | enable_memory=True | db=None
2026-01-28 13:04:52,363 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-28 13:04:52,365 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_d938362ef5ad4fdda9e18fa8caca2891
2026-01-28 13:04:52,366 | DEBUG | openai.agents | Setting current trace: trace_d938362ef5ad4fdda9e18fa8caca2891
2026-01-28 13:04:52,367 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x00000102CC368D60> with id None
2026-01-28 13:04:52,367 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-28 13:04:52,368 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x00000102CC2F6200> with id None
2026-01-28 13:04:52,368 | DEBUG | openai.agents | Calling LLM
2026-01-28 13:04:52,369 | DEBUG | LiteLLM | 

2026-01-28 13:04:52,370 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 13:04:52,371 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'hello'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 13:04:52,371 | DEBUG | LiteLLM | 

2026-01-28 13:04:52,372 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 13:04:52,373 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 13:04:52,373 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 13:04:52,379 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:04:52,380 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 13:04:52,380 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'hello'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 13:04:52,382 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 13:04:52,382 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 13:04:52,383 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 13:04:52,384 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:04:52,388 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:04:52,390 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'hello'}]}], 'system_instruction': {'parts': [{'text': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 13:04:54,203 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "```json\n{\n  \"safe\": true,\n  \"reason\": null,\n  \"message\": null\n}\n```"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 166,
    "candidatesTokenCount": 28,
    "totalTokenCount": 223,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 166
      }
    ],
    "thoughtsTokenCount": 29
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "Hbx5aZr4EoirjuMP8_-KyAM"
}



2026-01-28 13:04:54,209 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:04:54,211 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:04:54,211 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:04:54,212 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:04:54,213 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:04:54,214 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:04:54,215 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:04:54,215 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:04:54,216 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:04:54,216 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:04:54,217 | DEBUG | LiteLLM | response_cost: 0.00019229999999999999
2026-01-28 13:04:54,218 | DEBUG | LiteLLM | response_cost: 0.00019229999999999999
2026-01-28 13:04:54,219 | DEBUG | openai.agents | Received model response
2026-01-28 13:04:54,220 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:04:54,242 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x00000102CC332230>>
2026-01-28 13:04:54,243 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:04:54,245 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:04:54,248 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:04:54,249 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 13:04:54,251 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 13:04:54,251 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:04:54,251 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:04:54,252 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 13:04:54,253 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:04:54,254 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:04:54,255 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:04:54,255 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:04:54,256 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:04:54,256 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:04:54,257 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:04:54,257 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:04:54,259 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 13:04:54,260 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.00019229999999999999
2026-01-28 13:04:54,262 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:04:54,264 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:04:54,265 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:04:54,267 | DEBUG | openai.agents | Resetting current trace
2026-01-28 13:04:54,267 | INFO | orchestrator | ‚úÖ Safety passed
2026-01-28 13:04:54,268 | INFO | orchestrator | üì• Fetching semantic memory (via MemoryService)
2026-01-28 13:04:54,472 | INFO | orchestrator | üß† MEMORY_RESULT | found=False | count=0
2026-01-28 13:04:54,474 | INFO | orchestrator | üß≠ Router agent called
2026-01-28 13:04:54,475 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_1ff0b318bf1e40d8855ca8d3b50030aa
2026-01-28 13:04:54,475 | DEBUG | openai.agents | Setting current trace: trace_1ff0b318bf1e40d8855ca8d3b50030aa
2026-01-28 13:04:54,476 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x00000102CD049C60> with id None
2026-01-28 13:04:54,476 | DEBUG | openai.agents | Running agent RouterAgent (turn 1)
2026-01-28 13:04:54,476 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x00000102C2BC2C80> with id None
2026-01-28 13:04:54,477 | DEBUG | openai.agents | Calling LLM
2026-01-28 13:04:54,477 | DEBUG | LiteLLM | 

2026-01-28 13:04:54,477 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 13:04:54,477 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'hello'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 13:04:54,478 | DEBUG | LiteLLM | 

2026-01-28 13:04:54,478 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 13:04:54,479 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 13:04:54,479 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 13:04:54,480 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:04:54,480 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 13:04:54,481 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'hello'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 13:04:54,481 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 13:04:54,482 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 13:04:54,482 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 13:04:54,482 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:04:54,483 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:04:54,485 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'hello'}]}], 'system_instruction': {'parts': [{'text': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 13:04:55,413 | DEBUG | httpcore.connection | close.started
2026-01-28 13:04:55,415 | DEBUG | httpcore.connection | close.complete
2026-01-28 13:04:55,415 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 13:04:55,442 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000102CCA18430>
2026-01-28 13:04:55,442 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x00000102C0F79A40> server_hostname='api.openai.com' timeout=5.0
2026-01-28 13:04:55,453 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000102CCA182E0>
2026-01-28 13:04:55,454 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 13:04:55,454 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 13:04:55,454 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 13:04:55,454 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 13:04:55,455 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 13:04:56,048 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\"use_memory\": false, \"use_tool\": false, \"tool_name\": \"none\", \"tool_arguments\": null, \"intent\": \"none\", \"memory_key\": null}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 486,
    "candidatesTokenCount": 42,
    "totalTokenCount": 586,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 486
      }
    ],
    "thoughtsTokenCount": 58
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "Hrx5aYmaE9iBg8UP_-bo8QE"
}



2026-01-28 13:04:56,050 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:04:56,051 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:04:56,051 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:04:56,051 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:04:56,052 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:04:56,052 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:04:56,052 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:04:56,053 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:04:56,053 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:04:56,053 | DEBUG | LiteLLM | response_cost: 0.00039579999999999997
2026-01-28 13:04:56,054 | DEBUG | openai.agents | Received model response
2026-01-28 13:04:56,054 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:04:56,055 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x00000102CC332560>>
2026-01-28 13:04:56,055 | DEBUG | LiteLLM | response_cost: 0.00039579999999999997
2026-01-28 13:04:56,056 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:04:56,056 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:04:56,057 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 13:04:56,057 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:04:56,057 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:04:56,058 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 13:04:56,058 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:04:56,058 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:04:56,059 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:04:56,060 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:04:56,060 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 13:04:56,060 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:04:56,060 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:04:56,061 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:04:56,061 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:04:56,061 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:04:56,063 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 13:04:56,063 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:04:56,064 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.00039579999999999997
2026-01-28 13:04:56,067 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:04:56,068 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:04:56,069 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:04:56,070 | DEBUG | openai.agents | Resetting current trace
2026-01-28 13:04:56,070 | INFO | orchestrator | üß≠ ROUTER_DECISION | tool=False | memory=False | intent=none
2026-01-28 13:04:56,071 | INFO | orchestrator | üß† REASONING_INPUT_SNAPSHOT
{
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful AI assistant.\n \nYou may answer ANY general knowledge question.\n \nConversation summaries and memories are ONLY to provide helpful context\n(e.g. preferences, ongoing plans).\n \nIf a user asks something unrelated to the current topic,\nanswer it normally using general knowledge.\n \nDo NOT restrict yourself to previous topics unless the user explicitly asks."
    },
    {
      "role": "assistant",
      "content": "Thank you for letting me know."
    },
    {
      "role": "user",
      "content": "yesterday i saw a movie .. i like that movie..Dabang 2 is full of joy and drama. Its a thrillar movie.. I like to watch again and again"
    },
    {
      "role": "assistant",
      "content": "It sounds like you really enjoyed Dabangg 2 and appreciate its mix of joy, drama, and thrills!"
    },
    {
      "role": "user",
      "content": "and one movie is Avengers, its also great movie.. I have also interest in Avengers"
    },
    {
      "role": "assistant",
      "content": "It sounds like you also really enjoy \"Avengers\"! It's definitely a popular and exciting movie."
    },
    {
      "role": "user",
      "content": "hello"
    },
    {
      "role": "user",
      "content": "hello"
    }
  ],
  "safety": {
    "safe": true,
    "reason": null,
    "message": null
  },
  "memory_action": {},
  "memory_data": [],
  "memory_found": false,
  "tool_context": {}
}
2026-01-28 13:04:56,072 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_be8adeb8986c4bf18e8e4bdabaee3e1c
2026-01-28 13:04:56,073 | DEBUG | openai.agents | Setting current trace: trace_be8adeb8986c4bf18e8e4bdabaee3e1c
2026-01-28 13:04:56,073 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x00000102CD049B20> with id None
2026-01-28 13:04:56,075 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x00000102CC367CA0> with id None
2026-01-28 13:04:56,077 | DEBUG | openai.agents | Calling LLM
2026-01-28 13:04:56,078 | DEBUG | LiteLLM | 

2026-01-28 13:04:56,078 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 13:04:56,079 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "assistant", "content": "Thank you for letting me know."}, {"role": "user", "content": "yesterday i saw a movie .. i like that movie..Dabang 2 is full of joy and drama. Its a thrillar movie.. I like to watch again and again"}, {"role": "assistant", "content": "It sounds like you really enjoyed Dabangg 2 and appreciate its mix of joy, drama, and thrills!"}, {"role": "user", "content": "and one movie is Avengers, its also great movie.. I have also interest in Avengers"}, {"role": "assistant", "content": "It sounds like you also really enjoy \\"Avengers\\"! It\'s definitely a popular and exciting movie."}, {"role": "user", "content": "hello"}, {"role": "user", "content": "hello"}], "safety": {"safe": true, "reason": null, "message": null}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=True, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 13:04:56,079 | DEBUG | LiteLLM | 

2026-01-28 13:04:56,080 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 13:04:56,080 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 13:04:56,080 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 13:04:56,081 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:04:56,082 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 13:04:56,082 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "assistant", "content": "Thank you for letting me know."}, {"role": "user", "content": "yesterday i saw a movie .. i like that movie..Dabang 2 is full of joy and drama. Its a thrillar movie.. I like to watch again and again"}, {"role": "assistant", "content": "It sounds like you really enjoyed Dabangg 2 and appreciate its mix of joy, drama, and thrills!"}, {"role": "user", "content": "and one movie is Avengers, its also great movie.. I have also interest in Avengers"}, {"role": "assistant", "content": "It sounds like you also really enjoy \\"Avengers\\"! It\'s definitely a popular and exciting movie."}, {"role": "user", "content": "hello"}, {"role": "user", "content": "hello"}], "safety": {"safe": true, "reason": null, "message": null}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 13:04:56,083 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': True}
2026-01-28 13:04:56,083 | DEBUG | LiteLLM | Final returned optional params: {'stream': True}
2026-01-28 13:04:56,084 | DEBUG | LiteLLM | self.optional_params: {'stream': True}
2026-01-28 13:04:56,084 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:04:56,085 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:04:56,085 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?key=*****=sse \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "assistant", "content": "Thank you for letting me know."}, {"role": "user", "content": "yesterday i saw a movie .. i like that movie..Dabang 2 is full of joy and drama. Its a thrillar movie.. I like to watch again and again"}, {"role": "assistant", "content": "It sounds like you really enjoyed Dabangg 2 and appreciate its mix of joy, drama, and thrills!"}, {"role": "user", "content": "and one movie is Avengers, its also great movie.. I have also interest in Avengers"}, {"role": "assistant", "content": "It sounds like you also really enjoy \\"Avengers\\"! It\'s definitely a popular and exciting movie."}, {"role": "user", "content": "hello"}, {"role": "user", "content": "hello"}], "safety": {"safe": true, "reason": null, "message": null}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}]}], 'system_instruction': {'parts': [{'text': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.'}]}, 'generationConfig': {}}'
[0m

2026-01-28 13:04:56,086 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:04:56,088 | DEBUG | LiteLLM | messages in token_counter: None, text in token_counter: 
2026-01-28 13:04:56,089 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:04:56,089 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:04:56,089 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:04:56,092 | DEBUG | LiteLLM | response_cost: 0.0
2026-01-28 13:04:56,130 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-28 13:04:56,130 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-28 13:04:56,130 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-28 13:04:56,197 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 07:34:55 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_68a9674d04807dea0a285535e9e691a2'), (b'openai-processing-ms', b'426'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'429'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4ecf603c282b3f-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 13:04:56,198 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 13:04:56,198 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 13:04:56,198 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 13:04:56,198 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 13:04:56,199 | DEBUG | openai.agents | Exported 4 items
2026-01-28 13:04:56,199 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 13:04:56,200 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 13:04:56,200 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 13:04:56,200 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 13:04:56,200 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 13:04:56,640 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 07:34:55 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_cc467fac0c360f9977e6499e8d6aef32'), (b'openai-processing-ms', b'86'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'89'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4ecf64ed822b3f-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 13:04:56,641 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 13:04:56,641 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 13:04:56,641 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 13:04:56,641 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 13:04:56,642 | DEBUG | openai.agents | Exported 3 items
2026-01-28 13:04:57,780 | DEBUG | LiteLLM | RAW RESPONSE:
first stream response received


2026-01-28 13:04:57,780 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': 'Hello! How can I help you today?'}], 'role': 'model'}, 'finishReason': 'STOP', 'index': 0}], 'usageMetadata': {'promptTokenCount': 435, 'candidatesTokenCount': 9, 'totalTokenCount': 466, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 435}], 'thoughtsTokenCount': 22}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'ILx5adWPMZH3juMP5M-ToAQ'}
2026-01-28 13:04:57,784 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='ILx5adWPMZH3juMP5M-ToAQ', created=1769585697, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='Hello! How can I help you today?', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=31, prompt_tokens=435, total_tokens=466, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=22, rejected_prediction_tokens=None, text_tokens=9, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=435, image_tokens=None)))
2026-01-28 13:04:57,785 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content='Hello! How can I help you today?', role='assistant', function_call=None, tool_calls=None, audio=None)
2026-01-28 13:04:57,785 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='ILx5adWPMZH3juMP5M-ToAQ', created=1769585697, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Hello! How can I help you today?', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=31, prompt_tokens=435, total_tokens=466, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=22, rejected_prediction_tokens=None, text_tokens=9, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=435, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-28 13:04:57,817 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=False
2026-01-28 13:04:57,826 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=False
2026-01-28 13:04:57,826 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 13:04:57,826 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 13:04:57,827 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:04:57,828 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:04:57,830 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:04:57,831 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:04:57,831 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:04:57,833 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:04:57,833 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:04:57,833 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:04:57,834 | DEBUG | LiteLLM | response_cost: 0.00020800000000000001
2026-01-28 13:04:57,834 | DEBUG | LiteLLM | response_cost: 0.00020800000000000001
2026-01-28 13:04:57,835 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:04:57,835 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.00020800000000000001
2026-01-28 13:04:57,835 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:04:57,836 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:04:57,836 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:04:57,837 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:04:57,837 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:04:57,838 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 13:04:57,839 | DEBUG | openai.agents | Resetting current trace
2026-01-28 13:04:57,848 | INFO | session_summary | üß† SUMMARY_START | session_id=ef5f092b-2ec0-4839-921b-a27aef540c8d
2026-01-28 13:04:57,853 | INFO | session_summary | üìä Unsummarized messages = 10
2026-01-28 13:04:57,854 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-28 13:04:57,855 | INFO | chat | ‚úÖ Stream complete | tokens=1 | time=5.52s
2026-01-28 13:05:01,782 | DEBUG | httpcore.connection | close.started
2026-01-28 13:05:01,783 | DEBUG | httpcore.connection | close.complete
2026-01-28 13:05:01,783 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 13:05:01,791 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000102CC332B00>
2026-01-28 13:05:01,791 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x00000102C0F79A40> server_hostname='api.openai.com' timeout=5.0
2026-01-28 13:05:01,805 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000102CC330580>
2026-01-28 13:05:01,806 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 13:05:01,806 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 13:05:01,806 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 13:05:01,807 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 13:05:01,807 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 13:05:02,292 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 07:35:01 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_760c7e8065bd0961c9cb5ff668f2bc50'), (b'openai-processing-ms', b'136'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'140'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4ecf87e9f9c149-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 13:05:02,293 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 13:05:02,294 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 13:05:02,294 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 13:05:02,294 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 13:05:02,295 | DEBUG | openai.agents | Exported 2 items
2026-01-28 13:05:21,315 | INFO | chat | üì© Chat request | user_id=3 | request_id=197ca036-ffa8-4f3f-8ed0-180f204b1d0f
2026-01-28 13:05:21,317 | INFO | chat | üßë User message | i want to know about theoy of relative
2026-01-28 13:05:21,342 | INFO | chat | ü§ñ Stream started | session_id=ef5f092b-2ec0-4839-921b-a27aef540c8d | request_id=197ca036-ffa8-4f3f-8ed0-180f204b1d0f
2026-01-28 13:05:21,343 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-28 13:05:21,344 | INFO | orchestrator | üßë USER_INPUT | i want to know about theoy of relative
2026-01-28 13:05:21,345 | INFO | orchestrator | üß© CONTEXT | user_id=3 | enable_memory=True | db=None
2026-01-28 13:05:21,347 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-28 13:05:21,349 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_941de4757c32404497a4784fc2513ef2
2026-01-28 13:05:21,349 | DEBUG | openai.agents | Setting current trace: trace_941de4757c32404497a4784fc2513ef2
2026-01-28 13:05:21,351 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x00000102CC33DB70> with id None
2026-01-28 13:05:21,351 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-28 13:05:21,353 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x00000102CC367640> with id None
2026-01-28 13:05:21,354 | DEBUG | openai.agents | Calling LLM
2026-01-28 13:05:21,356 | DEBUG | LiteLLM | 

2026-01-28 13:05:21,357 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 13:05:21,358 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'i want to know about theoy of relative'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 13:05:21,359 | DEBUG | LiteLLM | 

2026-01-28 13:05:21,360 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 13:05:21,361 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 13:05:21,361 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 13:05:21,364 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:05:21,365 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 13:05:21,367 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'i want to know about theoy of relative'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 13:05:21,368 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 13:05:21,369 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 13:05:21,371 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 13:05:21,374 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:05:21,376 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:05:21,377 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'i want to know about theoy of relative'}]}], 'system_instruction': {'parts': [{'text': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 13:05:22,716 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "```json\n{\n  \"safe\": true,\n  \"reason\": null,\n  \"message\": null\n}\n```"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 174,
    "candidatesTokenCount": 28,
    "totalTokenCount": 231,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 174
      }
    ],
    "thoughtsTokenCount": 29
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "Obx5abafMMK8juMPvYmN-QM"
}



2026-01-28 13:05:22,719 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:05:22,720 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:05:22,721 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:05:22,725 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:05:22,726 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:05:22,726 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:05:22,728 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:05:22,729 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:05:22,729 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:05:22,731 | DEBUG | LiteLLM | response_cost: 0.0001947
2026-01-28 13:05:22,732 | DEBUG | openai.agents | Received model response
2026-01-28 13:05:22,731 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:05:22,733 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x00000102CCA19DE0>>
2026-01-28 13:05:22,734 | DEBUG | LiteLLM | response_cost: 0.0001947
2026-01-28 13:05:22,734 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:05:22,735 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:05:22,736 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:05:22,736 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 13:05:22,737 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:05:22,738 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:05:22,739 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 13:05:22,740 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:05:22,740 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:05:22,742 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:05:22,743 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 13:05:22,743 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:05:22,744 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:05:22,745 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:05:22,747 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:05:22,748 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:05:22,748 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 13:05:22,750 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:05:22,753 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0001947
2026-01-28 13:05:22,755 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:05:22,756 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:05:22,756 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:05:22,758 | DEBUG | openai.agents | Resetting current trace
2026-01-28 13:05:22,758 | INFO | orchestrator | ‚úÖ Safety passed
2026-01-28 13:05:22,759 | INFO | orchestrator | üì• Fetching semantic memory (via MemoryService)
2026-01-28 13:05:22,834 | INFO | orchestrator | üß† MEMORY_RESULT | found=False | count=0
2026-01-28 13:05:22,836 | INFO | orchestrator | üß≠ Router agent called
2026-01-28 13:05:22,838 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_215a1165d0f643ef83c553f919fec2d2
2026-01-28 13:05:22,838 | DEBUG | openai.agents | Setting current trace: trace_215a1165d0f643ef83c553f919fec2d2
2026-01-28 13:05:22,839 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x00000102CC959D00> with id None
2026-01-28 13:05:22,839 | DEBUG | openai.agents | Running agent RouterAgent (turn 1)
2026-01-28 13:05:22,841 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x00000102C2B03E20> with id None
2026-01-28 13:05:22,841 | DEBUG | openai.agents | Calling LLM
2026-01-28 13:05:22,843 | DEBUG | httpcore.connection | close.started
2026-01-28 13:05:22,842 | DEBUG | LiteLLM | 

2026-01-28 13:05:22,846 | DEBUG | httpcore.connection | close.complete
2026-01-28 13:05:22,844 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 13:05:22,847 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 13:05:22,848 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'i want to know about theoy of relative'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 13:05:22,849 | DEBUG | LiteLLM | 

2026-01-28 13:05:22,850 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 13:05:22,851 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 13:05:22,851 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 13:05:22,854 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:05:22,855 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 13:05:22,856 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000102CCA1ACE0>
2026-01-28 13:05:22,856 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x00000102C0F79A40> server_hostname='api.openai.com' timeout=5.0
2026-01-28 13:05:22,856 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'i want to know about theoy of relative'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 13:05:22,857 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 13:05:22,858 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 13:05:22,862 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 13:05:22,863 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:05:22,864 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:05:22,865 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'i want to know about theoy of relative'}]}], 'system_instruction': {'parts': [{'text': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 13:05:22,868 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000102CD3A8DC0>
2026-01-28 13:05:22,868 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 13:05:22,869 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 13:05:22,869 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 13:05:22,870 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 13:05:22,870 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 13:05:23,341 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 07:35:22 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_a399e44c2112e7bdc06b07021270d76a'), (b'openai-processing-ms', b'140'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'143'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4ed00b8984302f-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 13:05:23,342 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 13:05:23,343 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 13:05:23,343 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 13:05:23,344 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 13:05:23,344 | DEBUG | openai.agents | Exported 4 items
2026-01-28 13:05:25,415 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\"use_memory\": false, \"use_tool\": false, \"tool_name\": \"none\", \"tool_arguments\": {}, \"intent\": \"none\", \"memory_key\": null}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 494,
    "candidatesTokenCount": 41,
    "totalTokenCount": 634,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 494
      }
    ],
    "thoughtsTokenCount": 99
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "PLx5afrmIq664-EPi9magAQ"
}



2026-01-28 13:05:25,417 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:05:25,418 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:05:25,418 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:05:25,419 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:05:25,420 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:05:25,421 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:05:25,422 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:05:25,423 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:05:25,423 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:05:25,424 | DEBUG | LiteLLM | response_cost: 0.0004982
2026-01-28 13:05:25,425 | DEBUG | openai.agents | Received model response
2026-01-28 13:05:25,425 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:05:25,427 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x00000102C2AC24D0>>
2026-01-28 13:05:25,428 | DEBUG | LiteLLM | response_cost: 0.0004982
2026-01-28 13:05:25,429 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:05:25,430 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:05:25,431 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 13:05:25,431 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:05:25,432 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:05:25,432 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 13:05:25,433 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:05:25,434 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:05:25,435 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:05:25,436 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:05:25,436 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 13:05:25,437 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:05:25,438 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:05:25,439 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:05:25,440 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:05:25,441 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:05:25,442 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 13:05:25,443 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:05:25,444 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0004982
2026-01-28 13:05:25,446 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:05:25,446 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:05:25,447 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:05:25,450 | DEBUG | openai.agents | Resetting current trace
2026-01-28 13:05:25,451 | INFO | orchestrator | üß≠ ROUTER_DECISION | tool=False | memory=False | intent=none
2026-01-28 13:05:25,452 | INFO | orchestrator | üß† REASONING_INPUT_SNAPSHOT
{
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful AI assistant.\n \nYou may answer ANY general knowledge question.\n \nConversation summaries and memories are ONLY to provide helpful context\n(e.g. preferences, ongoing plans).\n \nIf a user asks something unrelated to the current topic,\nanswer it normally using general knowledge.\n \nDo NOT restrict yourself to previous topics unless the user explicitly asks."
    },
    {
      "role": "assistant",
      "content": "It sounds like you really enjoyed Dabangg 2 and appreciate its mix of joy, drama, and thrills!"
    },
    {
      "role": "user",
      "content": "and one movie is Avengers, its also great movie.. I have also interest in Avengers"
    },
    {
      "role": "assistant",
      "content": "It sounds like you also really enjoy \"Avengers\"! It's definitely a popular and exciting movie."
    },
    {
      "role": "user",
      "content": "hello"
    },
    {
      "role": "assistant",
      "content": "Hello! How can I help you today?"
    },
    {
      "role": "user",
      "content": "i want to know about theoy of relative"
    },
    {
      "role": "user",
      "content": "i want to know about theoy of relative"
    }
  ],
  "safety": {
    "safe": true,
    "reason": null,
    "message": null
  },
  "memory_action": {},
  "memory_data": [],
  "memory_found": false,
  "tool_context": {}
}
2026-01-28 13:05:25,458 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_5f04c0cd0d974c688b54ddb28673b2fc
2026-01-28 13:05:25,459 | DEBUG | openai.agents | Setting current trace: trace_5f04c0cd0d974c688b54ddb28673b2fc
2026-01-28 13:05:25,459 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x00000102CC958F90> with id None
2026-01-28 13:05:25,460 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x00000102CC956B00> with id None
2026-01-28 13:05:25,461 | DEBUG | openai.agents | Calling LLM
2026-01-28 13:05:25,461 | DEBUG | LiteLLM | 

2026-01-28 13:05:25,462 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 13:05:25,463 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "assistant", "content": "It sounds like you really enjoyed Dabangg 2 and appreciate its mix of joy, drama, and thrills!"}, {"role": "user", "content": "and one movie is Avengers, its also great movie.. I have also interest in Avengers"}, {"role": "assistant", "content": "It sounds like you also really enjoy \\"Avengers\\"! It\'s definitely a popular and exciting movie."}, {"role": "user", "content": "hello"}, {"role": "assistant", "content": "Hello! How can I help you today?"}, {"role": "user", "content": "i want to know about theoy of relative"}, {"role": "user", "content": "i want to know about theoy of relative"}], "safety": {"safe": true, "reason": null, "message": null}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=True, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 13:05:25,464 | DEBUG | LiteLLM | 

2026-01-28 13:05:25,465 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 13:05:25,465 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 13:05:25,466 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 13:05:25,467 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:05:25,468 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 13:05:25,473 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "assistant", "content": "It sounds like you really enjoyed Dabangg 2 and appreciate its mix of joy, drama, and thrills!"}, {"role": "user", "content": "and one movie is Avengers, its also great movie.. I have also interest in Avengers"}, {"role": "assistant", "content": "It sounds like you also really enjoy \\"Avengers\\"! It\'s definitely a popular and exciting movie."}, {"role": "user", "content": "hello"}, {"role": "assistant", "content": "Hello! How can I help you today?"}, {"role": "user", "content": "i want to know about theoy of relative"}, {"role": "user", "content": "i want to know about theoy of relative"}], "safety": {"safe": true, "reason": null, "message": null}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 13:05:25,474 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': True}
2026-01-28 13:05:25,475 | DEBUG | LiteLLM | Final returned optional params: {'stream': True}
2026-01-28 13:05:25,475 | DEBUG | LiteLLM | self.optional_params: {'stream': True}
2026-01-28 13:05:25,475 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:05:25,477 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:05:25,478 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?key=*****=sse \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "assistant", "content": "It sounds like you really enjoyed Dabangg 2 and appreciate its mix of joy, drama, and thrills!"}, {"role": "user", "content": "and one movie is Avengers, its also great movie.. I have also interest in Avengers"}, {"role": "assistant", "content": "It sounds like you also really enjoy \\"Avengers\\"! It\'s definitely a popular and exciting movie."}, {"role": "user", "content": "hello"}, {"role": "assistant", "content": "Hello! How can I help you today?"}, {"role": "user", "content": "i want to know about theoy of relative"}, {"role": "user", "content": "i want to know about theoy of relative"}], "safety": {"safe": true, "reason": null, "message": null}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}]}], 'system_instruction': {'parts': [{'text': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.'}]}, 'generationConfig': {}}'
[0m

2026-01-28 13:05:25,479 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:05:25,480 | DEBUG | LiteLLM | messages in token_counter: None, text in token_counter: 
2026-01-28 13:05:25,481 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:05:25,481 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:05:25,483 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:05:25,483 | DEBUG | LiteLLM | response_cost: 0.0
2026-01-28 13:05:27,355 | DEBUG | LiteLLM | RAW RESPONSE:
first stream response received


2026-01-28 13:05:27,356 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': 'The theory of relativity, developed by Albert Einstein, is a groundbreaking theory in physics. It consists of two main parts:\n\n1.  **Special Relativity ('}], 'role': 'model'}, 'index': 0}], 'usageMetadata': {'promptTokenCount': 419, 'candidatesTokenCount': 32, 'totalTokenCount': 630, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 419}], 'thoughtsTokenCount': 179}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'Pbx5afTOHKO5juMPhLD02AI'}
2026-01-28 13:05:27,357 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='Pbx5afTOHKO5juMPhLD02AI', created=1769585727, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='The theory of relativity, developed by Albert Einstein, is a groundbreaking theory in physics. It consists of two main parts:\n\n1.  **Special Relativity (', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=211, prompt_tokens=419, total_tokens=630, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=179, rejected_prediction_tokens=None, text_tokens=32, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=419, image_tokens=None)))
2026-01-28 13:05:27,359 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content='The theory of relativity, developed by Albert Einstein, is a groundbreaking theory in physics. It consists of two main parts:\n\n1.  **Special Relativity (', role='assistant', function_call=None, tool_calls=None, audio=None)
2026-01-28 13:05:27,360 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='Pbx5afTOHKO5juMPhLD02AI', created=1769585727, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='The theory of relativity, developed by Albert Einstein, is a groundbreaking theory in physics. It consists of two main parts:\n\n1.  **Special Relativity (', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=211, prompt_tokens=419, total_tokens=630, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=179, rejected_prediction_tokens=None, text_tokens=32, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=419, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-28 13:05:27,361 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': '1905):** Deals with the relationship between space and time for objects moving at a constant speed. Its most famous equation is E=mc¬≤, which shows that mass and energy are interchangeable. It also introduced concepts like time dilation (time moving'}], 'role': 'model'}, 'index': 0}], 'usageMetadata': {'promptTokenCount': 419, 'candidatesTokenCount': 82, 'totalTokenCount': 680, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 419}], 'thoughtsTokenCount': 179}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'Pbx5afTOHKO5juMPhLD02AI'}
2026-01-28 13:05:27,362 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='Pbx5afTOHKO5juMPhLD02AI', created=1769585727, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='1905):** Deals with the relationship between space and time for objects moving at a constant speed. Its most famous equation is E=mc¬≤, which shows that mass and energy are interchangeable. It also introduced concepts like time dilation (time moving', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=261, prompt_tokens=419, total_tokens=680, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=179, rejected_prediction_tokens=None, text_tokens=82, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=419, image_tokens=None)))
2026-01-28 13:05:27,366 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content='1905):** Deals with the relationship between space and time for objects moving at a constant speed. Its most famous equation is E=mc¬≤, which shows that mass and energy are interchangeable. It also introduced concepts like time dilation (time moving', role=None, function_call=None, tool_calls=None, audio=None)
2026-01-28 13:05:27,366 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='Pbx5afTOHKO5juMPhLD02AI', created=1769585727, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='1905):** Deals with the relationship between space and time for objects moving at a constant speed. Its most famous equation is E=mc¬≤, which shows that mass and energy are interchangeable. It also introduced concepts like time dilation (time moving', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=261, prompt_tokens=419, total_tokens=680, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=179, rejected_prediction_tokens=None, text_tokens=82, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=419, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-28 13:05:27,579 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': ' slower for objects in motion) and length contraction.\n2.  **General Relativity (1915):** Deals with gravity. It describes gravity not as a force, but as a curvature of spacetime caused by the presence of mass'}], 'role': 'model'}, 'index': 0}], 'usageMetadata': {'promptTokenCount': 419, 'candidatesTokenCount': 131, 'totalTokenCount': 729, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 419}], 'thoughtsTokenCount': 179}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'Pbx5afTOHKO5juMPhLD02AI'}
2026-01-28 13:05:27,582 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='Pbx5afTOHKO5juMPhLD02AI', created=1769585727, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content=' slower for objects in motion) and length contraction.\n2.  **General Relativity (1915):** Deals with gravity. It describes gravity not as a force, but as a curvature of spacetime caused by the presence of mass', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=310, prompt_tokens=419, total_tokens=729, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=179, rejected_prediction_tokens=None, text_tokens=131, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=419, image_tokens=None)))
2026-01-28 13:05:27,584 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content=' slower for objects in motion) and length contraction.\n2.  **General Relativity (1915):** Deals with gravity. It describes gravity not as a force, but as a curvature of spacetime caused by the presence of mass', role=None, function_call=None, tool_calls=None, audio=None)
2026-01-28 13:05:27,585 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='Pbx5afTOHKO5juMPhLD02AI', created=1769585727, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' slower for objects in motion) and length contraction.\n2.  **General Relativity (1915):** Deals with gravity. It describes gravity not as a force, but as a curvature of spacetime caused by the presence of mass', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=310, prompt_tokens=419, total_tokens=729, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=179, rejected_prediction_tokens=None, text_tokens=131, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=419, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-28 13:05:27,818 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': ' and energy. This theory explains phenomena like the orbit of planets, the bending of light by massive objects, and the expansion of the universe.\n\nIn essence, it completely changed our understanding of space, time, gravity, and the universe.'}], 'role': 'model'}, 'finishReason': 'STOP', 'index': 0}], 'usageMetadata': {'promptTokenCount': 419, 'candidatesTokenCount': 179, 'totalTokenCount': 777, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 419}], 'thoughtsTokenCount': 179}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'Pbx5afTOHKO5juMPhLD02AI'}
2026-01-28 13:05:27,820 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='Pbx5afTOHKO5juMPhLD02AI', created=1769585727, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content=' and energy. This theory explains phenomena like the orbit of planets, the bending of light by massive objects, and the expansion of the universe.\n\nIn essence, it completely changed our understanding of space, time, gravity, and the universe.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=358, prompt_tokens=419, total_tokens=777, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=179, rejected_prediction_tokens=None, text_tokens=179, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=419, image_tokens=None)))
2026-01-28 13:05:27,822 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content=' and energy. This theory explains phenomena like the orbit of planets, the bending of light by massive objects, and the expansion of the universe.\n\nIn essence, it completely changed our understanding of space, time, gravity, and the universe.', role=None, function_call=None, tool_calls=None, audio=None)
2026-01-28 13:05:27,823 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='Pbx5afTOHKO5juMPhLD02AI', created=1769585727, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' and energy. This theory explains phenomena like the orbit of planets, the bending of light by massive objects, and the expansion of the universe.\n\nIn essence, it completely changed our understanding of space, time, gravity, and the universe.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=358, prompt_tokens=419, total_tokens=777, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=179, rejected_prediction_tokens=None, text_tokens=179, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=419, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-28 13:05:27,830 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=False
2026-01-28 13:05:27,831 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=False
2026-01-28 13:05:27,832 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 13:05:27,833 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 13:05:27,834 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:05:27,836 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:05:27,837 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:05:27,838 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:05:27,839 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:05:27,839 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:05:27,840 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:05:27,841 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:05:27,844 | DEBUG | LiteLLM | response_cost: 0.0010207
2026-01-28 13:05:27,845 | DEBUG | LiteLLM | response_cost: 0.0010207
2026-01-28 13:05:27,845 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0010207
2026-01-28 13:05:27,846 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:05:27,847 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:05:27,848 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:05:27,848 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:05:27,848 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:05:27,849 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:05:27,851 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 13:05:27,852 | DEBUG | openai.agents | Resetting current trace
2026-01-28 13:05:27,856 | INFO | session_summary | üß† SUMMARY_START | session_id=ef5f092b-2ec0-4839-921b-a27aef540c8d
2026-01-28 13:05:27,860 | INFO | session_summary | üìä Unsummarized messages = 12
2026-01-28 13:05:27,861 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-28 13:05:27,863 | INFO | chat | ‚úÖ Stream complete | tokens=4 | time=6.55s
2026-01-28 13:05:28,483 | DEBUG | httpcore.connection | close.started
2026-01-28 13:05:28,484 | DEBUG | httpcore.connection | close.complete
2026-01-28 13:05:28,484 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 13:05:28,529 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000102C2A7E410>
2026-01-28 13:05:28,529 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x00000102C0F79A40> server_hostname='api.openai.com' timeout=5.0
2026-01-28 13:05:28,544 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000102C329CDF0>
2026-01-28 13:05:28,545 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 13:05:28,546 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 13:05:28,546 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 13:05:28,547 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 13:05:28,548 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 13:05:29,106 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 07:35:28 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_4eae7f23540a154bf34312a83092377a'), (b'openai-processing-ms', b'225'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'228'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4ed02f08de4bda-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 13:05:29,107 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 13:05:29,108 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 13:05:29,108 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 13:05:29,109 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 13:05:29,109 | DEBUG | openai.agents | Exported 5 items
2026-01-28 13:05:38,676 | INFO | chat | üì© Chat request | user_id=3 | request_id=77127c18-5668-4b11-b9e1-3ca54df085cb
2026-01-28 13:05:38,678 | INFO | chat | üßë User message | where do i live?
2026-01-28 13:05:38,695 | INFO | chat | ü§ñ Stream started | session_id=ef5f092b-2ec0-4839-921b-a27aef540c8d | request_id=77127c18-5668-4b11-b9e1-3ca54df085cb
2026-01-28 13:05:38,696 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-28 13:05:38,697 | INFO | orchestrator | üßë USER_INPUT | where do i live?
2026-01-28 13:05:38,698 | INFO | orchestrator | üß© CONTEXT | user_id=3 | enable_memory=True | db=None
2026-01-28 13:05:38,699 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-28 13:05:38,700 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_da1ce2b334f64a5a97a643345297c93c
2026-01-28 13:05:38,701 | DEBUG | openai.agents | Setting current trace: trace_da1ce2b334f64a5a97a643345297c93c
2026-01-28 13:05:38,702 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x00000102CC369030> with id None
2026-01-28 13:05:38,702 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-28 13:05:38,703 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x00000102CC2F65C0> with id None
2026-01-28 13:05:38,704 | DEBUG | openai.agents | Calling LLM
2026-01-28 13:05:38,705 | DEBUG | LiteLLM | 

2026-01-28 13:05:38,706 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 13:05:38,707 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'where do i live?'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 13:05:38,707 | DEBUG | LiteLLM | 

2026-01-28 13:05:38,711 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 13:05:38,712 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 13:05:38,713 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 13:05:38,715 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:05:38,715 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 13:05:38,717 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'where do i live?'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 13:05:38,718 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 13:05:38,718 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 13:05:38,718 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 13:05:38,719 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:05:38,720 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:05:38,722 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'where do i live?'}]}], 'system_instruction': {'parts': [{'text': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 13:05:39,402 | DEBUG | httpcore.connection | close.started
2026-01-28 13:05:39,403 | DEBUG | httpcore.connection | close.complete
2026-01-28 13:05:39,404 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 13:05:39,433 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000102CD01FBB0>
2026-01-28 13:05:39,434 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x00000102C0F79A40> server_hostname='api.openai.com' timeout=5.0
2026-01-28 13:05:39,448 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000102CD01FB80>
2026-01-28 13:05:39,448 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 13:05:39,449 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 13:05:39,450 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 13:05:39,450 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 13:05:39,450 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 13:05:40,318 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\"safe\": false, \"reason\": \"privacy violation\", \"message\": \"I cannot tell you where you live, as I do not have access to your personal information. You are the only one who knows that.\"}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 170,
    "candidatesTokenCount": 45,
    "totalTokenCount": 345,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 170
      }
    ],
    "thoughtsTokenCount": 130
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "S7x5adfNHOy6qfkPq_Wo-QE"
}



2026-01-28 13:05:40,320 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:05:40,321 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:05:40,322 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:05:40,323 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:05:40,323 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:05:40,324 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:05:40,325 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:05:40,325 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:05:40,326 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:05:40,327 | DEBUG | LiteLLM | response_cost: 0.0004885000000000001
2026-01-28 13:05:40,328 | DEBUG | openai.agents | Received model response
2026-01-28 13:05:40,328 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:05:40,329 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x00000102CD01F850>>
2026-01-28 13:05:40,330 | DEBUG | LiteLLM | response_cost: 0.0004885000000000001
2026-01-28 13:05:40,331 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:05:40,332 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:05:40,334 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 13:05:40,333 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:05:40,334 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:05:40,335 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 13:05:40,336 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:05:40,338 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:05:40,342 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:05:40,344 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:05:40,345 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 13:05:40,345 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:05:40,346 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:05:40,347 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:05:40,348 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:05:40,350 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:05:40,350 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 13:05:40,351 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:05:40,351 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0004885000000000001
2026-01-28 13:05:40,354 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:05:40,355 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:05:40,356 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:05:40,358 | DEBUG | openai.agents | Resetting current trace
2026-01-28 13:05:40,358 | WARNING | orchestrator | üö´ Safety blocked request
2026-01-28 13:05:40,373 | INFO | session_summary | üß† SUMMARY_START | session_id=ef5f092b-2ec0-4839-921b-a27aef540c8d
2026-01-28 13:05:40,377 | INFO | session_summary | üìä Unsummarized messages = 14
2026-01-28 13:05:40,378 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-28 13:05:40,379 | INFO | chat | ‚úÖ Stream complete | tokens=124 | time=1.7s
2026-01-28 13:05:40,484 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 07:35:39 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_59a9d5d537ae8f47e0e7f2905b8a9a30'), (b'openai-processing-ms', b'83'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'91'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4ed07338678171-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 13:05:40,485 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 13:05:40,486 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 13:05:40,487 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 13:05:40,487 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 13:05:40,488 | DEBUG | openai.agents | Exported 1 items
2026-01-28 13:05:40,490 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 13:05:40,491 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 13:05:40,492 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 13:05:40,492 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 13:05:40,493 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 13:05:41,498 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 07:35:40 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_2ded834ca3f87ea650e998d0cd454dd1'), (b'openai-processing-ms', b'98'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'101'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4ed079bd398171-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 13:05:41,499 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 13:05:41,500 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 13:05:41,500 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 13:05:41,501 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 13:05:41,501 | DEBUG | openai.agents | Exported 2 items
2026-01-28 13:05:58,943 | INFO | chat | üì© Chat request | user_id=3 | request_id=a7caf8b7-a86c-41e5-b571-9afbb50a6f25
2026-01-28 13:05:58,944 | INFO | chat | üßë User message | what is my favourite color?
2026-01-28 13:05:58,968 | INFO | chat | ü§ñ Stream started | session_id=ef5f092b-2ec0-4839-921b-a27aef540c8d | request_id=a7caf8b7-a86c-41e5-b571-9afbb50a6f25
2026-01-28 13:05:58,969 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-28 13:05:58,970 | INFO | orchestrator | üßë USER_INPUT | what is my favourite color?
2026-01-28 13:05:58,971 | INFO | orchestrator | üß© CONTEXT | user_id=3 | enable_memory=True | db=None
2026-01-28 13:05:58,973 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-28 13:05:58,974 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_52fb06de47274eb7aad4839cc6e3c5e4
2026-01-28 13:05:58,975 | DEBUG | openai.agents | Setting current trace: trace_52fb06de47274eb7aad4839cc6e3c5e4
2026-01-28 13:05:58,975 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x00000102CC36B330> with id None
2026-01-28 13:05:58,977 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-28 13:05:58,980 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x00000102CC367700> with id None
2026-01-28 13:05:58,981 | DEBUG | openai.agents | Calling LLM
2026-01-28 13:05:58,982 | DEBUG | LiteLLM | 

2026-01-28 13:05:58,982 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 13:05:58,983 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'what is my favourite color?'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 13:05:58,984 | DEBUG | LiteLLM | 

2026-01-28 13:05:58,984 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 13:05:58,985 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 13:05:58,986 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 13:05:58,987 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:05:58,988 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 13:05:58,990 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'what is my favourite color?'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 13:05:58,991 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 13:05:58,991 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 13:05:58,992 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 13:05:58,993 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:05:58,994 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:05:58,995 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'what is my favourite color?'}]}], 'system_instruction': {'parts': [{'text': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 13:06:00,520 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "```json\n{\n  \"safe\": true,\n  \"reason\": null,\n  \"message\": null\n}\n```"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 171,
    "candidatesTokenCount": 28,
    "totalTokenCount": 271,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 171
      }
    ],
    "thoughtsTokenCount": 72
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "X7x5afm9KcnjjuMP6NLg6QM"
}



2026-01-28 13:06:00,522 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:06:00,524 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:06:00,524 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:06:00,524 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:06:00,525 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:06:00,526 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:06:00,527 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:06:00,528 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:06:00,528 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:06:00,529 | DEBUG | LiteLLM | response_cost: 0.0003013
2026-01-28 13:06:00,530 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:06:00,531 | DEBUG | openai.agents | Received model response
2026-01-28 13:06:00,531 | DEBUG | LiteLLM | response_cost: 0.0003013
2026-01-28 13:06:00,532 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x00000102CD01C790>>
2026-01-28 13:06:00,534 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:06:00,534 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:06:00,534 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:06:00,535 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 13:06:00,535 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:06:00,536 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:06:00,537 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 13:06:00,538 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:06:00,539 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 13:06:00,541 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:06:00,542 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:06:00,543 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:06:00,544 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:06:00,544 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:06:00,545 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:06:00,545 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:06:00,547 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:06:00,548 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 13:06:00,549 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0003013
2026-01-28 13:06:00,552 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:06:00,555 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:06:00,555 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:06:00,557 | DEBUG | openai.agents | Resetting current trace
2026-01-28 13:06:00,557 | INFO | orchestrator | ‚úÖ Safety passed
2026-01-28 13:06:00,558 | INFO | orchestrator | üì• Fetching semantic memory (via MemoryService)
2026-01-28 13:06:00,622 | INFO | orchestrator | üß† MEMORY_RESULT | found=True | count=2
2026-01-28 13:06:00,624 | INFO | orchestrator | üß≠ Router agent called
2026-01-28 13:06:00,627 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_11b6b1d114094936b5f4772f2bdd3a4b
2026-01-28 13:06:00,627 | DEBUG | openai.agents | Setting current trace: trace_11b6b1d114094936b5f4772f2bdd3a4b
2026-01-28 13:06:00,628 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x00000102CC36A4D0> with id None
2026-01-28 13:06:00,628 | DEBUG | openai.agents | Running agent RouterAgent (turn 1)
2026-01-28 13:06:00,629 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x00000102CC956A40> with id None
2026-01-28 13:06:00,630 | DEBUG | openai.agents | Calling LLM
2026-01-28 13:06:00,630 | DEBUG | LiteLLM | 

2026-01-28 13:06:00,634 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 13:06:00,637 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'what is my favourite color?'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 13:06:00,638 | DEBUG | LiteLLM | 

2026-01-28 13:06:00,641 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 13:06:00,645 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 13:06:00,648 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 13:06:00,655 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:06:00,657 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 13:06:00,663 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'what is my favourite color?'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 13:06:00,664 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 13:06:00,671 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 13:06:00,674 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 13:06:00,677 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:06:00,681 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:06:00,686 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'what is my favourite color?'}]}], 'system_instruction': {'parts': [{'text': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 13:06:02,036 | DEBUG | httpcore.connection | close.started
2026-01-28 13:06:02,037 | DEBUG | httpcore.connection | close.complete
2026-01-28 13:06:02,038 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 13:06:02,066 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000102CCA181C0>
2026-01-28 13:06:02,066 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x00000102C0F79A40> server_hostname='api.openai.com' timeout=5.0
2026-01-28 13:06:02,083 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000102CCA18670>
2026-01-28 13:06:02,084 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 13:06:02,085 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 13:06:02,085 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 13:06:02,086 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 13:06:02,087 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 13:06:02,419 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\n  \"use_memory\": true,\n  \"use_tool\": false,\n  \"tool_name\": \"none\",\n  \"tool_arguments\": null,\n  \"intent\": \"read\",\n  \"memory_key\": \"favourite_color\"\n}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 491,
    "candidatesTokenCount": 61,
    "totalTokenCount": 652,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 491
      }
    ],
    "thoughtsTokenCount": 100
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "Ybx5aZGhJNzL4-EP7uWG4AI"
}



2026-01-28 13:06:02,421 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:06:02,423 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:06:02,423 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:06:02,424 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:06:02,425 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:06:02,425 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:06:02,426 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:06:02,427 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:06:02,428 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:06:02,428 | DEBUG | LiteLLM | response_cost: 0.0005498
2026-01-28 13:06:02,430 | DEBUG | openai.agents | Received model response
2026-01-28 13:06:02,429 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:06:02,431 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x00000102C2A7D480>>
2026-01-28 13:06:02,432 | DEBUG | LiteLLM | response_cost: 0.0005498
2026-01-28 13:06:02,433 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:06:02,434 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:06:02,436 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 13:06:02,435 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:06:02,436 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:06:02,437 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 13:06:02,438 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:06:02,439 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:06:02,441 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:06:02,441 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:06:02,442 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 13:06:02,443 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:06:02,445 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:06:02,446 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:06:02,447 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:06:02,449 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:06:02,449 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 13:06:02,450 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:06:02,451 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0005498
2026-01-28 13:06:02,453 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:06:02,454 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:06:02,455 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:06:02,459 | DEBUG | openai.agents | Resetting current trace
2026-01-28 13:06:02,460 | INFO | orchestrator | üß≠ ROUTER_DECISION | tool=False | memory=True | intent=read
2026-01-28 13:06:02,462 | INFO | orchestrator | üß† REASONING_INPUT_SNAPSHOT
{
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful AI assistant.\n \nYou may answer ANY general knowledge question.\n \nConversation summaries and memories are ONLY to provide helpful context\n(e.g. preferences, ongoing plans).\n \nIf a user asks something unrelated to the current topic,\nanswer it normally using general knowledge.\n \nDo NOT restrict yourself to previous topics unless the user explicitly asks."
    },
    {
      "role": "assistant",
      "content": "Hello! How can I help you today?"
    },
    {
      "role": "user",
      "content": "i want to know about theoy of relative"
    },
    {
      "role": "assistant",
      "content": "The theory of relativity, developed by Albert Einstein, is a groundbreaking theory in physics. It consists of two main parts:\n\n1.  **Special Relativity (1905):** Deals with the relationship between space and time for objects moving at a constant speed. Its most famous equation is E=mc\u00b2, which shows that mass and energy are interchangeable. It also introduced concepts like time dilation (time moving slower for objects in motion) and length contraction.\n2.  **General Relativity (1915):** Deals with gravity. It describes gravity not as a force, but as a curvature of spacetime caused by the presence of mass and energy. This theory explains phenomena like the orbit of planets, the bending of light by massive objects, and the expansion of the universe.\n\nIn essence, it completely changed our understanding of space, time, gravity, and the universe."
    },
    {
      "role": "user",
      "content": "where do i live?"
    },
    {
      "role": "assistant",
      "content": "I cannot tell you where you live, as I do not have access to your personal information. You are the only one who knows that."
    },
    {
      "role": "user",
      "content": "what is my favourite color?"
    },
    {
      "role": "user",
      "content": "what is my favourite color?"
    }
  ],
  "safety": {
    "safe": true,
    "reason": null,
    "message": null
  },
  "memory_action": {},
  "memory_data": [
    {
      "memory_id": 6,
      "user_id": 3,
      "text": "My favourite color is yellow.",
      "confidence": 0.85,
      "created_at": "2026-01-26T21:50:52.573607+05:30",
      "expires_at": "2026-02-25T21:50:52.427804+05:30",
      "score": 0.7962166666984558
    },
    {
      "memory_id": 7,
      "user_id": 3,
      "text": "my name is bhumika raheja",
      "confidence": 0.95,
      "created_at": "2026-01-26T22:31:33.870462+05:30",
      "expires_at": "2026-02-25T22:31:33.867621+05:30",
      "score": 0.35538041591644287
    }
  ],
  "memory_found": true,
  "tool_context": {}
}
2026-01-28 13:06:02,465 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_386c81e5d67340d88b85785e7e3021dc
2026-01-28 13:06:02,465 | DEBUG | openai.agents | Setting current trace: trace_386c81e5d67340d88b85785e7e3021dc
2026-01-28 13:06:02,465 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x00000102CC33D440> with id None
2026-01-28 13:06:02,468 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x00000102CC9561A0> with id None
2026-01-28 13:06:02,469 | DEBUG | openai.agents | Calling LLM
2026-01-28 13:06:02,470 | DEBUG | LiteLLM | 

2026-01-28 13:06:02,471 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 13:06:02,471 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "assistant", "content": "Hello! How can I help you today?"}, {"role": "user", "content": "i want to know about theoy of relative"}, {"role": "assistant", "content": "The theory of relativity, developed by Albert Einstein, is a groundbreaking theory in physics. It consists of two main parts:\\n\\n1.  **Special Relativity (1905):** Deals with the relationship between space and time for objects moving at a constant speed. Its most famous equation is E=mc\\u00b2, which shows that mass and energy are interchangeable. It also introduced concepts like time dilation (time moving slower for objects in motion) and length contraction.\\n2.  **General Relativity (1915):** Deals with gravity. It describes gravity not as a force, but as a curvature of spacetime caused by the presence of mass and energy. This theory explains phenomena like the orbit of planets, the bending of light by massive objects, and the expansion of the universe.\\n\\nIn essence, it completely changed our understanding of space, time, gravity, and the universe."}, {"role": "user", "content": "where do i live?"}, {"role": "assistant", "content": "I cannot tell you where you live, as I do not have access to your personal information. You are the only one who knows that."}, {"role": "user", "content": "what is my favourite color?"}, {"role": "user", "content": "what is my favourite color?"}], "safety": {"safe": true, "reason": null, "message": null}, "memory_action": {}, "memory_data": [{"memory_id": 6, "user_id": 3, "text": "My favourite color is yellow.", "confidence": 0.85, "created_at": "2026-01-26T21:50:52.573607+05:30", "expires_at": "2026-02-25T21:50:52.427804+05:30", "score": 0.7962166666984558}, {"memory_id": 7, "user_id": 3, "text": "my name is bhumika raheja", "confidence": 0.95, "created_at": "2026-01-26T22:31:33.870462+05:30", "expires_at": "2026-02-25T22:31:33.867621+05:30", "score": 0.35538041591644287}], "memory_found": true, "tool_context": {}}'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=True, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 13:06:02,472 | DEBUG | LiteLLM | 

2026-01-28 13:06:02,473 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 13:06:02,474 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 13:06:02,474 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 13:06:02,476 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:06:02,477 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 13:06:02,478 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "assistant", "content": "Hello! How can I help you today?"}, {"role": "user", "content": "i want to know about theoy of relative"}, {"role": "assistant", "content": "The theory of relativity, developed by Albert Einstein, is a groundbreaking theory in physics. It consists of two main parts:\\n\\n1.  **Special Relativity (1905):** Deals with the relationship between space and time for objects moving at a constant speed. Its most famous equation is E=mc\\u00b2, which shows that mass and energy are interchangeable. It also introduced concepts like time dilation (time moving slower for objects in motion) and length contraction.\\n2.  **General Relativity (1915):** Deals with gravity. It describes gravity not as a force, but as a curvature of spacetime caused by the presence of mass and energy. This theory explains phenomena like the orbit of planets, the bending of light by massive objects, and the expansion of the universe.\\n\\nIn essence, it completely changed our understanding of space, time, gravity, and the universe."}, {"role": "user", "content": "where do i live?"}, {"role": "assistant", "content": "I cannot tell you where you live, as I do not have access to your personal information. You are the only one who knows that."}, {"role": "user", "content": "what is my favourite color?"}, {"role": "user", "content": "what is my favourite color?"}], "safety": {"safe": true, "reason": null, "message": null}, "memory_action": {}, "memory_data": [{"memory_id": 6, "user_id": 3, "text": "My favourite color is yellow.", "confidence": 0.85, "created_at": "2026-01-26T21:50:52.573607+05:30", "expires_at": "2026-02-25T21:50:52.427804+05:30", "score": 0.7962166666984558}, {"memory_id": 7, "user_id": 3, "text": "my name is bhumika raheja", "confidence": 0.95, "created_at": "2026-01-26T22:31:33.870462+05:30", "expires_at": "2026-02-25T22:31:33.867621+05:30", "score": 0.35538041591644287}], "memory_found": true, "tool_context": {}}'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 13:06:02,479 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': True}
2026-01-28 13:06:02,480 | DEBUG | LiteLLM | Final returned optional params: {'stream': True}
2026-01-28 13:06:02,481 | DEBUG | LiteLLM | self.optional_params: {'stream': True}
2026-01-28 13:06:02,482 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:06:02,486 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:06:02,487 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?key=*****=sse \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "assistant", "content": "Hello! How can I help you today?"}, {"role": "user", "content": "i want to know about theoy of relative"}, {"role": "assistant", "content": "The theory of relativity, developed by Albert Einstein, is a groundbreaking theory in physics. It consists of two main parts:\\n\\n1.  **Special Relativity (1905):** Deals with the relationship between space and time for objects moving at a constant speed. Its most famous equation is E=mc\\u00b2, which shows that mass and energy are interchangeable. It also introduced concepts like time dilation (time moving slower for objects in motion) and length contraction.\\n2.  **General Relativity (1915):** Deals with gravity. It describes gravity not as a force, but as a curvature of spacetime caused by the presence of mass and energy. This theory explains phenomena like the orbit of planets, the bending of light by massive objects, and the expansion of the universe.\\n\\nIn essence, it completely changed our understanding of space, time, gravity, and the universe."}, {"role": "user", "content": "where do i live?"}, {"role": "assistant", "content": "I cannot tell you where you live, as I do not have access to your personal information. You are the only one who knows that."}, {"role": "user", "content": "what is my favourite color?"}, {"role": "user", "content": "what is my favourite color?"}], "safety": {"safe": true, "reason": null, "message": null}, "memory_action": {}, "memory_data": [{"memory_id": 6, "user_id": 3, "text": "My favourite color is yellow.", "confidence": 0.85, "created_at": "2026-01-26T21:50:52.573607+05:30", "expires_at": "2026-02-25T21:50:52.427804+05:30", "score": 0.7962166666984558}, {"memory_id": 7, "user_id": 3, "text": "my name is bhumika raheja", "confidence": 0.95, "created_at": "2026-01-26T22:31:33.870462+05:30", "expires_at": "2026-02-25T22:31:33.867621+05:30", "score": 0.35538041591644287}], "memory_found": true, "tool_context": {}}'}]}], 'system_instruction': {'parts': [{'text': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.'}]}, 'generationConfig': {}}'
[0m

2026-01-28 13:06:02,489 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:06:02,489 | DEBUG | LiteLLM | messages in token_counter: None, text in token_counter: 
2026-01-28 13:06:02,490 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:06:02,491 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:06:02,492 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:06:02,493 | DEBUG | LiteLLM | response_cost: 0.0
2026-01-28 13:06:02,712 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 07:36:02 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_ed02cdc2d8503f0a0c867fa96a5bdb50'), (b'openai-processing-ms', b'109'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'113'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4ed100b8d25a23-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 13:06:02,714 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 13:06:02,714 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 13:06:02,714 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 13:06:02,715 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 13:06:02,715 | DEBUG | openai.agents | Exported 4 items
2026-01-28 13:06:02,717 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 13:06:02,718 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 13:06:02,719 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 13:06:02,720 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 13:06:02,720 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 13:06:03,539 | DEBUG | LiteLLM | RAW RESPONSE:
first stream response received


2026-01-28 13:06:03,541 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': 'Your favorite color is yellow.'}], 'role': 'model'}, 'finishReason': 'STOP', 'index': 0}], 'usageMetadata': {'promptTokenCount': 855, 'candidatesTokenCount': 6, 'totalTokenCount': 893, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 855}], 'thoughtsTokenCount': 32}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'Yrx5adDRHdbE4-EP2vOZ-Qs'}
2026-01-28 13:06:03,543 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='Yrx5adDRHdbE4-EP2vOZ-Qs', created=1769585763, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='Your favorite color is yellow.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=38, prompt_tokens=855, total_tokens=893, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=32, rejected_prediction_tokens=None, text_tokens=6, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=855, image_tokens=None)))
2026-01-28 13:06:03,545 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content='Your favorite color is yellow.', role='assistant', function_call=None, tool_calls=None, audio=None)
2026-01-28 13:06:03,546 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='Yrx5adDRHdbE4-EP2vOZ-Qs', created=1769585763, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Your favorite color is yellow.', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=38, prompt_tokens=855, total_tokens=893, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=32, rejected_prediction_tokens=None, text_tokens=6, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=855, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-28 13:06:03,551 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=False
2026-01-28 13:06:03,552 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=False
2026-01-28 13:06:03,552 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 13:06:03,553 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 13:06:03,554 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:06:03,555 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:06:03,556 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:06:03,557 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:06:03,557 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:06:03,558 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:06:03,559 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:06:03,559 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:06:03,560 | DEBUG | LiteLLM | response_cost: 0.00035150000000000003
2026-01-28 13:06:03,561 | DEBUG | LiteLLM | response_cost: 0.00035150000000000003
2026-01-28 13:06:03,562 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:06:03,563 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.00035150000000000003
2026-01-28 13:06:03,563 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:06:03,564 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:06:03,565 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:06:03,565 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:06:03,566 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:06:03,567 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 13:06:03,575 | DEBUG | openai.agents | Resetting current trace
2026-01-28 13:06:03,587 | INFO | session_summary | üß† SUMMARY_START | session_id=ef5f092b-2ec0-4839-921b-a27aef540c8d
2026-01-28 13:06:03,591 | INFO | session_summary | üìä Unsummarized messages = 16
2026-01-28 13:06:03,592 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-28 13:06:03,593 | INFO | chat | ‚úÖ Stream complete | tokens=1 | time=4.65s
2026-01-28 13:06:03,729 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 07:36:03 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_45c72147bf233fa7f0d4d32d7c1ec822'), (b'openai-processing-ms', b'99'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'102'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4ed104ab585a23-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 13:06:03,730 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 13:06:03,731 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 13:06:03,731 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 13:06:03,732 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 13:06:03,732 | DEBUG | openai.agents | Exported 3 items
2026-01-28 13:06:03,734 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 13:06:03,734 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 13:06:03,735 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 13:06:03,736 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 13:06:03,736 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 13:06:04,213 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 07:36:03 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_c521ffaadc616c27512e8f6eb88249eb'), (b'openai-processing-ms', b'147'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'151'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4ed10b0f505a23-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 13:06:04,214 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 13:06:04,214 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 13:06:04,215 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 13:06:04,215 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 13:06:04,215 | DEBUG | openai.agents | Exported 2 items
2026-01-28 13:06:33,516 | INFO | chat | üì© Chat request | user_id=3 | request_id=a5ad8029-224f-42e8-b75a-873f2b459806
2026-01-28 13:06:33,517 | INFO | chat | üßë User message | where do  i studied in
2026-01-28 13:06:33,540 | INFO | chat | ü§ñ Stream started | session_id=ef5f092b-2ec0-4839-921b-a27aef540c8d | request_id=a5ad8029-224f-42e8-b75a-873f2b459806
2026-01-28 13:06:33,541 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-28 13:06:33,543 | INFO | orchestrator | üßë USER_INPUT | where do  i studied in
2026-01-28 13:06:33,545 | INFO | orchestrator | üß© CONTEXT | user_id=3 | enable_memory=True | db=None
2026-01-28 13:06:33,548 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-28 13:06:33,550 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_39ef59c803e840a29d1d9a6b0ac08720
2026-01-28 13:06:33,550 | DEBUG | openai.agents | Setting current trace: trace_39ef59c803e840a29d1d9a6b0ac08720
2026-01-28 13:06:33,551 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x00000102CC95A070> with id None
2026-01-28 13:06:33,551 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-28 13:06:33,553 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x00000102CC955BA0> with id None
2026-01-28 13:06:33,553 | DEBUG | openai.agents | Calling LLM
2026-01-28 13:06:33,554 | DEBUG | LiteLLM | 

2026-01-28 13:06:33,554 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 13:06:33,555 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'where do  i studied in'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 13:06:33,556 | DEBUG | LiteLLM | 

2026-01-28 13:06:33,556 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 13:06:33,557 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 13:06:33,558 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 13:06:33,559 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:06:33,560 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 13:06:33,561 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'where do  i studied in'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 13:06:33,562 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 13:06:33,563 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 13:06:33,563 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 13:06:33,564 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:06:33,565 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:06:33,565 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'where do  i studied in'}]}], 'system_instruction': {'parts': [{'text': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 13:06:34,941 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\n  \"safe\": false,\n  \"reason\": \"Cannot access personal information or user data.\",\n  \"message\": \"As an AI, I don't have access to your personal information, including where you've studied. You would know the answer to that question!\"\n}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 171,
    "candidatesTokenCount": 61,
    "totalTokenCount": 314,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 171
      }
    ],
    "thoughtsTokenCount": 82
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "grx5aa__BPX_juMPy4n16QE"
}



2026-01-28 13:06:34,942 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:06:34,944 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:06:34,944 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:06:34,945 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:06:34,946 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:06:34,947 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:06:34,947 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:06:34,949 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:06:34,950 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:06:34,951 | DEBUG | LiteLLM | response_cost: 0.00040880000000000007
2026-01-28 13:06:34,952 | DEBUG | openai.agents | Received model response
2026-01-28 13:06:34,951 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:06:34,953 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x00000102CCA1A980>>
2026-01-28 13:06:34,954 | DEBUG | LiteLLM | response_cost: 0.00040880000000000007
2026-01-28 13:06:34,954 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:06:34,956 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:06:34,957 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 13:06:34,957 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:06:34,957 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:06:34,958 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 13:06:34,960 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:06:34,960 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:06:34,962 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:06:34,963 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:06:34,964 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 13:06:34,964 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:06:34,965 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:06:34,969 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:06:34,970 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:06:34,972 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:06:34,972 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 13:06:34,973 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:06:34,974 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.00040880000000000007
2026-01-28 13:06:34,976 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:06:34,977 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:06:34,978 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:06:34,980 | DEBUG | openai.agents | Resetting current trace
2026-01-28 13:06:34,981 | WARNING | orchestrator | üö´ Safety blocked request
2026-01-28 13:06:34,989 | INFO | session_summary | üß† SUMMARY_START | session_id=ef5f092b-2ec0-4839-921b-a27aef540c8d
2026-01-28 13:06:34,993 | INFO | session_summary | üìä Unsummarized messages = 18
2026-01-28 13:06:35,000 | INFO | session_summary | üìÑ Existing summary = False
2026-01-28 13:06:35,005 | INFO | session_summary | üßæ Messages fetched = 12
2026-01-28 13:06:35,006 | INFO | session_summary | ‚úçÔ∏è Calling LLM for MERGED summary
2026-01-28 13:06:35,007 | DEBUG | LiteLLM | 

2026-01-28 13:06:35,007 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 13:06:35,008 | DEBUG | LiteLLM | [92mlitellm.completion(model='gemini/gemini-2.5-flash', provider='gemini', messages=[{'role': 'system', 'content': 'You are a precise memory compression engine.'}, {'role': 'user', 'content': '\nYou are a memory compression system.\n\nTASK:\nUpdate the existing conversation summary using the new dialogue below.\n\nRULES:\n- Preserve facts, decisions, preferences, and open tasks\n- Preserve key events and discussion topics\n- Remove chit-chat and repetition\n- Do NOT invent new information\n- Keep the summary concise and factual\n\nEXISTING SUMMARY:\nNone\n\nNEW DIALOGUE:\nUSER: where do i live?\nASSISTANT: I am an AI and do not have access to your personal information, including where you live.\nUSER: i lives in fatehabad\nASSISTANT: Thank you for letting me know.\nUSER: yesterday i saw a movie .. i like that movie..Dabang 2 is full of joy and drama. Its a thrillar movie.. I like to watch again and again\nASSISTANT: It sounds like you really enjoyed Dabangg 2 and appreciate its mix of joy, drama, and thrills!\nUSER: and one movie is Avengers, its also great movie.. I have also interest in Avengers\nASSISTANT: It sounds like you also really enjoy "Avengers"! It\'s definitely a popular and exciting movie.\nUSER: hello\nASSISTANT: Hello! How can I help you today?\nUSER: i want to know about theoy of relative\nASSISTANT: The theory of relativity, developed by Albert Einstein, is a groundbreaking theory in physics. It consists of two main parts:\n\n1.  **Special Relativity (1905):** Deals with the relationship between space and time for objects moving at a constant speed. Its most famous equation is E=mc¬≤, which shows that mass and energy are interchangeable. It also introduced concepts like time dilation (time moving slower for objects in motion) and length contraction.\n2.  **General Relativity (1915):** Deals with gravity. It describes gravity not as a force, but as a curvature of spacetime caused by the presence of mass and energy. This theory explains phenomena like the orbit of planets, the bending of light by massive objects, and the expansion of the universe.\n\nIn essence, it completely changed our understanding of space, time, gravity, and the universe.\n\nOUTPUT:\nUpdated summary only.\n'}], temperature=0.2, max_tokens=400)[0m
2026-01-28 13:06:35,009 | DEBUG | LiteLLM | 

2026-01-28 13:06:35,010 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 13:06:35,011 | DEBUG | LiteLLM | SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False
2026-01-28 13:06:35,015 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:06:35,015 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 13:06:35,016 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': 0.2, 'top_p': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': 400, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': 'You are a precise memory compression engine.'}, {'role': 'user', 'content': '\nYou are a memory compression system.\n\nTASK:\nUpdate the existing conversation summary using the new dialogue below.\n\nRULES:\n- Preserve facts, decisions, preferences, and open tasks\n- Preserve key events and discussion topics\n- Remove chit-chat and repetition\n- Do NOT invent new information\n- Keep the summary concise and factual\n\nEXISTING SUMMARY:\nNone\n\nNEW DIALOGUE:\nUSER: where do i live?\nASSISTANT: I am an AI and do not have access to your personal information, including where you live.\nUSER: i lives in fatehabad\nASSISTANT: Thank you for letting me know.\nUSER: yesterday i saw a movie .. i like that movie..Dabang 2 is full of joy and drama. Its a thrillar movie.. I like to watch again and again\nASSISTANT: It sounds like you really enjoyed Dabangg 2 and appreciate its mix of joy, drama, and thrills!\nUSER: and one movie is Avengers, its also great movie.. I have also interest in Avengers\nASSISTANT: It sounds like you also really enjoy "Avengers"! It\'s definitely a popular and exciting movie.\nUSER: hello\nASSISTANT: Hello! How can I help you today?\nUSER: i want to know about theoy of relative\nASSISTANT: The theory of relativity, developed by Albert Einstein, is a groundbreaking theory in physics. It consists of two main parts:\n\n1.  **Special Relativity (1905):** Deals with the relationship between space and time for objects moving at a constant speed. Its most famous equation is E=mc¬≤, which shows that mass and energy are interchangeable. It also introduced concepts like time dilation (time moving slower for objects in motion) and length contraction.\n2.  **General Relativity (1915):** Deals with gravity. It describes gravity not as a force, but as a curvature of spacetime caused by the presence of mass and energy. This theory explains phenomena like the orbit of planets, the bending of light by massive objects, and the expansion of the universe.\n\nIn essence, it completely changed our understanding of space, time, gravity, and the universe.\n\nOUTPUT:\nUpdated summary only.\n'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None, 'provider': 'gemini'}
2026-01-28 13:06:35,017 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'temperature': 0.2, 'max_tokens': 400}
2026-01-28 13:06:35,018 | DEBUG | LiteLLM | Final returned optional params: {'temperature': 0.2, 'max_output_tokens': 400, 'provider': 'gemini'}
2026-01-28 13:06:35,019 | DEBUG | LiteLLM | self.optional_params: {'temperature': 0.2, 'max_tokens': 400, 'provider': 'gemini'}
2026-01-28 13:06:35,019 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:06:35,020 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:06:35,021 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '\nYou are a memory compression system.\n\nTASK:\nUpdate the existing conversation summary using the new dialogue below.\n\nRULES:\n- Preserve facts, decisions, preferences, and open tasks\n- Preserve key events and discussion topics\n- Remove chit-chat and repetition\n- Do NOT invent new information\n- Keep the summary concise and factual\n\nEXISTING SUMMARY:\nNone\n\nNEW DIALOGUE:\nUSER: where do i live?\nASSISTANT: I am an AI and do not have access to your personal information, including where you live.\nUSER: i lives in fatehabad\nASSISTANT: Thank you for letting me know.\nUSER: yesterday i saw a movie .. i like that movie..Dabang 2 is full of joy and drama. Its a thrillar movie.. I like to watch again and again\nASSISTANT: It sounds like you really enjoyed Dabangg 2 and appreciate its mix of joy, drama, and thrills!\nUSER: and one movie is Avengers, its also great movie.. I have also interest in Avengers\nASSISTANT: It sounds like you also really enjoy "Avengers"! It\'s definitely a popular and exciting movie.\nUSER: hello\nASSISTANT: Hello! How can I help you today?\nUSER: i want to know about theoy of relative\nASSISTANT: The theory of relativity, developed by Albert Einstein, is a groundbreaking theory in physics. It consists of two main parts:\n\n1.  **Special Relativity (1905):** Deals with the relationship between space and time for objects moving at a constant speed. Its most famous equation is E=mc¬≤, which shows that mass and energy are interchangeable. It also introduced concepts like time dilation (time moving slower for objects in motion) and length contraction.\n2.  **General Relativity (1915):** Deals with gravity. It describes gravity not as a force, but as a curvature of spacetime caused by the presence of mass and energy. This theory explains phenomena like the orbit of planets, the bending of light by massive objects, and the expansion of the universe.\n\nIn essence, it completely changed our understanding of space, time, gravity, and the universe.\n\nOUTPUT:\nUpdated summary only.\n'}]}], 'system_instruction': {'parts': [{'text': 'You are a precise memory compression engine.'}]}, 'generationConfig': {'temperature': 0.2, 'max_output_tokens': 400}}'
[0m

2026-01-28 13:06:35,023 | DEBUG | httpcore.connection | connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None
2026-01-28 13:06:35,033 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000102CCA18EE0>
2026-01-28 13:06:35,033 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x00000102CC26E1C0> server_hostname='generativelanguage.googleapis.com' timeout=600.0
2026-01-28 13:06:35,044 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000102CCA18D30>
2026-01-28 13:06:35,047 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 13:06:35,049 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 13:06:35,049 | DEBUG | httpcore.connection | close.started
2026-01-28 13:06:35,050 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 13:06:35,050 | DEBUG | httpcore.connection | close.complete
2026-01-28 13:06:35,051 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 13:06:35,051 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 13:06:35,052 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 13:06:35,113 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000102CD01E890>
2026-01-28 13:06:35,113 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x00000102C0F79A40> server_hostname='api.openai.com' timeout=5.0
2026-01-28 13:06:35,128 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000102CD01CA30>
2026-01-28 13:06:35,128 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 13:06:35,129 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 13:06:35,129 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 13:06:35,130 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 13:06:35,130 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 13:06:35,575 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 07:36:34 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_450aeacb8128cffc89700a12737d9a93'), (b'openai-processing-ms', b'130'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'133'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4ed1cf28de54e8-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 13:06:35,577 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 13:06:35,577 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 13:06:35,578 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 13:06:35,579 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 13:06:35,579 | DEBUG | openai.agents | Exported 3 items
2026-01-28 13:06:37,202 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Wed, 28 Jan 2026 07:36:36 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=2143'), (b'Alt-Svc', b'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])
2026-01-28 13:06:37,204 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 13:06:37,205 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 13:06:37,205 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 13:06:37,206 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 13:06:37,207 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "The user lives in Fatehabad. They enjoy movies, specifically \"Dabangg 2\" (liking its joy, drama, and thriller aspects, and wanting to rewatch it) and \"Avengers.\" The user also inquired about the theory of relativity, which the assistant explained by detailing Special Relativity (including E=mc¬≤, time dilation, and length contraction) and General Relativity (describing gravity as spacetime curvature)."
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 476,
    "candidatesTokenCount": 85,
    "totalTokenCount": 731,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 476
      }
    ],
    "thoughtsTokenCount": 170
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "hLx5aeT8FNvVqfkPl_2jqAM"
}



2026-01-28 13:06:37,210 | INFO | LiteLLM | Wrapper: Completed Call, calling success_handler
2026-01-28 13:06:37,212 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:06:37,212 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:06:37,213 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:06:37,214 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:06:37,215 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:06:37,216 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:06:37,217 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:06:37,218 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:06:37,219 | DEBUG | LiteLLM | response_cost: 0.0007803
2026-01-28 13:06:37,220 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:06:37,222 | DEBUG | LiteLLM | response_cost: 0.0007803
2026-01-28 13:06:37,221 | INFO | session_summary | üÜï CREATING NEW SUMMARY ROW
2026-01-28 13:06:37,225 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:06:37,230 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:06:37,231 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:06:37,233 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 13:06:37,240 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:06:37,242 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:06:37,243 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:06:37,251 | INFO | session_summary | ‚úÖ SUMMARY_MERGED_SUCCESSFULLY
2026-01-28 13:06:37,253 | INFO | chat | ‚úÖ Stream complete | tokens=135 | time=3.74s
2026-01-28 13:07:43,265 | DEBUG | LiteLLM | LoggingWorker cancelled during shutdown
2026-01-28 13:07:43,267 | DEBUG | LiteLLM | [LoggingWorker] atexit: Queue is empty
2026-01-28 13:07:43,267 | DEBUG | asyncio | Using proactor: IocpProactor
2026-01-28 13:07:43,268 | DEBUG | openai.agents | Shutting down trace provider
2026-01-28 13:07:43,268 | DEBUG | openai.agents | Shutting down trace processor <agents.tracing.processors.BatchTraceProcessor object at 0x00000102C0F7F9D0>
2026-01-28 13:07:43,565 | ERROR | asyncio | Fatal error on SSL transport
protocol: <asyncio.sslproto.SSLProtocol object at 0x00000102CC22C2E0>
transport: <_SelectorSocketTransport closing fd=4064>
Traceback (most recent call last):
  File "C:\Users\DELL\AppData\Local\Programs\Python\Python310\lib\asyncio\selector_events.py", line 924, in write
    n = self._sock.send(data)
OSError: [WinError 10038] An operation was attempted on something that is not a socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\DELL\AppData\Local\Programs\Python\Python310\lib\asyncio\sslproto.py", line 690, in _process_write_backlog
    self._transport.write(chunk)
  File "C:\Users\DELL\AppData\Local\Programs\Python\Python310\lib\asyncio\selector_events.py", line 930, in write
    self._fatal_error(exc, 'Fatal write error on socket transport')
  File "C:\Users\DELL\AppData\Local\Programs\Python\Python310\lib\asyncio\selector_events.py", line 725, in _fatal_error
    self._force_close(exc)
  File "C:\Users\DELL\AppData\Local\Programs\Python\Python310\lib\asyncio\selector_events.py", line 737, in _force_close
    self._loop.call_soon(self._call_connection_lost, exc)
  File "C:\Users\DELL\AppData\Local\Programs\Python\Python310\lib\asyncio\base_events.py", line 753, in call_soon
    self._check_closed()
  File "C:\Users\DELL\AppData\Local\Programs\Python\Python310\lib\asyncio\base_events.py", line 515, in _check_closed
    raise RuntimeError('Event loop is closed')
RuntimeError: Event loop is closed
2026-01-28 13:07:43,568 | ERROR | asyncio | Fatal error on SSL transport
protocol: <asyncio.sslproto.SSLProtocol object at 0x00000102CCA1A200>
transport: <_SelectorSocketTransport closing fd=4108>
Traceback (most recent call last):
  File "C:\Users\DELL\AppData\Local\Programs\Python\Python310\lib\asyncio\selector_events.py", line 924, in write
    n = self._sock.send(data)
OSError: [WinError 10038] An operation was attempted on something that is not a socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\DELL\AppData\Local\Programs\Python\Python310\lib\asyncio\sslproto.py", line 690, in _process_write_backlog
    self._transport.write(chunk)
  File "C:\Users\DELL\AppData\Local\Programs\Python\Python310\lib\asyncio\selector_events.py", line 930, in write
    self._fatal_error(exc, 'Fatal write error on socket transport')
  File "C:\Users\DELL\AppData\Local\Programs\Python\Python310\lib\asyncio\selector_events.py", line 725, in _fatal_error
    self._force_close(exc)
  File "C:\Users\DELL\AppData\Local\Programs\Python\Python310\lib\asyncio\selector_events.py", line 737, in _force_close
    self._loop.call_soon(self._call_connection_lost, exc)
  File "C:\Users\DELL\AppData\Local\Programs\Python\Python310\lib\asyncio\base_events.py", line 753, in call_soon
    self._check_closed()
  File "C:\Users\DELL\AppData\Local\Programs\Python\Python310\lib\asyncio\base_events.py", line 515, in _check_closed
    raise RuntimeError('Event loop is closed')
RuntimeError: Event loop is closed
2026-01-28 13:07:43,799 | DEBUG | httpcore.connection | close.started
2026-01-28 13:07:43,799 | DEBUG | httpcore.connection | close.complete
2026-01-28 13:07:55,344 | INFO | sentence_transformers.SentenceTransformer | Use pytorch device_name: cpu
2026-01-28 13:07:55,345 | INFO | sentence_transformers.SentenceTransformer | Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2026-01-28 13:07:55,350 | DEBUG | urllib3.connectionpool | Starting new HTTPS connection (1): huggingface.co:443
2026-01-28 13:07:55,669 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-28 13:07:55,684 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-28 13:07:55,974 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-28 13:07:56,043 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-28 13:07:56,592 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-28 13:07:56,603 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-28 13:07:56,889 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md HTTP/1.1" 307 0
2026-01-28 13:07:56,904 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/README.md HTTP/1.1" 200 0
2026-01-28 13:07:57,506 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-28 13:07:57,517 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-28 13:07:57,757 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json HTTP/1.1" 307 0
2026-01-28 13:07:57,769 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/sentence_bert_config.json HTTP/1.1" 200 0
2026-01-28 13:07:58,208 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json HTTP/1.1" 404 0
2026-01-28 13:07:58,439 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1" 307 0
2026-01-28 13:07:58,452 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json HTTP/1.1" 200 0
2026-01-28 13:07:58,755 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/1.1" 307 0
2026-01-28 13:07:58,766 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json HTTP/1.1" 200 0
2026-01-28 13:07:59,010 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1" 404 64
2026-01-28 13:07:59,263 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main?recursive=True&expand=False HTTP/1.1" 200 6465
2026-01-28 13:07:59,573 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json HTTP/1.1" 307 0
2026-01-28 13:07:59,584 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json HTTP/1.1" 200 0
2026-01-28 13:07:59,831 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2 HTTP/1.1" 200 6885
2026-01-28 13:08:00,845 | INFO | main | üöÄ FastAPI application starting
2026-01-28 13:09:26,634 | INFO | chat | üì© Chat request | user_id=3 | request_id=8697c28e-ed36-496d-9e5f-da6350f6eb02
2026-01-28 13:09:26,676 | INFO | chat | üßë User message | hello
2026-01-28 13:09:26,706 | INFO | chat | ü§ñ Stream started | session_id=ef5f092b-2ec0-4839-921b-a27aef540c8d | request_id=8697c28e-ed36-496d-9e5f-da6350f6eb02
2026-01-28 13:09:26,708 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-28 13:09:26,709 | INFO | orchestrator | üßë USER_INPUT | hello
2026-01-28 13:09:26,711 | INFO | orchestrator | üß© CONTEXT | user_id=3 | enable_memory=True | db=None
2026-01-28 13:09:26,712 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-28 13:09:26,715 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_ab32596e75894ddd8cc1389b9313e3f3
2026-01-28 13:09:26,716 | DEBUG | openai.agents | Setting current trace: trace_ab32596e75894ddd8cc1389b9313e3f3
2026-01-28 13:09:26,721 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x0000025FE8B8D6C0> with id None
2026-01-28 13:09:26,721 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-28 13:09:27,850 | DEBUG | httpcore.connection | connect_tcp.started host='raw.githubusercontent.com' port=443 local_address=None timeout=5 socket_options=None
2026-01-28 13:09:27,891 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000025FE944F280>
2026-01-28 13:09:27,892 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000025FE93ECCC0> server_hostname='raw.githubusercontent.com' timeout=5
2026-01-28 13:09:27,934 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000025FE944F250>
2026-01-28 13:09:27,935 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'GET']>
2026-01-28 13:09:27,936 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 13:09:27,936 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'GET']>
2026-01-28 13:09:27,937 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 13:09:27,937 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'GET']>
2026-01-28 13:09:27,973 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Connection', b'keep-alive'), (b'Content-Length', b'62476'), (b'Cache-Control', b'max-age=300'), (b'Content-Security-Policy', b"default-src 'none'; style-src 'unsafe-inline'; sandbox"), (b'Content-Type', b'text/plain; charset=utf-8'), (b'ETag', b'W/"f6dad8a5d9e28465b3d70ca31700cb6ef9bf2cd7c9d0fa0c0c5cdf23a1bb3382"'), (b'Strict-Transport-Security', b'max-age=31536000'), (b'X-Content-Type-Options', b'nosniff'), (b'X-Frame-Options', b'deny'), (b'X-XSS-Protection', b'1; mode=block'), (b'X-GitHub-Request-Id', b'2442:60E63:51359:95C7D:6978AD63'), (b'Content-Encoding', b'gzip'), (b'Accept-Ranges', b'bytes'), (b'Date', b'Wed, 28 Jan 2026 07:39:27 GMT'), (b'Via', b'1.1 varnish'), (b'X-Served-By', b'cache-bom-vanm7210088-BOM'), (b'X-Cache', b'HIT'), (b'X-Cache-Hits', b'35'), (b'X-Timer', b'S1769585967.283081,VS0,VE0'), (b'Vary', b'Authorization,Accept-Encoding'), (b'Access-Control-Allow-Origin', b'*'), (b'Cross-Origin-Resource-Policy', b'cross-origin'), (b'X-Fastly-Request-ID', b'ecca08f0a0acee3176981bb2fce5ce691c200663'), (b'Expires', b'Wed, 28 Jan 2026 07:44:27 GMT'), (b'Source-Age', b'175')])
2026-01-28 13:09:27,975 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'GET']>
2026-01-28 13:09:28,021 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 13:09:28,022 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 13:09:28,023 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 13:09:28,024 | DEBUG | httpcore.connection | close.started
2026-01-28 13:09:28,025 | DEBUG | httpcore.connection | close.complete
2026-01-28 13:09:29,293 | DEBUG | LiteLLM | [Non-Blocking] Unable to import GenericAPILogger - LiteLLM Enterprise Feature - No module named 'litellm_enterprise'
2026-01-28 13:09:30,481 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-28 13:09:30,482 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-28 13:09:30,483 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-28 13:09:30,502 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-28 13:09:30,503 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-28 13:09:30,503 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-28 13:09:30,504 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-28 13:09:30,505 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-28 13:09:30,505 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-28 13:09:30,507 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-28 13:09:30,507 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-28 13:09:30,508 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-28 13:09:30,536 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000025FE8BD09A0> with id None
2026-01-28 13:09:30,536 | DEBUG | openai.agents | Calling LLM
2026-01-28 13:09:30,537 | DEBUG | LiteLLM | 

2026-01-28 13:09:30,537 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 13:09:30,538 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'hello'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 13:09:30,539 | DEBUG | LiteLLM | 

2026-01-28 13:09:30,540 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 13:09:30,541 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 13:09:30,542 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 13:09:30,561 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:09:30,562 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 13:09:30,564 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'hello'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 13:09:30,566 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 13:09:30,568 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 13:09:30,568 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 13:09:30,569 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:09:31,013 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:09:31,014 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'hello'}]}], 'system_instruction': {'parts': [{'text': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 13:09:31,852 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 13:09:31,875 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000025FF2490F70>
2026-01-28 13:09:31,880 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000025FE6FC9BC0> server_hostname='api.openai.com' timeout=5.0
2026-01-28 13:09:31,892 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000025FF2490F40>
2026-01-28 13:09:31,892 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 13:09:31,893 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 13:09:31,893 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 13:09:31,894 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 13:09:31,894 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 13:09:32,517 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "```json\n{\n  \"safe\": true,\n  \"reason\": null,\n  \"message\": null\n}\n```"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 166,
    "candidatesTokenCount": 28,
    "totalTokenCount": 229,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 166
      }
    ],
    "thoughtsTokenCount": 35
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "M715aeHWKcK6qfkPp-DF4QM"
}



2026-01-28 13:09:32,520 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:09:32,521 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:09:32,522 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:09:32,522 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:09:32,523 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:09:32,523 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:09:32,524 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:09:32,524 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:09:32,524 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:09:32,525 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:09:32,525 | DEBUG | LiteLLM | response_cost: 0.00020730000000000002
2026-01-28 13:09:32,526 | DEBUG | LiteLLM | response_cost: 0.00020730000000000002
2026-01-28 13:09:32,526 | DEBUG | openai.agents | Received model response
2026-01-28 13:09:32,527 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:09:32,536 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x0000025FF24526B0>>
2026-01-28 13:09:32,537 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:09:32,538 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:09:32,538 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:09:32,539 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 13:09:32,540 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:09:32,541 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 13:09:32,540 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:09:32,541 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:09:32,541 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 13:09:32,541 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:09:32,543 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:09:32,543 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:09:32,544 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:09:32,544 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:09:32,545 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:09:32,545 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:09:32,546 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 13:09:32,546 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.00020730000000000002
2026-01-28 13:09:32,547 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:09:32,550 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:09:32,551 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:09:32,552 | DEBUG | openai.agents | Resetting current trace
2026-01-28 13:09:32,553 | INFO | orchestrator | ‚úÖ Safety passed
2026-01-28 13:09:32,553 | INFO | orchestrator | üì• Fetching semantic memory (via MemoryService)
2026-01-28 13:09:32,631 | INFO | orchestrator | üß† MEMORY_RESULT | found=False | count=0
2026-01-28 13:09:32,633 | INFO | orchestrator | üß≠ Router agent called
2026-01-28 13:09:32,634 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_49065dfa7ada4b188a2589c1e8dfb4a2
2026-01-28 13:09:32,634 | DEBUG | openai.agents | Setting current trace: trace_49065dfa7ada4b188a2589c1e8dfb4a2
2026-01-28 13:09:32,635 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x0000025FF31AE020> with id None
2026-01-28 13:09:32,635 | DEBUG | openai.agents | Running agent RouterAgent (turn 1)
2026-01-28 13:09:32,635 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000025FF2462920> with id None
2026-01-28 13:09:32,636 | DEBUG | openai.agents | Calling LLM
2026-01-28 13:09:32,636 | DEBUG | LiteLLM | 

2026-01-28 13:09:32,636 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 13:09:32,636 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'hello'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 13:09:32,637 | DEBUG | LiteLLM | 

2026-01-28 13:09:32,637 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 13:09:32,638 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 13:09:32,638 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 13:09:32,639 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:09:32,639 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 13:09:32,640 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'hello'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 13:09:32,640 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 13:09:32,641 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 13:09:32,641 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 13:09:32,641 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:09:32,642 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:09:32,643 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'hello'}]}], 'system_instruction': {'parts': [{'text': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 13:09:33,158 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 07:39:32 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_0e3d0369e565ef78294362879e0a3d76'), (b'openai-processing-ms', b'89'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'92'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=jNB.T7GHcW_O.FBTvXAkG1M8U7HNrMdfe8qTB6qXUSA-1769585972-1.0.1.1-TPZ2iRQyEJ6DWXoOEeNOxkm05A6URqL8NsokiKD8pFENeglQEKMjhxLQac1Kw65ejEaNg_2Osz6PWQZKzwhtwmmpYib.5hRbG1pbz0rTIfU; path=/; expires=Wed, 28-Jan-26 08:09:32 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=512JPaKBWaHMyeO3ABNMNC.QGClLfUNMz81yA0oujKk-1769585972454-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4ed61ffe8a59ac-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 13:09:33,158 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 13:09:33,158 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 13:09:33,159 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 13:09:33,159 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 13:09:33,160 | DEBUG | openai.agents | Exported 1 items
2026-01-28 13:09:33,160 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 13:09:33,161 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 13:09:33,161 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 13:09:33,161 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 13:09:33,161 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 13:09:33,575 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 07:39:32 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_a4a64e1e3bad5010f5b599ff11eb701d'), (b'openai-processing-ms', b'87'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'90'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4ed627ea0659ac-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 13:09:33,575 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 13:09:33,575 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 13:09:33,576 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 13:09:33,576 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 13:09:33,576 | DEBUG | openai.agents | Exported 3 items
2026-01-28 13:09:34,253 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\n  \"use_memory\": false,\n  \"use_tool\": false,\n  \"tool_name\": \"none\",\n  \"tool_arguments\": null,\n  \"intent\": \"none\",\n  \"memory_key\": null\n}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 486,
    "candidatesTokenCount": 56,
    "totalTokenCount": 568,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 486
      }
    ],
    "thoughtsTokenCount": 26
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "Nb15aYGJF-zVjuMPy-6C4QM"
}



2026-01-28 13:09:34,256 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:09:34,257 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:09:34,257 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:09:34,258 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:09:34,259 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:09:34,260 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:09:34,261 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:09:34,262 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:09:34,263 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:09:34,264 | DEBUG | LiteLLM | response_cost: 0.0003508
2026-01-28 13:09:34,265 | DEBUG | openai.agents | Received model response
2026-01-28 13:09:34,265 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:09:34,267 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x0000025FF2493BE0>>
2026-01-28 13:09:34,267 | DEBUG | LiteLLM | response_cost: 0.0003508
2026-01-28 13:09:34,268 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:09:34,270 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:09:34,272 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 13:09:34,271 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:09:34,272 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:09:34,273 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 13:09:34,275 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:09:34,275 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:09:34,277 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:09:34,278 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:09:34,279 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 13:09:34,280 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:09:34,280 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:09:34,282 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:09:34,282 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:09:34,287 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:09:34,288 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 13:09:34,288 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:09:34,289 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0003508
2026-01-28 13:09:34,290 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:09:34,291 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:09:34,292 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:09:34,293 | DEBUG | openai.agents | Resetting current trace
2026-01-28 13:09:34,294 | INFO | orchestrator | üß≠ ROUTER_DECISION | tool=False | memory=False | intent=none
2026-01-28 13:09:34,295 | INFO | orchestrator | üß† REASONING_INPUT_SNAPSHOT
{
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful AI assistant.\n \nYou may answer ANY general knowledge question.\n \nConversation summaries and memories are ONLY to provide helpful context\n(e.g. preferences, ongoing plans).\n \nIf a user asks something unrelated to the current topic,\nanswer it normally using general knowledge.\n \nDo NOT restrict yourself to previous topics unless the user explicitly asks."
    },
    {
      "role": "system",
      "content": "Conversation summary:\nThe user lives in Fatehabad. They enjoy movies, specifically \"Dabangg 2\" (liking its joy, drama, and thriller aspects, and wanting to rewatch it) and \"Avengers.\" The user also inquired about the theory of relativity, which the assistant explained by detailing Special Relativity (including E=mc\u00b2, time dilation, and length contraction) and General Relativity (describing gravity as spacetime curvature)."
    },
    {
      "role": "assistant",
      "content": "I cannot tell you where you live, as I do not have access to your personal information. You are the only one who knows that."
    },
    {
      "role": "user",
      "content": "what is my favourite color?"
    },
    {
      "role": "assistant",
      "content": "Your favorite color is yellow."
    },
    {
      "role": "user",
      "content": "where do  i studied in"
    },
    {
      "role": "assistant",
      "content": "As an AI, I don't have access to your personal information, including where you've studied. You would know the answer to that question!"
    },
    {
      "role": "user",
      "content": "hello"
    },
    {
      "role": "user",
      "content": "hello"
    }
  ],
  "safety": {
    "safe": true,
    "reason": null,
    "message": "OK"
  },
  "memory_action": {},
  "memory_data": [],
  "memory_found": false,
  "tool_context": {}
}
2026-01-28 13:09:34,298 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_d98ee85746044ab0be58af8d8b37c0a1
2026-01-28 13:09:34,301 | DEBUG | openai.agents | Setting current trace: trace_d98ee85746044ab0be58af8d8b37c0a1
2026-01-28 13:09:34,303 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x0000025FF31AF920> with id None
2026-01-28 13:09:34,304 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000025FF31C4100> with id None
2026-01-28 13:09:34,304 | DEBUG | openai.agents | Calling LLM
2026-01-28 13:09:34,305 | DEBUG | LiteLLM | 

2026-01-28 13:09:34,305 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 13:09:34,306 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "system", "content": "Conversation summary:\\nThe user lives in Fatehabad. They enjoy movies, specifically \\"Dabangg 2\\" (liking its joy, drama, and thriller aspects, and wanting to rewatch it) and \\"Avengers.\\" The user also inquired about the theory of relativity, which the assistant explained by detailing Special Relativity (including E=mc\\u00b2, time dilation, and length contraction) and General Relativity (describing gravity as spacetime curvature)."}, {"role": "assistant", "content": "I cannot tell you where you live, as I do not have access to your personal information. You are the only one who knows that."}, {"role": "user", "content": "what is my favourite color?"}, {"role": "assistant", "content": "Your favorite color is yellow."}, {"role": "user", "content": "where do  i studied in"}, {"role": "assistant", "content": "As an AI, I don\'t have access to your personal information, including where you\'ve studied. You would know the answer to that question!"}, {"role": "user", "content": "hello"}, {"role": "user", "content": "hello"}], "safety": {"safe": true, "reason": null, "message": "OK"}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=True, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 13:09:34,307 | DEBUG | LiteLLM | 

2026-01-28 13:09:34,307 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 13:09:34,308 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 13:09:34,308 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 13:09:34,310 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:09:34,311 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 13:09:34,312 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "system", "content": "Conversation summary:\\nThe user lives in Fatehabad. They enjoy movies, specifically \\"Dabangg 2\\" (liking its joy, drama, and thriller aspects, and wanting to rewatch it) and \\"Avengers.\\" The user also inquired about the theory of relativity, which the assistant explained by detailing Special Relativity (including E=mc\\u00b2, time dilation, and length contraction) and General Relativity (describing gravity as spacetime curvature)."}, {"role": "assistant", "content": "I cannot tell you where you live, as I do not have access to your personal information. You are the only one who knows that."}, {"role": "user", "content": "what is my favourite color?"}, {"role": "assistant", "content": "Your favorite color is yellow."}, {"role": "user", "content": "where do  i studied in"}, {"role": "assistant", "content": "As an AI, I don\'t have access to your personal information, including where you\'ve studied. You would know the answer to that question!"}, {"role": "user", "content": "hello"}, {"role": "user", "content": "hello"}], "safety": {"safe": true, "reason": null, "message": "OK"}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 13:09:34,313 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': True}
2026-01-28 13:09:34,314 | DEBUG | LiteLLM | Final returned optional params: {'stream': True}
2026-01-28 13:09:34,315 | DEBUG | LiteLLM | self.optional_params: {'stream': True}
2026-01-28 13:09:34,317 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:09:34,319 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:09:34,320 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?key=*****=sse \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "system", "content": "Conversation summary:\\nThe user lives in Fatehabad. They enjoy movies, specifically \\"Dabangg 2\\" (liking its joy, drama, and thriller aspects, and wanting to rewatch it) and \\"Avengers.\\" The user also inquired about the theory of relativity, which the assistant explained by detailing Special Relativity (including E=mc\\u00b2, time dilation, and length contraction) and General Relativity (describing gravity as spacetime curvature)."}, {"role": "assistant", "content": "I cannot tell you where you live, as I do not have access to your personal information. You are the only one who knows that."}, {"role": "user", "content": "what is my favourite color?"}, {"role": "assistant", "content": "Your favorite color is yellow."}, {"role": "user", "content": "where do  i studied in"}, {"role": "assistant", "content": "As an AI, I don\'t have access to your personal information, including where you\'ve studied. You would know the answer to that question!"}, {"role": "user", "content": "hello"}, {"role": "user", "content": "hello"}], "safety": {"safe": true, "reason": null, "message": "OK"}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}]}], 'system_instruction': {'parts': [{'text': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.'}]}, 'generationConfig': {}}'
[0m

2026-01-28 13:09:34,322 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:09:34,325 | DEBUG | LiteLLM | messages in token_counter: None, text in token_counter: 
2026-01-28 13:09:34,326 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:09:34,327 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:09:34,327 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:09:34,328 | DEBUG | LiteLLM | response_cost: 0.0
2026-01-28 13:09:34,429 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-28 13:09:34,430 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-28 13:09:34,430 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-28 13:09:35,434 | DEBUG | LiteLLM | RAW RESPONSE:
first stream response received


2026-01-28 13:09:35,436 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': 'Hello there! How can I help you today?'}], 'role': 'model'}, 'finishReason': 'STOP', 'index': 0}], 'usageMetadata': {'promptTokenCount': 515, 'candidatesTokenCount': 10, 'totalTokenCount': 545, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 515}], 'thoughtsTokenCount': 20}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'Nr15afzIG52s4-EPxdmKoAQ'}
2026-01-28 13:09:35,443 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='Nr15afzIG52s4-EPxdmKoAQ', created=1769585975, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='Hello there! How can I help you today?', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=30, prompt_tokens=515, total_tokens=545, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=20, rejected_prediction_tokens=None, text_tokens=10, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=515, image_tokens=None)))
2026-01-28 13:09:35,448 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content='Hello there! How can I help you today?', role='assistant', function_call=None, tool_calls=None, audio=None)
2026-01-28 13:09:35,451 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='Nr15afzIG52s4-EPxdmKoAQ', created=1769585975, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Hello there! How can I help you today?', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=30, prompt_tokens=515, total_tokens=545, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=20, rejected_prediction_tokens=None, text_tokens=10, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=515, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-28 13:09:35,500 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=False
2026-01-28 13:09:35,500 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=False
2026-01-28 13:09:35,501 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 13:09:35,501 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 13:09:35,501 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:09:35,502 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:09:35,502 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:09:35,503 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:09:35,503 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:09:35,503 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:09:35,504 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:09:35,504 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:09:35,504 | DEBUG | LiteLLM | response_cost: 0.0002295
2026-01-28 13:09:35,505 | DEBUG | LiteLLM | response_cost: 0.0002295
2026-01-28 13:09:35,505 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:09:35,506 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0002295
2026-01-28 13:09:35,506 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:09:35,507 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:09:35,507 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:09:35,508 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:09:35,509 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:09:35,510 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 13:09:35,511 | DEBUG | openai.agents | Resetting current trace
2026-01-28 13:09:35,527 | INFO | session_summary | üß† SUMMARY_START | session_id=ef5f092b-2ec0-4839-921b-a27aef540c8d
2026-01-28 13:09:35,538 | INFO | session_summary | üìä Unsummarized messages = 14
2026-01-28 13:09:35,538 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-28 13:09:35,538 | INFO | chat | ‚úÖ Stream complete | tokens=1 | time=8.9s
2026-01-28 13:09:38,722 | DEBUG | httpcore.connection | close.started
2026-01-28 13:09:38,723 | DEBUG | httpcore.connection | close.complete
2026-01-28 13:09:38,723 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 13:09:38,751 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000025FF3182CB0>
2026-01-28 13:09:38,751 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000025FE6FC9BC0> server_hostname='api.openai.com' timeout=5.0
2026-01-28 13:09:38,765 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000025FF3182CE0>
2026-01-28 13:09:38,765 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 13:09:38,766 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 13:09:38,766 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 13:09:38,767 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 13:09:38,767 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 13:09:39,215 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 07:39:38 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_ed517e4f6ce631c4b70b32ff67ff8661'), (b'openai-processing-ms', b'122'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'125'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4ed64aeba1d97b-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 13:09:39,216 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 13:09:39,217 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 13:09:39,217 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 13:09:39,218 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 13:09:39,218 | DEBUG | openai.agents | Exported 5 items
2026-01-28 13:09:45,930 | INFO | chat | üì© Chat request | user_id=3 | request_id=3bdbeef1-2caa-4583-9df4-6a66b46011ab
2026-01-28 13:09:45,931 | INFO | chat | üßë User message | what is my favourite color?
2026-01-28 13:09:45,954 | INFO | chat | ü§ñ Stream started | session_id=ef5f092b-2ec0-4839-921b-a27aef540c8d | request_id=3bdbeef1-2caa-4583-9df4-6a66b46011ab
2026-01-28 13:09:45,955 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-28 13:09:45,957 | INFO | orchestrator | üßë USER_INPUT | what is my favourite color?
2026-01-28 13:09:45,958 | INFO | orchestrator | üß© CONTEXT | user_id=3 | enable_memory=True | db=None
2026-01-28 13:09:45,959 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-28 13:09:45,960 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_e83ad5a6bf7f4533be731d17a0848d68
2026-01-28 13:09:45,963 | DEBUG | openai.agents | Setting current trace: trace_e83ad5a6bf7f4533be731d17a0848d68
2026-01-28 13:09:45,964 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x0000025FF31AD8F0> with id None
2026-01-28 13:09:45,964 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-28 13:09:45,966 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000025FF31C6020> with id None
2026-01-28 13:09:45,966 | DEBUG | openai.agents | Calling LLM
2026-01-28 13:09:45,966 | DEBUG | LiteLLM | 

2026-01-28 13:09:45,968 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 13:09:45,968 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'what is my favourite color?'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 13:09:45,969 | DEBUG | LiteLLM | 

2026-01-28 13:09:45,969 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 13:09:45,970 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 13:09:45,971 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 13:09:45,973 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:09:45,973 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 13:09:45,975 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'what is my favourite color?'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 13:09:45,976 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 13:09:45,976 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 13:09:45,977 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 13:09:45,978 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:09:45,981 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:09:45,984 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'what is my favourite color?'}]}], 'system_instruction': {'parts': [{'text': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 13:09:49,390 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "```json\n{\n  \"safe\": false,\n  \"reason\": \"Cannot know personal preferences\",\n  \"message\": \"As an AI, I don't have access to personal information or your preferences, so I can't know your favorite color.\"\n}\n```"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 171,
    "candidatesTokenCount": 59,
    "totalTokenCount": 474,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 171
      }
    ],
    "thoughtsTokenCount": 244
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "RL15aY-UHa664-EPi9magAQ"
}



2026-01-28 13:09:49,393 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:09:49,395 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:09:49,395 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:09:49,396 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:09:49,397 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:09:49,398 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:09:49,399 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:09:49,400 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:09:49,401 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:09:49,402 | DEBUG | LiteLLM | response_cost: 0.0008088000000000001
2026-01-28 13:09:49,403 | DEBUG | openai.agents | Received model response
2026-01-28 13:09:49,403 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:09:49,404 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x0000025FF24910F0>>
2026-01-28 13:09:49,405 | DEBUG | LiteLLM | response_cost: 0.0008088000000000001
2026-01-28 13:09:49,405 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:09:49,408 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:09:49,409 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 13:09:49,408 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:09:49,409 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:09:49,410 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 13:09:49,412 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:09:49,414 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:09:49,417 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:09:49,417 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:09:49,418 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 13:09:49,419 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:09:49,419 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:09:49,421 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:09:49,422 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:09:49,423 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:09:49,424 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 13:09:49,425 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:09:49,425 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0008088000000000001
2026-01-28 13:09:49,427 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:09:49,428 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:09:49,430 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:09:49,433 | DEBUG | openai.agents | Resetting current trace
2026-01-28 13:09:49,434 | WARNING | orchestrator | üö´ Safety blocked request
2026-01-28 13:09:49,438 | INFO | session_summary | üß† SUMMARY_START | session_id=ef5f092b-2ec0-4839-921b-a27aef540c8d
2026-01-28 13:09:49,441 | INFO | session_summary | üìä Unsummarized messages = 16
2026-01-28 13:09:49,443 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-28 13:09:49,445 | INFO | chat | ‚úÖ Stream complete | tokens=0 | time=3.51s
2026-01-28 13:09:49,503 | DEBUG | httpcore.connection | close.started
2026-01-28 13:09:49,504 | DEBUG | httpcore.connection | close.complete
2026-01-28 13:09:49,505 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 13:09:49,514 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000025FE8AD45E0>
2026-01-28 13:09:49,514 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000025FE6FC9BC0> server_hostname='api.openai.com' timeout=5.0
2026-01-28 13:09:49,528 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000025FE8AD4FD0>
2026-01-28 13:09:49,528 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 13:09:49,529 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 13:09:49,530 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 13:09:49,531 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 13:09:49,531 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 13:09:49,960 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 07:39:49 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_44956722978571f6ab3539faca3dd152'), (b'openai-processing-ms', b'109'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'114'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4ed68e2ad1dc7f-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 13:09:49,962 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 13:09:49,962 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 13:09:49,963 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 13:09:49,963 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 13:09:49,964 | DEBUG | openai.agents | Exported 3 items
2026-01-28 13:10:51,611 | INFO | chat | üì© Chat request | user_id=3 | request_id=53d9794c-56d8-4a71-ba9a-b50cf435f9cd
2026-01-28 13:10:51,611 | INFO | chat | üßë User message | what is my favourite color?
2026-01-28 13:10:51,640 | INFO | chat | ü§ñ Stream started | session_id=b8bafaac-76e6-43b7-b11b-9650423a713e | request_id=53d9794c-56d8-4a71-ba9a-b50cf435f9cd
2026-01-28 13:10:51,641 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-28 13:10:51,643 | INFO | orchestrator | üßë USER_INPUT | what is my favourite color?
2026-01-28 13:10:51,644 | INFO | orchestrator | üß© CONTEXT | user_id=3 | enable_memory=True | db=None
2026-01-28 13:10:51,645 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-28 13:10:51,646 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_66afa5389f5343e487a0d01d6a8042d0
2026-01-28 13:10:51,647 | DEBUG | openai.agents | Setting current trace: trace_66afa5389f5343e487a0d01d6a8042d0
2026-01-28 13:10:51,647 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x0000025FF2A81760> with id None
2026-01-28 13:10:51,648 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-28 13:10:51,649 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000025FF2A1CBE0> with id None
2026-01-28 13:10:51,650 | DEBUG | openai.agents | Calling LLM
2026-01-28 13:10:51,650 | DEBUG | LiteLLM | 

2026-01-28 13:10:51,652 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 13:10:51,652 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'what is my favourite color?'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 13:10:51,654 | DEBUG | LiteLLM | 

2026-01-28 13:10:51,656 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 13:10:51,657 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 13:10:51,658 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 13:10:51,659 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:10:51,660 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 13:10:51,661 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'what is my favourite color?'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 13:10:51,662 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 13:10:51,662 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 13:10:51,663 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 13:10:51,664 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:10:51,665 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:10:51,666 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'what is my favourite color?'}]}], 'system_instruction': {'parts': [{'text': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 13:10:54,526 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\"safe\": true, \"reason\": null, \"message\": null}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 171,
    "candidatesTokenCount": 15,
    "totalTokenCount": 526,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 171
      }
    ],
    "thoughtsTokenCount": 340
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "hb15abSYKYirjuMP8_-KyAM"
}



2026-01-28 13:10:54,529 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:10:54,531 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:10:54,531 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:10:54,532 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:10:54,533 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:10:54,534 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:10:54,535 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:10:54,536 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:10:54,537 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:10:54,538 | DEBUG | LiteLLM | response_cost: 0.0009388
2026-01-28 13:10:54,540 | DEBUG | openai.agents | Received model response
2026-01-28 13:10:54,539 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:10:54,541 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x0000025FF3183220>>
2026-01-28 13:10:54,542 | DEBUG | LiteLLM | response_cost: 0.0009388
2026-01-28 13:10:54,543 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:10:54,545 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:10:54,546 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 13:10:54,546 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:10:54,546 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:10:54,547 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 13:10:54,549 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:10:54,550 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:10:54,554 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:10:54,556 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:10:54,557 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 13:10:54,557 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:10:54,558 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:10:54,560 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:10:54,561 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:10:54,563 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:10:54,563 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 13:10:54,564 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:10:54,565 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0009388
2026-01-28 13:10:54,566 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:10:54,568 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:10:54,568 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:10:54,573 | DEBUG | openai.agents | Resetting current trace
2026-01-28 13:10:54,573 | INFO | orchestrator | ‚úÖ Safety passed
2026-01-28 13:10:54,574 | INFO | orchestrator | üì• Fetching semantic memory (via MemoryService)
2026-01-28 13:10:54,615 | INFO | orchestrator | üß† MEMORY_RESULT | found=True | count=2
2026-01-28 13:10:54,616 | INFO | orchestrator | üß≠ Router agent called
2026-01-28 13:10:54,619 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_ed2089c105e148f4a7e45df765579089
2026-01-28 13:10:54,620 | DEBUG | openai.agents | Setting current trace: trace_ed2089c105e148f4a7e45df765579089
2026-01-28 13:10:54,621 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x0000025FF2A076A0> with id None
2026-01-28 13:10:54,621 | DEBUG | openai.agents | Running agent RouterAgent (turn 1)
2026-01-28 13:10:54,622 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000025FF2A1C880> with id None
2026-01-28 13:10:54,623 | DEBUG | openai.agents | Calling LLM
2026-01-28 13:10:54,623 | DEBUG | LiteLLM | 

2026-01-28 13:10:54,623 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 13:10:54,625 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'what is my favourite color?'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 13:10:54,626 | DEBUG | LiteLLM | 

2026-01-28 13:10:54,627 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 13:10:54,627 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 13:10:54,628 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 13:10:54,630 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:10:54,630 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 13:10:54,632 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'what is my favourite color?'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 13:10:54,633 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 13:10:54,634 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 13:10:54,635 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 13:10:54,635 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:10:54,639 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:10:54,640 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'what is my favourite color?'}]}], 'system_instruction': {'parts': [{'text': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 13:10:56,368 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\n  \"use_memory\": true,\n  \"use_tool\": false,\n  \"tool_name\": \"none\",\n  \"tool_arguments\": null,\n  \"intent\": \"read\",\n  \"memory_key\": \"favorite_color\"\n}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 491,
    "candidatesTokenCount": 60,
    "totalTokenCount": 642,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 491
      }
    ],
    "thoughtsTokenCount": 91
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "h715aejcIJvk4-EPpZe6iQQ"
}



2026-01-28 13:10:56,370 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:10:56,372 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:10:56,372 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:10:56,372 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:10:56,373 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:10:56,374 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:10:56,374 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:10:56,375 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:10:56,376 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:10:56,377 | DEBUG | LiteLLM | response_cost: 0.0005248
2026-01-28 13:10:56,377 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:10:56,378 | DEBUG | openai.agents | Received model response
2026-01-28 13:10:56,379 | DEBUG | LiteLLM | response_cost: 0.0005248
2026-01-28 13:10:56,380 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x0000025FE8AD53F0>>
2026-01-28 13:10:56,381 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:10:56,381 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:10:56,383 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:10:56,387 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 13:10:56,387 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:10:56,388 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:10:56,391 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 13:10:56,393 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:10:56,394 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 13:10:56,395 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:10:56,396 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:10:56,397 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:10:56,398 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:10:56,399 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:10:56,399 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:10:56,400 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:10:56,403 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:10:56,405 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 13:10:56,407 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0005248
2026-01-28 13:10:56,408 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:10:56,409 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:10:56,410 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:10:56,412 | DEBUG | openai.agents | Resetting current trace
2026-01-28 13:10:56,413 | INFO | orchestrator | üß≠ ROUTER_DECISION | tool=False | memory=True | intent=read
2026-01-28 13:10:56,414 | INFO | orchestrator | üß† REASONING_INPUT_SNAPSHOT
{
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful AI assistant.\n \nYou may answer ANY general knowledge question.\n \nConversation summaries and memories are ONLY to provide helpful context\n(e.g. preferences, ongoing plans).\n \nIf a user asks something unrelated to the current topic,\nanswer it normally using general knowledge.\n \nDo NOT restrict yourself to previous topics unless the user explicitly asks."
    },
    {
      "role": "user",
      "content": "hi"
    },
    {
      "role": "assistant",
      "content": "Hi there! How can I help you today?"
    },
    {
      "role": "user",
      "content": "what is my favourite color?"
    },
    {
      "role": "user",
      "content": "what is my favourite color?"
    }
  ],
  "safety": {
    "safe": true,
    "reason": null,
    "message": "OK"
  },
  "memory_action": {},
  "memory_data": [
    {
      "memory_id": 6,
      "user_id": 3,
      "text": "My favourite color is yellow.",
      "confidence": 0.85,
      "created_at": "2026-01-26T21:50:52.573607+05:30",
      "expires_at": "2026-02-25T21:50:52.427804+05:30",
      "score": 0.7962166666984558
    },
    {
      "memory_id": 7,
      "user_id": 3,
      "text": "my name is bhumika raheja",
      "confidence": 0.95,
      "created_at": "2026-01-26T22:31:33.870462+05:30",
      "expires_at": "2026-02-25T22:31:33.867621+05:30",
      "score": 0.35538041591644287
    }
  ],
  "memory_found": true,
  "tool_context": {}
}
2026-01-28 13:10:56,417 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_afaccabfe95f4c52839fa13ac35e6e8d
2026-01-28 13:10:56,418 | DEBUG | openai.agents | Setting current trace: trace_afaccabfe95f4c52839fa13ac35e6e8d
2026-01-28 13:10:56,418 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x0000025FE8A46930> with id None
2026-01-28 13:10:56,420 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000025FF2A1C520> with id None
2026-01-28 13:10:56,422 | DEBUG | openai.agents | Calling LLM
2026-01-28 13:10:56,423 | DEBUG | LiteLLM | 

2026-01-28 13:10:56,423 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 13:10:56,424 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "hi"}, {"role": "assistant", "content": "Hi there! How can I help you today?"}, {"role": "user", "content": "what is my favourite color?"}, {"role": "user", "content": "what is my favourite color?"}], "safety": {"safe": true, "reason": null, "message": "OK"}, "memory_action": {}, "memory_data": [{"memory_id": 6, "user_id": 3, "text": "My favourite color is yellow.", "confidence": 0.85, "created_at": "2026-01-26T21:50:52.573607+05:30", "expires_at": "2026-02-25T21:50:52.427804+05:30", "score": 0.7962166666984558}, {"memory_id": 7, "user_id": 3, "text": "my name is bhumika raheja", "confidence": 0.95, "created_at": "2026-01-26T22:31:33.870462+05:30", "expires_at": "2026-02-25T22:31:33.867621+05:30", "score": 0.35538041591644287}], "memory_found": true, "tool_context": {}}'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=True, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 13:10:56,425 | DEBUG | LiteLLM | 

2026-01-28 13:10:56,426 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 13:10:56,427 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 13:10:56,427 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 13:10:56,429 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:10:56,430 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 13:10:56,431 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "hi"}, {"role": "assistant", "content": "Hi there! How can I help you today?"}, {"role": "user", "content": "what is my favourite color?"}, {"role": "user", "content": "what is my favourite color?"}], "safety": {"safe": true, "reason": null, "message": "OK"}, "memory_action": {}, "memory_data": [{"memory_id": 6, "user_id": 3, "text": "My favourite color is yellow.", "confidence": 0.85, "created_at": "2026-01-26T21:50:52.573607+05:30", "expires_at": "2026-02-25T21:50:52.427804+05:30", "score": 0.7962166666984558}, {"memory_id": 7, "user_id": 3, "text": "my name is bhumika raheja", "confidence": 0.95, "created_at": "2026-01-26T22:31:33.870462+05:30", "expires_at": "2026-02-25T22:31:33.867621+05:30", "score": 0.35538041591644287}], "memory_found": true, "tool_context": {}}'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 13:10:56,432 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': True}
2026-01-28 13:10:56,433 | DEBUG | LiteLLM | Final returned optional params: {'stream': True}
2026-01-28 13:10:56,434 | DEBUG | LiteLLM | self.optional_params: {'stream': True}
2026-01-28 13:10:56,434 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:10:56,437 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:10:56,439 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?key=*****=sse \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "hi"}, {"role": "assistant", "content": "Hi there! How can I help you today?"}, {"role": "user", "content": "what is my favourite color?"}, {"role": "user", "content": "what is my favourite color?"}], "safety": {"safe": true, "reason": null, "message": "OK"}, "memory_action": {}, "memory_data": [{"memory_id": 6, "user_id": 3, "text": "My favourite color is yellow.", "confidence": 0.85, "created_at": "2026-01-26T21:50:52.573607+05:30", "expires_at": "2026-02-25T21:50:52.427804+05:30", "score": 0.7962166666984558}, {"memory_id": 7, "user_id": 3, "text": "my name is bhumika raheja", "confidence": 0.95, "created_at": "2026-01-26T22:31:33.870462+05:30", "expires_at": "2026-02-25T22:31:33.867621+05:30", "score": 0.35538041591644287}], "memory_found": true, "tool_context": {}}'}]}], 'system_instruction': {'parts': [{'text': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.'}]}, 'generationConfig': {}}'
[0m

2026-01-28 13:10:56,440 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:10:56,441 | DEBUG | LiteLLM | messages in token_counter: None, text in token_counter: 
2026-01-28 13:10:56,442 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:10:56,442 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:10:56,443 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:10:56,444 | DEBUG | LiteLLM | response_cost: 0.0
2026-01-28 13:10:56,683 | DEBUG | httpcore.connection | close.started
2026-01-28 13:10:56,686 | DEBUG | httpcore.connection | close.complete
2026-01-28 13:10:56,687 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 13:10:56,728 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000025FF2A30FD0>
2026-01-28 13:10:56,729 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000025FE6FC9BC0> server_hostname='api.openai.com' timeout=5.0
2026-01-28 13:10:56,742 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000025FF2A30FA0>
2026-01-28 13:10:56,743 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 13:10:56,745 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 13:10:56,746 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 13:10:56,747 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 13:10:56,747 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 13:10:57,219 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 07:40:56 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_af6443f1787404ac9669261e8fad4b2b'), (b'openai-processing-ms', b'153'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'157'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4ed83249eed04b-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 13:10:57,220 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 13:10:57,221 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 13:10:57,222 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 13:10:57,222 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 13:10:57,223 | DEBUG | openai.agents | Exported 7 items
2026-01-28 13:10:57,434 | DEBUG | LiteLLM | RAW RESPONSE:
first stream response received


2026-01-28 13:10:57,437 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': 'Your favorite color is yellow.'}], 'role': 'model'}, 'finishReason': 'STOP', 'index': 0}], 'usageMetadata': {'promptTokenCount': 597, 'candidatesTokenCount': 6, 'totalTokenCount': 635, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 597}], 'thoughtsTokenCount': 32}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'iL15aeeSGtvQg8UPhNXAsAQ'}
2026-01-28 13:10:57,438 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='iL15aeeSGtvQg8UPhNXAsAQ', created=1769586057, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='Your favorite color is yellow.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=38, prompt_tokens=597, total_tokens=635, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=32, rejected_prediction_tokens=None, text_tokens=6, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=597, image_tokens=None)))
2026-01-28 13:10:57,440 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content='Your favorite color is yellow.', role='assistant', function_call=None, tool_calls=None, audio=None)
2026-01-28 13:10:57,441 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='iL15aeeSGtvQg8UPhNXAsAQ', created=1769586057, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Your favorite color is yellow.', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=38, prompt_tokens=597, total_tokens=635, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=32, rejected_prediction_tokens=None, text_tokens=6, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=597, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-28 13:10:57,447 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=False
2026-01-28 13:10:57,447 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=False
2026-01-28 13:10:57,448 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 13:10:57,448 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 13:10:57,449 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:10:57,450 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:10:57,451 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:10:57,452 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:10:57,453 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:10:57,453 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:10:57,454 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:10:57,455 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:10:57,456 | DEBUG | LiteLLM | response_cost: 0.0002741
2026-01-28 13:10:57,459 | DEBUG | LiteLLM | response_cost: 0.0002741
2026-01-28 13:10:57,461 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:10:57,462 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0002741
2026-01-28 13:10:57,463 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:10:57,464 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:10:57,464 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:10:57,465 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:10:57,466 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:10:57,467 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 13:10:57,469 | DEBUG | openai.agents | Resetting current trace
2026-01-28 13:10:57,483 | INFO | session_summary | üß† SUMMARY_START | session_id=b8bafaac-76e6-43b7-b11b-9650423a713e
2026-01-28 13:10:57,490 | INFO | session_summary | üìä Unsummarized messages = 4
2026-01-28 13:10:57,492 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-28 13:10:57,493 | INFO | chat | ‚úÖ Stream complete | tokens=1 | time=5.88s
2026-01-28 13:11:02,358 | DEBUG | httpcore.connection | close.started
2026-01-28 13:11:02,359 | DEBUG | httpcore.connection | close.complete
2026-01-28 13:11:02,360 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 13:11:02,389 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000025FF3183C70>
2026-01-28 13:11:02,389 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000025FE6FC9BC0> server_hostname='api.openai.com' timeout=5.0
2026-01-28 13:11:02,402 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000025FF3181FF0>
2026-01-28 13:11:02,402 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 13:11:02,403 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 13:11:02,403 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 13:11:02,404 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 13:11:02,404 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 13:11:02,832 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 07:41:02 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_efe1a6fa5b1b5ea33eeacca872cf18c5'), (b'openai-processing-ms', b'109'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'113'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4ed855aed1aa33-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 13:11:02,833 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 13:11:02,834 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 13:11:02,834 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 13:11:02,835 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 13:11:02,835 | DEBUG | openai.agents | Exported 2 items
2026-01-28 13:11:37,032 | INFO | chat | üì© Chat request | user_id=3 | request_id=94236cf9-51cf-48ab-914b-65fc74c82ec9
2026-01-28 13:11:37,033 | INFO | chat | üßë User message | what is my favourite pizza?
2026-01-28 13:11:37,060 | INFO | chat | ü§ñ Stream started | session_id=29af8d62-64a8-4520-910a-d9462c3cddbc | request_id=94236cf9-51cf-48ab-914b-65fc74c82ec9
2026-01-28 13:11:37,064 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-28 13:11:37,066 | INFO | orchestrator | üßë USER_INPUT | what is my favourite pizza?
2026-01-28 13:11:37,067 | INFO | orchestrator | üß© CONTEXT | user_id=3 | enable_memory=True | db=None
2026-01-28 13:11:37,068 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-28 13:11:37,069 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_4f71756dc2304748a7c4a6ae8439cff1
2026-01-28 13:11:37,070 | DEBUG | openai.agents | Setting current trace: trace_4f71756dc2304748a7c4a6ae8439cff1
2026-01-28 13:11:37,071 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x0000025FF31ADC60> with id None
2026-01-28 13:11:37,071 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-28 13:11:37,073 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000025FF2A1FDC0> with id None
2026-01-28 13:11:37,073 | DEBUG | openai.agents | Calling LLM
2026-01-28 13:11:37,074 | DEBUG | LiteLLM | 

2026-01-28 13:11:37,075 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 13:11:37,075 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'what is my favourite pizza?'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 13:11:37,076 | DEBUG | LiteLLM | 

2026-01-28 13:11:37,077 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 13:11:37,078 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 13:11:37,079 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 13:11:37,084 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:11:37,085 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 13:11:37,086 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'what is my favourite pizza?'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 13:11:37,087 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 13:11:37,087 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 13:11:37,088 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 13:11:37,089 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:11:37,090 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:11:37,091 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'what is my favourite pizza?'}]}], 'system_instruction': {'parts': [{'text': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 13:11:38,574 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\"safe\": true, \"reason\": null, \"message\": null}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 171,
    "candidatesTokenCount": 15,
    "totalTokenCount": 250,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 171
      }
    ],
    "thoughtsTokenCount": 64
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "sb15afnbKqTbqfkPtLbc0Qw"
}



2026-01-28 13:11:38,577 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:11:38,578 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:11:38,579 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:11:38,580 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:11:38,581 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:11:38,581 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:11:38,582 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:11:38,583 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:11:38,583 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:11:38,585 | DEBUG | LiteLLM | response_cost: 0.00024880000000000003
2026-01-28 13:11:38,585 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:11:38,586 | DEBUG | openai.agents | Received model response
2026-01-28 13:11:38,587 | DEBUG | LiteLLM | response_cost: 0.00024880000000000003
2026-01-28 13:11:38,588 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x0000025FF2A31660>>
2026-01-28 13:11:38,589 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:11:38,590 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:11:38,591 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:11:38,592 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:11:38,592 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 13:11:38,592 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:11:38,594 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:11:38,595 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 13:11:38,597 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 13:11:38,599 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:11:38,601 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:11:38,602 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:11:38,603 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:11:38,604 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:11:38,604 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:11:38,606 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:11:38,607 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:11:38,607 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 13:11:38,608 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.00024880000000000003
2026-01-28 13:11:38,609 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:11:38,610 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:11:38,610 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:11:38,621 | DEBUG | openai.agents | Resetting current trace
2026-01-28 13:11:38,622 | INFO | orchestrator | ‚úÖ Safety passed
2026-01-28 13:11:38,624 | INFO | orchestrator | üì• Fetching semantic memory (via MemoryService)
2026-01-28 13:11:38,666 | INFO | orchestrator | üß† MEMORY_RESULT | found=True | count=3
2026-01-28 13:11:38,667 | INFO | orchestrator | üß≠ Router agent called
2026-01-28 13:11:38,668 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_924ffd25a98e493abfa0316b36f6dddd
2026-01-28 13:11:38,668 | DEBUG | openai.agents | Setting current trace: trace_924ffd25a98e493abfa0316b36f6dddd
2026-01-28 13:11:38,669 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x0000025FF2A29C10> with id None
2026-01-28 13:11:38,669 | DEBUG | openai.agents | Running agent RouterAgent (turn 1)
2026-01-28 13:11:38,670 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000025FF2A1FD60> with id None
2026-01-28 13:11:38,670 | DEBUG | openai.agents | Calling LLM
2026-01-28 13:11:38,671 | DEBUG | LiteLLM | 

2026-01-28 13:11:38,671 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 13:11:38,672 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'what is my favourite pizza?'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 13:11:38,673 | DEBUG | LiteLLM | 

2026-01-28 13:11:38,674 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 13:11:38,674 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 13:11:38,675 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 13:11:38,677 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:11:38,680 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 13:11:38,683 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'what is my favourite pizza?'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 13:11:38,684 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 13:11:38,684 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 13:11:38,685 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 13:11:38,685 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:11:38,687 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:11:38,687 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'what is my favourite pizza?'}]}], 'system_instruction': {'parts': [{'text': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 13:11:38,763 | DEBUG | httpcore.connection | close.started
2026-01-28 13:11:38,764 | DEBUG | httpcore.connection | close.complete
2026-01-28 13:11:38,764 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 13:11:38,794 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000025FF3181F30>
2026-01-28 13:11:38,794 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000025FE6FC9BC0> server_hostname='api.openai.com' timeout=5.0
2026-01-28 13:11:38,808 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000025FF3183F40>
2026-01-28 13:11:38,809 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 13:11:38,809 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 13:11:38,810 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 13:11:38,811 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 13:11:38,811 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 13:11:39,294 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 07:41:38 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_742da4f6bbcd9d49079a42cb301729b1'), (b'openai-processing-ms', b'96'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'99'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4ed9392ed77599-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 13:11:39,295 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 13:11:39,296 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 13:11:39,297 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 13:11:39,297 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 13:11:39,298 | DEBUG | openai.agents | Exported 4 items
2026-01-28 13:11:40,291 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\n  \"use_memory\": true,\n  \"use_tool\": false,\n  \"tool_name\": \"none\",\n  \"tool_arguments\": null,\n  \"intent\": \"read\",\n  \"memory_key\": \"food_preference\"\n}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 491,
    "candidatesTokenCount": 60,
    "totalTokenCount": 609,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 491
      }
    ],
    "thoughtsTokenCount": 58
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "s715adLEGdasjuMP2quf8QM"
}



2026-01-28 13:11:40,293 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:11:40,295 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:11:40,295 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:11:40,296 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:11:40,297 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:11:40,297 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:11:40,298 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:11:40,299 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:11:40,299 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:11:40,300 | DEBUG | LiteLLM | response_cost: 0.0004423
2026-01-28 13:11:40,302 | DEBUG | openai.agents | Received model response
2026-01-28 13:11:40,301 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:11:40,303 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x0000025FF2A31FC0>>
2026-01-28 13:11:40,304 | DEBUG | LiteLLM | response_cost: 0.0004423
2026-01-28 13:11:40,305 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:11:40,306 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:11:40,307 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 13:11:40,307 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:11:40,308 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:11:40,309 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 13:11:40,310 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:11:40,312 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:11:40,315 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:11:40,316 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:11:40,317 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 13:11:40,318 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:11:40,319 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:11:40,320 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:11:40,321 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:11:40,322 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:11:40,324 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 13:11:40,324 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:11:40,325 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0004423
2026-01-28 13:11:40,327 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:11:40,328 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:11:40,331 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:11:40,333 | DEBUG | openai.agents | Resetting current trace
2026-01-28 13:11:40,334 | INFO | orchestrator | üß≠ ROUTER_DECISION | tool=False | memory=True | intent=read
2026-01-28 13:11:40,336 | INFO | orchestrator | üß† REASONING_INPUT_SNAPSHOT
{
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful AI assistant.\n \nYou may answer ANY general knowledge question.\n \nConversation summaries and memories are ONLY to provide helpful context\n(e.g. preferences, ongoing plans).\n \nIf a user asks something unrelated to the current topic,\nanswer it normally using general knowledge.\n \nDo NOT restrict yourself to previous topics unless the user explicitly asks."
    },
    {
      "role": "system",
      "content": "Conversation summary:\nThe user's name is Bhumika Raheja. Their"
    },
    {
      "role": "assistant",
      "content": "I understand you have an interest in a movie you love the most, but you haven't told me the title of that specific movie yet. Could you please tell me which movie it is?"
    },
    {
      "role": "user",
      "content": "jab we met i have told you"
    },
    {
      "role": "assistant",
      "content": "Ah, yes! Thank you for reminding me. You love the movie \"Jab We Met.\" I've made a note of that."
    },
    {
      "role": "user",
      "content": "i love watch series .. yesterday i have one series , i found it intersting.  The Strangers  is a wonderful series"
    },
    {
      "role": "assistant",
      "content": "It's great to hear you enjoyed \"The Strangers\" series! I'll make a note that you love watching series and found that one particularly interesting."
    },
    {
      "role": "user",
      "content": "what is my favourite pizza?"
    },
    {
      "role": "user",
      "content": "what is my favourite pizza?"
    }
  ],
  "safety": {
    "safe": true,
    "reason": null,
    "message": "OK"
  },
  "memory_action": {},
  "memory_data": [
    {
      "memory_id": 4,
      "user_id": 3,
      "text": "my favourite pizza is margneita.",
      "confidence": 0.85,
      "created_at": "2026-01-23T22:15:19.533318+05:30",
      "expires_at": "2026-02-26T10:09:28.217743+05:30",
      "score": 0.8055028319358826
    },
    {
      "memory_id": 10,
      "user_id": 3,
      "text": "I love sweet dishes.",
      "confidence": 0.85,
      "created_at": "2026-01-27T10:16:57.730737+05:30",
      "expires_at": "2026-02-26T17:41:35.888018+05:30",
      "score": 0.4141779839992523
    },
    {
      "memory_id": 6,
      "user_id": 3,
      "text": "My favourite color is yellow.",
      "confidence": 0.85,
      "created_at": "2026-01-26T21:50:52.573607+05:30",
      "expires_at": "2026-02-25T21:50:52.427804+05:30",
      "score": 0.37809517979621887
    }
  ],
  "memory_found": true,
  "tool_context": {}
}
2026-01-28 13:11:40,339 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_6c3f6d52bffd49dd8648de65e7cddc68
2026-01-28 13:11:40,340 | DEBUG | openai.agents | Setting current trace: trace_6c3f6d52bffd49dd8648de65e7cddc68
2026-01-28 13:11:40,341 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x0000025FF2A516C0> with id None
2026-01-28 13:11:40,342 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000025FF2A1EB60> with id None
2026-01-28 13:11:40,343 | DEBUG | openai.agents | Calling LLM
2026-01-28 13:11:40,343 | DEBUG | LiteLLM | 

2026-01-28 13:11:40,345 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 13:11:40,347 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "system", "content": "Conversation summary:\\nThe user\'s name is Bhumika Raheja. Their"}, {"role": "assistant", "content": "I understand you have an interest in a movie you love the most, but you haven\'t told me the title of that specific movie yet. Could you please tell me which movie it is?"}, {"role": "user", "content": "jab we met i have told you"}, {"role": "assistant", "content": "Ah, yes! Thank you for reminding me. You love the movie \\"Jab We Met.\\" I\'ve made a note of that."}, {"role": "user", "content": "i love watch series .. yesterday i have one series , i found it intersting.  The Strangers  is a wonderful series"}, {"role": "assistant", "content": "It\'s great to hear you enjoyed \\"The Strangers\\" series! I\'ll make a note that you love watching series and found that one particularly interesting."}, {"role": "user", "content": "what is my favourite pizza?"}, {"role": "user", "content": "what is my favourite pizza?"}], "safety": {"safe": true, "reason": null, "message": "OK"}, "memory_action": {}, "memory_data": [{"memory_id": 4, "user_id": 3, "text": "my favourite pizza is margneita.", "confidence": 0.85, "created_at": "2026-01-23T22:15:19.533318+05:30", "expires_at": "2026-02-26T10:09:28.217743+05:30", "score": 0.8055028319358826}, {"memory_id": 10, "user_id": 3, "text": "I love sweet dishes.", "confidence": 0.85, "created_at": "2026-01-27T10:16:57.730737+05:30", "expires_at": "2026-02-26T17:41:35.888018+05:30", "score": 0.4141779839992523}, {"memory_id": 6, "user_id": 3, "text": "My favourite color is yellow.", "confidence": 0.85, "created_at": "2026-01-26T21:50:52.573607+05:30", "expires_at": "2026-02-25T21:50:52.427804+05:30", "score": 0.37809517979621887}], "memory_found": true, "tool_context": {}}'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=True, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 13:11:40,348 | DEBUG | LiteLLM | 

2026-01-28 13:11:40,349 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 13:11:40,350 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 13:11:40,351 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 13:11:40,353 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:11:40,354 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 13:11:40,355 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "system", "content": "Conversation summary:\\nThe user\'s name is Bhumika Raheja. Their"}, {"role": "assistant", "content": "I understand you have an interest in a movie you love the most, but you haven\'t told me the title of that specific movie yet. Could you please tell me which movie it is?"}, {"role": "user", "content": "jab we met i have told you"}, {"role": "assistant", "content": "Ah, yes! Thank you for reminding me. You love the movie \\"Jab We Met.\\" I\'ve made a note of that."}, {"role": "user", "content": "i love watch series .. yesterday i have one series , i found it intersting.  The Strangers  is a wonderful series"}, {"role": "assistant", "content": "It\'s great to hear you enjoyed \\"The Strangers\\" series! I\'ll make a note that you love watching series and found that one particularly interesting."}, {"role": "user", "content": "what is my favourite pizza?"}, {"role": "user", "content": "what is my favourite pizza?"}], "safety": {"safe": true, "reason": null, "message": "OK"}, "memory_action": {}, "memory_data": [{"memory_id": 4, "user_id": 3, "text": "my favourite pizza is margneita.", "confidence": 0.85, "created_at": "2026-01-23T22:15:19.533318+05:30", "expires_at": "2026-02-26T10:09:28.217743+05:30", "score": 0.8055028319358826}, {"memory_id": 10, "user_id": 3, "text": "I love sweet dishes.", "confidence": 0.85, "created_at": "2026-01-27T10:16:57.730737+05:30", "expires_at": "2026-02-26T17:41:35.888018+05:30", "score": 0.4141779839992523}, {"memory_id": 6, "user_id": 3, "text": "My favourite color is yellow.", "confidence": 0.85, "created_at": "2026-01-26T21:50:52.573607+05:30", "expires_at": "2026-02-25T21:50:52.427804+05:30", "score": 0.37809517979621887}], "memory_found": true, "tool_context": {}}'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 13:11:40,356 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': True}
2026-01-28 13:11:40,357 | DEBUG | LiteLLM | Final returned optional params: {'stream': True}
2026-01-28 13:11:40,358 | DEBUG | LiteLLM | self.optional_params: {'stream': True}
2026-01-28 13:11:40,359 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:11:40,360 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:11:40,363 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?key=*****=sse \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "system", "content": "Conversation summary:\\nThe user\'s name is Bhumika Raheja. Their"}, {"role": "assistant", "content": "I understand you have an interest in a movie you love the most, but you haven\'t told me the title of that specific movie yet. Could you please tell me which movie it is?"}, {"role": "user", "content": "jab we met i have told you"}, {"role": "assistant", "content": "Ah, yes! Thank you for reminding me. You love the movie \\"Jab We Met.\\" I\'ve made a note of that."}, {"role": "user", "content": "i love watch series .. yesterday i have one series , i found it intersting.  The Strangers  is a wonderful series"}, {"role": "assistant", "content": "It\'s great to hear you enjoyed \\"The Strangers\\" series! I\'ll make a note that you love watching series and found that one particularly interesting."}, {"role": "user", "content": "what is my favourite pizza?"}, {"role": "user", "content": "what is my favourite pizza?"}], "safety": {"safe": true, "reason": null, "message": "OK"}, "memory_action": {}, "memory_data": [{"memory_id": 4, "user_id": 3, "text": "my favourite pizza is margneita.", "confidence": 0.85, "created_at": "2026-01-23T22:15:19.533318+05:30", "expires_at": "2026-02-26T10:09:28.217743+05:30", "score": 0.8055028319358826}, {"memory_id": 10, "user_id": 3, "text": "I love sweet dishes.", "confidence": 0.85, "created_at": "2026-01-27T10:16:57.730737+05:30", "expires_at": "2026-02-26T17:41:35.888018+05:30", "score": 0.4141779839992523}, {"memory_id": 6, "user_id": 3, "text": "My favourite color is yellow.", "confidence": 0.85, "created_at": "2026-01-26T21:50:52.573607+05:30", "expires_at": "2026-02-25T21:50:52.427804+05:30", "score": 0.37809517979621887}], "memory_found": true, "tool_context": {}}'}]}], 'system_instruction': {'parts': [{'text': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.'}]}, 'generationConfig': {}}'
[0m

2026-01-28 13:11:40,366 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:11:40,367 | DEBUG | LiteLLM | messages in token_counter: None, text in token_counter: 
2026-01-28 13:11:40,368 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:11:40,369 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:11:40,370 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:11:40,371 | DEBUG | LiteLLM | response_cost: 0.0
2026-01-28 13:11:41,695 | DEBUG | LiteLLM | RAW RESPONSE:
first stream response received


2026-01-28 13:11:41,696 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': 'Your favorite pizza is Margherita.'}], 'role': 'model'}, 'finishReason': 'STOP', 'index': 0}], 'usageMetadata': {'promptTokenCount': 912, 'candidatesTokenCount': 6, 'totalTokenCount': 964, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 912}], 'thoughtsTokenCount': 46}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'tL15aeOOH6v84-EP8fTmqQQ'}
2026-01-28 13:11:41,698 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='tL15aeOOH6v84-EP8fTmqQQ', created=1769586101, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='Your favorite pizza is Margherita.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=52, prompt_tokens=912, total_tokens=964, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=46, rejected_prediction_tokens=None, text_tokens=6, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=912, image_tokens=None)))
2026-01-28 13:11:41,701 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content='Your favorite pizza is Margherita.', role='assistant', function_call=None, tool_calls=None, audio=None)
2026-01-28 13:11:41,701 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='tL15aeOOH6v84-EP8fTmqQQ', created=1769586101, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Your favorite pizza is Margherita.', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=52, prompt_tokens=912, total_tokens=964, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=46, rejected_prediction_tokens=None, text_tokens=6, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=912, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-28 13:11:41,705 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=False
2026-01-28 13:11:41,705 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=False
2026-01-28 13:11:41,705 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 13:11:41,706 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 13:11:41,707 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:11:41,708 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:11:41,709 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:11:41,710 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:11:41,711 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:11:41,711 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:11:41,713 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:11:41,713 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:11:41,716 | DEBUG | LiteLLM | response_cost: 0.0004036
2026-01-28 13:11:41,718 | DEBUG | LiteLLM | response_cost: 0.0004036
2026-01-28 13:11:41,719 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:11:41,719 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0004036
2026-01-28 13:11:41,720 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:11:41,721 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:11:41,722 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:11:41,722 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:11:41,723 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:11:41,725 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 13:11:41,727 | DEBUG | openai.agents | Resetting current trace
2026-01-28 13:11:41,734 | INFO | session_summary | üß† SUMMARY_START | session_id=29af8d62-64a8-4520-910a-d9462c3cddbc
2026-01-28 13:11:41,737 | INFO | session_summary | üìä Unsummarized messages = 16
2026-01-28 13:11:41,739 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-28 13:11:41,740 | INFO | chat | ‚úÖ Stream complete | tokens=1 | time=4.71s
2026-01-28 13:11:44,427 | DEBUG | httpcore.connection | close.started
2026-01-28 13:11:44,429 | DEBUG | httpcore.connection | close.complete
2026-01-28 13:11:44,429 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 13:11:44,438 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000025FE8AD55A0>
2026-01-28 13:11:44,439 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000025FE6FC9BC0> server_hostname='api.openai.com' timeout=5.0
2026-01-28 13:11:44,451 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000025FE8AD4790>
2026-01-28 13:11:44,453 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 13:11:44,454 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 13:11:44,455 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 13:11:44,456 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 13:11:44,456 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 13:11:44,907 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 07:41:44 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_7c181fd94b6456debe43d3710e67f0c2'), (b'openai-processing-ms', b'122'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'125'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4ed95c7efaa89b-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 13:11:44,908 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 13:11:44,909 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 13:11:44,909 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 13:11:44,910 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 13:11:44,910 | DEBUG | openai.agents | Exported 5 items
2026-01-28 13:11:53,663 | INFO | chat | üì© Chat request | user_id=3 | request_id=943f4d81-6512-41ca-8339-6a90af294a54
2026-01-28 13:11:53,664 | INFO | chat | üßë User message | where do i studied in?
2026-01-28 13:11:53,681 | INFO | chat | ü§ñ Stream started | session_id=29af8d62-64a8-4520-910a-d9462c3cddbc | request_id=943f4d81-6512-41ca-8339-6a90af294a54
2026-01-28 13:11:53,682 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-28 13:11:53,683 | INFO | orchestrator | üßë USER_INPUT | where do i studied in?
2026-01-28 13:11:53,684 | INFO | orchestrator | üß© CONTEXT | user_id=3 | enable_memory=True | db=None
2026-01-28 13:11:53,685 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-28 13:11:53,686 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_9b58f3fc2f1d4b51b21530e250b7f104
2026-01-28 13:11:53,686 | DEBUG | openai.agents | Setting current trace: trace_9b58f3fc2f1d4b51b21530e250b7f104
2026-01-28 13:11:53,687 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x0000025FF2A291C0> with id None
2026-01-28 13:11:53,687 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-28 13:11:53,688 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000025FF2A1FBE0> with id None
2026-01-28 13:11:53,689 | DEBUG | openai.agents | Calling LLM
2026-01-28 13:11:53,690 | DEBUG | LiteLLM | 

2026-01-28 13:11:53,690 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 13:11:53,695 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'where do i studied in?'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 13:11:53,696 | DEBUG | LiteLLM | 

2026-01-28 13:11:53,698 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 13:11:53,699 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 13:11:53,700 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 13:11:53,702 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:11:53,703 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 13:11:53,704 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'where do i studied in?'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 13:11:53,705 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 13:11:53,705 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 13:11:53,706 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 13:11:53,708 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:11:53,713 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:11:53,715 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'where do i studied in?'}]}], 'system_instruction': {'parts': [{'text': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 13:11:55,061 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "```json\n{\n  \"safe\": true,\n  \"reason\": null,\n  \"message\": null\n}\n```"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 171,
    "candidatesTokenCount": 28,
    "totalTokenCount": 249,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 171
      }
    ],
    "thoughtsTokenCount": 50
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "wr15aYKvDZivjuMP7IOIgAI"
}



2026-01-28 13:11:55,064 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:11:55,065 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:11:55,066 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:11:55,067 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:11:55,068 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:11:55,068 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:11:55,069 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:11:55,070 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:11:55,071 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:11:55,072 | DEBUG | LiteLLM | response_cost: 0.0002463
2026-01-28 13:11:55,073 | DEBUG | openai.agents | Received model response
2026-01-28 13:11:55,072 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:11:55,074 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x0000025FF2A31FC0>>
2026-01-28 13:11:55,075 | DEBUG | LiteLLM | response_cost: 0.0002463
2026-01-28 13:11:55,076 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:11:55,077 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:11:55,078 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:11:55,078 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 13:11:55,078 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:11:55,080 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:11:55,081 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 13:11:55,083 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:11:55,085 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:11:55,087 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:11:55,087 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 13:11:55,088 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:11:55,088 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:11:55,090 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:11:55,091 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:11:55,092 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:11:55,092 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 13:11:55,094 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:11:55,096 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0002463
2026-01-28 13:11:55,098 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:11:55,098 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:11:55,099 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:11:55,102 | DEBUG | openai.agents | Resetting current trace
2026-01-28 13:11:55,102 | INFO | orchestrator | ‚úÖ Safety passed
2026-01-28 13:11:55,103 | INFO | orchestrator | üì• Fetching semantic memory (via MemoryService)
2026-01-28 13:11:55,146 | INFO | orchestrator | üß† MEMORY_RESULT | found=True | count=1
2026-01-28 13:11:55,148 | INFO | orchestrator | üß≠ Router agent called
2026-01-28 13:11:55,149 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_1d5f9ff8ad5d423f98ad3729fdb1ab3f
2026-01-28 13:11:55,150 | DEBUG | openai.agents | Setting current trace: trace_1d5f9ff8ad5d423f98ad3729fdb1ab3f
2026-01-28 13:11:55,150 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x0000025FF2A52F70> with id None
2026-01-28 13:11:55,152 | DEBUG | openai.agents | Running agent RouterAgent (turn 1)
2026-01-28 13:11:55,152 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000025FF2A61F00> with id None
2026-01-28 13:11:55,153 | DEBUG | openai.agents | Calling LLM
2026-01-28 13:11:55,153 | DEBUG | LiteLLM | 

2026-01-28 13:11:55,154 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 13:11:55,155 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'where do i studied in?'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 13:11:55,156 | DEBUG | LiteLLM | 

2026-01-28 13:11:55,157 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 13:11:55,158 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 13:11:55,159 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 13:11:55,163 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:11:55,164 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 13:11:55,166 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'where do i studied in?'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 13:11:55,167 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 13:11:55,168 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 13:11:55,168 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 13:11:55,169 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:11:55,170 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:11:55,171 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'where do i studied in?'}]}], 'system_instruction': {'parts': [{'text': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 13:11:55,194 | DEBUG | httpcore.connection | close.started
2026-01-28 13:11:55,195 | DEBUG | httpcore.connection | close.complete
2026-01-28 13:11:55,195 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 13:11:55,212 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000025FF2A444F0>
2026-01-28 13:11:55,213 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000025FE6FC9BC0> server_hostname='api.openai.com' timeout=5.0
2026-01-28 13:11:55,224 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000025FF2A44490>
2026-01-28 13:11:55,224 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 13:11:55,225 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 13:11:55,225 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 13:11:55,226 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 13:11:55,226 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 13:11:55,701 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 07:41:54 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_19a6f639db8ff6753c0a645b6f256066'), (b'openai-processing-ms', b'123'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'126'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4ed99fef88d23d-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 13:11:55,704 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 13:11:55,705 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 13:11:55,705 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 13:11:55,706 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 13:11:55,706 | DEBUG | openai.agents | Exported 4 items
2026-01-28 13:11:56,592 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\n  \"use_memory\": true,\n  \"use_tool\": false,\n  \"tool_name\": \"none\",\n  \"tool_arguments\": null,\n  \"intent\": \"read\",\n  \"memory_key\": null\n}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 491,
    "candidatesTokenCount": 56,
    "totalTokenCount": 600,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 491
      }
    ],
    "thoughtsTokenCount": 53
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "w715aZ7zLcnjjuMP6NLg6QM"
}



2026-01-28 13:11:56,595 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:11:56,596 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:11:56,596 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:11:56,597 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:11:56,598 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:11:56,599 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:11:56,600 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:11:56,601 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:11:56,601 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:11:56,602 | DEBUG | LiteLLM | response_cost: 0.00041980000000000007
2026-01-28 13:11:56,603 | DEBUG | openai.agents | Received model response
2026-01-28 13:11:56,603 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:11:56,604 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x0000025FF2493CD0>>
2026-01-28 13:11:56,605 | DEBUG | LiteLLM | response_cost: 0.00041980000000000007
2026-01-28 13:11:56,606 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:11:56,607 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:11:56,608 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 13:11:56,607 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:11:56,608 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:11:56,609 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 13:11:56,610 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:11:56,610 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:11:56,612 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:11:56,613 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:11:56,613 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 13:11:56,616 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:11:56,617 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:11:56,619 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:11:56,620 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:11:56,621 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:11:56,622 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 13:11:56,622 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:11:56,623 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.00041980000000000007
2026-01-28 13:11:56,625 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:11:56,626 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:11:56,627 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:11:56,631 | DEBUG | openai.agents | Resetting current trace
2026-01-28 13:11:56,632 | INFO | orchestrator | üß≠ ROUTER_DECISION | tool=False | memory=True | intent=read
2026-01-28 13:11:56,633 | INFO | orchestrator | üß† REASONING_INPUT_SNAPSHOT
{
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful AI assistant.\n \nYou may answer ANY general knowledge question.\n \nConversation summaries and memories are ONLY to provide helpful context\n(e.g. preferences, ongoing plans).\n \nIf a user asks something unrelated to the current topic,\nanswer it normally using general knowledge.\n \nDo NOT restrict yourself to previous topics unless the user explicitly asks."
    },
    {
      "role": "system",
      "content": "Conversation summary:\nThe user's name is Bhumika Raheja. Their"
    },
    {
      "role": "assistant",
      "content": "Ah, yes! Thank you for reminding me. You love the movie \"Jab We Met.\" I've made a note of that."
    },
    {
      "role": "user",
      "content": "i love watch series .. yesterday i have one series , i found it intersting.  The Strangers  is a wonderful series"
    },
    {
      "role": "assistant",
      "content": "It's great to hear you enjoyed \"The Strangers\" series! I'll make a note that you love watching series and found that one particularly interesting."
    },
    {
      "role": "user",
      "content": "what is my favourite pizza?"
    },
    {
      "role": "assistant",
      "content": "Your favorite pizza is Margherita."
    },
    {
      "role": "user",
      "content": "where do i studied in?"
    },
    {
      "role": "user",
      "content": "where do i studied in?"
    }
  ],
  "safety": {
    "safe": true,
    "reason": null,
    "message": "OK"
  },
  "memory_action": {},
  "memory_data": [
    {
      "memory_id": 3,
      "user_id": 3,
      "text": "BML Munjal University",
      "confidence": 0.9,
      "created_at": "2026-01-23T22:12:42.486515+05:30",
      "expires_at": "2026-02-22T22:12:42.434719+05:30",
      "score": 0.35781705379486084
    }
  ],
  "memory_found": true,
  "tool_context": {}
}
2026-01-28 13:11:56,636 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_5e7f3414cecb41bf88ce72b15fab73fd
2026-01-28 13:11:56,636 | DEBUG | openai.agents | Setting current trace: trace_5e7f3414cecb41bf88ce72b15fab73fd
2026-01-28 13:11:56,637 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x0000025FF2A294E0> with id None
2026-01-28 13:11:56,638 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000025FF31C5840> with id None
2026-01-28 13:11:56,638 | DEBUG | openai.agents | Calling LLM
2026-01-28 13:11:56,639 | DEBUG | LiteLLM | 

2026-01-28 13:11:56,640 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 13:11:56,640 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "system", "content": "Conversation summary:\\nThe user\'s name is Bhumika Raheja. Their"}, {"role": "assistant", "content": "Ah, yes! Thank you for reminding me. You love the movie \\"Jab We Met.\\" I\'ve made a note of that."}, {"role": "user", "content": "i love watch series .. yesterday i have one series , i found it intersting.  The Strangers  is a wonderful series"}, {"role": "assistant", "content": "It\'s great to hear you enjoyed \\"The Strangers\\" series! I\'ll make a note that you love watching series and found that one particularly interesting."}, {"role": "user", "content": "what is my favourite pizza?"}, {"role": "assistant", "content": "Your favorite pizza is Margherita."}, {"role": "user", "content": "where do i studied in?"}, {"role": "user", "content": "where do i studied in?"}], "safety": {"safe": true, "reason": null, "message": "OK"}, "memory_action": {}, "memory_data": [{"memory_id": 3, "user_id": 3, "text": "BML Munjal University", "confidence": 0.9, "created_at": "2026-01-23T22:12:42.486515+05:30", "expires_at": "2026-02-22T22:12:42.434719+05:30", "score": 0.35781705379486084}], "memory_found": true, "tool_context": {}}'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=True, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 13:11:56,641 | DEBUG | LiteLLM | 

2026-01-28 13:11:56,642 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 13:11:56,644 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 13:11:56,645 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 13:11:56,648 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:11:56,648 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 13:11:56,649 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "system", "content": "Conversation summary:\\nThe user\'s name is Bhumika Raheja. Their"}, {"role": "assistant", "content": "Ah, yes! Thank you for reminding me. You love the movie \\"Jab We Met.\\" I\'ve made a note of that."}, {"role": "user", "content": "i love watch series .. yesterday i have one series , i found it intersting.  The Strangers  is a wonderful series"}, {"role": "assistant", "content": "It\'s great to hear you enjoyed \\"The Strangers\\" series! I\'ll make a note that you love watching series and found that one particularly interesting."}, {"role": "user", "content": "what is my favourite pizza?"}, {"role": "assistant", "content": "Your favorite pizza is Margherita."}, {"role": "user", "content": "where do i studied in?"}, {"role": "user", "content": "where do i studied in?"}], "safety": {"safe": true, "reason": null, "message": "OK"}, "memory_action": {}, "memory_data": [{"memory_id": 3, "user_id": 3, "text": "BML Munjal University", "confidence": 0.9, "created_at": "2026-01-23T22:12:42.486515+05:30", "expires_at": "2026-02-22T22:12:42.434719+05:30", "score": 0.35781705379486084}], "memory_found": true, "tool_context": {}}'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 13:11:56,650 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': True}
2026-01-28 13:11:56,651 | DEBUG | LiteLLM | Final returned optional params: {'stream': True}
2026-01-28 13:11:56,651 | DEBUG | LiteLLM | self.optional_params: {'stream': True}
2026-01-28 13:11:56,652 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:11:56,654 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:11:56,655 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?key=*****=sse \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "system", "content": "Conversation summary:\\nThe user\'s name is Bhumika Raheja. Their"}, {"role": "assistant", "content": "Ah, yes! Thank you for reminding me. You love the movie \\"Jab We Met.\\" I\'ve made a note of that."}, {"role": "user", "content": "i love watch series .. yesterday i have one series , i found it intersting.  The Strangers  is a wonderful series"}, {"role": "assistant", "content": "It\'s great to hear you enjoyed \\"The Strangers\\" series! I\'ll make a note that you love watching series and found that one particularly interesting."}, {"role": "user", "content": "what is my favourite pizza?"}, {"role": "assistant", "content": "Your favorite pizza is Margherita."}, {"role": "user", "content": "where do i studied in?"}, {"role": "user", "content": "where do i studied in?"}], "safety": {"safe": true, "reason": null, "message": "OK"}, "memory_action": {}, "memory_data": [{"memory_id": 3, "user_id": 3, "text": "BML Munjal University", "confidence": 0.9, "created_at": "2026-01-23T22:12:42.486515+05:30", "expires_at": "2026-02-22T22:12:42.434719+05:30", "score": 0.35781705379486084}], "memory_found": true, "tool_context": {}}'}]}], 'system_instruction': {'parts': [{'text': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.'}]}, 'generationConfig': {}}'
[0m

2026-01-28 13:11:56,656 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:11:56,657 | DEBUG | LiteLLM | messages in token_counter: None, text in token_counter: 
2026-01-28 13:11:56,658 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:11:56,659 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:11:56,660 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:11:56,663 | DEBUG | LiteLLM | response_cost: 0.0
2026-01-28 13:11:58,095 | DEBUG | LiteLLM | RAW RESPONSE:
first stream response received


2026-01-28 13:11:58,096 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': 'You studied at BML Munjal University.'}], 'role': 'model'}, 'finishReason': 'STOP', 'index': 0}], 'usageMetadata': {'promptTokenCount': 603, 'candidatesTokenCount': 10, 'totalTokenCount': 662, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 603}], 'thoughtsTokenCount': 49}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'xL15adGhOPi8juMP36nR6AE'}
2026-01-28 13:11:58,098 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='xL15adGhOPi8juMP36nR6AE', created=1769586118, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='You studied at BML Munjal University.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=59, prompt_tokens=603, total_tokens=662, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=49, rejected_prediction_tokens=None, text_tokens=10, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=603, image_tokens=None)))
2026-01-28 13:11:58,101 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content='You studied at BML Munjal University.', role='assistant', function_call=None, tool_calls=None, audio=None)
2026-01-28 13:11:58,102 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='xL15adGhOPi8juMP36nR6AE', created=1769586118, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='You studied at BML Munjal University.', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=59, prompt_tokens=603, total_tokens=662, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=49, rejected_prediction_tokens=None, text_tokens=10, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=603, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-28 13:11:58,106 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=False
2026-01-28 13:11:58,107 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=False
2026-01-28 13:11:58,107 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 13:11:58,108 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 13:11:58,109 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:11:58,110 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:11:58,111 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:11:58,112 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:11:58,113 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:11:58,113 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:11:58,114 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:11:58,115 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:11:58,116 | DEBUG | LiteLLM | response_cost: 0.0003284
2026-01-28 13:11:58,117 | DEBUG | LiteLLM | response_cost: 0.0003284
2026-01-28 13:11:58,121 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:11:58,122 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0003284
2026-01-28 13:11:58,123 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:11:58,124 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:11:58,125 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:11:58,126 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:11:58,128 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:11:58,131 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 13:11:58,133 | DEBUG | openai.agents | Resetting current trace
2026-01-28 13:11:58,147 | INFO | session_summary | üß† SUMMARY_START | session_id=29af8d62-64a8-4520-910a-d9462c3cddbc
2026-01-28 13:11:58,151 | INFO | session_summary | üìä Unsummarized messages = 18
2026-01-28 13:11:58,156 | INFO | session_summary | üìÑ Existing summary = True
2026-01-28 13:11:58,162 | INFO | session_summary | üßæ Messages fetched = 12
2026-01-28 13:11:58,163 | INFO | session_summary | ‚úçÔ∏è Calling LLM for MERGED summary
2026-01-28 13:11:58,164 | DEBUG | LiteLLM | 

2026-01-28 13:11:58,165 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 13:11:58,167 | DEBUG | LiteLLM | [92mlitellm.completion(model='gemini/gemini-2.5-flash', provider='gemini', messages=[{'role': 'system', 'content': 'You are a precise memory compression engine.'}, {'role': 'user', 'content': '\nYou are a memory compression system.\n\nTASK:\nUpdate the existing conversation summary using the new dialogue below.\n\nRULES:\n- Preserve facts, decisions, preferences, and open tasks\n- Preserve key events and discussion topics\n- Remove chit-chat and repetition\n- Do NOT invent new information\n- Keep the summary concise and factual\n\nEXISTING SUMMARY:\nThe user\'s name is Bhumika Raheja. Their\n\nNEW DIALOGUE:\nUSER: what is my favourite color?\nASSISTANT: Your favorite color is yellow.\nUSER: do you know where i am studied in?\nASSISTANT: You studied at BML Munjal University.\nUSER: do you know any movie preference of me?\nASSISTANT: I know you love to watch movies, but I don\'t know your specific preferences for genres, actors, or anything else yet.\nUSER: i told u about specific movie\nASSISTANT: I don\'t recall you telling me about a specific movie. Can you tell me again?\nUSER: i have interst on movie  that i love the most  \nASSISTANT: I understand you have an interest in a movie you love the most, but you haven\'t told me the title of that specific movie yet. Could you please tell me which movie it is?\nUSER: jab we met i have told you\nASSISTANT: Ah, yes! Thank you for reminding me. You love the movie "Jab We Met." I\'ve made a note of that.\n\nOUTPUT:\nUpdated summary only.\n'}], temperature=0.2, max_tokens=400)[0m
2026-01-28 13:11:58,169 | DEBUG | LiteLLM | 

2026-01-28 13:11:58,170 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 13:11:58,170 | DEBUG | LiteLLM | SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False
2026-01-28 13:11:58,173 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:11:58,173 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 13:11:58,176 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': 0.2, 'top_p': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': 400, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': 'You are a precise memory compression engine.'}, {'role': 'user', 'content': '\nYou are a memory compression system.\n\nTASK:\nUpdate the existing conversation summary using the new dialogue below.\n\nRULES:\n- Preserve facts, decisions, preferences, and open tasks\n- Preserve key events and discussion topics\n- Remove chit-chat and repetition\n- Do NOT invent new information\n- Keep the summary concise and factual\n\nEXISTING SUMMARY:\nThe user\'s name is Bhumika Raheja. Their\n\nNEW DIALOGUE:\nUSER: what is my favourite color?\nASSISTANT: Your favorite color is yellow.\nUSER: do you know where i am studied in?\nASSISTANT: You studied at BML Munjal University.\nUSER: do you know any movie preference of me?\nASSISTANT: I know you love to watch movies, but I don\'t know your specific preferences for genres, actors, or anything else yet.\nUSER: i told u about specific movie\nASSISTANT: I don\'t recall you telling me about a specific movie. Can you tell me again?\nUSER: i have interst on movie  that i love the most  \nASSISTANT: I understand you have an interest in a movie you love the most, but you haven\'t told me the title of that specific movie yet. Could you please tell me which movie it is?\nUSER: jab we met i have told you\nASSISTANT: Ah, yes! Thank you for reminding me. You love the movie "Jab We Met." I\'ve made a note of that.\n\nOUTPUT:\nUpdated summary only.\n'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None, 'provider': 'gemini'}
2026-01-28 13:11:58,178 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'temperature': 0.2, 'max_tokens': 400}
2026-01-28 13:11:58,180 | DEBUG | LiteLLM | Final returned optional params: {'temperature': 0.2, 'max_output_tokens': 400, 'provider': 'gemini'}
2026-01-28 13:11:58,181 | DEBUG | LiteLLM | self.optional_params: {'temperature': 0.2, 'max_tokens': 400, 'provider': 'gemini'}
2026-01-28 13:11:58,182 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:11:58,182 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:11:58,183 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '\nYou are a memory compression system.\n\nTASK:\nUpdate the existing conversation summary using the new dialogue below.\n\nRULES:\n- Preserve facts, decisions, preferences, and open tasks\n- Preserve key events and discussion topics\n- Remove chit-chat and repetition\n- Do NOT invent new information\n- Keep the summary concise and factual\n\nEXISTING SUMMARY:\nThe user\'s name is Bhumika Raheja. Their\n\nNEW DIALOGUE:\nUSER: what is my favourite color?\nASSISTANT: Your favorite color is yellow.\nUSER: do you know where i am studied in?\nASSISTANT: You studied at BML Munjal University.\nUSER: do you know any movie preference of me?\nASSISTANT: I know you love to watch movies, but I don\'t know your specific preferences for genres, actors, or anything else yet.\nUSER: i told u about specific movie\nASSISTANT: I don\'t recall you telling me about a specific movie. Can you tell me again?\nUSER: i have interst on movie  that i love the most  \nASSISTANT: I understand you have an interest in a movie you love the most, but you haven\'t told me the title of that specific movie yet. Could you please tell me which movie it is?\nUSER: jab we met i have told you\nASSISTANT: Ah, yes! Thank you for reminding me. You love the movie "Jab We Met." I\'ve made a note of that.\n\nOUTPUT:\nUpdated summary only.\n'}]}], 'system_instruction': {'parts': [{'text': 'You are a precise memory compression engine.'}]}, 'generationConfig': {'temperature': 0.2, 'max_output_tokens': 400}}'
[0m

2026-01-28 13:11:58,185 | DEBUG | httpcore.connection | connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None
2026-01-28 13:11:58,220 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000025FF2A31150>
2026-01-28 13:11:58,220 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000025FF23DE1C0> server_hostname='generativelanguage.googleapis.com' timeout=600.0
2026-01-28 13:11:58,239 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000025FF2A31510>
2026-01-28 13:11:58,239 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 13:11:58,240 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 13:11:58,242 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 13:11:58,242 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 13:11:58,242 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 13:12:00,146 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Wed, 28 Jan 2026 07:41:59 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=1896'), (b'Alt-Svc', b'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])
2026-01-28 13:12:00,147 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 13:12:00,149 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 13:12:00,149 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 13:12:00,150 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 13:12:00,151 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "The user's name is Bhumika Raheja. Their favorite color is yellow. They studied at BML Munjal University. They love the movie \"Jab We Met.\""
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 338,
    "candidatesTokenCount": 37,
    "totalTokenCount": 497,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 338
      }
    ],
    "thoughtsTokenCount": 122
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "x715af6hDtmY4-EPoNODsQQ"
}



2026-01-28 13:12:00,154 | INFO | LiteLLM | Wrapper: Completed Call, calling success_handler
2026-01-28 13:12:00,159 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:12:00,159 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:12:00,160 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:12:00,161 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:12:00,162 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:12:00,163 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:12:00,164 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:12:00,165 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:12:00,166 | DEBUG | LiteLLM | response_cost: 0.0004989
2026-01-28 13:12:00,167 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:12:00,167 | INFO | session_summary | üîÑ MERGING INTO EXISTING SUMMARY
2026-01-28 13:12:00,168 | DEBUG | LiteLLM | response_cost: 0.0004989
2026-01-28 13:12:00,175 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:12:00,177 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:12:00,177 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:12:00,179 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 13:12:00,182 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:12:00,183 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:12:00,184 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:12:00,193 | INFO | session_summary | ‚úÖ SUMMARY_MERGED_SUCCESSFULLY
2026-01-28 13:12:00,194 | INFO | chat | ‚úÖ Stream complete | tokens=1 | time=6.53s
2026-01-28 13:12:00,854 | DEBUG | httpcore.connection | close.started
2026-01-28 13:12:00,855 | DEBUG | httpcore.connection | close.complete
2026-01-28 13:12:00,856 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 13:12:00,893 | DEBUG | httpcore.connection | connect_tcp.failed exception=ConnectError(gaierror(11001, 'getaddrinfo failed'))
2026-01-28 13:12:00,894 | WARNING | openai.agents | [non-fatal] Tracing: request failed: [Errno 11001] getaddrinfo failed
2026-01-28 13:12:01,971 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 13:12:02,002 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000025FE8AD4F70>
2026-01-28 13:12:02,002 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000025FE6FC9BC0> server_hostname='api.openai.com' timeout=5.0
2026-01-28 13:12:02,015 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000025FE8AD4CD0>
2026-01-28 13:12:02,016 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 13:12:02,017 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 13:12:02,017 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 13:12:02,019 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 13:12:02,020 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 13:12:02,474 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 07:42:01 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_f70499eed8eb03fb88ba899f8fe7e7a7'), (b'openai-processing-ms', b'134'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'138'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4ed9ca3831de4c-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 13:12:02,475 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 13:12:02,476 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 13:12:02,476 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 13:12:02,477 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 13:12:02,477 | DEBUG | openai.agents | Exported 5 items
2026-01-28 13:12:22,578 | INFO | chat | üì© Chat request | user_id=3 | request_id=f652420c-7f5f-4b57-8213-a06cd4a2e5f6
2026-01-28 13:12:22,580 | INFO | chat | üßë User message | tell me the weather in my city.
2026-01-28 13:12:22,609 | INFO | chat | ü§ñ Stream started | session_id=29af8d62-64a8-4520-910a-d9462c3cddbc | request_id=f652420c-7f5f-4b57-8213-a06cd4a2e5f6
2026-01-28 13:12:22,611 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-28 13:12:22,612 | INFO | orchestrator | üßë USER_INPUT | tell me the weather in my city.
2026-01-28 13:12:22,613 | INFO | orchestrator | üß© CONTEXT | user_id=3 | enable_memory=True | db=None
2026-01-28 13:12:22,614 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-28 13:12:22,616 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_b3dae4a2146a4fda9929a763bdc943ec
2026-01-28 13:12:22,616 | DEBUG | openai.agents | Setting current trace: trace_b3dae4a2146a4fda9929a763bdc943ec
2026-01-28 13:12:22,618 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x0000025FF2A2A480> with id None
2026-01-28 13:12:22,618 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-28 13:12:22,620 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000025FF2A1C040> with id None
2026-01-28 13:12:22,621 | DEBUG | openai.agents | Calling LLM
2026-01-28 13:12:22,622 | DEBUG | LiteLLM | 

2026-01-28 13:12:22,624 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 13:12:22,625 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'tell me the weather in my city.'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 13:12:22,626 | DEBUG | LiteLLM | 

2026-01-28 13:12:22,627 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 13:12:22,628 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 13:12:22,628 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 13:12:22,630 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:12:22,631 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 13:12:22,632 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'tell me the weather in my city.'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 13:12:22,633 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 13:12:22,634 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 13:12:22,635 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 13:12:22,635 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:12:22,637 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:12:22,638 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'tell me the weather in my city.'}]}], 'system_instruction': {'parts': [{'text': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 13:12:23,002 | DEBUG | httpcore.connection | close.started
2026-01-28 13:12:23,003 | DEBUG | httpcore.connection | close.complete
2026-01-28 13:12:23,003 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 13:12:23,013 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000025FF2A30280>
2026-01-28 13:12:23,014 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000025FE6FC9BC0> server_hostname='api.openai.com' timeout=5.0
2026-01-28 13:12:23,033 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000025FF2A33C70>
2026-01-28 13:12:23,033 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 13:12:23,034 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 13:12:23,035 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 13:12:23,035 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 13:12:23,036 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 13:12:23,465 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 07:42:22 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_dbde5fc29877345e0cec38b664f8bcdc'), (b'openai-processing-ms', b'102'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'105'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4eda4d9e13918d-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 13:12:23,466 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 13:12:23,466 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 13:12:23,467 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 13:12:23,467 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 13:12:23,468 | DEBUG | openai.agents | Exported 1 items
2026-01-28 13:12:23,886 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\"safe\": true, \"reason\": null, \"message\": null}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 173,
    "candidatesTokenCount": 15,
    "totalTokenCount": 285,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 173
      }
    ],
    "thoughtsTokenCount": 97
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "3715af4ZtLyO4w_0qeOIBA"
}



2026-01-28 13:12:23,889 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:12:23,890 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:12:23,891 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:12:23,891 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:12:23,893 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:12:23,893 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:12:23,894 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:12:23,895 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:12:23,896 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:12:23,897 | DEBUG | LiteLLM | response_cost: 0.00033190000000000004
2026-01-28 13:12:23,898 | DEBUG | openai.agents | Received model response
2026-01-28 13:12:23,897 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:12:23,899 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x0000025FF2A30D30>>
2026-01-28 13:12:23,900 | DEBUG | LiteLLM | response_cost: 0.00033190000000000004
2026-01-28 13:12:23,901 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:12:23,905 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:12:23,906 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 13:12:23,906 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:12:23,907 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:12:23,908 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 13:12:23,910 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:12:23,910 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:12:23,912 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:12:23,913 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:12:23,913 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 13:12:23,914 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:12:23,915 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:12:23,916 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:12:23,917 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:12:23,920 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:12:23,922 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 13:12:23,923 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:12:23,924 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.00033190000000000004
2026-01-28 13:12:23,926 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:12:23,928 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:12:23,928 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:12:23,930 | DEBUG | openai.agents | Resetting current trace
2026-01-28 13:12:23,930 | INFO | orchestrator | ‚úÖ Safety passed
2026-01-28 13:12:23,931 | INFO | orchestrator | üì• Fetching semantic memory (via MemoryService)
2026-01-28 13:12:23,971 | INFO | orchestrator | üß† MEMORY_RESULT | found=True | count=1
2026-01-28 13:12:23,972 | INFO | orchestrator | üß≠ Router agent called
2026-01-28 13:12:23,974 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_fea42e9673cc4bb484f0cd7c61890c64
2026-01-28 13:12:23,974 | DEBUG | openai.agents | Setting current trace: trace_fea42e9673cc4bb484f0cd7c61890c64
2026-01-28 13:12:23,975 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x0000025FF2A516C0> with id None
2026-01-28 13:12:23,975 | DEBUG | openai.agents | Running agent RouterAgent (turn 1)
2026-01-28 13:12:23,977 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000025FF2A603A0> with id None
2026-01-28 13:12:23,977 | DEBUG | openai.agents | Calling LLM
2026-01-28 13:12:23,978 | DEBUG | LiteLLM | 

2026-01-28 13:12:23,978 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 13:12:23,979 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'tell me the weather in my city.'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 13:12:23,980 | DEBUG | LiteLLM | 

2026-01-28 13:12:23,981 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 13:12:23,982 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 13:12:23,982 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 13:12:23,984 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:12:23,985 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 13:12:23,989 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'tell me the weather in my city.'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 13:12:23,990 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 13:12:23,991 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 13:12:23,992 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 13:12:23,992 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:12:23,994 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:12:23,995 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'tell me the weather in my city.'}]}], 'system_instruction': {'parts': [{'text': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 13:12:25,817 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\n  \"use_memory\": false,\n  \"use_tool\": true,\n  \"tool_name\": \"weather\",\n  \"tool_arguments\": {},\n  \"intent\": \"none\",\n  \"memory_key\": null\n}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 493,
    "candidatesTokenCount": 55,
    "totalTokenCount": 721,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 493
      }
    ],
    "thoughtsTokenCount": 173
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "4L15aYeyOdefjuMP5ouH6QE"
}



2026-01-28 13:12:25,820 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:12:25,821 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:12:25,821 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:12:25,822 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:12:25,822 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:12:25,823 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:12:25,824 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:12:25,825 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:12:25,825 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:12:25,826 | DEBUG | LiteLLM | response_cost: 0.0007179
2026-01-28 13:12:25,828 | DEBUG | openai.agents | Received model response
2026-01-28 13:12:25,827 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:12:25,829 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x0000025FF2A30B80>>
2026-01-28 13:12:25,829 | DEBUG | LiteLLM | response_cost: 0.0007179
2026-01-28 13:12:25,830 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:12:25,831 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:12:25,832 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 13:12:25,832 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:12:25,833 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:12:25,834 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 13:12:25,835 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:12:25,837 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:12:25,840 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:12:25,840 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:12:25,842 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 13:12:25,843 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:12:25,843 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:12:25,845 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:12:25,846 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:12:25,847 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:12:25,848 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 13:12:25,849 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:12:25,850 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0007179
2026-01-28 13:12:25,852 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:12:25,853 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:12:25,854 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:12:25,858 | DEBUG | openai.agents | Resetting current trace
2026-01-28 13:12:25,859 | INFO | orchestrator | üß≠ ROUTER_DECISION | tool=True | memory=False | intent=none
2026-01-28 13:12:25,860 | INFO | orchestrator | üõ†Ô∏è Tool agent called
2026-01-28 13:12:25,861 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_f2c92aac19a34fc28cd5de12fc6b22b0
2026-01-28 13:12:25,862 | DEBUG | openai.agents | Setting current trace: trace_f2c92aac19a34fc28cd5de12fc6b22b0
2026-01-28 13:12:25,862 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x0000025FE8CAF7E0> with id None
2026-01-28 13:12:25,863 | DEBUG | openai.agents | Running agent ToolAgent (turn 1)
2026-01-28 13:12:25,864 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000025FF2A60940> with id None
2026-01-28 13:12:25,864 | DEBUG | openai.agents | Calling LLM
2026-01-28 13:12:25,865 | DEBUG | LiteLLM | 

2026-01-28 13:12:25,866 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 13:12:25,866 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a Tool Agent.\n \nYour task is to decide whether the user\'s query requires a tool.\n \nAvailable tools:\n- weather ‚Üí for weather-related queries (city-based)\n- calculator ‚Üí for mathematical calculations\n- web_search ‚Üí for latest information, prices, news, current events\n- browser ‚Üí for visiting, fetching, or summarizing a webpage\n \nRules:\n- You MUST output valid JSON only.\n- If a tool is required, return:\n  { "tool": "<tool_name>", "arguments": { ... } }\n- If no tool is required, return:\n  { "tool": "none", "arguments": {} }\n \nSTRICT RULES:\n- Do NOT answer the user.\n- Do NOT explain.\n- Do NOT include markdown.\n- Do NOT include extra keys.\n- Do NOT wrap JSON in text.\n- Output JSON ONLY.\n \nExamples:\n \nUser: What is the weather in Delhi today?\nOutput:\n{ "tool": "weather", "arguments": { "city": "Delhi" } }\n \nUser: Calculate (10 + 5) * 2\nOutput:\n{ "tool": "calculator", "arguments": { "expression": "(10 + 5) * 2" } }\n \nUser: Explain machine learning\nOutput:\n{ "tool": "none", "arguments": {} }\n ', 'role': 'system'}, {'role': 'user', 'content': 'tell me the weather in my city.'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 13:12:25,867 | DEBUG | LiteLLM | 

2026-01-28 13:12:25,868 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 13:12:25,868 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 13:12:25,869 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 13:12:25,873 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:12:25,874 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 13:12:25,875 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Tool Agent.\n \nYour task is to decide whether the user\'s query requires a tool.\n \nAvailable tools:\n- weather ‚Üí for weather-related queries (city-based)\n- calculator ‚Üí for mathematical calculations\n- web_search ‚Üí for latest information, prices, news, current events\n- browser ‚Üí for visiting, fetching, or summarizing a webpage\n \nRules:\n- You MUST output valid JSON only.\n- If a tool is required, return:\n  { "tool": "<tool_name>", "arguments": { ... } }\n- If no tool is required, return:\n  { "tool": "none", "arguments": {} }\n \nSTRICT RULES:\n- Do NOT answer the user.\n- Do NOT explain.\n- Do NOT include markdown.\n- Do NOT include extra keys.\n- Do NOT wrap JSON in text.\n- Output JSON ONLY.\n \nExamples:\n \nUser: What is the weather in Delhi today?\nOutput:\n{ "tool": "weather", "arguments": { "city": "Delhi" } }\n \nUser: Calculate (10 + 5) * 2\nOutput:\n{ "tool": "calculator", "arguments": { "expression": "(10 + 5) * 2" } }\n \nUser: Explain machine learning\nOutput:\n{ "tool": "none", "arguments": {} }\n ', 'role': 'system'}, {'role': 'user', 'content': 'tell me the weather in my city.'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 13:12:25,876 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 13:12:25,876 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 13:12:25,878 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 13:12:25,878 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:12:25,879 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:12:25,880 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'tell me the weather in my city.'}]}], 'system_instruction': {'parts': [{'text': 'You are a Tool Agent.\n \nYour task is to decide whether the user\'s query requires a tool.\n \nAvailable tools:\n- weather ‚Üí for weather-related queries (city-based)\n- calculator ‚Üí for mathematical calculations\n- web_search ‚Üí for latest information, prices, news, current events\n- browser ‚Üí for visiting, fetching, or summarizing a webpage\n \nRules:\n- You MUST output valid JSON only.\n- If a tool is required, return:\n  { "tool": "<tool_name>", "arguments": { ... } }\n- If no tool is required, return:\n  { "tool": "none", "arguments": {} }\n \nSTRICT RULES:\n- Do NOT answer the user.\n- Do NOT explain.\n- Do NOT include markdown.\n- Do NOT include extra keys.\n- Do NOT wrap JSON in text.\n- Output JSON ONLY.\n \nExamples:\n \nUser: What is the weather in Delhi today?\nOutput:\n{ "tool": "weather", "arguments": { "city": "Delhi" } }\n \nUser: Calculate (10 + 5) * 2\nOutput:\n{ "tool": "calculator", "arguments": { "expression": "(10 + 5) * 2" } }\n \nUser: Explain machine learning\nOutput:\n{ "tool": "none", "arguments": {} }\n '}]}, 'generationConfig': {}}'
[0m

2026-01-28 13:12:27,240 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{ \"tool\": \"none\", \"arguments\": {} }"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 320,
    "candidatesTokenCount": 12,
    "totalTokenCount": 441,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 320
      }
    ],
    "thoughtsTokenCount": 109
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "4r15aeDHE7qu4-EPt5GUmQQ"
}



2026-01-28 13:12:27,242 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:12:27,244 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:12:27,244 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:12:27,245 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:12:27,246 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:12:27,247 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:12:27,248 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:12:27,249 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:12:27,249 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:12:27,250 | DEBUG | LiteLLM | response_cost: 0.00039850000000000004
2026-01-28 13:12:27,252 | DEBUG | openai.agents | Received model response
2026-01-28 13:12:27,251 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:12:27,253 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x0000025FF2A31300>>
2026-01-28 13:12:27,254 | DEBUG | LiteLLM | response_cost: 0.00039850000000000004
2026-01-28 13:12:27,255 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:12:27,256 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:12:27,257 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 13:12:27,257 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:12:27,258 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:12:27,259 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 13:12:27,260 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:12:27,261 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:12:27,262 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:12:27,264 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:12:27,267 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 13:12:27,268 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:12:27,269 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:12:27,270 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:12:27,271 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:12:27,272 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:12:27,273 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 13:12:27,275 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:12:27,276 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.00039850000000000004
2026-01-28 13:12:27,278 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:12:27,279 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:12:27,280 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:12:27,281 | DEBUG | openai.agents | Resetting current trace
2026-01-28 13:12:27,282 | INFO | orchestrator | üõ†Ô∏è TOOL_EXEC | name=none | args={}
2026-01-28 13:12:27,283 | INFO | orchestrator | üß† REASONING_INPUT_SNAPSHOT
{
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful AI assistant.\n \nYou may answer ANY general knowledge question.\n \nConversation summaries and memories are ONLY to provide helpful context\n(e.g. preferences, ongoing plans).\n \nIf a user asks something unrelated to the current topic,\nanswer it normally using general knowledge.\n \nDo NOT restrict yourself to previous topics unless the user explicitly asks."
    },
    {
      "role": "system",
      "content": "Conversation summary:\nThe user's name is Bhumika Raheja. Their favorite color is yellow. They studied at BML Munjal University. They love the movie \"Jab We Met.\""
    },
    {
      "role": "assistant",
      "content": "It's great to hear you enjoyed \"The Strangers\" series! I'll make a note that you love watching series and found that one particularly interesting."
    },
    {
      "role": "user",
      "content": "what is my favourite pizza?"
    },
    {
      "role": "assistant",
      "content": "Your favorite pizza is Margherita."
    },
    {
      "role": "user",
      "content": "where do i studied in?"
    },
    {
      "role": "assistant",
      "content": "You studied at BML Munjal University."
    },
    {
      "role": "user",
      "content": "tell me the weather in my city."
    },
    {
      "role": "user",
      "content": "tell me the weather in my city."
    }
  ],
  "safety": {
    "safe": true,
    "reason": null,
    "message": "OK"
  },
  "memory_action": {},
  "memory_data": [
    {
      "memory_id": 8,
      "user_id": 3,
      "text": "I like hilly areas.",
      "confidence": 0.85,
      "created_at": "2026-01-26T22:35:31.683204+05:30",
      "expires_at": "2026-02-25T23:24:06.165795+05:30",
      "score": 0.3659367561340332
    }
  ],
  "memory_found": true,
  "tool_context": null
}
2026-01-28 13:12:27,285 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_c34fa50524e446d7abd8a508bc96c718
2026-01-28 13:12:27,286 | DEBUG | openai.agents | Setting current trace: trace_c34fa50524e446d7abd8a508bc96c718
2026-01-28 13:12:27,286 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x0000025FF2A2A610> with id None
2026-01-28 13:12:27,288 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000025FF2A62C80> with id None
2026-01-28 13:12:27,290 | DEBUG | openai.agents | Calling LLM
2026-01-28 13:12:27,290 | DEBUG | LiteLLM | 

2026-01-28 13:12:27,291 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 13:12:27,291 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "system", "content": "Conversation summary:\\nThe user\'s name is Bhumika Raheja. Their favorite color is yellow. They studied at BML Munjal University. They love the movie \\"Jab We Met.\\""}, {"role": "assistant", "content": "It\'s great to hear you enjoyed \\"The Strangers\\" series! I\'ll make a note that you love watching series and found that one particularly interesting."}, {"role": "user", "content": "what is my favourite pizza?"}, {"role": "assistant", "content": "Your favorite pizza is Margherita."}, {"role": "user", "content": "where do i studied in?"}, {"role": "assistant", "content": "You studied at BML Munjal University."}, {"role": "user", "content": "tell me the weather in my city."}, {"role": "user", "content": "tell me the weather in my city."}], "safety": {"safe": true, "reason": null, "message": "OK"}, "memory_action": {}, "memory_data": [{"memory_id": 8, "user_id": 3, "text": "I like hilly areas.", "confidence": 0.85, "created_at": "2026-01-26T22:35:31.683204+05:30", "expires_at": "2026-02-25T23:24:06.165795+05:30", "score": 0.3659367561340332}], "memory_found": true, "tool_context": null}'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=True, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 13:12:27,292 | DEBUG | LiteLLM | 

2026-01-28 13:12:27,294 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 13:12:27,295 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 13:12:27,295 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 13:12:27,297 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:12:27,298 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 13:12:27,299 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "system", "content": "Conversation summary:\\nThe user\'s name is Bhumika Raheja. Their favorite color is yellow. They studied at BML Munjal University. They love the movie \\"Jab We Met.\\""}, {"role": "assistant", "content": "It\'s great to hear you enjoyed \\"The Strangers\\" series! I\'ll make a note that you love watching series and found that one particularly interesting."}, {"role": "user", "content": "what is my favourite pizza?"}, {"role": "assistant", "content": "Your favorite pizza is Margherita."}, {"role": "user", "content": "where do i studied in?"}, {"role": "assistant", "content": "You studied at BML Munjal University."}, {"role": "user", "content": "tell me the weather in my city."}, {"role": "user", "content": "tell me the weather in my city."}], "safety": {"safe": true, "reason": null, "message": "OK"}, "memory_action": {}, "memory_data": [{"memory_id": 8, "user_id": 3, "text": "I like hilly areas.", "confidence": 0.85, "created_at": "2026-01-26T22:35:31.683204+05:30", "expires_at": "2026-02-25T23:24:06.165795+05:30", "score": 0.3659367561340332}], "memory_found": true, "tool_context": null}'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 13:12:27,300 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': True}
2026-01-28 13:12:27,300 | DEBUG | LiteLLM | Final returned optional params: {'stream': True}
2026-01-28 13:12:27,301 | DEBUG | LiteLLM | self.optional_params: {'stream': True}
2026-01-28 13:12:27,302 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:12:27,303 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:12:27,307 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?key=*****=sse \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "system", "content": "Conversation summary:\\nThe user\'s name is Bhumika Raheja. Their favorite color is yellow. They studied at BML Munjal University. They love the movie \\"Jab We Met.\\""}, {"role": "assistant", "content": "It\'s great to hear you enjoyed \\"The Strangers\\" series! I\'ll make a note that you love watching series and found that one particularly interesting."}, {"role": "user", "content": "what is my favourite pizza?"}, {"role": "assistant", "content": "Your favorite pizza is Margherita."}, {"role": "user", "content": "where do i studied in?"}, {"role": "assistant", "content": "You studied at BML Munjal University."}, {"role": "user", "content": "tell me the weather in my city."}, {"role": "user", "content": "tell me the weather in my city."}], "safety": {"safe": true, "reason": null, "message": "OK"}, "memory_action": {}, "memory_data": [{"memory_id": 8, "user_id": 3, "text": "I like hilly areas.", "confidence": 0.85, "created_at": "2026-01-26T22:35:31.683204+05:30", "expires_at": "2026-02-25T23:24:06.165795+05:30", "score": 0.3659367561340332}], "memory_found": true, "tool_context": null}'}]}], 'system_instruction': {'parts': [{'text': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.'}]}, 'generationConfig': {}}'
[0m

2026-01-28 13:12:27,309 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:12:27,309 | DEBUG | LiteLLM | messages in token_counter: None, text in token_counter: 
2026-01-28 13:12:27,310 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:12:27,311 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:12:27,312 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:12:27,313 | DEBUG | LiteLLM | response_cost: 0.0
2026-01-28 13:12:28,292 | DEBUG | LiteLLM | RAW RESPONSE:
first stream response received


2026-01-28 13:12:28,295 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': "I don't"}], 'role': 'model'}, 'index': 0}], 'usageMetadata': {'promptTokenCount': 591, 'candidatesTokenCount': 4, 'totalTokenCount': 659, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 591}], 'thoughtsTokenCount': 64}, 'modelVersion': 'gemini-2.5-flash', 'responseId': '4715aeGnBNiBg8UP_-bo8QE'}
2026-01-28 13:12:28,296 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='4715aeGnBNiBg8UP_-bo8QE', created=1769586148, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content="I don't", role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=68, prompt_tokens=591, total_tokens=659, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=64, rejected_prediction_tokens=None, text_tokens=4, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=591, image_tokens=None)))
2026-01-28 13:12:28,299 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content="I don't", role='assistant', function_call=None, tool_calls=None, audio=None)
2026-01-28 13:12:28,300 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='4715aeGnBNiBg8UP_-bo8QE', created=1769586148, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content="I don't", role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=68, prompt_tokens=591, total_tokens=659, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=64, rejected_prediction_tokens=None, text_tokens=4, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=591, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-28 13:12:28,304 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': " know what your current city is. Could you please tell me which city you'd like the weather for?"}], 'role': 'model'}, 'finishReason': 'STOP', 'index': 0}], 'usageMetadata': {'promptTokenCount': 591, 'candidatesTokenCount': 26, 'totalTokenCount': 681, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 591}], 'thoughtsTokenCount': 64}, 'modelVersion': 'gemini-2.5-flash', 'responseId': '4715aeGnBNiBg8UP_-bo8QE'}
2026-01-28 13:12:28,307 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='4715aeGnBNiBg8UP_-bo8QE', created=1769586148, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content=" know what your current city is. Could you please tell me which city you'd like the weather for?", role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=90, prompt_tokens=591, total_tokens=681, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=64, rejected_prediction_tokens=None, text_tokens=26, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=591, image_tokens=None)))
2026-01-28 13:12:28,310 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content=" know what your current city is. Could you please tell me which city you'd like the weather for?", role=None, function_call=None, tool_calls=None, audio=None)
2026-01-28 13:12:28,311 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='4715aeGnBNiBg8UP_-bo8QE', created=1769586148, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=" know what your current city is. Could you please tell me which city you'd like the weather for?", role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=90, prompt_tokens=591, total_tokens=681, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=64, rejected_prediction_tokens=None, text_tokens=26, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=591, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-28 13:12:28,317 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=False
2026-01-28 13:12:28,317 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=False
2026-01-28 13:12:28,318 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 13:12:28,319 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 13:12:28,320 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:12:28,321 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:12:28,322 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:12:28,323 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:12:28,326 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:12:28,327 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:12:28,327 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:12:28,328 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:12:28,329 | DEBUG | LiteLLM | response_cost: 0.0004023
2026-01-28 13:12:28,330 | DEBUG | LiteLLM | response_cost: 0.0004023
2026-01-28 13:12:28,332 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:12:28,332 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0004023
2026-01-28 13:12:28,333 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:12:28,334 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:12:28,334 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:12:28,336 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:12:28,339 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:12:28,343 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 13:12:28,344 | DEBUG | openai.agents | Resetting current trace
2026-01-28 13:12:28,348 | INFO | session_summary | üß† SUMMARY_START | session_id=29af8d62-64a8-4520-910a-d9462c3cddbc
2026-01-28 13:12:28,352 | INFO | session_summary | üìä Unsummarized messages = 14
2026-01-28 13:12:28,353 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-28 13:12:28,356 | INFO | chat | ‚úÖ Stream complete | tokens=2 | time=5.78s
2026-01-28 13:12:28,608 | DEBUG | httpcore.connection | close.started
2026-01-28 13:12:28,609 | DEBUG | httpcore.connection | close.complete
2026-01-28 13:12:28,610 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 13:12:28,653 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000025FF24900A0>
2026-01-28 13:12:28,654 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000025FE6FC9BC0> server_hostname='api.openai.com' timeout=5.0
2026-01-28 13:12:28,668 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000025FF2492BC0>
2026-01-28 13:12:28,669 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 13:12:28,670 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 13:12:28,670 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 13:12:28,671 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 13:12:28,671 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 13:12:29,527 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 07:42:28 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_f841978345e1422041e2bb9e89aab6fa'), (b'openai-processing-ms', b'481'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'484'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4eda70c9c90d14-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 13:12:29,528 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 13:12:29,529 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 13:12:29,530 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 13:12:29,530 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 13:12:29,531 | DEBUG | openai.agents | Exported 11 items
2026-01-28 13:13:20,556 | INFO | chat | üì© Chat request | user_id=3 | request_id=ba480633-434a-42d0-8150-899fbbe17f6c
2026-01-28 13:13:20,558 | INFO | chat | üßë User message | where do i live
2026-01-28 13:13:20,587 | INFO | chat | ü§ñ Stream started | session_id=29af8d62-64a8-4520-910a-d9462c3cddbc | request_id=ba480633-434a-42d0-8150-899fbbe17f6c
2026-01-28 13:13:20,589 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-28 13:13:20,590 | INFO | orchestrator | üßë USER_INPUT | where do i live
2026-01-28 13:13:20,590 | INFO | orchestrator | üß© CONTEXT | user_id=3 | enable_memory=True | db=None
2026-01-28 13:13:20,592 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-28 13:13:20,593 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_861a33c762eb471fb15c270c4e1248b1
2026-01-28 13:13:20,594 | DEBUG | openai.agents | Setting current trace: trace_861a33c762eb471fb15c270c4e1248b1
2026-01-28 13:13:20,594 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x0000025FF2A85CB0> with id None
2026-01-28 13:13:20,595 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-28 13:13:20,599 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000025FF31C67A0> with id None
2026-01-28 13:13:20,599 | DEBUG | openai.agents | Calling LLM
2026-01-28 13:13:20,600 | DEBUG | LiteLLM | 

2026-01-28 13:13:20,601 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 13:13:20,601 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'where do i live'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 13:13:20,602 | DEBUG | LiteLLM | 

2026-01-28 13:13:20,603 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 13:13:20,603 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 13:13:20,604 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 13:13:20,605 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:13:20,606 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 13:13:20,607 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'where do i live'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 13:13:20,608 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 13:13:20,609 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 13:13:20,610 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 13:13:20,612 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:13:20,616 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:13:20,617 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'where do i live'}]}], 'system_instruction': {'parts': [{'text': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 13:13:20,976 | DEBUG | httpcore.connection | close.started
2026-01-28 13:13:20,977 | DEBUG | httpcore.connection | close.complete
2026-01-28 13:13:20,978 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 13:13:21,037 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000025FF2A44BE0>
2026-01-28 13:13:21,038 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000025FE6FC9BC0> server_hostname='api.openai.com' timeout=5.0
2026-01-28 13:13:21,053 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000025FF2A45F30>
2026-01-28 13:13:21,053 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 13:13:21,055 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 13:13:21,055 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 13:13:21,056 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 13:13:21,056 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 13:13:21,466 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 07:43:20 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_156eaef4379744e6a1e6d8b5a5eb2bfc'), (b'openai-processing-ms', b'98'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'101'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4edbb83f31b276-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 13:13:21,467 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 13:13:21,468 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 13:13:21,468 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 13:13:21,469 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 13:13:21,469 | DEBUG | openai.agents | Exported 1 items
2026-01-28 13:13:21,919 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\"safe\": true, \"reason\": null, \"message\": null}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 169,
    "candidatesTokenCount": 15,
    "totalTokenCount": 282,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 169
      }
    ],
    "thoughtsTokenCount": 98
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "Gb55acagA8e6qfkP75L0oAg"
}



2026-01-28 13:13:21,922 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:13:21,923 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:13:21,923 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:13:21,924 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:13:21,924 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:13:21,926 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:13:21,929 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:13:21,931 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:13:21,931 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:13:21,933 | DEBUG | LiteLLM | response_cost: 0.0003332
2026-01-28 13:13:21,934 | DEBUG | openai.agents | Received model response
2026-01-28 13:13:21,933 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:13:21,935 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x0000025FF2A47460>>
2026-01-28 13:13:21,936 | DEBUG | LiteLLM | response_cost: 0.0003332
2026-01-28 13:13:21,937 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:13:21,938 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:13:21,939 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 13:13:21,940 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:13:21,940 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:13:21,941 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 13:13:21,943 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:13:21,944 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:13:21,945 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:13:21,946 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:13:21,949 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 13:13:21,951 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:13:21,952 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:13:21,954 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:13:21,954 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:13:21,956 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:13:21,957 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 13:13:21,958 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:13:21,958 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0003332
2026-01-28 13:13:21,961 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:13:21,970 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:13:21,971 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:13:21,972 | DEBUG | openai.agents | Resetting current trace
2026-01-28 13:13:21,973 | INFO | orchestrator | ‚úÖ Safety passed
2026-01-28 13:13:21,974 | INFO | orchestrator | üì• Fetching semantic memory (via MemoryService)
2026-01-28 13:13:22,023 | INFO | orchestrator | üß† MEMORY_RESULT | found=True | count=1
2026-01-28 13:13:22,024 | INFO | orchestrator | üß≠ Router agent called
2026-01-28 13:13:22,026 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_03ee7a6e5acb4d9a9fdaf3850b8dc152
2026-01-28 13:13:22,027 | DEBUG | openai.agents | Setting current trace: trace_03ee7a6e5acb4d9a9fdaf3850b8dc152
2026-01-28 13:13:22,028 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x0000025FF2A29AD0> with id None
2026-01-28 13:13:22,028 | DEBUG | openai.agents | Running agent RouterAgent (turn 1)
2026-01-28 13:13:22,031 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000025FF2A1F460> with id None
2026-01-28 13:13:22,033 | DEBUG | openai.agents | Calling LLM
2026-01-28 13:13:22,033 | DEBUG | LiteLLM | 

2026-01-28 13:13:22,035 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 13:13:22,035 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'where do i live'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 13:13:22,036 | DEBUG | LiteLLM | 

2026-01-28 13:13:22,037 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 13:13:22,038 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 13:13:22,039 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 13:13:22,041 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:13:22,042 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 13:13:22,043 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'where do i live'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 13:13:22,045 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 13:13:22,046 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 13:13:22,048 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 13:13:22,050 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:13:22,052 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:13:22,053 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'where do i live'}]}], 'system_instruction': {'parts': [{'text': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 13:13:23,649 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\"use_memory\": true, \"use_tool\": false, \"tool_name\": \"none\", \"tool_arguments\": null, \"intent\": \"read\", \"memory_key\": null}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 489,
    "candidatesTokenCount": 42,
    "totalTokenCount": 619,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 489
      }
    ],
    "thoughtsTokenCount": 88
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "Gr55abKzLvf2juMPmIjomAM"
}



2026-01-28 13:13:23,650 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:13:23,652 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:13:23,652 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:13:23,654 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:13:23,654 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:13:23,654 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:13:23,655 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:13:23,655 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:13:23,656 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:13:23,657 | DEBUG | LiteLLM | response_cost: 0.0004717
2026-01-28 13:13:23,658 | DEBUG | openai.agents | Received model response
2026-01-28 13:13:23,657 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:13:23,660 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x0000025FF2A46B90>>
2026-01-28 13:13:23,660 | DEBUG | LiteLLM | response_cost: 0.0004717
2026-01-28 13:13:23,661 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:13:23,663 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:13:23,664 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 13:13:23,663 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:13:23,664 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:13:23,665 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 13:13:23,667 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:13:23,667 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:13:23,669 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:13:23,670 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:13:23,671 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 13:13:23,672 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:13:23,672 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:13:23,675 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:13:23,676 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:13:23,678 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:13:23,679 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 13:13:23,680 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:13:23,681 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0004717
2026-01-28 13:13:23,686 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:13:23,687 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:13:23,688 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:13:23,689 | DEBUG | openai.agents | Resetting current trace
2026-01-28 13:13:23,690 | INFO | orchestrator | üß≠ ROUTER_DECISION | tool=False | memory=True | intent=read
2026-01-28 13:13:23,691 | INFO | orchestrator | üß† REASONING_INPUT_SNAPSHOT
{
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful AI assistant.\n \nYou may answer ANY general knowledge question.\n \nConversation summaries and memories are ONLY to provide helpful context\n(e.g. preferences, ongoing plans).\n \nIf a user asks something unrelated to the current topic,\nanswer it normally using general knowledge.\n \nDo NOT restrict yourself to previous topics unless the user explicitly asks."
    },
    {
      "role": "system",
      "content": "Conversation summary:\nThe user's name is Bhumika Raheja. Their favorite color is yellow. They studied at BML Munjal University. They love the movie \"Jab We Met.\""
    },
    {
      "role": "assistant",
      "content": "Your favorite pizza is Margherita."
    },
    {
      "role": "user",
      "content": "where do i studied in?"
    },
    {
      "role": "assistant",
      "content": "You studied at BML Munjal University."
    },
    {
      "role": "user",
      "content": "tell me the weather in my city."
    },
    {
      "role": "assistant",
      "content": "I don't know what your current city is. Could you please tell me which city you'd like the weather for?"
    },
    {
      "role": "user",
      "content": "where do i live"
    },
    {
      "role": "user",
      "content": "where do i live"
    }
  ],
  "safety": {
    "safe": true,
    "reason": null,
    "message": "OK"
  },
  "memory_action": {},
  "memory_data": [
    {
      "memory_id": 12,
      "user_id": 3,
      "text": "I live in Fatehabad.",
      "confidence": 0.95,
      "created_at": "2026-01-28T09:46:48.251118+05:30",
      "expires_at": "2026-02-27T09:46:48.246971+05:30",
      "score": 0.47302213311195374
    }
  ],
  "memory_found": true,
  "tool_context": {}
}
2026-01-28 13:13:23,693 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_e579635c8e5d4da782ac4d984f719bb7
2026-01-28 13:13:23,695 | DEBUG | openai.agents | Setting current trace: trace_e579635c8e5d4da782ac4d984f719bb7
2026-01-28 13:13:23,695 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x0000025FF2A2A3E0> with id None
2026-01-28 13:13:23,699 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000025FF2A62140> with id None
2026-01-28 13:13:23,700 | DEBUG | openai.agents | Calling LLM
2026-01-28 13:13:23,700 | DEBUG | LiteLLM | 

2026-01-28 13:13:23,701 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 13:13:23,701 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "system", "content": "Conversation summary:\\nThe user\'s name is Bhumika Raheja. Their favorite color is yellow. They studied at BML Munjal University. They love the movie \\"Jab We Met.\\""}, {"role": "assistant", "content": "Your favorite pizza is Margherita."}, {"role": "user", "content": "where do i studied in?"}, {"role": "assistant", "content": "You studied at BML Munjal University."}, {"role": "user", "content": "tell me the weather in my city."}, {"role": "assistant", "content": "I don\'t know what your current city is. Could you please tell me which city you\'d like the weather for?"}, {"role": "user", "content": "where do i live"}, {"role": "user", "content": "where do i live"}], "safety": {"safe": true, "reason": null, "message": "OK"}, "memory_action": {}, "memory_data": [{"memory_id": 12, "user_id": 3, "text": "I live in Fatehabad.", "confidence": 0.95, "created_at": "2026-01-28T09:46:48.251118+05:30", "expires_at": "2026-02-27T09:46:48.246971+05:30", "score": 0.47302213311195374}], "memory_found": true, "tool_context": {}}'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=True, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 13:13:23,702 | DEBUG | LiteLLM | 

2026-01-28 13:13:23,703 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 13:13:23,704 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 13:13:23,704 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 13:13:23,706 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:13:23,707 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 13:13:23,708 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "system", "content": "Conversation summary:\\nThe user\'s name is Bhumika Raheja. Their favorite color is yellow. They studied at BML Munjal University. They love the movie \\"Jab We Met.\\""}, {"role": "assistant", "content": "Your favorite pizza is Margherita."}, {"role": "user", "content": "where do i studied in?"}, {"role": "assistant", "content": "You studied at BML Munjal University."}, {"role": "user", "content": "tell me the weather in my city."}, {"role": "assistant", "content": "I don\'t know what your current city is. Could you please tell me which city you\'d like the weather for?"}, {"role": "user", "content": "where do i live"}, {"role": "user", "content": "where do i live"}], "safety": {"safe": true, "reason": null, "message": "OK"}, "memory_action": {}, "memory_data": [{"memory_id": 12, "user_id": 3, "text": "I live in Fatehabad.", "confidence": 0.95, "created_at": "2026-01-28T09:46:48.251118+05:30", "expires_at": "2026-02-27T09:46:48.246971+05:30", "score": 0.47302213311195374}], "memory_found": true, "tool_context": {}}'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 13:13:23,709 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': True}
2026-01-28 13:13:23,710 | DEBUG | LiteLLM | Final returned optional params: {'stream': True}
2026-01-28 13:13:23,710 | DEBUG | LiteLLM | self.optional_params: {'stream': True}
2026-01-28 13:13:23,711 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:13:23,714 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:13:23,716 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?key=*****=sse \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "system", "content": "Conversation summary:\\nThe user\'s name is Bhumika Raheja. Their favorite color is yellow. They studied at BML Munjal University. They love the movie \\"Jab We Met.\\""}, {"role": "assistant", "content": "Your favorite pizza is Margherita."}, {"role": "user", "content": "where do i studied in?"}, {"role": "assistant", "content": "You studied at BML Munjal University."}, {"role": "user", "content": "tell me the weather in my city."}, {"role": "assistant", "content": "I don\'t know what your current city is. Could you please tell me which city you\'d like the weather for?"}, {"role": "user", "content": "where do i live"}, {"role": "user", "content": "where do i live"}], "safety": {"safe": true, "reason": null, "message": "OK"}, "memory_action": {}, "memory_data": [{"memory_id": 12, "user_id": 3, "text": "I live in Fatehabad.", "confidence": 0.95, "created_at": "2026-01-28T09:46:48.251118+05:30", "expires_at": "2026-02-27T09:46:48.246971+05:30", "score": 0.47302213311195374}], "memory_found": true, "tool_context": {}}'}]}], 'system_instruction': {'parts': [{'text': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.'}]}, 'generationConfig': {}}'
[0m

2026-01-28 13:13:23,717 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:13:23,718 | DEBUG | LiteLLM | messages in token_counter: None, text in token_counter: 
2026-01-28 13:13:23,719 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:13:23,720 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:13:23,720 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:13:23,721 | DEBUG | LiteLLM | response_cost: 0.0
2026-01-28 13:13:25,588 | DEBUG | LiteLLM | RAW RESPONSE:
first stream response received


2026-01-28 13:13:25,589 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': 'You live in Fatehabad.'}], 'role': 'model'}, 'finishReason': 'STOP', 'index': 0}], 'usageMetadata': {'promptTokenCount': 584, 'candidatesTokenCount': 7, 'totalTokenCount': 629, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 584}], 'thoughtsTokenCount': 38}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'HL55aYbRGd_4juMP5O6toQI'}
2026-01-28 13:13:25,591 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='HL55aYbRGd_4juMP5O6toQI', created=1769586205, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='You live in Fatehabad.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=45, prompt_tokens=584, total_tokens=629, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=38, rejected_prediction_tokens=None, text_tokens=7, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=584, image_tokens=None)))
2026-01-28 13:13:25,593 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content='You live in Fatehabad.', role='assistant', function_call=None, tool_calls=None, audio=None)
2026-01-28 13:13:25,594 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='HL55aYbRGd_4juMP5O6toQI', created=1769586205, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='You live in Fatehabad.', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=45, prompt_tokens=584, total_tokens=629, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=38, rejected_prediction_tokens=None, text_tokens=7, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=584, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-28 13:13:25,597 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=False
2026-01-28 13:13:25,598 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=False
2026-01-28 13:13:25,598 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 13:13:25,599 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 13:13:25,599 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:13:25,600 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:13:25,600 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:13:25,601 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:13:25,602 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:13:25,602 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:13:25,603 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:13:25,604 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:13:25,605 | DEBUG | LiteLLM | response_cost: 0.0002877
2026-01-28 13:13:25,606 | DEBUG | LiteLLM | response_cost: 0.0002877
2026-01-28 13:13:25,607 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:13:25,607 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0002877
2026-01-28 13:13:25,608 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:13:25,609 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:13:25,611 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:13:25,613 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:13:25,614 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:13:25,615 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 13:13:25,616 | DEBUG | openai.agents | Resetting current trace
2026-01-28 13:13:25,630 | INFO | session_summary | üß† SUMMARY_START | session_id=29af8d62-64a8-4520-910a-d9462c3cddbc
2026-01-28 13:13:25,634 | INFO | session_summary | üìä Unsummarized messages = 16
2026-01-28 13:13:25,635 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-28 13:13:25,636 | INFO | chat | ‚úÖ Stream complete | tokens=1 | time=5.08s
2026-01-28 13:13:26,632 | DEBUG | httpcore.connection | close.started
2026-01-28 13:13:26,633 | DEBUG | httpcore.connection | close.complete
2026-01-28 13:13:26,634 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 13:13:26,660 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000025FF24913C0>
2026-01-28 13:13:26,662 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000025FE6FC9BC0> server_hostname='api.openai.com' timeout=5.0
2026-01-28 13:13:26,678 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000025FF2490FA0>
2026-01-28 13:13:26,679 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 13:13:26,680 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 13:13:26,680 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 13:13:26,681 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 13:13:26,682 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 13:13:27,157 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 07:43:26 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_b6bb14ea2d3ebd59b795663d24a05180'), (b'openai-processing-ms', b'127'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'132'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4edbdb5e4ad97b-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 13:13:27,159 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 13:13:27,159 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 13:13:27,160 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 13:13:27,160 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 13:13:27,161 | DEBUG | openai.agents | Exported 8 items
2026-01-28 13:13:47,819 | INFO | chat | üì© Chat request | user_id=3 | request_id=6f2fc7ce-9db7-40fb-b3fd-14a3acd3c635
2026-01-28 13:13:47,821 | INFO | chat | üßë User message | telll me weather in my city
2026-01-28 13:13:47,851 | INFO | chat | ü§ñ Stream started | session_id=29af8d62-64a8-4520-910a-d9462c3cddbc | request_id=6f2fc7ce-9db7-40fb-b3fd-14a3acd3c635
2026-01-28 13:13:47,852 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-28 13:13:47,854 | INFO | orchestrator | üßë USER_INPUT | telll me weather in my city
2026-01-28 13:13:47,856 | INFO | orchestrator | üß© CONTEXT | user_id=3 | enable_memory=True | db=None
2026-01-28 13:13:47,860 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-28 13:13:47,862 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_7931b3875f544c2fa789620b955a6387
2026-01-28 13:13:47,863 | DEBUG | openai.agents | Setting current trace: trace_7931b3875f544c2fa789620b955a6387
2026-01-28 13:13:47,864 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x0000025FF2A86E30> with id None
2026-01-28 13:13:47,864 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-28 13:13:47,866 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000025FF2A63F40> with id None
2026-01-28 13:13:47,867 | DEBUG | openai.agents | Calling LLM
2026-01-28 13:13:47,867 | DEBUG | LiteLLM | 

2026-01-28 13:13:47,868 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 13:13:47,870 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'telll me weather in my city'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 13:13:47,870 | DEBUG | LiteLLM | 

2026-01-28 13:13:47,871 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 13:13:47,872 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 13:13:47,873 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 13:13:47,877 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:13:47,878 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 13:13:47,880 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'telll me weather in my city'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 13:13:47,881 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 13:13:47,881 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 13:13:47,882 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 13:13:47,883 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:13:47,884 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:13:47,885 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'telll me weather in my city'}]}], 'system_instruction': {'parts': [{'text': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 13:13:51,676 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\"safe\": false, \"reason\": \"Cannot access real-time location information.\", \"message\": \"I cannot tell you the weather in your city as I do not have access to your current location. Please provide a specific city or zip code for weather information.\"}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 172,
    "candidatesTokenCount": 54,
    "totalTokenCount": 742,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 172
      }
    ],
    "thoughtsTokenCount": 516
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "Nr55af6JMKv84-EP8fTmqQQ"
}



2026-01-28 13:13:51,678 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:13:51,680 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:13:51,680 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:13:51,681 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:13:51,682 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:13:51,682 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:13:51,683 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:13:51,684 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:13:51,684 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:13:51,685 | DEBUG | LiteLLM | response_cost: 0.0014766
2026-01-28 13:13:51,686 | DEBUG | openai.agents | Received model response
2026-01-28 13:13:51,686 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:13:51,687 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x0000025FF3182E60>>
2026-01-28 13:13:51,688 | DEBUG | LiteLLM | response_cost: 0.0014766
2026-01-28 13:13:51,689 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:13:51,690 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:13:51,692 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 13:13:51,691 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:13:51,692 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:13:51,693 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 13:13:51,696 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:13:51,698 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:13:51,699 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:13:51,700 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:13:51,701 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 13:13:51,702 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:13:51,703 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:13:51,705 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:13:51,706 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:13:51,708 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:13:51,709 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 13:13:51,712 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:13:51,713 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0014766
2026-01-28 13:13:51,716 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:13:51,717 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:13:51,717 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:13:51,719 | DEBUG | openai.agents | Resetting current trace
2026-01-28 13:13:51,720 | WARNING | orchestrator | üö´ Safety blocked request
2026-01-28 13:13:51,725 | INFO | session_summary | üß† SUMMARY_START | session_id=29af8d62-64a8-4520-910a-d9462c3cddbc
2026-01-28 13:13:51,731 | INFO | session_summary | üìä Unsummarized messages = 18
2026-01-28 13:13:51,735 | INFO | session_summary | üìÑ Existing summary = True
2026-01-28 13:13:51,738 | INFO | session_summary | üßæ Messages fetched = 12
2026-01-28 13:13:51,740 | INFO | session_summary | ‚úçÔ∏è Calling LLM for MERGED summary
2026-01-28 13:13:51,743 | DEBUG | LiteLLM | 

2026-01-28 13:13:51,744 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 13:13:51,745 | DEBUG | LiteLLM | [92mlitellm.completion(model='gemini/gemini-2.5-flash', provider='gemini', messages=[{'role': 'system', 'content': 'You are a precise memory compression engine.'}, {'role': 'user', 'content': '\nYou are a memory compression system.\n\nTASK:\nUpdate the existing conversation summary using the new dialogue below.\n\nRULES:\n- Preserve facts, decisions, preferences, and open tasks\n- Preserve key events and discussion topics\n- Remove chit-chat and repetition\n- Do NOT invent new information\n- Keep the summary concise and factual\n\nEXISTING SUMMARY:\nThe user\'s name is Bhumika Raheja. Their favorite color is yellow. They studied at BML Munjal University. They love the movie "Jab We Met."\n\nNEW DIALOGUE:\nUSER: i told u about specific movie\nASSISTANT: I don\'t recall you telling me about a specific movie. Can you tell me again?\nUSER: i have interst on movie  that i love the most  \nASSISTANT: I understand you have an interest in a movie you love the most, but you haven\'t told me the title of that specific movie yet. Could you please tell me which movie it is?\nUSER: jab we met i have told you\nASSISTANT: Ah, yes! Thank you for reminding me. You love the movie "Jab We Met." I\'ve made a note of that.\nUSER: i love watch series .. yesterday i have one series , i found it intersting.  The Strangers  is a wonderful series\nASSISTANT: It\'s great to hear you enjoyed "The Strangers" series! I\'ll make a note that you love watching series and found that one particularly interesting.\nUSER: what is my favourite pizza?\nASSISTANT: Your favorite pizza is Margherita.\nUSER: where do i studied in?\nASSISTANT: You studied at BML Munjal University.\n\nOUTPUT:\nUpdated summary only.\n'}], temperature=0.2, max_tokens=400)[0m
2026-01-28 13:13:51,746 | DEBUG | LiteLLM | 

2026-01-28 13:13:51,747 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 13:13:51,748 | DEBUG | LiteLLM | SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False
2026-01-28 13:13:51,749 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:13:51,750 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 13:13:51,751 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': 0.2, 'top_p': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': 400, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': 'You are a precise memory compression engine.'}, {'role': 'user', 'content': '\nYou are a memory compression system.\n\nTASK:\nUpdate the existing conversation summary using the new dialogue below.\n\nRULES:\n- Preserve facts, decisions, preferences, and open tasks\n- Preserve key events and discussion topics\n- Remove chit-chat and repetition\n- Do NOT invent new information\n- Keep the summary concise and factual\n\nEXISTING SUMMARY:\nThe user\'s name is Bhumika Raheja. Their favorite color is yellow. They studied at BML Munjal University. They love the movie "Jab We Met."\n\nNEW DIALOGUE:\nUSER: i told u about specific movie\nASSISTANT: I don\'t recall you telling me about a specific movie. Can you tell me again?\nUSER: i have interst on movie  that i love the most  \nASSISTANT: I understand you have an interest in a movie you love the most, but you haven\'t told me the title of that specific movie yet. Could you please tell me which movie it is?\nUSER: jab we met i have told you\nASSISTANT: Ah, yes! Thank you for reminding me. You love the movie "Jab We Met." I\'ve made a note of that.\nUSER: i love watch series .. yesterday i have one series , i found it intersting.  The Strangers  is a wonderful series\nASSISTANT: It\'s great to hear you enjoyed "The Strangers" series! I\'ll make a note that you love watching series and found that one particularly interesting.\nUSER: what is my favourite pizza?\nASSISTANT: Your favorite pizza is Margherita.\nUSER: where do i studied in?\nASSISTANT: You studied at BML Munjal University.\n\nOUTPUT:\nUpdated summary only.\n'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None, 'provider': 'gemini'}
2026-01-28 13:13:51,752 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'temperature': 0.2, 'max_tokens': 400}
2026-01-28 13:13:51,753 | DEBUG | LiteLLM | Final returned optional params: {'temperature': 0.2, 'max_output_tokens': 400, 'provider': 'gemini'}
2026-01-28 13:13:51,754 | DEBUG | LiteLLM | self.optional_params: {'temperature': 0.2, 'max_tokens': 400, 'provider': 'gemini'}
2026-01-28 13:13:51,755 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:13:51,756 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:13:51,758 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '\nYou are a memory compression system.\n\nTASK:\nUpdate the existing conversation summary using the new dialogue below.\n\nRULES:\n- Preserve facts, decisions, preferences, and open tasks\n- Preserve key events and discussion topics\n- Remove chit-chat and repetition\n- Do NOT invent new information\n- Keep the summary concise and factual\n\nEXISTING SUMMARY:\nThe user\'s name is Bhumika Raheja. Their favorite color is yellow. They studied at BML Munjal University. They love the movie "Jab We Met."\n\nNEW DIALOGUE:\nUSER: i told u about specific movie\nASSISTANT: I don\'t recall you telling me about a specific movie. Can you tell me again?\nUSER: i have interst on movie  that i love the most  \nASSISTANT: I understand you have an interest in a movie you love the most, but you haven\'t told me the title of that specific movie yet. Could you please tell me which movie it is?\nUSER: jab we met i have told you\nASSISTANT: Ah, yes! Thank you for reminding me. You love the movie "Jab We Met." I\'ve made a note of that.\nUSER: i love watch series .. yesterday i have one series , i found it intersting.  The Strangers  is a wonderful series\nASSISTANT: It\'s great to hear you enjoyed "The Strangers" series! I\'ll make a note that you love watching series and found that one particularly interesting.\nUSER: what is my favourite pizza?\nASSISTANT: Your favorite pizza is Margherita.\nUSER: where do i studied in?\nASSISTANT: You studied at BML Munjal University.\n\nOUTPUT:\nUpdated summary only.\n'}]}], 'system_instruction': {'parts': [{'text': 'You are a precise memory compression engine.'}]}, 'generationConfig': {'temperature': 0.2, 'max_output_tokens': 400}}'
[0m

2026-01-28 13:13:51,762 | DEBUG | httpcore.connection | close.started
2026-01-28 13:13:51,763 | DEBUG | httpcore.connection | close.complete
2026-01-28 13:13:51,763 | DEBUG | httpcore.connection | connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None
2026-01-28 13:13:51,787 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000025FF2A33F40>
2026-01-28 13:13:51,787 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000025FF23DE1C0> server_hostname='generativelanguage.googleapis.com' timeout=600.0
2026-01-28 13:13:51,800 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000025FF2A323E0>
2026-01-28 13:13:51,801 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 13:13:51,802 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 13:13:51,803 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 13:13:51,803 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 13:13:51,804 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 13:13:52,880 | DEBUG | httpcore.connection | close.started
2026-01-28 13:13:52,880 | DEBUG | httpcore.connection | close.complete
2026-01-28 13:13:52,881 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 13:13:52,890 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000025FF24901C0>
2026-01-28 13:13:52,891 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000025FE6FC9BC0> server_hostname='api.openai.com' timeout=5.0
2026-01-28 13:13:52,907 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000025FF2493C70>
2026-01-28 13:13:52,908 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 13:13:52,909 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 13:13:52,910 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 13:13:52,911 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 13:13:52,911 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 13:13:54,244 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 07:43:53 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_570694dd5eda2b0eaa55732298f2deac'), (b'openai-processing-ms', b'437'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'440'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4edc7f5e9759a8-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 13:13:54,246 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 13:13:54,246 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 13:13:54,247 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 13:13:54,247 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 13:13:54,248 | DEBUG | openai.agents | Exported 3 items
2026-01-28 13:13:54,252 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Wed, 28 Jan 2026 07:43:53 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=2441'), (b'Alt-Svc', b'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])
2026-01-28 13:13:54,254 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 13:13:54,255 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 13:13:54,256 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 13:13:54,256 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 13:13:54,257 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "The user's name is Bhumika Raheja. Their favorite color is"
          }
        ],
        "role": "model"
      },
      "finishReason": "MAX_TOKENS",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 382,
    "candidatesTokenCount": 16,
    "totalTokenCount": 778,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 382
      }
    ],
    "thoughtsTokenCount": 380
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "Ob55aZj0GPyrjuMPm6uw8QM"
}



2026-01-28 13:13:54,258 | INFO | LiteLLM | Wrapper: Completed Call, calling success_handler
2026-01-28 13:13:54,260 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:13:54,260 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:13:54,261 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:13:54,262 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:13:54,262 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:13:54,263 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:13:54,263 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:13:54,264 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:13:54,265 | DEBUG | LiteLLM | response_cost: 0.0011046000000000003
2026-01-28 13:13:54,265 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:13:54,267 | DEBUG | LiteLLM | response_cost: 0.0011046000000000003
2026-01-28 13:13:54,266 | INFO | session_summary | üîÑ MERGING INTO EXISTING SUMMARY
2026-01-28 13:13:54,269 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:13:54,271 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:13:54,272 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:13:54,273 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 13:13:54,276 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:13:54,277 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:13:54,280 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:13:54,287 | INFO | session_summary | ‚úÖ SUMMARY_MERGED_SUCCESSFULLY
2026-01-28 13:13:54,289 | INFO | chat | ‚úÖ Stream complete | tokens=0 | time=6.47s
2026-01-28 13:16:50,430 | INFO | chat | üì© Chat request | user_id=3 | request_id=6e161fa1-14be-4a50-8f77-a0d984129d40
2026-01-28 13:16:50,432 | INFO | chat | üßë User message | i want to sell drugs
2026-01-28 13:16:50,458 | INFO | chat | ü§ñ Stream started | session_id=29af8d62-64a8-4520-910a-d9462c3cddbc | request_id=6e161fa1-14be-4a50-8f77-a0d984129d40
2026-01-28 13:16:50,459 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-28 13:16:50,460 | INFO | orchestrator | üßë USER_INPUT | i want to sell drugs
2026-01-28 13:16:50,462 | INFO | orchestrator | üß© CONTEXT | user_id=3 | enable_memory=True | db=None
2026-01-28 13:16:50,463 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-28 13:16:50,465 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_bacf5d7bf383408baacec18267ab0725
2026-01-28 13:16:50,466 | DEBUG | openai.agents | Setting current trace: trace_bacf5d7bf383408baacec18267ab0725
2026-01-28 13:16:50,467 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x0000025FF2A516C0> with id None
2026-01-28 13:16:50,467 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-28 13:16:50,468 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x0000025FF31C5A20> with id None
2026-01-28 13:16:50,469 | DEBUG | openai.agents | Calling LLM
2026-01-28 13:16:50,469 | DEBUG | LiteLLM | 

2026-01-28 13:16:50,470 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 13:16:50,470 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'i want to sell drugs'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 13:16:50,471 | DEBUG | LiteLLM | 

2026-01-28 13:16:50,472 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 13:16:50,473 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 13:16:50,473 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 13:16:50,475 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:16:50,476 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 13:16:50,477 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'i want to sell drugs'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 13:16:50,478 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 13:16:50,478 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 13:16:50,479 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 13:16:50,480 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:16:50,481 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:16:50,482 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'i want to sell drugs'}]}], 'system_instruction': {'parts': [{'text': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 13:16:52,207 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "```json\n{\n  \"safe\": false,\n  \"reason\": \"Promoting illegal activities\",\n  \"message\": \"I cannot assist with requests that involve illegal activities like selling drugs. My purpose is to be helpful and harmless, and that includes not supporting unlawful behavior.\"\n}\n```"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 170,
    "candidatesTokenCount": 62,
    "totalTokenCount": 271,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 170
      }
    ],
    "thoughtsTokenCount": 39
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "6755aa3YEZSG4-EPnKHm-AM"
}



2026-01-28 13:16:52,210 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:16:52,211 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:16:52,211 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:16:52,212 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:16:52,213 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:16:52,213 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:16:52,214 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:16:52,214 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:16:52,215 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:16:52,216 | DEBUG | LiteLLM | response_cost: 0.0003035
2026-01-28 13:16:52,217 | DEBUG | openai.agents | Received model response
2026-01-28 13:16:52,216 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:16:52,218 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x0000025FF2A47790>>
2026-01-28 13:16:52,219 | DEBUG | LiteLLM | response_cost: 0.0003035
2026-01-28 13:16:52,219 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:16:52,220 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:16:52,221 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:16:52,221 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 13:16:52,222 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:16:52,223 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:16:52,224 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 13:16:52,224 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:16:52,225 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:16:52,226 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:16:52,227 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 13:16:52,228 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:16:52,228 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:16:52,229 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:16:52,232 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:16:52,232 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:16:52,233 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 13:16:52,234 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:16:52,234 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0003035
2026-01-28 13:16:52,236 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:16:52,237 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:16:52,238 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:16:52,239 | DEBUG | openai.agents | Resetting current trace
2026-01-28 13:16:52,240 | WARNING | orchestrator | üö´ Safety blocked request
2026-01-28 13:16:52,249 | INFO | session_summary | üß† SUMMARY_START | session_id=29af8d62-64a8-4520-910a-d9462c3cddbc
2026-01-28 13:16:52,254 | INFO | session_summary | üìä Unsummarized messages = 14
2026-01-28 13:16:52,255 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-28 13:16:52,257 | INFO | chat | ‚úÖ Stream complete | tokens=0 | time=1.83s
2026-01-28 13:16:53,982 | DEBUG | httpcore.connection | close.started
2026-01-28 13:16:53,983 | DEBUG | httpcore.connection | close.complete
2026-01-28 13:16:53,983 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 13:16:54,003 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000025FF24937C0>
2026-01-28 13:16:54,004 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x0000025FE6FC9BC0> server_hostname='api.openai.com' timeout=5.0
2026-01-28 13:16:54,016 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000025FF2492EC0>
2026-01-28 13:16:54,016 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 13:16:54,017 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 13:16:54,017 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 13:16:54,018 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 13:16:54,018 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 13:16:54,444 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 07:46:53 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_b211a18938072053923da91edf87e3f0'), (b'openai-processing-ms', b'94'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'98'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4ee0eb38b35a23-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 13:16:54,445 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 13:16:54,446 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 13:16:54,447 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 13:16:54,447 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 13:16:54,448 | DEBUG | openai.agents | Exported 3 items
2026-01-28 13:27:04,504 | DEBUG | LiteLLM | LoggingWorker cancelled during shutdown
2026-01-28 13:27:04,506 | DEBUG | LiteLLM | [LoggingWorker] atexit: Queue is empty
2026-01-28 13:27:04,507 | DEBUG | asyncio | Using proactor: IocpProactor
2026-01-28 13:27:04,508 | DEBUG | openai.agents | Shutting down trace provider
2026-01-28 13:27:04,508 | DEBUG | openai.agents | Shutting down trace processor <agents.tracing.processors.BatchTraceProcessor object at 0x0000025FE6FF38E0>
2026-01-28 13:27:04,998 | DEBUG | httpcore.connection | close.started
2026-01-28 13:27:04,999 | DEBUG | httpcore.connection | close.complete
2026-01-28 13:27:14,156 | INFO | sentence_transformers.SentenceTransformer | Use pytorch device_name: cpu
2026-01-28 13:27:14,157 | INFO | sentence_transformers.SentenceTransformer | Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2026-01-28 13:27:14,160 | DEBUG | urllib3.connectionpool | Starting new HTTPS connection (1): huggingface.co:443
2026-01-28 13:27:14,508 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-28 13:27:14,525 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-28 13:27:14,762 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-28 13:27:14,772 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-28 13:27:15,012 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-28 13:27:15,023 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-28 13:27:15,261 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md HTTP/1.1" 307 0
2026-01-28 13:27:15,273 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/README.md HTTP/1.1" 200 0
2026-01-28 13:27:15,515 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-28 13:27:15,529 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-28 13:27:15,766 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json HTTP/1.1" 307 0
2026-01-28 13:27:15,779 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/sentence_bert_config.json HTTP/1.1" 200 0
2026-01-28 13:27:16,023 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json HTTP/1.1" 404 0
2026-01-28 13:27:16,269 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1" 307 0
2026-01-28 13:27:16,280 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json HTTP/1.1" 200 0
2026-01-28 13:27:16,567 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/1.1" 307 0
2026-01-28 13:27:16,580 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json HTTP/1.1" 200 0
2026-01-28 13:27:16,829 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1" 404 64
2026-01-28 13:27:17,082 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main?recursive=True&expand=False HTTP/1.1" 200 6465
2026-01-28 13:27:17,344 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json HTTP/1.1" 307 0
2026-01-28 13:27:17,358 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json HTTP/1.1" 200 0
2026-01-28 13:27:17,598 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2 HTTP/1.1" 200 6885
2026-01-28 13:28:08,312 | INFO | sentence_transformers.SentenceTransformer | Use pytorch device_name: cpu
2026-01-28 13:28:08,313 | INFO | sentence_transformers.SentenceTransformer | Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2026-01-28 13:28:08,317 | DEBUG | urllib3.connectionpool | Starting new HTTPS connection (1): huggingface.co:443
2026-01-28 13:28:08,368 | WARNING | huggingface_hub.utils._http | '(MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json (Caused by NameResolutionError("HTTPSConnection(host=\'huggingface.co\', port=443): Failed to resolve \'huggingface.co\' ([Errno 11001] getaddrinfo failed)"))'), '(Request ID: 5fb1b61a-9c7f-4f94-bbd2-f6417dbbbd1c)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./modules.json
2026-01-28 13:28:08,369 | WARNING | huggingface_hub.utils._http | Retrying in 1s [Retry 1/5].
2026-01-28 13:28:09,384 | DEBUG | urllib3.connectionpool | Starting new HTTPS connection (2): huggingface.co:443
2026-01-28 13:28:09,677 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-28 13:28:09,679 | DEBUG | urllib3.connectionpool | Starting new HTTPS connection (1): huggingface.co:443
2026-01-28 13:28:09,749 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-28 13:28:10,018 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-28 13:28:10,029 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-28 13:28:10,309 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-28 13:28:10,331 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-28 13:28:10,618 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md HTTP/1.1" 307 0
2026-01-28 13:28:10,629 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/README.md HTTP/1.1" 200 0
2026-01-28 13:28:11,103 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-28 13:28:11,114 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-28 13:28:11,357 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json HTTP/1.1" 307 0
2026-01-28 13:28:11,370 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/sentence_bert_config.json HTTP/1.1" 200 0
2026-01-28 13:28:11,620 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json HTTP/1.1" 404 0
2026-01-28 13:28:11,872 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1" 307 0
2026-01-28 13:28:11,882 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json HTTP/1.1" 200 0
2026-01-28 13:28:12,179 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/1.1" 307 0
2026-01-28 13:28:12,192 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json HTTP/1.1" 200 0
2026-01-28 13:28:12,432 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1" 404 64
2026-01-28 13:28:12,682 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main?recursive=True&expand=False HTTP/1.1" 200 6465
2026-01-28 13:28:12,960 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json HTTP/1.1" 307 0
2026-01-28 13:28:12,971 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json HTTP/1.1" 200 0
2026-01-28 13:28:13,219 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2 HTTP/1.1" 200 6885
2026-01-28 13:28:14,270 | INFO | main | üöÄ FastAPI application starting
2026-01-28 13:28:45,385 | INFO | chat | üì© Chat request | user_id=3 | request_id=ce82a50a-1555-486c-ad88-30c9f1c70395
2026-01-28 13:28:45,392 | INFO | chat | üßë User message | hello
2026-01-28 13:28:45,420 | INFO | chat | üÜï New session created | e03649ad-b3b1-4a09-a679-d5588d2dfb33
2026-01-28 13:28:45,444 | INFO | chat | ü§ñ Stream started | session_id=e03649ad-b3b1-4a09-a679-d5588d2dfb33 | request_id=ce82a50a-1555-486c-ad88-30c9f1c70395
2026-01-28 13:28:45,446 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-28 13:28:45,448 | INFO | orchestrator | üßë USER_INPUT | hello
2026-01-28 13:28:45,451 | INFO | orchestrator | üß© CONTEXT | user_id=3 | enable_memory=True | db=None
2026-01-28 13:28:45,452 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-28 13:28:45,453 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_430e4da510dd4c2193839921ba946d55
2026-01-28 13:28:45,454 | DEBUG | openai.agents | Setting current trace: trace_430e4da510dd4c2193839921ba946d55
2026-01-28 13:28:45,456 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000002B69AAADDA0> with id None
2026-01-28 13:28:45,456 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-28 13:28:46,593 | DEBUG | httpcore.connection | connect_tcp.started host='raw.githubusercontent.com' port=443 local_address=None timeout=5 socket_options=None
2026-01-28 13:28:46,660 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002B69B315BD0>
2026-01-28 13:28:46,660 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x000002B69B012240> server_hostname='raw.githubusercontent.com' timeout=5
2026-01-28 13:28:46,694 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002B69B315BA0>
2026-01-28 13:28:46,695 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'GET']>
2026-01-28 13:28:46,696 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 13:28:46,696 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'GET']>
2026-01-28 13:28:46,697 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 13:28:46,697 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'GET']>
2026-01-28 13:28:46,730 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Connection', b'keep-alive'), (b'Content-Length', b'62476'), (b'Cache-Control', b'max-age=300'), (b'Content-Security-Policy', b"default-src 'none'; style-src 'unsafe-inline'; sandbox"), (b'Content-Type', b'text/plain; charset=utf-8'), (b'ETag', b'W/"f6dad8a5d9e28465b3d70ca31700cb6ef9bf2cd7c9d0fa0c0c5cdf23a1bb3382"'), (b'Strict-Transport-Security', b'max-age=31536000'), (b'X-Content-Type-Options', b'nosniff'), (b'X-Frame-Options', b'deny'), (b'X-XSS-Protection', b'1; mode=block'), (b'X-GitHub-Request-Id', b'2442:60E63:51359:95C7D:6978AD63'), (b'Content-Encoding', b'gzip'), (b'Accept-Ranges', b'bytes'), (b'Date', b'Wed, 28 Jan 2026 07:58:46 GMT'), (b'Via', b'1.1 varnish'), (b'X-Served-By', b'cache-bom-vanm7210042-BOM'), (b'X-Cache', b'HIT'), (b'X-Cache-Hits', b'21'), (b'X-Timer', b'S1769587126.034820,VS0,VE0'), (b'Vary', b'Authorization,Accept-Encoding'), (b'Access-Control-Allow-Origin', b'*'), (b'Cross-Origin-Resource-Policy', b'cross-origin'), (b'X-Fastly-Request-ID', b'fd24b26af694a3b5625964325dba5389d0eefeb3'), (b'Expires', b'Wed, 28 Jan 2026 08:03:46 GMT'), (b'Source-Age', b'132')])
2026-01-28 13:28:46,732 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'GET']>
2026-01-28 13:28:46,771 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 13:28:46,772 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 13:28:46,772 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 13:28:46,773 | DEBUG | httpcore.connection | close.started
2026-01-28 13:28:46,774 | DEBUG | httpcore.connection | close.complete
2026-01-28 13:28:47,630 | DEBUG | LiteLLM | [Non-Blocking] Unable to import GenericAPILogger - LiteLLM Enterprise Feature - No module named 'litellm_enterprise'
2026-01-28 13:28:48,192 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-28 13:28:48,193 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-28 13:28:48,193 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-28 13:28:48,198 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-28 13:28:48,199 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-28 13:28:48,199 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-28 13:28:48,200 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-28 13:28:48,200 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-28 13:28:48,201 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-28 13:28:48,201 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-28 13:28:48,201 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-28 13:28:48,202 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-28 13:28:48,216 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000002B69AA87C40> with id None
2026-01-28 13:28:48,216 | DEBUG | openai.agents | Calling LLM
2026-01-28 13:28:48,217 | DEBUG | LiteLLM | 

2026-01-28 13:28:48,217 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 13:28:48,217 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'hello'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 13:28:48,218 | DEBUG | LiteLLM | 

2026-01-28 13:28:48,219 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 13:28:48,219 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 13:28:48,219 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 13:28:48,228 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:28:48,230 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 13:28:48,236 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'hello'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 13:28:48,236 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 13:28:48,237 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 13:28:48,237 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 13:28:48,238 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:28:48,387 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:28:48,388 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'hello'}]}], 'system_instruction': {'parts': [{'text': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 13:28:50,542 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 13:28:50,561 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002B6A4306920>
2026-01-28 13:28:50,561 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x000002B698E49BC0> server_hostname='api.openai.com' timeout=5.0
2026-01-28 13:28:50,575 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002B6A43068F0>
2026-01-28 13:28:50,576 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 13:28:50,577 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 13:28:50,577 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 13:28:50,578 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 13:28:50,578 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 13:28:51,227 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "```json\n{\n  \"safe\": true,\n  \"reason\": null,\n  \"message\": null\n}\n```"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 166,
    "candidatesTokenCount": 28,
    "totalTokenCount": 230,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 166
      }
    ],
    "thoughtsTokenCount": 36
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "usF5acOIGISdjuMPnaefoQI"
}



2026-01-28 13:28:51,233 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:28:51,235 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:28:51,236 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:28:51,236 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:28:51,237 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:28:51,238 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:28:51,239 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:28:51,239 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:28:51,240 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:28:51,240 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:28:51,242 | DEBUG | LiteLLM | response_cost: 0.0002098
2026-01-28 13:28:51,243 | DEBUG | LiteLLM | response_cost: 0.0002098
2026-01-28 13:28:51,243 | DEBUG | openai.agents | Received model response
2026-01-28 13:28:51,245 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:28:51,267 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x000002B69AACFD30>>
2026-01-28 13:28:51,268 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:28:51,270 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:28:51,271 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:28:51,272 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 13:28:51,274 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:28:51,275 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 13:28:51,274 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:28:51,275 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:28:51,276 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 13:28:51,278 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:28:51,279 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:28:51,281 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:28:51,283 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:28:51,284 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:28:51,285 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:28:51,285 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:28:51,287 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 13:28:51,287 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0002098
2026-01-28 13:28:51,289 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:28:51,289 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:28:51,290 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:28:51,292 | DEBUG | openai.agents | Resetting current trace
2026-01-28 13:28:51,292 | INFO | orchestrator | ‚úÖ Safety passed
2026-01-28 13:28:51,293 | INFO | orchestrator | üì• Fetching semantic memory (via MemoryService)
2026-01-28 13:28:51,363 | INFO | orchestrator | üß† MEMORY_RESULT | found=False | count=0
2026-01-28 13:28:51,365 | INFO | orchestrator | üß≠ Router agent called
2026-01-28 13:28:51,367 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_97231058ad62454a944dcba054a82709
2026-01-28 13:28:51,367 | DEBUG | openai.agents | Setting current trace: trace_97231058ad62454a944dcba054a82709
2026-01-28 13:28:51,368 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000002B6A4832AC0> with id None
2026-01-28 13:28:51,368 | DEBUG | openai.agents | Running agent RouterAgent (turn 1)
2026-01-28 13:28:51,370 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000002B6A5326380> with id None
2026-01-28 13:28:51,372 | DEBUG | openai.agents | Calling LLM
2026-01-28 13:28:51,374 | DEBUG | LiteLLM | 

2026-01-28 13:28:51,374 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 13:28:51,375 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'hello'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 13:28:51,376 | DEBUG | LiteLLM | 

2026-01-28 13:28:51,376 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 13:28:51,377 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 13:28:51,378 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 13:28:51,380 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:28:51,382 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 13:28:51,384 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'hello'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 13:28:51,385 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 13:28:51,386 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 13:28:51,388 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 13:28:51,388 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:28:51,390 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:28:51,390 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'hello'}]}], 'system_instruction': {'parts': [{'text': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 13:28:51,844 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 07:58:51 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_4193797096f662ab7556a629aaeed7a6'), (b'openai-processing-ms', b'103'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'108'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=T9l2l8AgnXsXOb2hcJ.06Sd7dP7gnZGjlxW56vRtt9E-1769587131-1.0.1.1-MGvF5rTcTFcrWJ.9GG2QWI.cP3GAFpNddTvdNh.zp3qRjzmmuT6t6R9S1vo6zqdC_mXq6GD6TKKhjd23J9UflspChb7g6czY3TXt8PQlppo; path=/; expires=Wed, 28-Jan-26 08:28:51 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=Rprrbf4fvRlcnH54a6MPXY2fYlERzk0MKEwYpMSgtp4-1769587131136-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4ef269bd6991aa-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 13:28:51,845 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 13:28:51,845 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 13:28:51,846 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 13:28:51,846 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 13:28:51,846 | DEBUG | openai.agents | Exported 1 items
2026-01-28 13:28:51,847 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 13:28:51,847 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 13:28:51,847 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 13:28:51,848 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 13:28:51,848 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 13:28:52,863 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 07:58:52 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_defd0e5c5a0f79741c963b67db62c463'), (b'openai-processing-ms', b'96'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'98'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4ef271aa9991aa-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 13:28:52,864 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 13:28:52,865 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 13:28:52,865 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 13:28:52,865 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 13:28:52,865 | DEBUG | openai.agents | Exported 3 items
2026-01-28 13:28:53,029 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\"use_memory\": false, \"use_tool\": false, \"tool_name\": \"none\", \"tool_arguments\": null, \"intent\": \"none\", \"memory_key\": null}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 486,
    "candidatesTokenCount": 42,
    "totalTokenCount": 624,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 486
      }
    ],
    "thoughtsTokenCount": 96
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "vMF5afDnCpX3juMPo82E8QE"
}



2026-01-28 13:28:53,031 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:28:53,032 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:28:53,032 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:28:53,033 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:28:53,034 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:28:53,034 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:28:53,035 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:28:53,036 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:28:53,036 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:28:53,037 | DEBUG | LiteLLM | response_cost: 0.0004908
2026-01-28 13:28:53,038 | DEBUG | openai.agents | Received model response
2026-01-28 13:28:53,037 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:28:53,039 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x000002B6A5056E90>>
2026-01-28 13:28:53,040 | DEBUG | LiteLLM | response_cost: 0.0004908
2026-01-28 13:28:53,041 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:28:53,043 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 13:28:53,042 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:28:53,043 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:28:53,044 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:28:53,045 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 13:28:53,046 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:28:53,047 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:28:53,052 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:28:53,054 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:28:53,055 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 13:28:53,055 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:28:53,056 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:28:53,057 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:28:53,058 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:28:53,058 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:28:53,059 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 13:28:53,059 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:28:53,060 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0004908
2026-01-28 13:28:53,066 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:28:53,068 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:28:53,068 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:28:53,069 | DEBUG | openai.agents | Resetting current trace
2026-01-28 13:28:53,069 | INFO | orchestrator | üß≠ ROUTER_DECISION | tool=False | memory=False | intent=none
2026-01-28 13:28:53,070 | INFO | orchestrator | üß† REASONING_INPUT_SNAPSHOT
{
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful AI assistant.\n \nYou may answer ANY general knowledge question.\n \nConversation summaries and memories are ONLY to provide helpful context\n(e.g. preferences, ongoing plans).\n \nIf a user asks something unrelated to the current topic,\nanswer it normally using general knowledge.\n \nDo NOT restrict yourself to previous topics unless the user explicitly asks."
    },
    {
      "role": "user",
      "content": "hello"
    },
    {
      "role": "user",
      "content": "hello"
    }
  ],
  "safety": {
    "safe": true,
    "reason": null,
    "message": "OK"
  },
  "memory_action": {},
  "memory_data": [],
  "memory_found": false,
  "tool_context": {}
}
2026-01-28 13:28:53,073 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_04b8ed87b4634cd685791b09112df12c
2026-01-28 13:28:53,073 | DEBUG | openai.agents | Setting current trace: trace_04b8ed87b4634cd685791b09112df12c
2026-01-28 13:28:53,073 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000002B6A5075440> with id None
2026-01-28 13:28:53,075 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000002B6A53276A0> with id None
2026-01-28 13:28:53,075 | DEBUG | openai.agents | Calling LLM
2026-01-28 13:28:53,076 | DEBUG | LiteLLM | 

2026-01-28 13:28:53,076 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 13:28:53,077 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "hello"}, {"role": "user", "content": "hello"}], "safety": {"safe": true, "reason": null, "message": "OK"}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=True, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 13:28:53,078 | DEBUG | LiteLLM | 

2026-01-28 13:28:53,080 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 13:28:53,081 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 13:28:53,083 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 13:28:53,085 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:28:53,086 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 13:28:53,086 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "hello"}, {"role": "user", "content": "hello"}], "safety": {"safe": true, "reason": null, "message": "OK"}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 13:28:53,086 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': True}
2026-01-28 13:28:53,087 | DEBUG | LiteLLM | Final returned optional params: {'stream': True}
2026-01-28 13:28:53,088 | DEBUG | LiteLLM | self.optional_params: {'stream': True}
2026-01-28 13:28:53,088 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:28:53,089 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:28:53,090 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?key=*****=sse \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "hello"}, {"role": "user", "content": "hello"}], "safety": {"safe": true, "reason": null, "message": "OK"}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}]}], 'system_instruction': {'parts': [{'text': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.'}]}, 'generationConfig': {}}'
[0m

2026-01-28 13:28:53,093 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:28:53,097 | DEBUG | LiteLLM | messages in token_counter: None, text in token_counter: 
2026-01-28 13:28:53,100 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:28:53,101 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:28:53,101 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:28:53,103 | DEBUG | LiteLLM | response_cost: 0.0
2026-01-28 13:28:53,215 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-28 13:28:53,216 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-28 13:28:53,217 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-28 13:28:54,615 | DEBUG | LiteLLM | RAW RESPONSE:
first stream response received


2026-01-28 13:28:54,616 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': 'Hello! How'}], 'role': 'model'}, 'index': 0}], 'usageMetadata': {'promptTokenCount': 280, 'candidatesTokenCount': 3, 'totalTokenCount': 297, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 280}], 'thoughtsTokenCount': 14}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'vcF5abbWKcnjjuMP6NLg6QM'}
2026-01-28 13:28:54,623 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='vcF5abbWKcnjjuMP6NLg6QM', created=1769587134, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='Hello! How', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=17, prompt_tokens=280, total_tokens=297, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=14, rejected_prediction_tokens=None, text_tokens=3, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=280, image_tokens=None)))
2026-01-28 13:28:54,626 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content='Hello! How', role='assistant', function_call=None, tool_calls=None, audio=None)
2026-01-28 13:28:54,626 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='vcF5abbWKcnjjuMP6NLg6QM', created=1769587134, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Hello! How', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=17, prompt_tokens=280, total_tokens=297, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=14, rejected_prediction_tokens=None, text_tokens=3, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=280, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-28 13:28:54,676 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': ' can I help you today?'}], 'role': 'model'}, 'finishReason': 'STOP', 'index': 0}], 'usageMetadata': {'promptTokenCount': 280, 'candidatesTokenCount': 9, 'totalTokenCount': 303, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 280}], 'thoughtsTokenCount': 14}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'vcF5abbWKcnjjuMP6NLg6QM'}
2026-01-28 13:28:54,677 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='vcF5abbWKcnjjuMP6NLg6QM', created=1769587134, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content=' can I help you today?', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=23, prompt_tokens=280, total_tokens=303, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=14, rejected_prediction_tokens=None, text_tokens=9, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=280, image_tokens=None)))
2026-01-28 13:28:54,679 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content=' can I help you today?', role=None, function_call=None, tool_calls=None, audio=None)
2026-01-28 13:28:54,680 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='vcF5abbWKcnjjuMP6NLg6QM', created=1769587134, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' can I help you today?', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=23, prompt_tokens=280, total_tokens=303, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=14, rejected_prediction_tokens=None, text_tokens=9, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=280, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-28 13:28:54,706 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=False
2026-01-28 13:28:54,739 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 13:28:54,740 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=False
2026-01-28 13:28:54,740 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:28:54,741 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 13:28:54,742 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:28:54,743 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:28:54,744 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:28:54,745 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:28:54,746 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:28:54,746 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:28:54,747 | DEBUG | LiteLLM | response_cost: 0.0001415
2026-01-28 13:28:54,748 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:28:54,749 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:28:54,750 | DEBUG | LiteLLM | response_cost: 0.0001415
2026-01-28 13:28:54,751 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:28:54,752 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0001415
2026-01-28 13:28:54,753 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:28:54,754 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:28:54,756 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:28:54,757 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:28:54,758 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 13:28:54,760 | DEBUG | openai.agents | Resetting current trace
2026-01-28 13:28:54,776 | INFO | session_summary | üß† SUMMARY_START | session_id=e03649ad-b3b1-4a09-a679-d5588d2dfb33
2026-01-28 13:28:54,787 | INFO | session_summary | üìä Unsummarized messages = 2
2026-01-28 13:28:54,789 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-28 13:28:54,790 | INFO | chat | ‚úÖ Stream complete | tokens=2 | time=9.4s
2026-01-28 13:28:58,021 | DEBUG | httpcore.connection | close.started
2026-01-28 13:28:58,022 | DEBUG | httpcore.connection | close.complete
2026-01-28 13:28:58,023 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 13:28:58,050 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002B6A4307310>
2026-01-28 13:28:58,050 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x000002B698E49BC0> server_hostname='api.openai.com' timeout=5.0
2026-01-28 13:28:58,064 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002B6A4A40040>
2026-01-28 13:28:58,064 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 13:28:58,065 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 13:28:58,065 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 13:28:58,066 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 13:28:58,066 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 13:28:58,504 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 07:58:57 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_db380b1647078e553fb06cc7be03829e'), (b'openai-processing-ms', b'111'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'114'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4ef2988d3a2b0d-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 13:28:58,506 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 13:28:58,506 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 13:28:58,507 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 13:28:58,507 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 13:28:58,508 | DEBUG | openai.agents | Exported 5 items
2026-01-28 13:29:03,994 | INFO | chat | üì© Chat request | user_id=3 | request_id=2e565ce0-8091-4628-8de5-3a407e785b9f
2026-01-28 13:29:03,996 | INFO | chat | üßë User message | where do i live?
2026-01-28 13:29:04,025 | INFO | chat | ü§ñ Stream started | session_id=e03649ad-b3b1-4a09-a679-d5588d2dfb33 | request_id=2e565ce0-8091-4628-8de5-3a407e785b9f
2026-01-28 13:29:04,027 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-28 13:29:04,029 | INFO | orchestrator | üßë USER_INPUT | where do i live?
2026-01-28 13:29:04,032 | INFO | orchestrator | üß© CONTEXT | user_id=3 | enable_memory=True | db=None
2026-01-28 13:29:04,035 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-28 13:29:04,037 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_6cbba99f71d94da0b13aa87dd7637ea2
2026-01-28 13:29:04,037 | DEBUG | openai.agents | Setting current trace: trace_6cbba99f71d94da0b13aa87dd7637ea2
2026-01-28 13:29:04,038 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000002B6A47D2840> with id None
2026-01-28 13:29:04,039 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-28 13:29:04,041 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000002B69AB543A0> with id None
2026-01-28 13:29:04,041 | DEBUG | openai.agents | Calling LLM
2026-01-28 13:29:04,042 | DEBUG | LiteLLM | 

2026-01-28 13:29:04,042 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 13:29:04,043 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'where do i live?'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 13:29:04,044 | DEBUG | LiteLLM | 

2026-01-28 13:29:04,046 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 13:29:04,048 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 13:29:04,049 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 13:29:04,050 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:29:04,051 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 13:29:04,053 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'where do i live?'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 13:29:04,053 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 13:29:04,054 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 13:29:04,055 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 13:29:04,055 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:29:04,056 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:29:04,057 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'where do i live?'}]}], 'system_instruction': {'parts': [{'text': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output Schema:\n{\n  "safe": boolean,\n  "reason": string | null,\n  "message": string | null\n}\n\n### Rules:\n\nIf SAFE:\n{\n  "safe": true,\n  "reason": null,\n  "message": null\n}\n\nIf UNSAFE:\n{\n  "safe": false,\n  "reason": "<short reason>",\n  "message": "<polite, calm, context-aware refusal>"\n}\n\nReturn ONLY valid JSON.\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 13:29:05,393 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\"safe\": false, \"reason\": \"The user is asking for personal information.\", \"message\": \"I cannot access personal information about you. I am an AI and do not know where you live.\"}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 170,
    "candidatesTokenCount": 42,
    "totalTokenCount": 300,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 170
      }
    ],
    "thoughtsTokenCount": 88
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "yMF5aaSpHd7WjuMPlvfd8AE"
}



2026-01-28 13:29:05,395 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:29:05,396 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:29:05,396 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:29:05,397 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:29:05,398 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:29:05,398 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:29:05,399 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:29:05,400 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:29:05,400 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:29:05,402 | DEBUG | LiteLLM | response_cost: 0.000376
2026-01-28 13:29:05,403 | DEBUG | openai.agents | Received model response
2026-01-28 13:29:05,402 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:29:05,404 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x000002B6A485CD30>>
2026-01-28 13:29:05,406 | DEBUG | LiteLLM | response_cost: 0.000376
2026-01-28 13:29:05,407 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:29:05,408 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:29:05,409 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:29:05,409 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 13:29:05,409 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:29:05,410 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:29:05,412 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 13:29:05,412 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:29:05,413 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:29:05,414 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:29:05,415 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 13:29:05,416 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:29:05,417 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:29:05,421 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:29:05,422 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:29:05,423 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:29:05,424 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 13:29:05,425 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:29:05,426 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.000376
2026-01-28 13:29:05,431 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:29:05,431 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:29:05,432 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:29:05,434 | DEBUG | openai.agents | Resetting current trace
2026-01-28 13:29:05,435 | WARNING | orchestrator | üö´ Safety blocked request
2026-01-28 13:29:05,446 | INFO | session_summary | üß† SUMMARY_START | session_id=e03649ad-b3b1-4a09-a679-d5588d2dfb33
2026-01-28 13:29:05,450 | INFO | session_summary | üìä Unsummarized messages = 4
2026-01-28 13:29:05,451 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-28 13:29:05,452 | INFO | chat | ‚úÖ Stream complete | tokens=0 | time=1.46s
2026-01-28 13:29:08,777 | DEBUG | httpcore.connection | close.started
2026-01-28 13:29:08,777 | DEBUG | httpcore.connection | close.complete
2026-01-28 13:29:08,778 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 13:29:08,793 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002B6A485EE00>
2026-01-28 13:29:08,793 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x000002B698E49BC0> server_hostname='api.openai.com' timeout=5.0
2026-01-28 13:29:08,807 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002B6A485EE30>
2026-01-28 13:29:08,807 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 13:29:08,807 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 13:29:08,808 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 13:29:08,808 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 13:29:08,808 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 13:29:09,984 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 07:59:09 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_139606488da6a0ce51e825950621795a'), (b'openai-processing-ms', b'186'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'190'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4ef2dbaf015525-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 13:29:09,985 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 13:29:09,985 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 13:29:09,986 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 13:29:09,986 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 13:29:09,986 | DEBUG | openai.agents | Exported 3 items
2026-01-28 13:30:26,365 | DEBUG | LiteLLM | LoggingWorker cancelled during shutdown
2026-01-28 13:30:26,367 | DEBUG | LiteLLM | [LoggingWorker] atexit: Queue is empty
2026-01-28 13:30:26,367 | DEBUG | asyncio | Using proactor: IocpProactor
2026-01-28 13:30:26,368 | DEBUG | openai.agents | Shutting down trace provider
2026-01-28 13:30:26,368 | DEBUG | openai.agents | Shutting down trace processor <agents.tracing.processors.BatchTraceProcessor object at 0x000002B698E738E0>
2026-01-28 13:30:26,619 | ERROR | asyncio | Fatal error on SSL transport
protocol: <asyncio.sslproto.SSLProtocol object at 0x000002B69B1870A0>
transport: <_SelectorSocketTransport closing fd=744>
Traceback (most recent call last):
  File "C:\Users\DELL\AppData\Local\Programs\Python\Python310\lib\asyncio\selector_events.py", line 924, in write
    n = self._sock.send(data)
OSError: [WinError 10038] An operation was attempted on something that is not a socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\DELL\AppData\Local\Programs\Python\Python310\lib\asyncio\sslproto.py", line 690, in _process_write_backlog
    self._transport.write(chunk)
  File "C:\Users\DELL\AppData\Local\Programs\Python\Python310\lib\asyncio\selector_events.py", line 930, in write
    self._fatal_error(exc, 'Fatal write error on socket transport')
  File "C:\Users\DELL\AppData\Local\Programs\Python\Python310\lib\asyncio\selector_events.py", line 725, in _fatal_error
    self._force_close(exc)
  File "C:\Users\DELL\AppData\Local\Programs\Python\Python310\lib\asyncio\selector_events.py", line 737, in _force_close
    self._loop.call_soon(self._call_connection_lost, exc)
  File "C:\Users\DELL\AppData\Local\Programs\Python\Python310\lib\asyncio\base_events.py", line 753, in call_soon
    self._check_closed()
  File "C:\Users\DELL\AppData\Local\Programs\Python\Python310\lib\asyncio\base_events.py", line 515, in _check_closed
    raise RuntimeError('Event loop is closed')
RuntimeError: Event loop is closed
2026-01-28 13:30:26,622 | ERROR | asyncio | Fatal error on SSL transport
protocol: <asyncio.sslproto.SSLProtocol object at 0x000002B6A5056620>
transport: <_SelectorSocketTransport closing fd=3088>
Traceback (most recent call last):
  File "C:\Users\DELL\AppData\Local\Programs\Python\Python310\lib\asyncio\selector_events.py", line 924, in write
    n = self._sock.send(data)
OSError: [WinError 10038] An operation was attempted on something that is not a socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\DELL\AppData\Local\Programs\Python\Python310\lib\asyncio\sslproto.py", line 690, in _process_write_backlog
    self._transport.write(chunk)
  File "C:\Users\DELL\AppData\Local\Programs\Python\Python310\lib\asyncio\selector_events.py", line 930, in write
    self._fatal_error(exc, 'Fatal write error on socket transport')
  File "C:\Users\DELL\AppData\Local\Programs\Python\Python310\lib\asyncio\selector_events.py", line 725, in _fatal_error
    self._force_close(exc)
  File "C:\Users\DELL\AppData\Local\Programs\Python\Python310\lib\asyncio\selector_events.py", line 737, in _force_close
    self._loop.call_soon(self._call_connection_lost, exc)
  File "C:\Users\DELL\AppData\Local\Programs\Python\Python310\lib\asyncio\base_events.py", line 753, in call_soon
    self._check_closed()
  File "C:\Users\DELL\AppData\Local\Programs\Python\Python310\lib\asyncio\base_events.py", line 515, in _check_closed
    raise RuntimeError('Event loop is closed')
RuntimeError: Event loop is closed
2026-01-28 13:30:35,291 | INFO | sentence_transformers.SentenceTransformer | Use pytorch device_name: cpu
2026-01-28 13:30:35,292 | INFO | sentence_transformers.SentenceTransformer | Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2026-01-28 13:30:35,295 | DEBUG | urllib3.connectionpool | Starting new HTTPS connection (1): huggingface.co:443
2026-01-28 13:30:35,637 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-28 13:30:35,658 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-28 13:30:35,899 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-28 13:30:35,912 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-28 13:30:36,152 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-28 13:30:36,164 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-28 13:30:36,410 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md HTTP/1.1" 307 0
2026-01-28 13:30:36,426 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/README.md HTTP/1.1" 200 0
2026-01-28 13:30:36,677 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-28 13:30:36,689 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-28 13:30:36,918 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json HTTP/1.1" 307 0
2026-01-28 13:30:36,929 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/sentence_bert_config.json HTTP/1.1" 200 0
2026-01-28 13:30:37,173 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json HTTP/1.1" 404 0
2026-01-28 13:30:37,412 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1" 307 0
2026-01-28 13:30:37,424 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json HTTP/1.1" 200 0
2026-01-28 13:30:37,723 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/1.1" 307 0
2026-01-28 13:30:37,741 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json HTTP/1.1" 200 0
2026-01-28 13:30:37,993 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1" 404 64
2026-01-28 13:30:38,239 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main?recursive=True&expand=False HTTP/1.1" 200 6465
2026-01-28 13:30:38,512 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json HTTP/1.1" 307 0
2026-01-28 13:30:38,523 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json HTTP/1.1" 200 0
2026-01-28 13:30:38,774 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2 HTTP/1.1" 200 6885
2026-01-28 13:30:39,528 | INFO | main | üöÄ FastAPI application starting
2026-01-28 13:31:39,794 | INFO | chat | üì© Chat request | user_id=3 | request_id=f1948320-d653-4571-a479-fc4947fe474b
2026-01-28 13:31:39,829 | INFO | chat | üßë User message | hello
2026-01-28 13:31:39,872 | INFO | chat | ü§ñ Stream started | session_id=e03649ad-b3b1-4a09-a679-d5588d2dfb33 | request_id=f1948320-d653-4571-a479-fc4947fe474b
2026-01-28 13:31:39,873 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-28 13:31:39,874 | INFO | orchestrator | üßë USER_INPUT | hello
2026-01-28 13:31:39,875 | INFO | orchestrator | üß© CONTEXT | user_id=3 | enable_memory=True | db=None
2026-01-28 13:31:39,877 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-28 13:31:39,878 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_635ec9a491fb452184ba0bab1aeb33f9
2026-01-28 13:31:39,880 | DEBUG | openai.agents | Setting current trace: trace_635ec9a491fb452184ba0bab1aeb33f9
2026-01-28 13:31:39,882 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000002BECFFABEC0> with id None
2026-01-28 13:31:39,883 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-28 13:31:40,950 | DEBUG | httpcore.connection | connect_tcp.started host='raw.githubusercontent.com' port=443 local_address=None timeout=5 socket_options=None
2026-01-28 13:31:41,004 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002BED07DF2B0>
2026-01-28 13:31:41,013 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x000002BED07EAAC0> server_hostname='raw.githubusercontent.com' timeout=5
2026-01-28 13:31:41,047 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002BED07DF280>
2026-01-28 13:31:41,048 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'GET']>
2026-01-28 13:31:41,049 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 13:31:41,050 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'GET']>
2026-01-28 13:31:41,050 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 13:31:41,051 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'GET']>
2026-01-28 13:31:41,079 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Connection', b'keep-alive'), (b'Content-Length', b'62476'), (b'Cache-Control', b'max-age=300'), (b'Content-Security-Policy', b"default-src 'none'; style-src 'unsafe-inline'; sandbox"), (b'Content-Type', b'text/plain; charset=utf-8'), (b'ETag', b'W/"f6dad8a5d9e28465b3d70ca31700cb6ef9bf2cd7c9d0fa0c0c5cdf23a1bb3382"'), (b'Strict-Transport-Security', b'max-age=31536000'), (b'X-Content-Type-Options', b'nosniff'), (b'X-Frame-Options', b'deny'), (b'X-XSS-Protection', b'1; mode=block'), (b'X-GitHub-Request-Id', b'2442:60E63:51359:95C7D:6978AD63'), (b'Content-Encoding', b'gzip'), (b'Accept-Ranges', b'bytes'), (b'Date', b'Wed, 28 Jan 2026 08:01:40 GMT'), (b'Via', b'1.1 varnish'), (b'X-Served-By', b'cache-bom-vanm7210092-BOM'), (b'X-Cache', b'HIT'), (b'X-Cache-Hits', b'1'), (b'X-Timer', b'S1769587300.385988,VS0,VE1'), (b'Vary', b'Authorization,Accept-Encoding'), (b'Access-Control-Allow-Origin', b'*'), (b'Cross-Origin-Resource-Policy', b'cross-origin'), (b'X-Fastly-Request-ID', b'61738cd1405b30a72182838ad806afd929c8914c'), (b'Expires', b'Wed, 28 Jan 2026 08:06:40 GMT'), (b'Source-Age', b'6')])
2026-01-28 13:31:41,081 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'GET']>
2026-01-28 13:31:41,125 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 13:31:41,126 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 13:31:41,126 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 13:31:41,127 | DEBUG | httpcore.connection | close.started
2026-01-28 13:31:41,128 | DEBUG | httpcore.connection | close.complete
2026-01-28 13:31:42,336 | DEBUG | LiteLLM | [Non-Blocking] Unable to import GenericAPILogger - LiteLLM Enterprise Feature - No module named 'litellm_enterprise'
2026-01-28 13:31:43,476 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-28 13:31:43,477 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-28 13:31:43,478 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-28 13:31:43,492 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-28 13:31:43,493 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-28 13:31:43,494 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-28 13:31:43,495 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-28 13:31:43,496 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-28 13:31:43,496 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-28 13:31:43,497 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-28 13:31:43,498 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-28 13:31:43,499 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-28 13:31:43,527 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000002BECFF59E40> with id None
2026-01-28 13:31:43,528 | DEBUG | openai.agents | Calling LLM
2026-01-28 13:31:43,528 | DEBUG | LiteLLM | 

2026-01-28 13:31:43,529 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 13:31:43,529 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output JSON Schema (MANDATORY):\n\n{\n  "safe": boolean,\n  "reason": string | null\n}\n\n### Rules:\n- SAFE:\n  { "safe": true, "reason": null }\n\n- UNSAFE:\n  { "safe": false, "reason": "<short reason>" }\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'hello'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 13:31:43,530 | DEBUG | LiteLLM | 

2026-01-28 13:31:43,531 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 13:31:43,531 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 13:31:43,532 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 13:31:43,548 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:31:43,549 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 13:31:43,550 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output JSON Schema (MANDATORY):\n\n{\n  "safe": boolean,\n  "reason": string | null\n}\n\n### Rules:\n- SAFE:\n  { "safe": true, "reason": null }\n\n- UNSAFE:\n  { "safe": false, "reason": "<short reason>" }\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'hello'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 13:31:43,551 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 13:31:43,554 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 13:31:43,556 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 13:31:43,557 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:31:43,913 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:31:43,914 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'hello'}]}], 'system_instruction': {'parts': [{'text': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output JSON Schema (MANDATORY):\n\n{\n  "safe": boolean,\n  "reason": string | null\n}\n\n### Rules:\n- SAFE:\n  { "safe": true, "reason": null }\n\n- UNSAFE:\n  { "safe": false, "reason": "<short reason>" }\n\nReturn ONLY valid JSON.\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 13:31:44,937 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 13:31:44,967 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002BED9704FA0>
2026-01-28 13:31:44,967 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x000002BECE369A40> server_hostname='api.openai.com' timeout=5.0
2026-01-28 13:31:44,983 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002BED9704F70>
2026-01-28 13:31:44,983 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 13:31:44,984 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 13:31:44,985 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 13:31:44,985 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 13:31:44,985 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 13:31:45,317 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\"safe\": true, \"reason\": null}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 131,
    "candidatesTokenCount": 10,
    "totalTokenCount": 190,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 131
      }
    ],
    "thoughtsTokenCount": 49
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "aMJ5abbqG8Xf4-EPq9HKsQQ"
}



2026-01-28 13:31:45,322 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:31:45,324 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:31:45,325 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:31:45,326 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:31:45,327 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:31:45,327 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:31:45,328 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:31:45,329 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:31:45,329 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:31:45,330 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:31:45,331 | DEBUG | LiteLLM | response_cost: 0.00018680000000000004
2026-01-28 13:31:45,332 | DEBUG | LiteLLM | response_cost: 0.00018680000000000004
2026-01-28 13:31:45,332 | DEBUG | openai.agents | Received model response
2026-01-28 13:31:45,334 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:31:45,356 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x000002BED96C26E0>>
2026-01-28 13:31:45,357 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:31:45,359 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:31:45,360 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:31:45,360 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 13:31:45,363 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:31:45,363 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 13:31:45,363 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:31:45,363 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:31:45,365 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 13:31:45,365 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:31:45,366 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:31:45,367 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:31:45,368 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:31:45,370 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:31:45,371 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:31:45,373 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:31:45,375 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 13:31:45,376 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.00018680000000000004
2026-01-28 13:31:45,377 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:31:45,379 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:31:45,379 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:31:45,382 | DEBUG | openai.agents | Resetting current trace
2026-01-28 13:31:45,382 | INFO | orchestrator | ‚úÖ Safety passed
2026-01-28 13:31:45,384 | INFO | orchestrator | üì• Fetching semantic memory (via MemoryService)
2026-01-28 13:31:45,476 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 08:01:44 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_35819ae8cfd07314e8e83f406c141aa7'), (b'openai-processing-ms', b'120'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'123'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=DTz0oslIAwiZPTLWpkLGv_WSOI_1.k7w73HW3hJgvj4-1769587304-1.0.1.1-Lj20s0tCzc8eQh_mk5eO3AUMHwiZnxsIyZ7i9jy0KMWqnxeXayb1wmHdobV2OvOFEzaZVVSWxRpVn7D32ZkegfSz16lAq5WSiX2OxiPmfKA; path=/; expires=Wed, 28-Jan-26 08:31:44 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=nSfgzju264gMrd5QsxiINMJd9CBUq.jxmZClqd9CFWQ-1769587304769-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4ef6abce48596a-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 13:31:45,478 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 13:31:45,478 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 13:31:45,478 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 13:31:45,479 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 13:31:45,479 | DEBUG | openai.agents | Exported 1 items
2026-01-28 13:31:45,481 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 13:31:45,481 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 13:31:45,482 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 13:31:45,482 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 13:31:45,482 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 13:31:45,496 | INFO | orchestrator | üß† MEMORY_RESULT | found=False | count=0
2026-01-28 13:31:45,498 | INFO | orchestrator | üß≠ Router agent called
2026-01-28 13:31:45,498 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_539d6d828e9546ba809fa4ebd3a9597d
2026-01-28 13:31:45,499 | DEBUG | openai.agents | Setting current trace: trace_539d6d828e9546ba809fa4ebd3a9597d
2026-01-28 13:31:45,499 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000002BED94C9990> with id None
2026-01-28 13:31:45,499 | DEBUG | openai.agents | Running agent RouterAgent (turn 1)
2026-01-28 13:31:45,500 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000002BED96D2BC0> with id None
2026-01-28 13:31:45,501 | DEBUG | openai.agents | Calling LLM
2026-01-28 13:31:45,501 | DEBUG | LiteLLM | 

2026-01-28 13:31:45,502 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 13:31:45,502 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'hello'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 13:31:45,503 | DEBUG | LiteLLM | 

2026-01-28 13:31:45,504 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 13:31:45,504 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 13:31:45,505 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 13:31:45,507 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:31:45,508 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 13:31:45,509 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'hello'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 13:31:45,510 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 13:31:45,512 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 13:31:45,515 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 13:31:45,516 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:31:45,517 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:31:45,519 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'hello'}]}], 'system_instruction': {'parts': [{'text': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 13:31:45,902 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 08:01:45 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_813908c1744dbdc733e4b52d7097975b'), (b'openai-processing-ms', b'106'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'109'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4ef6aee853596a-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 13:31:45,903 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 13:31:45,904 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 13:31:45,905 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 13:31:45,905 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 13:31:45,906 | DEBUG | openai.agents | Exported 2 items
2026-01-28 13:31:45,908 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 13:31:45,909 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 13:31:45,909 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 13:31:45,911 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 13:31:45,911 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 13:31:46,326 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 08:01:45 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_193970dd1edf144fd8b5d34136325c4e'), (b'openai-processing-ms', b'101'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'105'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4ef6b189eb596a-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 13:31:46,327 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 13:31:46,328 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 13:31:46,328 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 13:31:46,328 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 13:31:46,329 | DEBUG | openai.agents | Exported 1 items
2026-01-28 13:31:47,010 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\n  \"use_memory\": false,\n  \"use_tool\": false,\n  \"tool_name\": \"none\",\n  \"tool_arguments\": null,\n  \"intent\": \"none\",\n  \"memory_key\": null\n}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 486,
    "candidatesTokenCount": 56,
    "totalTokenCount": 603,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 486
      }
    ],
    "thoughtsTokenCount": 61
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "asJ5ae_DBbS8juMP9KnjiAQ"
}



2026-01-28 13:31:47,013 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:31:47,014 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:31:47,015 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:31:47,016 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:31:47,017 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:31:47,018 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:31:47,018 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:31:47,019 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:31:47,019 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:31:47,020 | DEBUG | LiteLLM | response_cost: 0.0004383
2026-01-28 13:31:47,021 | DEBUG | openai.agents | Received model response
2026-01-28 13:31:47,021 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:31:47,023 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x000002BED9707BE0>>
2026-01-28 13:31:47,023 | DEBUG | LiteLLM | response_cost: 0.0004383
2026-01-28 13:31:47,024 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:31:47,026 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:31:47,026 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:31:47,027 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 13:31:47,027 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:31:47,029 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:31:47,033 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 13:31:47,034 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:31:47,034 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:31:47,035 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:31:47,036 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 13:31:47,037 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:31:47,037 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:31:47,038 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:31:47,042 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:31:47,043 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:31:47,044 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 13:31:47,044 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:31:47,045 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0004383
2026-01-28 13:31:47,046 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:31:47,047 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:31:47,048 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:31:47,049 | DEBUG | openai.agents | Resetting current trace
2026-01-28 13:31:47,050 | INFO | orchestrator | üß≠ ROUTER_DECISION | tool=False | memory=False | intent=none
2026-01-28 13:31:47,051 | INFO | orchestrator | üß† REASONING_INPUT_SNAPSHOT
{
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful AI assistant.\n \nYou may answer ANY general knowledge question.\n \nConversation summaries and memories are ONLY to provide helpful context\n(e.g. preferences, ongoing plans).\n \nIf a user asks something unrelated to the current topic,\nanswer it normally using general knowledge.\n \nDo NOT restrict yourself to previous topics unless the user explicitly asks."
    },
    {
      "role": "user",
      "content": "hello"
    },
    {
      "role": "assistant",
      "content": "Hello! How can I help you today?"
    },
    {
      "role": "user",
      "content": "where do i live?"
    },
    {
      "role": "assistant",
      "content": ""
    },
    {
      "role": "user",
      "content": "hello"
    },
    {
      "role": "user",
      "content": "hello"
    }
  ],
  "safety": {
    "safe": true,
    "reason": null,
    "message": "OK"
  },
  "memory_action": {},
  "memory_data": [],
  "memory_found": false,
  "tool_context": {}
}
2026-01-28 13:31:47,056 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_fd1701bb97554e2c8ad9dc4b02043e09
2026-01-28 13:31:47,057 | DEBUG | openai.agents | Setting current trace: trace_fd1701bb97554e2c8ad9dc4b02043e09
2026-01-28 13:31:47,058 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000002BEDA42C6D0> with id None
2026-01-28 13:31:47,059 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000002BEDA449720> with id None
2026-01-28 13:31:47,059 | DEBUG | openai.agents | Calling LLM
2026-01-28 13:31:47,060 | DEBUG | LiteLLM | 

2026-01-28 13:31:47,061 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 13:31:47,061 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "hello"}, {"role": "assistant", "content": "Hello! How can I help you today?"}, {"role": "user", "content": "where do i live?"}, {"role": "assistant", "content": ""}, {"role": "user", "content": "hello"}, {"role": "user", "content": "hello"}], "safety": {"safe": true, "reason": null, "message": "OK"}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=True, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 13:31:47,063 | DEBUG | LiteLLM | 

2026-01-28 13:31:47,063 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 13:31:47,064 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 13:31:47,065 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 13:31:47,066 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:31:47,067 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 13:31:47,070 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "hello"}, {"role": "assistant", "content": "Hello! How can I help you today?"}, {"role": "user", "content": "where do i live?"}, {"role": "assistant", "content": ""}, {"role": "user", "content": "hello"}, {"role": "user", "content": "hello"}], "safety": {"safe": true, "reason": null, "message": "OK"}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 13:31:47,072 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': True}
2026-01-28 13:31:47,073 | DEBUG | LiteLLM | Final returned optional params: {'stream': True}
2026-01-28 13:31:47,073 | DEBUG | LiteLLM | self.optional_params: {'stream': True}
2026-01-28 13:31:47,074 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:31:47,075 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:31:47,076 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?key=*****=sse \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "user", "content": "hello"}, {"role": "assistant", "content": "Hello! How can I help you today?"}, {"role": "user", "content": "where do i live?"}, {"role": "assistant", "content": ""}, {"role": "user", "content": "hello"}, {"role": "user", "content": "hello"}], "safety": {"safe": true, "reason": null, "message": "OK"}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}]}], 'system_instruction': {'parts': [{'text': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.'}]}, 'generationConfig': {}}'
[0m

2026-01-28 13:31:47,077 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:31:47,081 | DEBUG | LiteLLM | messages in token_counter: None, text in token_counter: 
2026-01-28 13:31:47,083 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:31:47,084 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:31:47,084 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:31:47,086 | DEBUG | LiteLLM | response_cost: 0.0
2026-01-28 13:31:47,190 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-28 13:31:47,191 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-28 13:31:47,192 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-28 13:31:48,490 | DEBUG | LiteLLM | RAW RESPONSE:
first stream response received


2026-01-28 13:31:48,491 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': 'Hello! How can I help you?'}], 'role': 'model'}, 'finishReason': 'STOP', 'index': 0}], 'usageMetadata': {'promptTokenCount': 339, 'candidatesTokenCount': 8, 'totalTokenCount': 426, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 339}], 'thoughtsTokenCount': 79}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'a8J5adCsC8e6qfkP75L0oAg'}
2026-01-28 13:31:48,494 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='a8J5adCsC8e6qfkP75L0oAg', created=1769587308, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='Hello! How can I help you?', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=87, prompt_tokens=339, total_tokens=426, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=79, rejected_prediction_tokens=None, text_tokens=8, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=339, image_tokens=None)))
2026-01-28 13:31:48,495 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content='Hello! How can I help you?', role='assistant', function_call=None, tool_calls=None, audio=None)
2026-01-28 13:31:48,495 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='a8J5adCsC8e6qfkP75L0oAg', created=1769587308, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Hello! How can I help you?', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=87, prompt_tokens=339, total_tokens=426, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=79, rejected_prediction_tokens=None, text_tokens=8, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=339, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-28 13:31:48,527 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=False
2026-01-28 13:31:48,537 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=False
2026-01-28 13:31:48,537 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 13:31:48,538 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 13:31:48,538 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:31:48,538 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:31:48,539 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:31:48,539 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:31:48,540 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:31:48,540 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:31:48,540 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:31:48,541 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:31:48,541 | DEBUG | LiteLLM | response_cost: 0.0003192
2026-01-28 13:31:48,541 | DEBUG | LiteLLM | response_cost: 0.0003192
2026-01-28 13:31:48,543 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:31:48,543 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0003192
2026-01-28 13:31:48,544 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:31:48,544 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:31:48,545 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:31:48,545 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:31:48,546 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:31:48,546 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 13:31:48,547 | DEBUG | openai.agents | Resetting current trace
2026-01-28 13:31:48,556 | INFO | session_summary | üß† SUMMARY_START | session_id=e03649ad-b3b1-4a09-a679-d5588d2dfb33
2026-01-28 13:31:48,562 | INFO | session_summary | üìä Unsummarized messages = 6
2026-01-28 13:31:48,563 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-28 13:31:48,563 | INFO | chat | ‚úÖ Stream complete | tokens=1 | time=8.77s
2026-01-28 13:31:51,478 | DEBUG | httpcore.connection | close.started
2026-01-28 13:31:51,479 | DEBUG | httpcore.connection | close.complete
2026-01-28 13:31:51,480 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 13:31:51,506 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002BEDA402320>
2026-01-28 13:31:51,506 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x000002BECE369A40> server_hostname='api.openai.com' timeout=5.0
2026-01-28 13:31:51,523 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002BEDA402260>
2026-01-28 13:31:51,523 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 13:31:51,524 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 13:31:51,524 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 13:31:51,525 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 13:31:51,525 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 13:31:51,976 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 08:01:51 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_e71a592b765d9e0bcccbf2397fe5100a'), (b'openai-processing-ms', b'124'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'129'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4ef6d4aab2d9c0-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 13:31:51,977 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 13:31:51,978 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 13:31:51,978 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 13:31:51,979 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 13:31:51,979 | DEBUG | openai.agents | Exported 5 items
2026-01-28 13:32:02,392 | INFO | chat | üì© Chat request | user_id=3 | request_id=09eeb941-3e73-4f23-b691-aa1cfce38146
2026-01-28 13:32:02,394 | INFO | chat | üßë User message | where do i study?
2026-01-28 13:32:02,422 | INFO | chat | ü§ñ Stream started | session_id=e03649ad-b3b1-4a09-a679-d5588d2dfb33 | request_id=09eeb941-3e73-4f23-b691-aa1cfce38146
2026-01-28 13:32:02,426 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-28 13:32:02,427 | INFO | orchestrator | üßë USER_INPUT | where do i study?
2026-01-28 13:32:02,428 | INFO | orchestrator | üß© CONTEXT | user_id=3 | enable_memory=True | db=None
2026-01-28 13:32:02,428 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-28 13:32:02,429 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_d2d8c10eb9434cce88047bf78c608c14
2026-01-28 13:32:02,430 | DEBUG | openai.agents | Setting current trace: trace_d2d8c10eb9434cce88047bf78c608c14
2026-01-28 13:32:02,430 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000002BED9C87880> with id None
2026-01-28 13:32:02,431 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-28 13:32:02,433 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000002BEDA44A980> with id None
2026-01-28 13:32:02,433 | DEBUG | openai.agents | Calling LLM
2026-01-28 13:32:02,434 | DEBUG | LiteLLM | 

2026-01-28 13:32:02,435 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 13:32:02,437 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output JSON Schema (MANDATORY):\n\n{\n  "safe": boolean,\n  "reason": string | null\n}\n\n### Rules:\n- SAFE:\n  { "safe": true, "reason": null }\n\n- UNSAFE:\n  { "safe": false, "reason": "<short reason>" }\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'where do i study?'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 13:32:02,439 | DEBUG | LiteLLM | 

2026-01-28 13:32:02,440 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 13:32:02,441 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 13:32:02,441 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 13:32:02,443 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:32:02,444 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 13:32:02,445 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output JSON Schema (MANDATORY):\n\n{\n  "safe": boolean,\n  "reason": string | null\n}\n\n### Rules:\n- SAFE:\n  { "safe": true, "reason": null }\n\n- UNSAFE:\n  { "safe": false, "reason": "<short reason>" }\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'where do i study?'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 13:32:02,446 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 13:32:02,447 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 13:32:02,447 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 13:32:02,447 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:02,450 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:02,453 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'where do i study?'}]}], 'system_instruction': {'parts': [{'text': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output JSON Schema (MANDATORY):\n\n{\n  "safe": boolean,\n  "reason": string | null\n}\n\n### Rules:\n- SAFE:\n  { "safe": true, "reason": null }\n\n- UNSAFE:\n  { "safe": false, "reason": "<short reason>" }\n\nReturn ONLY valid JSON.\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 13:32:03,781 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\"safe\": true, \"reason\": null}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 135,
    "candidatesTokenCount": 10,
    "totalTokenCount": 184,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 135
      }
    ],
    "thoughtsTokenCount": 39
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "esJ5aa62NpSG4-EPnKHm-AM"
}



2026-01-28 13:32:03,784 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:32:03,785 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:32:03,785 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:32:03,786 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:03,787 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:32:03,788 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:03,789 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:03,790 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:32:03,791 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:03,792 | DEBUG | LiteLLM | response_cost: 0.000163
2026-01-28 13:32:03,793 | DEBUG | openai.agents | Received model response
2026-01-28 13:32:03,792 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:32:03,794 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x000002BECFE64820>>
2026-01-28 13:32:03,796 | DEBUG | LiteLLM | response_cost: 0.000163
2026-01-28 13:32:03,796 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:32:03,797 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:32:03,798 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 13:32:03,798 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:32:03,799 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:03,800 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 13:32:03,801 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:32:03,802 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:32:03,804 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:32:03,807 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:03,809 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 13:32:03,809 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:03,810 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:32:03,812 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:32:03,813 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:32:03,814 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:03,815 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 13:32:03,817 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:32:03,819 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.000163
2026-01-28 13:32:03,825 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:32:03,825 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:03,827 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:32:03,829 | DEBUG | openai.agents | Resetting current trace
2026-01-28 13:32:03,829 | INFO | orchestrator | ‚úÖ Safety passed
2026-01-28 13:32:03,831 | INFO | orchestrator | üì• Fetching semantic memory (via MemoryService)
2026-01-28 13:32:03,880 | INFO | orchestrator | üß† MEMORY_RESULT | found=False | count=0
2026-01-28 13:32:03,881 | INFO | orchestrator | üß≠ Router agent called
2026-01-28 13:32:03,884 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_8314f52df0b549cd844586a372864103
2026-01-28 13:32:03,886 | DEBUG | openai.agents | Setting current trace: trace_8314f52df0b549cd844586a372864103
2026-01-28 13:32:03,887 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000002BECFDC6A70> with id None
2026-01-28 13:32:03,887 | DEBUG | openai.agents | Running agent RouterAgent (turn 1)
2026-01-28 13:32:03,888 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000002BEDA44A740> with id None
2026-01-28 13:32:03,888 | DEBUG | openai.agents | Calling LLM
2026-01-28 13:32:03,889 | DEBUG | LiteLLM | 

2026-01-28 13:32:03,889 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 13:32:03,890 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'where do i study?'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 13:32:03,891 | DEBUG | LiteLLM | 

2026-01-28 13:32:03,892 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 13:32:03,892 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 13:32:03,893 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 13:32:03,895 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:32:03,895 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 13:32:03,897 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'where do i study?'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 13:32:03,899 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 13:32:03,900 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 13:32:03,903 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 13:32:03,904 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:03,905 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:03,906 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'where do i study?'}]}], 'system_instruction': {'parts': [{'text': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 13:32:05,582 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\n  \"use_memory\": true,\n  \"use_tool\": false,\n  \"tool_name\": \"none\",\n  \"tool_arguments\": null,\n  \"intent\": \"read\",\n  \"memory_key\": null\n}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 490,
    "candidatesTokenCount": 56,
    "totalTokenCount": 643,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 490
      }
    ],
    "thoughtsTokenCount": 97
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "fMJ5aeW3K9zn4-EP7Ii_gAQ"
}



2026-01-28 13:32:05,584 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:32:05,585 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:32:05,585 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:32:05,586 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:05,587 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:32:05,587 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:05,587 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:05,588 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:32:05,589 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:05,589 | DEBUG | LiteLLM | response_cost: 0.0005295
2026-01-28 13:32:05,590 | DEBUG | openai.agents | Received model response
2026-01-28 13:32:05,590 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:32:05,591 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x000002BED9707070>>
2026-01-28 13:32:05,593 | DEBUG | LiteLLM | response_cost: 0.0005295
2026-01-28 13:32:05,596 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:32:05,597 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:32:05,597 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 13:32:05,597 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:32:05,597 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:05,598 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 13:32:05,599 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:32:05,602 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:32:05,604 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:32:05,605 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:05,605 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 13:32:05,606 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:05,606 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:32:05,607 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:32:05,607 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:32:05,608 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:05,608 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 13:32:05,608 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:32:05,609 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0005295
2026-01-28 13:32:05,610 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:32:05,610 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:05,611 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:32:05,612 | DEBUG | openai.agents | Resetting current trace
2026-01-28 13:32:05,613 | INFO | orchestrator | üß≠ ROUTER_DECISION | tool=False | memory=True | intent=read
2026-01-28 13:32:05,614 | INFO | orchestrator | üß† REASONING_INPUT_SNAPSHOT
{
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful AI assistant.\n \nYou may answer ANY general knowledge question.\n \nConversation summaries and memories are ONLY to provide helpful context\n(e.g. preferences, ongoing plans).\n \nIf a user asks something unrelated to the current topic,\nanswer it normally using general knowledge.\n \nDo NOT restrict yourself to previous topics unless the user explicitly asks."
    },
    {
      "role": "assistant",
      "content": "Hello! How can I help you today?"
    },
    {
      "role": "user",
      "content": "where do i live?"
    },
    {
      "role": "assistant",
      "content": ""
    },
    {
      "role": "user",
      "content": "hello"
    },
    {
      "role": "assistant",
      "content": "Hello! How can I help you?"
    },
    {
      "role": "user",
      "content": "where do i study?"
    },
    {
      "role": "user",
      "content": "where do i study?"
    }
  ],
  "safety": {
    "safe": true,
    "reason": null,
    "message": "OK"
  },
  "memory_action": {},
  "memory_data": [],
  "memory_found": false,
  "tool_context": {}
}
2026-01-28 13:32:05,619 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_b6b4dd1ac8944d8e96596be149bd52d8
2026-01-28 13:32:05,619 | DEBUG | openai.agents | Setting current trace: trace_b6b4dd1ac8944d8e96596be149bd52d8
2026-01-28 13:32:05,619 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000002BECFD4A700> with id None
2026-01-28 13:32:05,620 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000002BED9CA0FA0> with id None
2026-01-28 13:32:05,620 | DEBUG | openai.agents | Calling LLM
2026-01-28 13:32:05,621 | DEBUG | LiteLLM | 

2026-01-28 13:32:05,621 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 13:32:05,621 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "assistant", "content": "Hello! How can I help you today?"}, {"role": "user", "content": "where do i live?"}, {"role": "assistant", "content": ""}, {"role": "user", "content": "hello"}, {"role": "assistant", "content": "Hello! How can I help you?"}, {"role": "user", "content": "where do i study?"}, {"role": "user", "content": "where do i study?"}], "safety": {"safe": true, "reason": null, "message": "OK"}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=True, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 13:32:05,622 | DEBUG | LiteLLM | 

2026-01-28 13:32:05,622 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 13:32:05,622 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 13:32:05,623 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 13:32:05,624 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:32:05,627 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 13:32:05,629 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "assistant", "content": "Hello! How can I help you today?"}, {"role": "user", "content": "where do i live?"}, {"role": "assistant", "content": ""}, {"role": "user", "content": "hello"}, {"role": "assistant", "content": "Hello! How can I help you?"}, {"role": "user", "content": "where do i study?"}, {"role": "user", "content": "where do i study?"}], "safety": {"safe": true, "reason": null, "message": "OK"}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 13:32:05,631 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': True}
2026-01-28 13:32:05,635 | DEBUG | LiteLLM | Final returned optional params: {'stream': True}
2026-01-28 13:32:05,636 | DEBUG | LiteLLM | self.optional_params: {'stream': True}
2026-01-28 13:32:05,636 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:05,637 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:05,637 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?key=*****=sse \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "assistant", "content": "Hello! How can I help you today?"}, {"role": "user", "content": "where do i live?"}, {"role": "assistant", "content": ""}, {"role": "user", "content": "hello"}, {"role": "assistant", "content": "Hello! How can I help you?"}, {"role": "user", "content": "where do i study?"}, {"role": "user", "content": "where do i study?"}], "safety": {"safe": true, "reason": null, "message": "OK"}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}]}], 'system_instruction': {'parts': [{'text': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.'}]}, 'generationConfig': {}}'
[0m

2026-01-28 13:32:05,638 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:32:05,639 | DEBUG | LiteLLM | messages in token_counter: None, text in token_counter: 
2026-01-28 13:32:05,639 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:05,639 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:05,640 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:32:05,640 | DEBUG | LiteLLM | response_cost: 0.0
2026-01-28 13:32:06,958 | DEBUG | LiteLLM | RAW RESPONSE:
first stream response received


2026-01-28 13:32:06,959 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': 'I don'}], 'role': 'model'}, 'index': 0}], 'usageMetadata': {'promptTokenCount': 365, 'candidatesTokenCount': 2, 'totalTokenCount': 433, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 365}], 'thoughtsTokenCount': 66}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'fcJ5aausKuzVjuMPy-6C4QM'}
2026-01-28 13:32:06,959 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='fcJ5aausKuzVjuMPy-6C4QM', created=1769587326, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='I don', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=68, prompt_tokens=365, total_tokens=433, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=66, rejected_prediction_tokens=None, text_tokens=2, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=365, image_tokens=None)))
2026-01-28 13:32:06,960 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content='I don', role='assistant', function_call=None, tool_calls=None, audio=None)
2026-01-28 13:32:06,961 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='fcJ5aausKuzVjuMPy-6C4QM', created=1769587326, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='I don', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=68, prompt_tokens=365, total_tokens=433, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=66, rejected_prediction_tokens=None, text_tokens=2, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=365, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-28 13:32:06,961 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': "'t have access to personal information like where you study. I am a large language model, trained by Google."}], 'role': 'model'}, 'finishReason': 'STOP', 'index': 0}], 'usageMetadata': {'promptTokenCount': 365, 'candidatesTokenCount': 25, 'totalTokenCount': 456, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 365}], 'thoughtsTokenCount': 66}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'fcJ5aausKuzVjuMPy-6C4QM'}
2026-01-28 13:32:06,962 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='fcJ5aausKuzVjuMPy-6C4QM', created=1769587326, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content="'t have access to personal information like where you study. I am a large language model, trained by Google.", role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=91, prompt_tokens=365, total_tokens=456, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=66, rejected_prediction_tokens=None, text_tokens=25, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=365, image_tokens=None)))
2026-01-28 13:32:06,963 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content="'t have access to personal information like where you study. I am a large language model, trained by Google.", role=None, function_call=None, tool_calls=None, audio=None)
2026-01-28 13:32:06,963 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='fcJ5aausKuzVjuMPy-6C4QM', created=1769587326, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content="'t have access to personal information like where you study. I am a large language model, trained by Google.", role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=91, prompt_tokens=365, total_tokens=456, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=66, rejected_prediction_tokens=None, text_tokens=25, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=365, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-28 13:32:06,966 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=False
2026-01-28 13:32:06,966 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=False
2026-01-28 13:32:06,966 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 13:32:06,966 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 13:32:06,968 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:32:06,968 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:32:06,968 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:06,969 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:06,969 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:06,969 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:06,969 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:32:06,970 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:32:06,970 | DEBUG | LiteLLM | response_cost: 0.000337
2026-01-28 13:32:06,970 | DEBUG | LiteLLM | response_cost: 0.000337
2026-01-28 13:32:06,971 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:32:06,971 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.000337
2026-01-28 13:32:06,971 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:06,972 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:32:06,972 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:32:06,972 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:06,975 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:32:06,978 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 13:32:06,978 | DEBUG | openai.agents | Resetting current trace
2026-01-28 13:32:06,985 | INFO | session_summary | üß† SUMMARY_START | session_id=e03649ad-b3b1-4a09-a679-d5588d2dfb33
2026-01-28 13:32:06,987 | INFO | session_summary | üìä Unsummarized messages = 8
2026-01-28 13:32:06,988 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-28 13:32:06,988 | INFO | chat | ‚úÖ Stream complete | tokens=2 | time=4.6s
2026-01-28 13:32:07,343 | DEBUG | httpcore.connection | close.started
2026-01-28 13:32:07,344 | DEBUG | httpcore.connection | close.complete
2026-01-28 13:32:07,344 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 13:32:07,375 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002BED068D2A0>
2026-01-28 13:32:07,375 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x000002BECE369A40> server_hostname='api.openai.com' timeout=5.0
2026-01-28 13:32:07,384 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002BED9706920>
2026-01-28 13:32:07,384 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 13:32:07,385 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 13:32:07,385 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 13:32:07,386 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 13:32:07,386 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 13:32:07,826 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 08:02:07 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_4b39d5accf0c422950ebf0a3ee01c708'), (b'openai-processing-ms', b'118'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'121'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4ef737cde31043-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 13:32:07,826 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 13:32:07,826 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 13:32:07,827 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 13:32:07,827 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 13:32:07,827 | DEBUG | openai.agents | Exported 9 items
2026-01-28 13:32:39,827 | INFO | chat | üì© Chat request | user_id=3 | request_id=fd0e1db6-6834-4df9-bc18-d154ca9d010f
2026-01-28 13:32:39,828 | INFO | chat | üßë User message | what is my fav pizza?
2026-01-28 13:32:39,843 | INFO | chat | ü§ñ Stream started | session_id=e03649ad-b3b1-4a09-a679-d5588d2dfb33 | request_id=fd0e1db6-6834-4df9-bc18-d154ca9d010f
2026-01-28 13:32:39,845 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-28 13:32:39,846 | INFO | orchestrator | üßë USER_INPUT | what is my fav pizza?
2026-01-28 13:32:39,848 | INFO | orchestrator | üß© CONTEXT | user_id=3 | enable_memory=True | db=None
2026-01-28 13:32:39,849 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-28 13:32:39,850 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_cc67b51bcc144864971de7ba2109d250
2026-01-28 13:32:39,851 | DEBUG | openai.agents | Setting current trace: trace_cc67b51bcc144864971de7ba2109d250
2026-01-28 13:32:39,851 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000002BED9C87470> with id None
2026-01-28 13:32:39,852 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-28 13:32:39,853 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000002BED9CA2560> with id None
2026-01-28 13:32:39,854 | DEBUG | openai.agents | Calling LLM
2026-01-28 13:32:39,855 | DEBUG | LiteLLM | 

2026-01-28 13:32:39,855 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 13:32:39,857 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output JSON Schema (MANDATORY):\n\n{\n  "safe": boolean,\n  "reason": string | null\n}\n\n### Rules:\n- SAFE:\n  { "safe": true, "reason": null }\n\n- UNSAFE:\n  { "safe": false, "reason": "<short reason>" }\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'what is my fav pizza?'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 13:32:39,858 | DEBUG | LiteLLM | 

2026-01-28 13:32:39,859 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 13:32:39,860 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 13:32:39,861 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 13:32:39,865 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:32:39,866 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 13:32:39,868 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output JSON Schema (MANDATORY):\n\n{\n  "safe": boolean,\n  "reason": string | null\n}\n\n### Rules:\n- SAFE:\n  { "safe": true, "reason": null }\n\n- UNSAFE:\n  { "safe": false, "reason": "<short reason>" }\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'what is my fav pizza?'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 13:32:39,869 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 13:32:39,870 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 13:32:39,871 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 13:32:39,872 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:39,873 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:39,874 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'what is my fav pizza?'}]}], 'system_instruction': {'parts': [{'text': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output JSON Schema (MANDATORY):\n\n{\n  "safe": boolean,\n  "reason": string | null\n}\n\n### Rules:\n- SAFE:\n  { "safe": true, "reason": null }\n\n- UNSAFE:\n  { "safe": false, "reason": "<short reason>" }\n\nReturn ONLY valid JSON.\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 13:32:41,426 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\"safe\": true, \"reason\": null}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 136,
    "candidatesTokenCount": 10,
    "totalTokenCount": 232,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 136
      }
    ],
    "thoughtsTokenCount": 86
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "oMJ5aeTLIrHL4-EP4P6r6QM"
}



2026-01-28 13:32:41,427 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:32:41,428 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:32:41,428 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:32:41,429 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:41,429 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:32:41,429 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:41,429 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:41,431 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:32:41,431 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:41,431 | DEBUG | LiteLLM | response_cost: 0.00028080000000000005
2026-01-28 13:32:41,432 | DEBUG | openai.agents | Received model response
2026-01-28 13:32:41,432 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:32:41,432 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x000002BED9CAC220>>
2026-01-28 13:32:41,433 | DEBUG | LiteLLM | response_cost: 0.00028080000000000005
2026-01-28 13:32:41,433 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:32:41,434 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:32:41,434 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 13:32:41,434 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:32:41,434 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:41,435 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 13:32:41,435 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:32:41,436 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:32:41,436 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:32:41,437 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:41,437 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 13:32:41,437 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:41,437 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:32:41,438 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:32:41,438 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:32:41,439 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:41,439 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 13:32:41,439 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:32:41,440 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.00028080000000000005
2026-01-28 13:32:41,441 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:32:41,443 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:41,445 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:32:41,446 | DEBUG | openai.agents | Resetting current trace
2026-01-28 13:32:41,447 | INFO | orchestrator | ‚úÖ Safety passed
2026-01-28 13:32:41,447 | INFO | orchestrator | üì• Fetching semantic memory (via MemoryService)
2026-01-28 13:32:41,480 | INFO | orchestrator | üß† MEMORY_RESULT | found=True | count=2
2026-01-28 13:32:41,481 | INFO | orchestrator | üß≠ Router agent called
2026-01-28 13:32:41,481 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_8fb515d8ae8340708965c00bd4c5d509
2026-01-28 13:32:41,481 | DEBUG | openai.agents | Setting current trace: trace_8fb515d8ae8340708965c00bd4c5d509
2026-01-28 13:32:41,482 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000002BED9C876F0> with id None
2026-01-28 13:32:41,482 | DEBUG | openai.agents | Running agent RouterAgent (turn 1)
2026-01-28 13:32:41,482 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000002BED9CA0C40> with id None
2026-01-28 13:32:41,483 | DEBUG | openai.agents | Calling LLM
2026-01-28 13:32:41,483 | DEBUG | LiteLLM | 

2026-01-28 13:32:41,483 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 13:32:41,484 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'what is my fav pizza?'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 13:32:41,484 | DEBUG | LiteLLM | 

2026-01-28 13:32:41,484 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 13:32:41,485 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 13:32:41,485 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 13:32:41,486 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:32:41,486 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 13:32:41,487 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'what is my fav pizza?'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 13:32:41,487 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 13:32:41,487 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 13:32:41,488 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 13:32:41,488 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:41,489 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:41,489 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'what is my fav pizza?'}]}], 'system_instruction': {'parts': [{'text': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 13:32:43,117 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\n  \"use_memory\": true,\n  \"use_tool\": false,\n  \"tool_name\": \"none\",\n  \"tool_arguments\": null,\n  \"intent\": \"read\",\n  \"memory_key\": \"food_preference\"\n}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 491,
    "candidatesTokenCount": 60,
    "totalTokenCount": 613,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 491
      }
    ],
    "thoughtsTokenCount": 62
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "osJ5aauiDq664-EPi9magAQ"
}



2026-01-28 13:32:43,119 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:32:43,120 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:32:43,120 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:32:43,120 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:43,121 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:32:43,121 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:43,121 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:43,122 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:32:43,122 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:43,122 | DEBUG | LiteLLM | response_cost: 0.00045230000000000004
2026-01-28 13:32:43,123 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:32:43,123 | DEBUG | openai.agents | Received model response
2026-01-28 13:32:43,124 | DEBUG | LiteLLM | response_cost: 0.00045230000000000004
2026-01-28 13:32:43,124 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x000002BED9CAEB30>>
2026-01-28 13:32:43,125 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:32:43,125 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:32:43,125 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:43,126 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 13:32:43,126 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:32:43,126 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:32:43,126 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 13:32:43,129 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:32:43,130 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 13:32:43,131 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:32:43,134 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:43,135 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:32:43,136 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:43,136 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:32:43,136 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:43,137 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:32:43,138 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:32:43,138 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 13:32:43,139 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.00045230000000000004
2026-01-28 13:32:43,140 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:32:43,140 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:43,141 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:32:43,143 | DEBUG | openai.agents | Resetting current trace
2026-01-28 13:32:43,144 | INFO | orchestrator | üß≠ ROUTER_DECISION | tool=False | memory=True | intent=read
2026-01-28 13:32:43,145 | INFO | orchestrator | üß† REASONING_INPUT_SNAPSHOT
{
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful AI assistant.\n \nYou may answer ANY general knowledge question.\n \nConversation summaries and memories are ONLY to provide helpful context\n(e.g. preferences, ongoing plans).\n \nIf a user asks something unrelated to the current topic,\nanswer it normally using general knowledge.\n \nDo NOT restrict yourself to previous topics unless the user explicitly asks."
    },
    {
      "role": "assistant",
      "content": ""
    },
    {
      "role": "user",
      "content": "hello"
    },
    {
      "role": "assistant",
      "content": "Hello! How can I help you?"
    },
    {
      "role": "user",
      "content": "where do i study?"
    },
    {
      "role": "assistant",
      "content": "I don't have access to personal information like where you study. I am a large language model, trained by Google."
    },
    {
      "role": "user",
      "content": "what is my fav pizza?"
    },
    {
      "role": "user",
      "content": "what is my fav pizza?"
    }
  ],
  "safety": {
    "safe": true,
    "reason": null,
    "message": "OK"
  },
  "memory_action": {},
  "memory_data": [
    {
      "memory_id": 4,
      "user_id": 3,
      "text": "my favourite pizza is margneita.",
      "confidence": 0.85,
      "created_at": "2026-01-23T22:15:19.533318+05:30",
      "expires_at": "2026-02-26T10:09:28.217743+05:30",
      "score": 0.7625826597213745
    },
    {
      "memory_id": 10,
      "user_id": 3,
      "text": "I love sweet dishes.",
      "confidence": 0.85,
      "created_at": "2026-01-27T10:16:57.730737+05:30",
      "expires_at": "2026-02-26T17:41:35.888018+05:30",
      "score": 0.38045892119407654
    }
  ],
  "memory_found": true,
  "tool_context": {}
}
2026-01-28 13:32:43,147 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_54baa80390484c9d84dbe34d4da3f468
2026-01-28 13:32:43,147 | DEBUG | openai.agents | Setting current trace: trace_54baa80390484c9d84dbe34d4da3f468
2026-01-28 13:32:43,148 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000002BED9C84310> with id None
2026-01-28 13:32:43,148 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000002BEDA448BE0> with id None
2026-01-28 13:32:43,148 | DEBUG | openai.agents | Calling LLM
2026-01-28 13:32:43,149 | DEBUG | LiteLLM | 

2026-01-28 13:32:43,149 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 13:32:43,149 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "assistant", "content": ""}, {"role": "user", "content": "hello"}, {"role": "assistant", "content": "Hello! How can I help you?"}, {"role": "user", "content": "where do i study?"}, {"role": "assistant", "content": "I don\'t have access to personal information like where you study. I am a large language model, trained by Google."}, {"role": "user", "content": "what is my fav pizza?"}, {"role": "user", "content": "what is my fav pizza?"}], "safety": {"safe": true, "reason": null, "message": "OK"}, "memory_action": {}, "memory_data": [{"memory_id": 4, "user_id": 3, "text": "my favourite pizza is margneita.", "confidence": 0.85, "created_at": "2026-01-23T22:15:19.533318+05:30", "expires_at": "2026-02-26T10:09:28.217743+05:30", "score": 0.7625826597213745}, {"memory_id": 10, "user_id": 3, "text": "I love sweet dishes.", "confidence": 0.85, "created_at": "2026-01-27T10:16:57.730737+05:30", "expires_at": "2026-02-26T17:41:35.888018+05:30", "score": 0.38045892119407654}], "memory_found": true, "tool_context": {}}'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=True, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 13:32:43,150 | DEBUG | LiteLLM | 

2026-01-28 13:32:43,150 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 13:32:43,151 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 13:32:43,151 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 13:32:43,152 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:32:43,152 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 13:32:43,152 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "assistant", "content": ""}, {"role": "user", "content": "hello"}, {"role": "assistant", "content": "Hello! How can I help you?"}, {"role": "user", "content": "where do i study?"}, {"role": "assistant", "content": "I don\'t have access to personal information like where you study. I am a large language model, trained by Google."}, {"role": "user", "content": "what is my fav pizza?"}, {"role": "user", "content": "what is my fav pizza?"}], "safety": {"safe": true, "reason": null, "message": "OK"}, "memory_action": {}, "memory_data": [{"memory_id": 4, "user_id": 3, "text": "my favourite pizza is margneita.", "confidence": 0.85, "created_at": "2026-01-23T22:15:19.533318+05:30", "expires_at": "2026-02-26T10:09:28.217743+05:30", "score": 0.7625826597213745}, {"memory_id": 10, "user_id": 3, "text": "I love sweet dishes.", "confidence": 0.85, "created_at": "2026-01-27T10:16:57.730737+05:30", "expires_at": "2026-02-26T17:41:35.888018+05:30", "score": 0.38045892119407654}], "memory_found": true, "tool_context": {}}'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 13:32:43,153 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': True}
2026-01-28 13:32:43,153 | DEBUG | LiteLLM | Final returned optional params: {'stream': True}
2026-01-28 13:32:43,153 | DEBUG | LiteLLM | self.optional_params: {'stream': True}
2026-01-28 13:32:43,154 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:43,154 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:43,155 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?key=*****=sse \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "assistant", "content": ""}, {"role": "user", "content": "hello"}, {"role": "assistant", "content": "Hello! How can I help you?"}, {"role": "user", "content": "where do i study?"}, {"role": "assistant", "content": "I don\'t have access to personal information like where you study. I am a large language model, trained by Google."}, {"role": "user", "content": "what is my fav pizza?"}, {"role": "user", "content": "what is my fav pizza?"}], "safety": {"safe": true, "reason": null, "message": "OK"}, "memory_action": {}, "memory_data": [{"memory_id": 4, "user_id": 3, "text": "my favourite pizza is margneita.", "confidence": 0.85, "created_at": "2026-01-23T22:15:19.533318+05:30", "expires_at": "2026-02-26T10:09:28.217743+05:30", "score": 0.7625826597213745}, {"memory_id": 10, "user_id": 3, "text": "I love sweet dishes.", "confidence": 0.85, "created_at": "2026-01-27T10:16:57.730737+05:30", "expires_at": "2026-02-26T17:41:35.888018+05:30", "score": 0.38045892119407654}], "memory_found": true, "tool_context": {}}'}]}], 'system_instruction': {'parts': [{'text': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.'}]}, 'generationConfig': {}}'
[0m

2026-01-28 13:32:43,156 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:32:43,156 | DEBUG | LiteLLM | messages in token_counter: None, text in token_counter: 
2026-01-28 13:32:43,158 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:43,160 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:43,161 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:32:43,161 | DEBUG | LiteLLM | response_cost: 0.0
2026-01-28 13:32:43,811 | DEBUG | httpcore.connection | close.started
2026-01-28 13:32:43,812 | DEBUG | httpcore.connection | close.complete
2026-01-28 13:32:43,813 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 13:32:43,858 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002BECD9DF7C0>
2026-01-28 13:32:43,858 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x000002BECE369A40> server_hostname='api.openai.com' timeout=5.0
2026-01-28 13:32:43,871 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002BECFE64D00>
2026-01-28 13:32:43,872 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 13:32:43,873 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 13:32:43,873 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 13:32:43,874 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 13:32:43,874 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 13:32:44,286 | DEBUG | LiteLLM | RAW RESPONSE:
first stream response received


2026-01-28 13:32:44,287 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': 'Your favorite pizza is Margherita.'}], 'role': 'model'}, 'finishReason': 'STOP', 'index': 0}], 'usageMetadata': {'promptTokenCount': 656, 'candidatesTokenCount': 6, 'totalTokenCount': 755, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 656}], 'thoughtsTokenCount': 93}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'osJ5adKqOp2s4-EPxdmKoAQ'}
2026-01-28 13:32:44,289 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='osJ5adKqOp2s4-EPxdmKoAQ', created=1769587364, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='Your favorite pizza is Margherita.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=99, prompt_tokens=656, total_tokens=755, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=93, rejected_prediction_tokens=None, text_tokens=6, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=656, image_tokens=None)))
2026-01-28 13:32:44,291 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content='Your favorite pizza is Margherita.', role='assistant', function_call=None, tool_calls=None, audio=None)
2026-01-28 13:32:44,292 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='osJ5adKqOp2s4-EPxdmKoAQ', created=1769587364, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Your favorite pizza is Margherita.', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=99, prompt_tokens=656, total_tokens=755, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=93, rejected_prediction_tokens=None, text_tokens=6, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=656, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-28 13:32:44,296 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=False
2026-01-28 13:32:44,297 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=False
2026-01-28 13:32:44,297 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 13:32:44,298 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 13:32:44,299 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:32:44,300 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:32:44,301 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:44,302 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:44,302 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:44,303 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:44,303 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:32:44,304 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:32:44,305 | DEBUG | LiteLLM | response_cost: 0.0004443
2026-01-28 13:32:44,306 | DEBUG | LiteLLM | response_cost: 0.0004443
2026-01-28 13:32:44,308 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:32:44,309 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0004443
2026-01-28 13:32:44,311 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:44,313 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:32:44,314 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:32:44,315 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:44,316 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:32:44,317 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 13:32:44,319 | DEBUG | openai.agents | Resetting current trace
2026-01-28 13:32:44,332 | INFO | session_summary | üß† SUMMARY_START | session_id=e03649ad-b3b1-4a09-a679-d5588d2dfb33
2026-01-28 13:32:44,336 | INFO | session_summary | üìä Unsummarized messages = 10
2026-01-28 13:32:44,338 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-28 13:32:44,339 | INFO | chat | ‚úÖ Stream complete | tokens=1 | time=4.51s
2026-01-28 13:32:44,342 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 08:02:43 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_76ea388d67789db4442d8f4ca01aa4af'), (b'openai-processing-ms', b'134'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'137'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4ef81bde515965-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 13:32:44,345 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 13:32:44,348 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 13:32:44,349 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 13:32:44,350 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 13:32:44,351 | DEBUG | openai.agents | Exported 7 items
2026-01-28 13:32:44,353 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 13:32:44,355 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 13:32:44,357 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 13:32:44,358 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 13:32:44,359 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 13:32:44,815 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 08:02:44 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_048ef3356cc64c0d9805be7a046d014d'), (b'openai-processing-ms', b'116'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'119'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4ef81edfd55965-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 13:32:44,817 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 13:32:44,817 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 13:32:44,818 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 13:32:44,818 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 13:32:44,819 | DEBUG | openai.agents | Exported 2 items
2026-01-28 13:32:51,262 | INFO | chat | üì© Chat request | user_id=3 | request_id=3bdadd51-cb2c-4e3b-ac29-720e6ba6d40d
2026-01-28 13:32:51,263 | INFO | chat | üßë User message | where do i live?
2026-01-28 13:32:51,280 | INFO | chat | ü§ñ Stream started | session_id=e03649ad-b3b1-4a09-a679-d5588d2dfb33 | request_id=3bdadd51-cb2c-4e3b-ac29-720e6ba6d40d
2026-01-28 13:32:51,281 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-28 13:32:51,283 | INFO | orchestrator | üßë USER_INPUT | where do i live?
2026-01-28 13:32:51,284 | INFO | orchestrator | üß© CONTEXT | user_id=3 | enable_memory=True | db=None
2026-01-28 13:32:51,285 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-28 13:32:51,286 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_e0dad4043445428cae1f41332d9d3eef
2026-01-28 13:32:51,287 | DEBUG | openai.agents | Setting current trace: trace_e0dad4043445428cae1f41332d9d3eef
2026-01-28 13:32:51,287 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000002BED9C87BF0> with id None
2026-01-28 13:32:51,288 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-28 13:32:51,289 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000002BEDA449000> with id None
2026-01-28 13:32:51,289 | DEBUG | openai.agents | Calling LLM
2026-01-28 13:32:51,290 | DEBUG | LiteLLM | 

2026-01-28 13:32:51,291 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 13:32:51,291 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output JSON Schema (MANDATORY):\n\n{\n  "safe": boolean,\n  "reason": string | null\n}\n\n### Rules:\n- SAFE:\n  { "safe": true, "reason": null }\n\n- UNSAFE:\n  { "safe": false, "reason": "<short reason>" }\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'where do i live?'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 13:32:51,292 | DEBUG | LiteLLM | 

2026-01-28 13:32:51,293 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 13:32:51,295 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 13:32:51,298 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 13:32:51,301 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:32:51,303 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 13:32:51,304 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output JSON Schema (MANDATORY):\n\n{\n  "safe": boolean,\n  "reason": string | null\n}\n\n### Rules:\n- SAFE:\n  { "safe": true, "reason": null }\n\n- UNSAFE:\n  { "safe": false, "reason": "<short reason>" }\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'where do i live?'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 13:32:51,305 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 13:32:51,306 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 13:32:51,306 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 13:32:51,307 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:51,309 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:51,312 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'where do i live?'}]}], 'system_instruction': {'parts': [{'text': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output JSON Schema (MANDATORY):\n\n{\n  "safe": boolean,\n  "reason": string | null\n}\n\n### Rules:\n- SAFE:\n  { "safe": true, "reason": null }\n\n- UNSAFE:\n  { "safe": false, "reason": "<short reason>" }\n\nReturn ONLY valid JSON.\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 13:32:52,351 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\"safe\": false, \"reason\": \"I cannot provide personal information about you.\"}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 135,
    "candidatesTokenCount": 18,
    "totalTokenCount": 216,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 135
      }
    ],
    "thoughtsTokenCount": 63
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "q8J5afG2H82xjuMPw7LB-AE"
}



2026-01-28 13:32:52,354 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:32:52,355 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:32:52,355 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:32:52,356 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:52,357 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:32:52,358 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:52,359 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:52,359 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:32:52,360 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:52,361 | DEBUG | LiteLLM | response_cost: 0.000243
2026-01-28 13:32:52,362 | DEBUG | openai.agents | Received model response
2026-01-28 13:32:52,362 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:32:52,363 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x000002BED9CAE8C0>>
2026-01-28 13:32:52,364 | DEBUG | LiteLLM | response_cost: 0.000243
2026-01-28 13:32:52,365 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:32:52,366 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:32:52,367 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 13:32:52,367 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:32:52,368 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:52,369 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 13:32:52,371 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:32:52,372 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:32:52,376 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:32:52,376 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:52,377 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 13:32:52,379 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:52,381 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:32:52,382 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:32:52,383 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:32:52,384 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:52,385 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 13:32:52,385 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:32:52,386 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.000243
2026-01-28 13:32:52,388 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:32:52,389 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:32:52,390 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:32:52,394 | DEBUG | openai.agents | Resetting current trace
2026-01-28 13:32:52,396 | WARNING | orchestrator | üö´ Safety blocked request
2026-01-28 13:32:52,400 | INFO | session_summary | üß† SUMMARY_START | session_id=e03649ad-b3b1-4a09-a679-d5588d2dfb33
2026-01-28 13:32:52,405 | INFO | session_summary | üìä Unsummarized messages = 12
2026-01-28 13:32:52,406 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-28 13:32:52,408 | INFO | chat | ‚úÖ Stream complete | tokens=0 | time=1.14s
2026-01-28 13:32:55,106 | DEBUG | httpcore.connection | close.started
2026-01-28 13:32:55,107 | DEBUG | httpcore.connection | close.complete
2026-01-28 13:32:55,107 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 13:32:55,170 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002BED9704880>
2026-01-28 13:32:55,170 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x000002BECE369A40> server_hostname='api.openai.com' timeout=5.0
2026-01-28 13:32:55,182 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002BED9705600>
2026-01-28 13:32:55,182 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 13:32:55,183 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 13:32:55,183 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 13:32:55,183 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 13:32:55,183 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 13:32:55,604 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 08:02:54 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_835715fe033713df4e1d43e82b033340'), (b'openai-processing-ms', b'109'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'112'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4ef86278aff556-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 13:32:55,604 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 13:32:55,605 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 13:32:55,605 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 13:32:55,605 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 13:32:55,605 | DEBUG | openai.agents | Exported 3 items
2026-01-28 13:47:40,825 | DEBUG | LiteLLM | LoggingWorker cancelled during shutdown
2026-01-28 13:47:40,827 | DEBUG | LiteLLM | [LoggingWorker] atexit: Queue is empty
2026-01-28 13:47:40,828 | DEBUG | asyncio | Using proactor: IocpProactor
2026-01-28 13:47:40,829 | DEBUG | openai.agents | Shutting down trace provider
2026-01-28 13:47:40,829 | DEBUG | openai.agents | Shutting down trace processor <agents.tracing.processors.BatchTraceProcessor object at 0x000002BECE36F940>
2026-01-28 13:47:50,137 | INFO | sentence_transformers.SentenceTransformer | Use pytorch device_name: cpu
2026-01-28 13:47:50,138 | INFO | sentence_transformers.SentenceTransformer | Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2026-01-28 13:47:50,141 | DEBUG | urllib3.connectionpool | Starting new HTTPS connection (1): huggingface.co:443
2026-01-28 13:47:50,456 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-28 13:47:50,466 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-28 13:47:50,699 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-28 13:47:50,716 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-28 13:47:50,955 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-28 13:47:50,967 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-28 13:47:51,210 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md HTTP/1.1" 307 0
2026-01-28 13:47:51,228 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/README.md HTTP/1.1" 200 0
2026-01-28 13:47:51,472 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-28 13:47:51,483 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-28 13:47:51,724 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json HTTP/1.1" 307 0
2026-01-28 13:47:51,742 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/sentence_bert_config.json HTTP/1.1" 200 0
2026-01-28 13:47:52,074 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json HTTP/1.1" 404 0
2026-01-28 13:47:52,311 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1" 307 0
2026-01-28 13:47:52,321 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json HTTP/1.1" 200 0
2026-01-28 13:47:52,617 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/1.1" 307 0
2026-01-28 13:47:52,626 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json HTTP/1.1" 200 0
2026-01-28 13:47:52,864 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1" 404 64
2026-01-28 13:47:53,122 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main?recursive=True&expand=False HTTP/1.1" 200 6465
2026-01-28 13:47:53,392 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json HTTP/1.1" 307 0
2026-01-28 13:47:53,403 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json HTTP/1.1" 200 0
2026-01-28 13:47:53,654 | DEBUG | urllib3.connectionpool | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2 HTTP/1.1" 200 6885
2026-01-28 13:47:54,604 | INFO | main | üöÄ FastAPI application starting
2026-01-28 13:51:49,510 | INFO | chat | üì© Chat request | user_id=3 | request_id=c169a99c-d2ab-4d76-a920-c186251eab4f
2026-01-28 13:51:49,551 | INFO | chat | üßë User message | hi
2026-01-28 13:51:49,575 | INFO | chat | ü§ñ Stream started | session_id=e03649ad-b3b1-4a09-a679-d5588d2dfb33 | request_id=c169a99c-d2ab-4d76-a920-c186251eab4f
2026-01-28 13:51:49,576 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-28 13:51:49,578 | INFO | orchestrator | üßë USER_INPUT | hi
2026-01-28 13:51:49,579 | INFO | orchestrator | üß© CONTEXT | user_id=3 | enable_memory=True | db=None
2026-01-28 13:51:49,580 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-28 13:51:49,583 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_f38a0c2c811d4d86b73a8ce212f294e1
2026-01-28 13:51:49,586 | DEBUG | openai.agents | Setting current trace: trace_f38a0c2c811d4d86b73a8ce212f294e1
2026-01-28 13:51:49,588 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000001CCEC8D81D0> with id None
2026-01-28 13:51:49,589 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-28 13:51:50,681 | DEBUG | httpcore.connection | connect_tcp.started host='raw.githubusercontent.com' port=443 local_address=None timeout=5 socket_options=None
2026-01-28 13:51:50,731 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001CCED12F370>
2026-01-28 13:51:50,732 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x000001CCED13AB40> server_hostname='raw.githubusercontent.com' timeout=5
2026-01-28 13:51:50,765 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001CCED12F340>
2026-01-28 13:51:50,765 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'GET']>
2026-01-28 13:51:50,766 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 13:51:50,767 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'GET']>
2026-01-28 13:51:50,767 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 13:51:50,768 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'GET']>
2026-01-28 13:51:50,796 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Connection', b'keep-alive'), (b'Content-Length', b'62476'), (b'Cache-Control', b'max-age=300'), (b'Content-Security-Policy', b"default-src 'none'; style-src 'unsafe-inline'; sandbox"), (b'Content-Type', b'text/plain; charset=utf-8'), (b'ETag', b'W/"f6dad8a5d9e28465b3d70ca31700cb6ef9bf2cd7c9d0fa0c0c5cdf23a1bb3382"'), (b'Strict-Transport-Security', b'max-age=31536000'), (b'X-Content-Type-Options', b'nosniff'), (b'X-Frame-Options', b'deny'), (b'X-XSS-Protection', b'1; mode=block'), (b'X-GitHub-Request-Id', b'2442:60E63:51359:95C7D:6978AD63'), (b'Content-Encoding', b'gzip'), (b'Accept-Ranges', b'bytes'), (b'Date', b'Wed, 28 Jan 2026 08:21:50 GMT'), (b'Via', b'1.1 varnish'), (b'X-Served-By', b'cache-bom-vanm7210038-BOM'), (b'X-Cache', b'HIT'), (b'X-Cache-Hits', b'4'), (b'X-Timer', b'S1769588510.097122,VS0,VE0'), (b'Vary', b'Authorization,Accept-Encoding'), (b'Access-Control-Allow-Origin', b'*'), (b'Cross-Origin-Resource-Policy', b'cross-origin'), (b'X-Fastly-Request-ID', b'0b509c9d366fd48a2c13a46d0ddff26a6a33560b'), (b'Expires', b'Wed, 28 Jan 2026 08:26:50 GMT'), (b'Source-Age', b'14')])
2026-01-28 13:51:50,798 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'GET']>
2026-01-28 13:51:50,843 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 13:51:50,844 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 13:51:50,844 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 13:51:50,845 | DEBUG | httpcore.connection | close.started
2026-01-28 13:51:50,845 | DEBUG | httpcore.connection | close.complete
2026-01-28 13:51:51,514 | DEBUG | LiteLLM | [Non-Blocking] Unable to import GenericAPILogger - LiteLLM Enterprise Feature - No module named 'litellm_enterprise'
2026-01-28 13:51:52,073 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-28 13:51:52,074 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-28 13:51:52,074 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-28 13:51:52,081 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-28 13:51:52,081 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-28 13:51:52,082 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-28 13:51:52,082 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-28 13:51:52,083 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-28 13:51:52,083 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-28 13:51:52,083 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-28 13:51:52,084 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-28 13:51:52,084 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-28 13:51:52,094 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000001CCEC8D59C0> with id None
2026-01-28 13:51:52,095 | DEBUG | openai.agents | Calling LLM
2026-01-28 13:51:52,096 | DEBUG | LiteLLM | 

2026-01-28 13:51:52,096 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 13:51:52,098 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output JSON Schema (MANDATORY):\n\n{\n  "safe": boolean,\n  "reason": string | null\n}\n\n### Rules:\n- SAFE:\n  { "safe": true, "reason": null }\n\n- UNSAFE:\n  { "safe": false, "reason": "<short reason>" }\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'hi'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 13:51:52,099 | DEBUG | LiteLLM | 

2026-01-28 13:51:52,101 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 13:51:52,101 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 13:51:52,101 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 13:51:52,112 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:51:52,112 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 13:51:52,114 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output JSON Schema (MANDATORY):\n\n{\n  "safe": boolean,\n  "reason": string | null\n}\n\n### Rules:\n- SAFE:\n  { "safe": true, "reason": null }\n\n- UNSAFE:\n  { "safe": false, "reason": "<short reason>" }\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'hi'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 13:51:52,114 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 13:51:52,115 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 13:51:52,115 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 13:51:52,116 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:51:52,291 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:51:52,292 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'hi'}]}], 'system_instruction': {'parts': [{'text': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output JSON Schema (MANDATORY):\n\n{\n  "safe": boolean,\n  "reason": string | null\n}\n\n### Rules:\n- SAFE:\n  { "safe": true, "reason": null }\n\n- UNSAFE:\n  { "safe": false, "reason": "<short reason>" }\n\nReturn ONLY valid JSON.\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 13:51:53,514 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\"safe\": true, \"reason\": null}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 131,
    "candidatesTokenCount": 10,
    "totalTokenCount": 174,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 131
      }
    ],
    "thoughtsTokenCount": 33
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "IMd5afi2KISdjuMPnaefoQI"
}



2026-01-28 13:51:53,516 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:51:53,517 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:51:53,518 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:51:53,518 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:51:53,519 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:51:53,519 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:51:53,519 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:51:53,520 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:51:53,520 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:51:53,520 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:51:53,522 | DEBUG | LiteLLM | response_cost: 0.00014680000000000002
2026-01-28 13:51:53,522 | DEBUG | LiteLLM | response_cost: 0.00014680000000000002
2026-01-28 13:51:53,522 | DEBUG | openai.agents | Received model response
2026-01-28 13:51:53,523 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:51:53,529 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001CCF6012C20>>
2026-01-28 13:51:53,530 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:51:53,530 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:51:53,531 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:51:53,532 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 13:51:53,533 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:51:53,533 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 13:51:53,533 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:51:53,533 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:51:53,534 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 13:51:53,534 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:51:53,534 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:51:53,535 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:51:53,535 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:51:53,536 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:51:53,536 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:51:53,536 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:51:53,537 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 13:51:53,537 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.00014680000000000002
2026-01-28 13:51:53,538 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:51:53,538 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:51:53,538 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:51:53,540 | DEBUG | openai.agents | Resetting current trace
2026-01-28 13:51:53,540 | INFO | orchestrator | ‚úÖ Safety passed
2026-01-28 13:51:53,540 | INFO | orchestrator | üì• Fetching semantic memory (via MemoryService)
2026-01-28 13:51:53,620 | INFO | orchestrator | üß† MEMORY_RESULT | found=False | count=0
2026-01-28 13:51:53,621 | INFO | orchestrator | üß≠ Router agent called
2026-01-28 13:51:53,621 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_46e88469fe044b2480145f40bb9bde80
2026-01-28 13:51:53,622 | DEBUG | openai.agents | Setting current trace: trace_46e88469fe044b2480145f40bb9bde80
2026-01-28 13:51:53,622 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000001CCF60B05E0> with id None
2026-01-28 13:51:53,622 | DEBUG | openai.agents | Running agent RouterAgent (turn 1)
2026-01-28 13:51:53,623 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000001CCF6022CE0> with id None
2026-01-28 13:51:53,623 | DEBUG | openai.agents | Calling LLM
2026-01-28 13:51:53,623 | DEBUG | LiteLLM | 

2026-01-28 13:51:53,623 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 13:51:53,623 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'hi'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 13:51:53,624 | DEBUG | LiteLLM | 

2026-01-28 13:51:53,624 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 13:51:53,625 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 13:51:53,625 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 13:51:53,626 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:51:53,626 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 13:51:53,626 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n', 'role': 'system'}, {'role': 'user', 'content': 'hi'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 13:51:53,627 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 13:51:53,627 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 13:51:53,628 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 13:51:53,628 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:51:53,632 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:51:53,632 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'hi'}]}], 'system_instruction': {'parts': [{'text': 'You are a Router Agent for a Digital Human system.\n\nYour task:\nDecide whether the user request requires:\n1) Memory access\n2) Tool usage\n3) Memory intent (read or write)\n4) Which tool to call (if any)\n\nReturn STRICT JSON ONLY in the following format:\n\n{\n  "use_memory": true | false,\n  "use_tool": true | false,\n  "tool_name": "weather" | "calculator" | "web_search" | "browser" | "none",\n  "tool_arguments": { } | null,\n  "intent": "read" | "write" | "none",\n  "memory_key": "string | null"\n}\n\nRouting rules:\n\n- Greetings, casual talk, general knowledge ‚Üí use_memory=false, use_tool=false, intent=none\n- Statements that reveal, change, or delete personal facts or preferences ‚Üí use_memory=true, intent=write\n- Questions asking about the user\'s own stored information ‚Üí use_memory=true, intent=read\n- Questions about the world that do NOT require live data ‚Üí no memory, no tool\n- Tool usage ONLY if external data, real-time info, search, or calculation is required\n\nTool selection rules:\n\n- Weather, temperature, forecast, climate, current conditions ‚Üí tool_name="weather"\n- Math, arithmetic, calculations ‚Üí tool_name="calculator"\n- Real-time facts, news, current events ‚Üí tool_name="web_search"\n- Browsing or extracting info from websites ‚Üí tool_name="browser"\n- If no tool is needed ‚Üí tool_name="none"\n\nWeather tool rules (MANDATORY):\n- If the user asks about weather AND mentions a place ‚Üí ALWAYS use tool_name="weather"\n- If the user asks about weather "now", "today", or "current" ‚Üí use_tool=true\n\nMemory key rules:\n- For food preferences ‚Üí "food_preference"\n- For any other preference ‚Üí use a short snake_case key\n- If unknown or irrelevant ‚Üí null\n\nImportant constraints:\n- Do NOT explain your decision\n- Do NOT add text outside JSON\n- Do NOT include markdown\n- Always return valid JSON\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 13:51:54,681 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\"use_memory\": false, \"use_tool\": false, \"tool_name\": \"none\", \"tool_arguments\": null, \"intent\": \"none\", \"memory_key\": null}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 486,
    "candidatesTokenCount": 42,
    "totalTokenCount": 584,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 486
      }
    ],
    "thoughtsTokenCount": 56
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "Icd5abrcM6e54-EPi4qn2Qc"
}



2026-01-28 13:51:54,683 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:51:54,683 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:51:54,684 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:51:54,684 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:51:54,685 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:51:54,685 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:51:54,685 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:51:54,686 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:51:54,686 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:51:54,687 | DEBUG | LiteLLM | response_cost: 0.00039079999999999996
2026-01-28 13:51:54,687 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:51:54,687 | DEBUG | openai.agents | Received model response
2026-01-28 13:51:54,688 | DEBUG | LiteLLM | response_cost: 0.00039079999999999996
2026-01-28 13:51:54,688 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001CCF605F250>>
2026-01-28 13:51:54,689 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:51:54,689 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:51:54,690 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:51:54,690 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 13:51:54,690 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:51:54,691 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:51:54,691 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 13:51:54,691 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:51:54,692 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 13:51:54,693 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:51:54,693 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:51:54,693 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:51:54,694 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:51:54,694 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:51:54,694 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:51:54,695 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:51:54,695 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:51:54,696 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 13:51:54,696 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.00039079999999999996
2026-01-28 13:51:54,697 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:51:54,697 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:51:54,697 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:51:54,698 | DEBUG | openai.agents | Resetting current trace
2026-01-28 13:51:54,698 | INFO | orchestrator | üß≠ ROUTER_DECISION | tool=False | memory=False | intent=none
2026-01-28 13:51:54,699 | INFO | orchestrator | üß† REASONING_INPUT_SNAPSHOT
{
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful AI assistant.\n \nYou may answer ANY general knowledge question.\n \nConversation summaries and memories are ONLY to provide helpful context\n(e.g. preferences, ongoing plans).\n \nIf a user asks something unrelated to the current topic,\nanswer it normally using general knowledge.\n \nDo NOT restrict yourself to previous topics unless the user explicitly asks."
    },
    {
      "role": "assistant",
      "content": "I don't have access to personal information like where you study. I am a large language model, trained by Google."
    },
    {
      "role": "user",
      "content": "what is my fav pizza?"
    },
    {
      "role": "assistant",
      "content": "Your favorite pizza is Margherita."
    },
    {
      "role": "user",
      "content": "where do i live?"
    },
    {
      "role": "assistant",
      "content": ""
    },
    {
      "role": "user",
      "content": "hi"
    },
    {
      "role": "user",
      "content": "hi"
    }
  ],
  "safety": {
    "safe": true,
    "reason": null,
    "message": "OK"
  },
  "memory_action": {},
  "memory_data": [],
  "memory_found": false,
  "tool_context": {}
}
2026-01-28 13:51:54,700 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_3b183fbe80e34f7db64551c7aef2ef05
2026-01-28 13:51:54,701 | DEBUG | openai.agents | Setting current trace: trace_3b183fbe80e34f7db64551c7aef2ef05
2026-01-28 13:51:54,701 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000001CCF6D7D940> with id None
2026-01-28 13:51:54,702 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000001CCF6D95240> with id None
2026-01-28 13:51:54,702 | DEBUG | openai.agents | Calling LLM
2026-01-28 13:51:54,702 | DEBUG | LiteLLM | 

2026-01-28 13:51:54,703 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 13:51:54,703 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "assistant", "content": "I don\'t have access to personal information like where you study. I am a large language model, trained by Google."}, {"role": "user", "content": "what is my fav pizza?"}, {"role": "assistant", "content": "Your favorite pizza is Margherita."}, {"role": "user", "content": "where do i live?"}, {"role": "assistant", "content": ""}, {"role": "user", "content": "hi"}, {"role": "user", "content": "hi"}], "safety": {"safe": true, "reason": null, "message": "OK"}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=True, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 13:51:54,703 | DEBUG | LiteLLM | 

2026-01-28 13:51:54,704 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 13:51:54,704 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 13:51:54,704 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 13:51:54,707 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:51:54,707 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 13:51:54,708 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.', 'role': 'system'}, {'role': 'user', 'content': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "assistant", "content": "I don\'t have access to personal information like where you study. I am a large language model, trained by Google."}, {"role": "user", "content": "what is my fav pizza?"}, {"role": "assistant", "content": "Your favorite pizza is Margherita."}, {"role": "user", "content": "where do i live?"}, {"role": "assistant", "content": ""}, {"role": "user", "content": "hi"}, {"role": "user", "content": "hi"}], "safety": {"safe": true, "reason": null, "message": "OK"}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 13:51:54,709 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': True}
2026-01-28 13:51:54,709 | DEBUG | LiteLLM | Final returned optional params: {'stream': True}
2026-01-28 13:51:54,710 | DEBUG | LiteLLM | self.optional_params: {'stream': True}
2026-01-28 13:51:54,710 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:51:54,711 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:51:54,711 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?key=*****=sse \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '{"messages": [{"role": "system", "content": "You are a helpful AI assistant.\\n \\nYou may answer ANY general knowledge question.\\n \\nConversation summaries and memories are ONLY to provide helpful context\\n(e.g. preferences, ongoing plans).\\n \\nIf a user asks something unrelated to the current topic,\\nanswer it normally using general knowledge.\\n \\nDo NOT restrict yourself to previous topics unless the user explicitly asks."}, {"role": "assistant", "content": "I don\'t have access to personal information like where you study. I am a large language model, trained by Google."}, {"role": "user", "content": "what is my fav pizza?"}, {"role": "assistant", "content": "Your favorite pizza is Margherita."}, {"role": "user", "content": "where do i live?"}, {"role": "assistant", "content": ""}, {"role": "user", "content": "hi"}, {"role": "user", "content": "hi"}], "safety": {"safe": true, "reason": null, "message": "OK"}, "memory_action": {}, "memory_data": [], "memory_found": false, "tool_context": {}}'}]}], 'system_instruction': {'parts': [{'text': 'You are a reasoning agent.\n\nIMPORTANT RULES:\n\nIf tool_context is provided and contains data:\n- You MUST use the tool_context to answer\n- You MUST NOT say you lack real-time access\n- You MUST NOT ignore tool output\n- Answer using tool results only\n\nYou may receive:\n- User preferences or memory context\n- A user question\n\nRules:\n- If preferences are present, ALWAYS use them when relevant.\n- Prefer personalized answers over generic ones.\n- Do NOT mention memory explicitly.\n- Be concise and accurate.'}]}, 'generationConfig': {}}'
[0m

2026-01-28 13:51:54,715 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:51:54,718 | DEBUG | LiteLLM | messages in token_counter: None, text in token_counter: 
2026-01-28 13:51:54,718 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:51:54,719 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:51:54,719 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:51:54,720 | DEBUG | LiteLLM | response_cost: 0.0
2026-01-28 13:51:54,767 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 13:51:54,766 | DEBUG | LiteLLM | Using AiohttpTransport...
2026-01-28 13:51:54,767 | DEBUG | LiteLLM | Creating AiohttpTransport...
2026-01-28 13:51:54,768 | DEBUG | LiteLLM | NEW SESSION: Creating new ClientSession (no shared session provided)
2026-01-28 13:51:54,808 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001CCF605D000>
2026-01-28 13:51:54,808 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x000001CCEACB9A40> server_hostname='api.openai.com' timeout=5.0
2026-01-28 13:51:54,819 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001CCF605C9A0>
2026-01-28 13:51:54,819 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 13:51:54,819 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 13:51:54,820 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 13:51:54,820 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 13:51:54,820 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 13:51:55,264 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 08:21:54 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_c9e666b564e797133358812eaa3bff06'), (b'openai-processing-ms', b'114'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'117'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=pEIxdq7FovMX60On8KS2eodE.ad0v.blU2_6LQFIUJw-1769588514-1.0.1.1-T5F1hemMY3hbIHsnm0SGCF5EbJFmQsghnwAl_CXMSkrcXCsEpEMHfGiKoXzOpdHBpsN9PNoZfjQC5w_CcmP2WWlCdoqeO6L_eHtCDD4rLpA; path=/; expires=Wed, 28-Jan-26 08:51:54 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=d5asUYLEJ3DlGWEO7uI.UWrHQhgt.P4lfLizpBgBGBU-1769588514551-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4f14353a975952-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 13:51:55,264 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 13:51:55,265 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 13:51:55,265 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 13:51:55,265 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 13:51:55,265 | DEBUG | openai.agents | Exported 7 items
2026-01-28 13:51:56,264 | DEBUG | LiteLLM | RAW RESPONSE:
first stream response received


2026-01-28 13:51:56,265 | DEBUG | LiteLLM | RAW GEMINI CHUNK: {'candidates': [{'content': {'parts': [{'text': 'Hi! How can I help you today?'}], 'role': 'model'}, 'finishReason': 'STOP', 'index': 0}], 'usageMetadata': {'promptTokenCount': 375, 'candidatesTokenCount': 9, 'totalTokenCount': 404, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 375}], 'thoughtsTokenCount': 20}, 'modelVersion': 'gemini-2.5-flash', 'responseId': 'I8d5ae_YDOzVjuMPy-6C4QM'}
2026-01-28 13:51:56,269 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: ModelResponseStream(id='I8d5ae_YDOzVjuMPy-6C4QM', created=1769588516, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='Hi! How can I help you today?', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=29, prompt_tokens=375, total_tokens=404, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=20, rejected_prediction_tokens=None, text_tokens=9, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=375, image_tokens=None)))
2026-01-28 13:51:56,271 | DEBUG | LiteLLM | model_response.choices[0].delta inside is_chunk_non_empty: Delta(provider_specific_fields=None, content='Hi! How can I help you today?', role='assistant', function_call=None, tool_calls=None, audio=None)
2026-01-28 13:51:56,271 | DEBUG | LiteLLM | PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='I8d5ae_YDOzVjuMPy-6C4QM', created=1769588516, model='gemini-2.5-flash', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Hi! How can I help you today?', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=29, prompt_tokens=375, total_tokens=404, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=20, rejected_prediction_tokens=None, text_tokens=9, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=375, image_tokens=None)), citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])
2026-01-28 13:51:56,310 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=False
2026-01-28 13:51:56,312 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=False
2026-01-28 13:51:56,312 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 13:51:56,313 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 13:51:56,313 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:51:56,314 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:51:56,314 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:51:56,315 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:51:56,315 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:51:56,315 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:51:56,315 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:51:56,316 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:51:56,316 | DEBUG | LiteLLM | response_cost: 0.000185
2026-01-28 13:51:56,317 | DEBUG | LiteLLM | response_cost: 0.000185
2026-01-28 13:51:56,317 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:51:56,317 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.000185
2026-01-28 13:51:56,318 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:51:56,318 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:51:56,319 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:51:56,319 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:51:56,319 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:51:56,321 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 13:51:56,321 | DEBUG | openai.agents | Resetting current trace
2026-01-28 13:51:56,330 | INFO | session_summary | üß† SUMMARY_START | session_id=e03649ad-b3b1-4a09-a679-d5588d2dfb33
2026-01-28 13:51:56,335 | INFO | session_summary | üìä Unsummarized messages = 14
2026-01-28 13:51:56,336 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-28 13:51:56,336 | INFO | chat | ‚úÖ Stream complete | tokens=1 | time=6.83s
2026-01-28 13:52:00,401 | DEBUG | httpcore.connection | close.started
2026-01-28 13:52:00,403 | DEBUG | httpcore.connection | close.complete
2026-01-28 13:52:00,404 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 13:52:00,421 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001CCF6D570D0>
2026-01-28 13:52:00,422 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x000001CCEACB9A40> server_hostname='api.openai.com' timeout=5.0
2026-01-28 13:52:00,435 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001CCF6D56FE0>
2026-01-28 13:52:00,436 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 13:52:00,438 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 13:52:00,438 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 13:52:00,439 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 13:52:00,440 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 13:52:01,493 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 08:22:00 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_0f2e482bedd74c5644095d02cbb6bf51'), (b'openai-processing-ms', b'112'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'115'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4f14585f0c8991-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 13:52:01,493 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 13:52:01,493 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 13:52:01,494 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 13:52:01,494 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 13:52:01,495 | DEBUG | openai.agents | Exported 2 items
2026-01-28 13:52:05,622 | INFO | chat | üì© Chat request | user_id=3 | request_id=76fd07eb-f636-4c7f-ae98-df31919716be
2026-01-28 13:52:05,624 | INFO | chat | üßë User message | i can kill myself
2026-01-28 13:52:05,645 | INFO | chat | ü§ñ Stream started | session_id=e03649ad-b3b1-4a09-a679-d5588d2dfb33 | request_id=76fd07eb-f636-4c7f-ae98-df31919716be
2026-01-28 13:52:05,649 | INFO | orchestrator | üî• LOG TEST: orchestrator started
2026-01-28 13:52:05,651 | INFO | orchestrator | üßë USER_INPUT | i can kill myself
2026-01-28 13:52:05,652 | INFO | orchestrator | üß© CONTEXT | user_id=3 | enable_memory=True | db=None
2026-01-28 13:52:05,654 | INFO | orchestrator | üõ°Ô∏è Safety agent called
2026-01-28 13:52:05,655 | DEBUG | openai.agents | Creating trace Agent workflow with id trace_70217be0c3c04897a2c08bb19c7caaa0
2026-01-28 13:52:05,656 | DEBUG | openai.agents | Setting current trace: trace_70217be0c3c04897a2c08bb19c7caaa0
2026-01-28 13:52:05,658 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.AgentSpanData object at 0x000001CCF65C86D0> with id None
2026-01-28 13:52:05,659 | DEBUG | openai.agents | Running agent Safety Agent (turn 1)
2026-01-28 13:52:05,662 | DEBUG | openai.agents | Creating span <agents.tracing.span_data.GenerationSpanData object at 0x000001CCF6D97820> with id None
2026-01-28 13:52:05,664 | DEBUG | openai.agents | Calling LLM
2026-01-28 13:52:05,664 | DEBUG | LiteLLM | 

2026-01-28 13:52:05,665 | DEBUG | LiteLLM | [92mRequest to litellm:[0m
2026-01-28 13:52:05,666 | DEBUG | LiteLLM | [92mlitellm.acompletion(model='gemini/gemini-2.5-flash', messages=[{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output JSON Schema (MANDATORY):\n\n{\n  "safe": boolean,\n  "reason": string | null\n}\n\n### Rules:\n- SAFE:\n  { "safe": true, "reason": null }\n\n- UNSAFE:\n  { "safe": false, "reason": "<short reason>" }\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'i can kill myself'}], tools=None, temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.6.4'}, api_key=None, base_url=None)[0m
2026-01-28 13:52:05,668 | DEBUG | LiteLLM | 

2026-01-28 13:52:05,670 | DEBUG | LiteLLM | self.optional_params: {}
2026-01-28 13:52:05,671 | DEBUG | LiteLLM | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2026-01-28 13:52:05,674 | DEBUG | LiteLLM | üîÑ NO SHARED SESSION: acompletion called without shared_session
2026-01-28 13:52:05,680 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:52:05,684 | INFO | LiteLLM | 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2026-01-28 13:52:05,700 | DEBUG | LiteLLM | 
LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output JSON Schema (MANDATORY):\n\n{\n  "safe": boolean,\n  "reason": string | null\n}\n\n### Rules:\n- SAFE:\n  { "safe": true, "reason": null }\n\n- UNSAFE:\n  { "safe": false, "reason": "<short reason>" }\n\nReturn ONLY valid JSON.\n', 'role': 'system'}, {'role': 'user', 'content': 'i can kill myself'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'custom_llm_provider': 'gemini', 'drop_params': None, 'model': 'gemini-2.5-flash', 'n': None, 'service_tier': None}
2026-01-28 13:52:05,704 | DEBUG | LiteLLM | 
LiteLLM: Non-Default params passed to completion() {'stream': False}
2026-01-28 13:52:05,711 | DEBUG | LiteLLM | Final returned optional params: {}
2026-01-28 13:52:05,712 | DEBUG | LiteLLM | self.optional_params: {'stream': False}
2026-01-28 13:52:05,714 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:52:05,716 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:52:05,717 | DEBUG | LiteLLM | [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=*****toyA \
-H 'Content-Type: application/json' -H 'User-Agent: Agents/Python 0.6.4' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'i can kill myself'}]}], 'system_instruction': {'parts': [{'text': '\nYou are a content safety classifier.\n\nYour task:\nDecide whether the user\'s message is SAFE or UNSAFE.\n\nYou MUST respond with a SINGLE JSON object.\nDo NOT include markdown, explanations, or extra text.\n\n### Output JSON Schema (MANDATORY):\n\n{\n  "safe": boolean,\n  "reason": string | null\n}\n\n### Rules:\n- SAFE:\n  { "safe": true, "reason": null }\n\n- UNSAFE:\n  { "safe": false, "reason": "<short reason>" }\n\nReturn ONLY valid JSON.\n'}]}, 'generationConfig': {}}'
[0m

2026-01-28 13:52:06,623 | DEBUG | httpcore.connection | close.started
2026-01-28 13:52:06,625 | DEBUG | httpcore.connection | close.complete
2026-01-28 13:52:06,625 | DEBUG | httpcore.connection | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2026-01-28 13:52:06,641 | DEBUG | httpcore.connection | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001CCF6600130>
2026-01-28 13:52:06,642 | DEBUG | httpcore.connection | start_tls.started ssl_context=<ssl.SSLContext object at 0x000001CCEACB9A40> server_hostname='api.openai.com' timeout=5.0
2026-01-28 13:52:06,655 | DEBUG | httpcore.connection | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001CCF6600100>
2026-01-28 13:52:06,655 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 13:52:06,657 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 13:52:06,657 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 13:52:06,659 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 13:52:06,659 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 13:52:06,962 | DEBUG | LiteLLM | RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\"safe\": false, \"reason\": \"The user is expressing suicidal intent.\"}"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 134,
    "candidatesTokenCount": 17,
    "totalTokenCount": 185,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 134
      }
    ],
    "thoughtsTokenCount": 34
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "Lsd5aeuGCOW8qfkPza63oAI"
}



2026-01-28 13:52:06,967 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:52:06,971 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:52:06,971 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:52:06,975 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:52:06,981 | DEBUG | LiteLLM | selected model name for cost calculation: gemini/gemini-2.5-flash
2026-01-28 13:52:06,982 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:52:06,983 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:52:06,983 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:52:06,983 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:52:06,984 | DEBUG | LiteLLM | response_cost: 0.0001677
2026-01-28 13:52:06,986 | DEBUG | openai.agents | Received model response
2026-01-28 13:52:06,984 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:52:06,987 | DEBUG | LiteLLM | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001CCEC7B4A30>>
2026-01-28 13:52:06,989 | DEBUG | LiteLLM | response_cost: 0.0001677
2026-01-28 13:52:06,989 | DEBUG | LiteLLM | Filtered callbacks: ['cache']
2026-01-28 13:52:06,991 | DEBUG | openai.agents | Processing output item type=message class=ResponseOutputMessage
2026-01-28 13:52:06,991 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:52:06,991 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call: Cache_hit=None
2026-01-28 13:52:06,991 | DEBUG | LiteLLM | Logging Details LiteLLM-Async Success Call, cache_hit=None
2026-01-28 13:52:06,992 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:52:06,993 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:52:06,994 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:52:07,000 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:52:07,000 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:52:07,001 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:52:07,002 | DEBUG | LiteLLM | Logging Details LiteLLM-Success Call streaming complete
2026-01-28 13:52:07,003 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:52:07,005 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:52:07,014 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:52:07,030 | DEBUG | LiteLLM | Async success callbacks: Got a complete streaming response
2026-01-28 13:52:07,033 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:52:07,040 | DEBUG | LiteLLM | Model=gemini-2.5-flash; cost=0.0001677
2026-01-28 13:52:07,047 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:52:07,048 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'vertex_ai/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-flash', 'custom_llm_provider': 'vertex_ai'}
2026-01-28 13:52:07,049 | DEBUG | LiteLLM | checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-flash', 'combined_model_name': 'gemini/gemini-2.5-flash', 'stripped_model_name': 'gemini-2.5-flash', 'combined_stripped_model_name': 'gemini/gemini-2.5-flash', 'custom_llm_provider': 'gemini'}
2026-01-28 13:52:07,049 | DEBUG | LiteLLM | model_info: {'key': 'gemini/gemini-2.5-flash', 'max_tokens': 65535, 'max_input_tokens': 1048576, 'max_output_tokens': 65535, 'input_cost_per_token': 3e-07, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 3e-08, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': 1e-06, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 2.5e-06, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': 2.5e-06, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': True, 'supports_reasoning': True, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 8000000, 'rpm': 100000, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2026-01-28 13:52:07,050 | DEBUG | openai.agents | Resetting current trace
2026-01-28 13:52:07,050 | WARNING | orchestrator | üö´ Safety blocked request
2026-01-28 13:52:07,055 | INFO | session_summary | üß† SUMMARY_START | session_id=e03649ad-b3b1-4a09-a679-d5588d2dfb33
2026-01-28 13:52:07,061 | INFO | session_summary | üìä Unsummarized messages = 16
2026-01-28 13:52:07,061 | INFO | session_summary | ‚è≠Ô∏è SUMMARY_SKIP | LOW_MESSAGE_COUNT
2026-01-28 13:52:07,064 | INFO | chat | ‚úÖ Stream complete | tokens=0 | time=1.44s
2026-01-28 13:52:07,183 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 08:22:06 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_3054119d75a62e20def6994a58d1ff2a'), (b'openai-processing-ms', b'217'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'220'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4f147f281b563c-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 13:52:07,184 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 13:52:07,184 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 13:52:07,184 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 13:52:07,185 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 13:52:07,185 | DEBUG | openai.agents | Exported 1 items
2026-01-28 13:52:07,186 | DEBUG | httpcore.http11 | send_request_headers.started request=<Request [b'POST']>
2026-01-28 13:52:07,186 | DEBUG | httpcore.http11 | send_request_headers.complete
2026-01-28 13:52:07,187 | DEBUG | httpcore.http11 | send_request_body.started request=<Request [b'POST']>
2026-01-28 13:52:07,187 | DEBUG | httpcore.http11 | send_request_body.complete
2026-01-28 13:52:07,187 | DEBUG | httpcore.http11 | receive_response_headers.started request=<Request [b'POST']>
2026-01-28 13:52:07,606 | DEBUG | httpcore.http11 | receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Wed, 28 Jan 2026 08:22:06 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_ea14ee8e7c72a8dc588033bce7f401db'), (b'openai-processing-ms', b'103'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'personal-tscrr9'), (b'openai-project', b'proj_3H6kNL2t8cFL0jtOE4pLhx2B'), (b'x-envoy-upstream-service-time', b'106'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9c4f14827e49563c-DEL'), (b'alt-svc', b'h3=":443"; ma=86400')])
2026-01-28 13:52:07,606 | DEBUG | httpcore.http11 | receive_response_body.started request=<Request [b'POST']>
2026-01-28 13:52:07,606 | DEBUG | httpcore.http11 | receive_response_body.complete
2026-01-28 13:52:07,607 | DEBUG | httpcore.http11 | response_closed.started
2026-01-28 13:52:07,607 | DEBUG | httpcore.http11 | response_closed.complete
2026-01-28 13:52:07,607 | DEBUG | openai.agents | Exported 2 items
